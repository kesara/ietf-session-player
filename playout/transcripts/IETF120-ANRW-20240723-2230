[
  {
    "startTime": "00:00:47",
    "text": "music for me Yes, it's super implants, no? Edwin. Edwin. Yes. Ah, you're the next one, right? Okay, okay, okay that's great. Yeah, because I was thinking like, what do you say? person? Ah, you have a lot of them. Where are the customers? Yes, I was That's weird, like I was there And I think it sounds like, I don't recall seeing you. Well, yeah, well meet you in Vancouver instead of Cool, cool. I'll go ahead good. I really enjoyed it was good. I really enjoyed it. It was quite a wide range of different topics this year. I think last, I was there last year as well focus on machine learning West this year was different is... I depends a lot, no There are always more demands for presentation and slots. Same with people Usually there is quite a few people that cannot make it because we have a room"
  },
  {
    "startTime": "00:02:02",
    "text": "There was nothing terrible, but yeah, no, for nothing. Thank you That's good. Yeah, we're not seeing buffings buffins We're not seeing Buffet I like this transcript. Can we play? with the transcription? No, I didn't say buffer Thank you right, welcome back everybody Oops, there is some echo. Okay sort of. So the next session is about security and energy, and the first presentation is about log low-carbGP, a carbon-aware inter-domain route extension to BGP, and it will be presented by Edwin Sutherland from Lovroy University Edwin, thank you. Hi, everyone I'm here to present low-car BGP together with Ian Phillips we I've been doing this research just under a year since we participated in the Carbon Aware Networks workshop in Oxford University last year So to talk about the most motivation, why are we interested in sustainability? on the internet in the first place? Well, depending on where you really this information, there's some reports up to 3 to 4% of the global CO2 emissions is contributed by the ICT industry and if you break that down a little bit more, you'll see that there's three"
  },
  {
    "startTime": "00:04:02",
    "text": "main elements that factors into this Data center infrastructure, which we all know with the rise of AI workloads is ever more expensive but there's also end user computing devices and networks and they're all of these are contributing to the energy emissions that as a planet we need to be aware of and to try and to reduce Another important metric to be aware of is that there's also the growing trend of the internet in terms of generating traffic So we have a predicted 60% global data growth over the over over year moving forward. So this and incumbency on us as network operators, developers to think about how we can make the networks more sustainable moving forward So the key idea I want to present to you today is that to make the internet and networks greener, effectively we need to make ISVs want to be greener and to do that we create an incentive or a motivation where ISC will decide to route traffic and prioritize traffic over greener ASS as opposed to just choosing the shortest path first And by incorporating greenness in routing policies, we believe that there's a mechanism where ISPs will be more profitable in terms of those receiving more traffic and potentially change the dynamics of the internet where we are creating a race to the bottom where everyone wants to be greener that's the ideal world maybe it's a fantasy but let's see how we can move towards that direction Okay, so how do we measure greenness in the first place? There's some ideas that I've been proposed by the ATF Some of these are, for example, taken into consideration the carbon intensity. Carbon intensity"
  },
  {
    "startTime": "00:06:03",
    "text": "is the CO2 grams per kilowatt hour of electricity used. And in this case, we can effectively measure the effectiveness the carbon efficiency of networks using this type of data source also in a way it's a bit more i would say equal in terms of whether you're a big operator or your a small operator, it doesn't really matter because what we're trying to focus on is how efficient you are in terms of you carbon emissions. Another way you could measure this is through something like a carbon performance rating. If you're familiar to how houses are often rated for the energy efficiency we can implement some kind of rate system around how different air network operators perform from a carbon footprint perspective in either case we envisage that there will be an external auditor that will carry out this work to hold networks accountable for the submission or the publishing of the carbon intensity or other green metrics that we might want to use in the future so fundamentally what we want to be able to do is to identify that an AS that is using renewable energy is more carbon efficient, therefore we want to route our traffic through those ASSs, and the ASS that are more to do is to identify that an AS that is using renewable energy is more carbon efficient, therefore we want to route our traffic through those ASSs. And the ASs that are using more pollutant energy, we want to discourage them from using pollutant energy, therefore one of the ways to do that is not to send our traffic through them and by doing that they may have a motive to say, well, actually, we want to restrict traffic So let's carry out some energy efficiency or carbon efficiency exercises or program in our infrastructure to appeal to anyone that wants to be on the path for making greener decisions"
  },
  {
    "startTime": "00:08:02",
    "text": "So we built a system called low-cut BJ and essentially low-cut PGP is an SDN controller that will sit within an ISP's network It will retrieve the green metric from some kind of central database It could be an RPCI system. It could be something else dedicated for greenness, and it will distribute the green metrics that belongs to that ISP through the autonomous border routes. So this information will get disseminated throughout the internet to every will be aware of the carbon intensity value for each AS The other function of local BGP is to retrieve prefixes that have been advertised to that AS and to work out what the carbon intensity for each path that is received for different prefixes. And we'll go into this in a little bit more detail We can then aggregate that information in some fashion that we can then use to make routine policy decisions on once we understand that we can formulate those within policy decisions and push that down to the network devices to make the decision So the dissemination what we played around with this so far, is to use extended communities And although this might be malleable in some sense, they could be other ways to provide attestation for example, maybe not using extended communities by using RPKI as an example but in our you know environment, we're essentially using an extended community to encode the carbon intensity metric in this case we do this in an additive fashion so if everyone plays nicely we get what we call a SIM chain, which is effectively a breadcrime of the carbon footprint of routing to a specific prefix. Once you have that, information, you can then start maybe"
  },
  {
    "startTime": "00:10:02",
    "text": "potentially doing reporting on that to understand your carbon footprint in terms of your contribution overall in the internet We can then aggregate that information we can for example have what we call the total carbon intensity metric, a prefix and we can make routine decisions based on that information We can calculate the median of the carbon intensity from that chain or maybe we want to be more picky and say for we want to avoid certain carbon intensity values So the highest carbon intensity value within a chain, we want to have that path and select a different path And then lastly, you could do some more data aggregation if it makes sense to reflect that. And this could be, for example, the median T T-SIM on a Peneva basis For policy formulation there are different ones, the ones that we tested in our environment was the greenest path fed first. So in this case, I'm interested in only routing picking a path with the lowest total carbon intensity metric. You could do things like having a budget for the carbon footprint of a path or prefix, and then you can do decide that beyond that budget you want to deeper deprioritize that path. It's an example other things to be considered is a multi-objective optimization where you're taking greenness as part of the equation in addition to the administrative and performance requirements of a provider So in our test bed, what we set up is essentially use electricity maps There's a platform out there that reports on the carbon intensity on a geographic basis and we use the cathars emulation platform to emulate the geon topology which fits not"
  },
  {
    "startTime": "00:12:02",
    "text": "nicely because most of the ASs within the geont topology are algebra-based. So that was a quite an easy test bed to put together for the for the purpose of testing What we've done is to try and figure out, well, can we calculate some of these metrics on a previous basis? and what does changing the routing policies to greenness? power implies, what are the results? compared to classic PGP? so to do this testing essentially from each AS in our topology, which was around 38 ASSs, we check in that every single destination prefix and checking their path and calculate the total T-SIM values So the results that we can and checking their path and calculating the total T-SIM values. So the results that we gained from this, when we did the comparisons on the X-axis, you see all the different ASs in our topology and on the y-axis is the median score So on your left-hand side, on your right-hand side, sorry, is the total, the median total carbon intensity metric and that represents for any particular areas on the excesses all possible destinations that they could get to what is the median T-SIM for that so as we can see in the graph there, there isn't a dramatic change, although in most cases, low-carb GPP performs better from a carbon efficient from a carbon footprint perspective and as you, the graph shows this some instances where there's a significant savings from a carbon footprint perspective on the right-hand side on the Hopcount side, you see the comparison between low-carb PGGP and classic BGP from a Hop-Camp perspective So has there anything changed from a Hop-Camp perspective? And the intuition that we got"
  },
  {
    "startTime": "00:14:03",
    "text": "from this is actually the greenest path is most often the shortest path because of the cumulative nature of the TTIM aggregated metric So again it shows you here that all the we introduce greenness into the equation, we're not made a drastic change to the way that the topology routing perspective is done in the outcome of the result So we look at two cases. So in this case, this is the best case of example where local BGP shows significant improvement and terms of lowering carbon footprint. So in this case, we have from AS 553 to AS-5392 And in this case, we're going from top to bottom with classic BGCP there are fewer hops but what we've traded for two extra hops is the reduction of the carbon footprint about 67% So in this case, you could say that's a good trade-off depending on the context and whether latency is a affected in a significant way The waste case example here we see from AS 2614 to AS 5614 to AS 5538. In this case here, although we're going to the same destination, low-car BGP decided to go avoid certain ASs because of the significant carbon footprint and decided to trade off five extra hops just to gain 30 reduction in carbon footprint So as you can see here, this is not exactly a nice outcome. And depending on practicality, this is definitely not something that some operators"
  },
  {
    "startTime": "00:16:03",
    "text": "will be happy with. And therefore, one of the things we want to look at in the future is the multi-objective scenario where we can actually enhance local BGP to be more considerate An interesting analysis we did here was, well, what happens? when local BGP gets put into effect where there are depending on the topology? put into effect, where there are, depending on the on the topology, whether there are ASs that are more central or? more transited to, what is the change here? And what we see is actually, I think, is quite significant and could be a motive So in the top five, let's there's no change in ASs from class BGP to low-car BGP bgbgp the top ases remain more or less the same And this is most likely due to the centrality of those AS within the department but also the carbon intensity value as well is advantageous in this case But when we look at the bottom five, there's been a dramatic change because there's um the AS is doing in classic BGP that were often transited through I know are longer there in L and local BGP because local BGP favours transiting through ASS will lower carbon intensity values. We see that this has significantly changed. So if you are in AAS, 57961, you're longer receiving traffic which might be a motivator to reduce the carbon footprint So overall, what we found in our testing was that low-cut BGP, actually reduced overall carbon footprint routing between the different AASs within our"
  },
  {
    "startTime": "00:18:02",
    "text": "topology to reduce the carbon footprint by around 20 Now, this might not be significant in terms of the percentage wise, but we believe that every little helps and improves the emissions rate of operating the internet So, conclusions what we we've learned from this work is that local BGP has a nice property of overlapping green economy with internet routing, and if we have the right incentive in place, we can motivate ISPs to start making greener routing decisions, which obviously from an internet perspective has a as a compounding effect in helping to move the move this forward low-cap beach is also adaptable to the current interdomain routine paradigm because we don't make any changes to the BGP algorithm it's up or we ask for BGP to advertise community values There's an SDN controller that has context of that information and is able to manipulate BGP using the existing best path selection algorithm What we want to do moving forward is to experiment with different into AS level to topologies, try different green metrics types, maybe instead of having a single green metric, we can have multiple metrics that we can evaluate against and also do some work on the multi-objective optimisation So that's all I wanted to present around low-car VHP open for questions and suggestions moving forward. Thank you very much. We have time for two or three questions, depending how fast they are. Sharat, you're next Thank you very much. This is really interesting piece of work and got my intellectual juices going, so, so thank you. So, uh, here's my question. Let me try and phrase it"
  },
  {
    "startTime": "00:20:00",
    "text": "But just, you know, I tend to think of hearing geographically. So, for example, you know, let's say i want to peer with a customer or or an isp it, which is, you know, I tend to think of peering geographically. So, for example, you know, let's say I want to peer with a customer or an ISP, you know, I will need to think about what is the catchment area that I want to try and get and like, okay, maybe I need to peer with them up here in Vancouver or maybe I need to peer with them like in the southern U.S. And so my question is the following, like when I'm sending traffic, right? What I'm often choosing between is like which of my peers, you know, here and the Pacific Northwest would I send traffic to? And up here, the majority of energy tends to be green. You know, we've got lots of, you know, water-based energy that we have here versus like maybe in the southern U.S. where maybe there's more, more dinosaur burning that's happening So what I'm wondering is like, you know, in your analysis, like where does the majority of difference come from? when you're sending traffic between a, choice of different peers in a particular location? Like where where is the source of difference in the carbon footprint come from in a particular geographic location because yeah, you know, I guess greenery depends on where you are Yes, there's a couple of things we want to dive into in relation to that. So obviously, we're always aware that the internet is becoming more, one hop away from anything, right? So the density there so what we want to figure out is the carbon intensity value to actually represent the ingrained and egress pairing of traffic going through an aAS is the carbon intensity value to actually represent the ingress and egress pairing of traffic going through an a s and that way we can be more granular around the selection of which people to send traffic through. But ultimately, we don't really care about the neighbor we care about the path so we want to go for greener paths rather than i'm going to pick this neighbor because it's green because your nearest neighbor might be green but downstream from them, they may be using more"
  },
  {
    "startTime": "00:22:02",
    "text": "pollutant ASs, and we want to avoid that okay all right thanks uh i'm sorry that queue is close i know other people want to make questions but I would really appreciate if people can make questions are relatively fast and the answers can be relatively fast because we have already run out of time Please, jonathan hui think you are the next one. Yeah, jonathan hoyland, Cloud Cloudflare fast because we have already run out of time. Please, jonathan hui think you are the next one. Yeah, jonathan hoyland, Cloud Player. So if you give an AS a score saying, oh, this is very green, and another one, a score saying this is not very green, is there a risk that people will be like, oh, I'm going to send all my traffic through the green AS, which is going to make it really congested which is going to mean there's lots of retransmissions? which means it's going to be much less green? I think the greener AAS will be happy to achieve more traffic if it impacts their revenue, right? So they will find ways to accept more traffic But in saying that, yes, there could be polarizing in doing it this way Yeah, you learn up with much less green i'm sure there's a debate there to be had which we will continue offline. Matt, you're next Yes, I have a sort of a different question along the same vein, and that is for a lot of technologies the base power dissipation of a routing device or whatever is independent load on it, and the incremental energy dependent on the load is different, and it matters a whole lot how you average that base capacity out in particular a heavily loaded router looks more efficient How do you calculate those numbers? and how do you justify them or audit them? That's a really great question. I think with the carbon intensity metric we are looking at the source of the energy rather than the utilization of the energy, right? So you're more carbon efficiency, if you're using sustainable energy like one professor told me if all you're using is solar"
  },
  {
    "startTime": "00:24:00",
    "text": "you can use as much energy as you want, because you know, you're still being carbon efficient. So we want to look at the carbon efficiency rather than the energy efficiency now the energy efficiency could influence that but the carbon efficiency is much more important So for instance, in networking technology, deep buffers are extremely expensive in terms of power and they have vastly different capabilities in terms of performance performance Thank you very much. Let's thank the speaker And let's welcome the next speaker, justin iurman talking about implementing and evaluating UIM integration integrity protection. Room resource it's complicated. Hi, everyone. So as the chair mentioned, I'm going to talk a little bit about integrity protection of ioam so this is a work related to a draft we have here in the IUT in the IPPM working group So we just went out of this group and so the draft is progressing well, sorry So yeah, let's just have a look at the journey we went through for this work so a bit of background on iOM for those we don't know that so basically you want to collect telemetry data on our network So what you do is that each hop on the path would just insert telemetry data in the packet itself, so you don't need you know, syntetical traffic for that In this case, this is just an example of IOM with the IPV6 data plan, but you have some other encapsulation protocols defined for IOM And so on the edge of your domain, even though the IETF doesn't have, you know, a definition or consensus on limited domains, but anyway, let's call the first node the encapsulating note. So it would just, you know, pre-allocate some space in the packet for the data. It would add its IOM data and then forward the packet. And so you got some"
  },
  {
    "startTime": "00:26:00",
    "text": "transit notes on the pass that would add if they are configured to do so, they would add the RIM data And then on the other side of the and so you got some transit notes on the pass that would add if they are configured to do so they would add the RIM data and then on the other side of your domain on the other edge you got the decapsulating note, which would just, you know, pop remove the IOM option, so the header and the data But the thing is, here, the data, so everything that is IBM related, isn't clear, right? And while you don't necessarily need confidentiality, because it runs inside your domain you probably want to have some integrity protection, right? Because what happens if you know? that is compromised with just, you know, modify your IM data? Well, it could, you know, hide some potential issues in your networks or the opposite actually makes you think that there is a problem why it's fine. So you really want some issues in your networks or the opposite actually makes you think that there is a problem why it's fine. So you really want something to make sure that the data is clean and so this is what we add integrity protection on iom make sure that the data is clean. And so this is why we add integrity protection on IOM. So basically we would just add an extra is clean. And so this is why we add integrity protection on IOM. So basically we would just, you know, add an extra header in between the IOM header and the IOM data. And so the ANCAP node would just proceed as usual and initialize that ICV The same for the transit nodes that would you know, just update the ICV step by step. So you kind of have a chain of ICS transit nodes that would you know just update the ICV step by step so you kind of have kind of have a chain of ICVs just like a proof of you know on the path And then the last node would just validate the ICV and so the output is either, yes, the integrity is intact So you can just ingest the IOM data or there is a problem with the integrity so just ignore this IOM data This is the XRio header that we add, so basically you have a method ID field. This guy is just there to tell a node which method it has, which algorithm it has to use"
  },
  {
    "startTime": "00:28:00",
    "text": "to compute the ICV, basically You got a nuns that is from a variable length, so you got a nonce length you know, to specify its length, even though some meters might force a specific size but anyway, you want a generic header for every method And you got the ICV, so this is Integrity Check Value Right, so I'm going to review a little bit all the possibilities we went through because, you know, there were a lot And all those, um, options, let's call them options we'll use GMAC, which is actually a mode of GCM. And just kind of using GCM with a new language encryption. So the only output you have is called an authentication tag, which will be our ICV here So we are obviously in symmetric keyword So what we do here is that the un-absulating node would just take the header only immutable fields its IOM data, and it would GMAC the whole thing That would be the ICV value Then it forwards the packet to transit nodes. A transit node would just check the header first, so it would have to, you know another GMAC to compare. And then if it's far, reGMAC the received ICV with its IOM data So you can't, you know, form a chain of ICVs as I mentioned. And so the DeKap node at the end, would have to recheck the whole chain. So it a little bit heavy, but this is the purpose of this method, which is the validation at the end And so obviously you get either check on transit nodes with it recheck the whole chain. So it's a bit heavy, but this is the purpose of this method, which is the validation at the end. And so obviously you get either check on transit notes with this technique. So this is the version we specified in the previous version of the draft. Right now, this is 09, so this was the last one The advantage is to have a Thank you check-on transit nodes, but it is really expensive"
  },
  {
    "startTime": "00:30:03",
    "text": "and it was made optional because of that. Why is it expensive? Because you have a chain of ICBs And so if you want to check the header, you're have to recompute the whole chain up to yourself which is really not what you want, right? and obviously this is not considered a um zero trust solution in that case because each transit node would need the key from the iCAP node in order to check the header, right? And we really want to find a solution that this is zero trust one right so let's try just to improve this one. And obviously the idea is to just, you know, add an extra ICV in the packet so that you get an ICV for the header, an ICV for the data. And so each transit node would just you know, be able to compute the ICV as a check the header as one step So that's the improvement, but on the other way, you got an extra ANONCV and extra ICV. And so it's not that desirable because we got some space confines, especially in the IPV6 data plan for IOM Also, you got the encapsulating node that performs 2G max. So while it's not you know, at the end of the world, it's against, maybe not perfect and not what you want And since you got check header on the transit nodes, it's again, you have to share your key from the end again, maybe not perfect and not what you want. And since you got check header on the transit nodes, it's, again, you have to share your key from the NCAP node. So again, it's not a zero trust solution So we need to think a little bit about this and answer this question Do we really need the header check on transit nodes, actually? So if you think about it from a perspective of the IOM processing, the answer is yes, maybe it's desirable, but if you take it from the other side and you think about the integrity protection of IOM data then the answer is no, it's not necessary, right? And actually it's fine because the initial goal we had when implementing this draft was to protect"
  },
  {
    "startTime": "00:32:00",
    "text": "the IUM data, not necessarily the header, especially on the transit notes, right? And obviously, if you want a zero-trust solution so if you don't want your notes to share their key in between them, you have to use a method that does not implement header check on transit notes So let's have a look at how it works So again, it's called validation at the end, but this time with no header check on transit. So basically, the ANCAP node would just GMAG the header, and it's IOM data fields, and this time the header would be only a selection of imitable fields, but that are really tied to context and provide meaning to IUM data right if it's only an immutable field required for the processing, you just don't care about it So you don't need to include it A transit note would just, you know, it won't check the header, so it's just one step in this time. So you take the previous ICV you receive and you GMAC it with, sorry, you GMAC it with the IOM data. Same up to the end and so the decap again would check the header uh, would check the whole chain, sorry. So the advantage is that as I mentioned this is a zero trust solution this time. And I guess that we could you know, have as a cons that this is the more either check on transit nodes right but as I'm mentioned, this is kind of a compromise and an improvement from option one A and one B. So we can live with it In case, we cannot leave with it we still got some options So this one was suggested initially during a second year review from Ben on the meeting list. Let's call this one a name neighbor validation This one is actually easy. So basically what you do is that the encamp note would gemmack the whole thing. So here you"
  },
  {
    "startTime": "00:34:00",
    "text": "don't exclude some fields. You just jimag the whole option the header and the data fields. And you forward it to the next note. The transit node would just check you know, the ICV received from the neighbor and then do the same. It was regimac the whole thing. So it's really a point-to-point neighbor-to-neighbor validation in this case So back to the advantage is the headway check on transit nodes But again, we are back in the no zero trust solution. So it's kind of a compromise again Right, we have IPSEC too. So let's call it option four The advantage is that you can just reuse know, reuse what's already there. So it's provided to you. You don't need to redefine a new protocol, right? But it's kind of messy because you would need IP sector configure all over the place, in between all of them IOM nodes, right? And we're still in a no zero trust solution but the most important point here is that it may change the past taken by packets. And this is certainly not what you want here because the goal of IOM is to be, you know, when a packet takes a certain path if you insert IOM in the packet you want it to make sure you want it to take the same pass you know, so that's really important to have this, this disadvantage. So it's just you know, cited here, but probably in sure you want it to take the same pass, you know. So that's really important to have this disadvantage. So it's just, you know, cited here, but probably not that good right let's have a summary of all the options so that I don't get people lost. So the first column is the number of ICV in the headers So if you look at option 1B, this is the only one that has two ICVs So remember this is the one that we wanted to add an extra ICV in order to make a one-step transit header to check on the transit notes. Then you got the header protected. It's so it"
  },
  {
    "startTime": "00:36:02",
    "text": "protected? Is it protected? Yes, no or partially inside parenthesis do we have either check on transit nodes? Yes, no, or optionally, into parenthesis Is it a zero transolution? Yes or no? And then we got the number of GMAC operations for the ANCAP node for a transit node, and for a DECAP node And so as I mentioned, this is really a goal to have a zero trust solution So if we look at this column, we have only one solution, right? We have only option two. So of course, it's a little bit heavy on the decap node because it has to revalidate the whole chain and so if you look at the last column for this one you got n minus one g max to do, right? But again, this is this is a compromise because it matches what you want on the other side so So option two was kind of an impact what you want on the other side. So option two was kind of an improvement and compromise with one A and one B, so you can just forget about those two. But if you look at option three and four, you can see that all values are the same. And specifically, the decap node is constant so there is only one decap and there is only one GMAC on the decap, sorry. So since they are the same and from what I mentioned on the previous slide, we will just pick option 3 because it's lighter than using IPSEX or option 4. So I retried, you know, to to we try to implement those two and to evaluate them. So keep in mind the last three columns. So if we look at three option two and three for the NCAP transit and decap, we got basically the same value for the N-cap. So there is only one G-Mac for the transit node option two is one G-mac while option three is two G-max. And for the de-cap node, option 2 is n minus 1 G-max, so you have to validate the whole chain, while"
  },
  {
    "startTime": "00:38:03",
    "text": "option 3 is only 1Gmax. So obviously you should see on the graph some equivalence for the uncap and trussit and a difference on the D-cap So top left, the encamp node And yeah, by the way, and the blue line is vanilla IOM, so just IOM without integration protection, which will be our baseline here The orange line is option two so validation at the end with no other check And option three, neighbor validation is the green one the same for all graphs So if we look at the Ancap node top left we can see that they are quite equivalent right You see a little bit more drops for options two, up to a certain percentage of interest of IOM, but it's quite the same, which is expected, right? Then at the bottom, transition node, we see that after 10% of injection of IOM in the traffic, you got more drops with option 3, which is if I go back to the slides, it's kind of expected because you have two GMAX instead of one right but again it looks like it's not, you know, that much Now let's have a look at the top right graph, which is the DeKap node And so here might be surprising because you see that the green and the orange lines are perfectly aligned or almost. The reason for that is that we wanted to have a common basis for comparison. So in that case, the orange line, so the validation at the end with no other check on transit node is the case. This is the best case So this is with only one validation validation So I want to know what happens if we had"
  },
  {
    "startTime": "00:40:02",
    "text": "more validations to do, then here we are So obviously here you see that it fits up to 10 validation validations If you don't exceed 1% of insertion right? If you don't exceed 0.1% of insertion, you can just fit the maximum number of validation And it may seem a bad news from performance perspective, but actually I I've discussed with a lot of operators who are right here And the average size of domains that they told me is usually five maybe 10 nodes. So, you know, it's in that range But based on this graph, the recommendation is so certainly to have either not more than 0.1% of insertion or not exceed 1% for up to 10 nodes to validate Right So a quick conclusion on this So as I mentioned, the draft as a new version which is 09, and now it's specific the option 2 and the option 1A, if you remember, which was specific in a draft and previous version was abandoned We also provide, you know, because it might seem scary this one, but we provide some mitigation for risk for performance risk, by allowing the decap node to delegate the drone of validator so that you put the validator job out of the for running past and that could be a good alternative We still maybe want to have option three defined somehow, so we can just, you know, it in a separate document later"
  },
  {
    "startTime": "00:42:00",
    "text": "if there is a need from someone. This is actually our plan. Should you want to use option 1B or option 4? Well, it's not that word it. If you need that, then you probably can just use option 3, because they are quite equivalent for an advantage and disadvantage comparison Although option four, so IPSEC could be useful for inter-domain use cases. So say you have your domain, domain A, and another removed domain, domain B And you want your IOM data to transit over that unsecure path Well, just configure an IP section of between the two, right? And I guess the conclusion, the real one, is that it was a journey and it was really a story of compromise And obviously there is no perfect solution So we got option two and option three and depending on the context and the situation of a purpose they might use one or the other. So that's why we want to provide the two And that's it for me. So if you have any question just ask. Thank you. Thank you very much. Questions? to Juan Camelo Hello, Justin. Thank you very much and congratulations because this work not only was accepted here, but it seems had a reflection in a way working work draft so congratulations. I'm not an expert on the protocol, so I was just one wondering will all situations in all situations that the protocol defines, is it enough that you have the validation at the end? Or does the protocol have any kind of cases in which? it doesn't need to look into the packets in a transit note to do some kind of operation? Yeah, so basically here I use the trace option type as an example because that's kind of a special beast that is problematic for the integrity protection"
  },
  {
    "startTime": "00:44:03",
    "text": "So usually the other option types would fit well with even option two or three So I think the use case for option two so the validation at the end, is three option 2 or 3. So I think the use case for option 2, so the validation at the end, is really if you don't want to trust even your IOM node So you don't want to trust anyone in your domain even if it's your domain And if you're not that paranoid, you could go with option 3. In that case, you would trust your IOM nodes but let's say you've got middle boxes or so of the boxes in between, those you don't trust. So yeah it depends on the situation and I think it's really up to the operator to decide what to use Okay, thank you very well. I think you very much let's thank the speaker again thank you our next presenter, remote is Christina Kada No, sorry, Shiam Krishna Kata Yes who will be presenting assessing the security of internet platform A case study of Dutch critical infrastructure so the room is yours. Hello everyone Can you hear me? Yes. Okay, that's great Welcome to my presentation. I am Sam Krishna Ghaqa a PhD student at University of Trenta. Today I will talk about assessing the security of internet paths where we did a case study of a few dots critical infrastructures Oh, I cannot move my slides. Can you help me? Yes, you will need to say next one you want the next slide"
  },
  {
    "startTime": "00:46:02",
    "text": "Oh, okay. Next. Okay is that okay if I see my screen it sounds a bit disturbing Okay, first I want to start with a news article published in March 2024 That was published in NOS, which is a mainstream news organized in Dutch critical in the Netherlands It basically shows Excuse me, can I steer my computer because I have to do next? multiple times? Sorry, assignments into you before. Either you serve the screen and you control the slides on your side or I serve them on my side and then you have to say next when you go on the next slide. OK, next please don't see see Have you requested to share the screen? oh no okay then I was, okay. Now I have requested requested to share the screen? Oh, no. Okay, then I was... Now you have... Now you have Oh, oh One second there you are Okay, thanks. So that news basically shows that the dominance of cloud-based services many big organizations, and critical information in the Netherlands are moving to the cloud operators like big hyper science"
  },
  {
    "startTime": "00:48:00",
    "text": "Microsoft, and Google for their daily operations So what might be the problem? The problem is that those critical infrastructures, they should have secure paths to those cloud service providers according to European Union Commission critical infrastructures are those physical and information technology facilities, network services and assets. The disruption of OIS impacts the health, safety, and security or economic well-being of citizens In most of the cases, CI's needs to traverse multiple autonomous systems to reach their cloud providers And the problem is that those CIs have limited insights about the security status of the path The reason is that first they have limited visibility They do not know how many paths exist from the to the cloud provider and the second problem is that there exists no mechanism to measure the security of the whole path Now I want to start with a very high level overview of how BGP works As you see in the figure, there is five autonomous system where autonomous system one is one CI, critical infrastructure and AS5 is a cloud provider they are connected by BGP sessions. Let us imagine a situation where cloud provider is announcing a block of IP addresses, 5.5.0 slash 24, then it sends a BGP message to its never AS4. It basically gives the reachability information to reach the prefix. Then after receiving this announcement, then it forwards it to its other neighbors, depending its AS number So in this way, OSCI will have two paths to reach to that cloud providers And remember that data path is just opposite of the method path. So in this way, we have path one and"
  },
  {
    "startTime": "00:50:03",
    "text": "path two for CI to reach to this cloud provider And there is a situation where S4 might not announce its that preference to this cloud provider and there is a situation where S4 might not announce its that prefix to its S2 which is known as selective announcement and one of our earlier presenter saba also talked about this selective announcement thing. In this way, CI will see only one path which is path one, to reach its cloud provider Now let's talk about prefix hijacking and route origin validations things Imagine a situation where a service is hosted with IP 5.5.1 and in this way AS4 will have a routing table with only one entry to reach a that server so whenever us CI does request for this IP, then it will propagate to 3, 4 and four, and in this way, it will reach to cloud provider Consider the situation where another AS-1-00-coms and it starts announcing more specific prefix which is of length 25 Then in this case, AS4 will have another entry to reach to that server and according to the standard routing routing model which is gower expert model the more specific prefix path is preferred over less specific path. In this way, as for might forward its traffic to that AS 1000 and this phenomenon is known as prefix hijacking and it's a common incident in the current internet. So though, AS 1000. And this phenomenon is known as prefix hijacking and it's a common incident in the current internet. So the way to prevent this prefix hijacking is route origin validation. And this route origin validation is a filter that basically takes the list of the prefixes and as is authorized to announce"
  },
  {
    "startTime": "00:52:03",
    "text": "So in this case, if AS4 had implemented, ROV filtering, then then would discard the route towards that high curve Now let's come to the real scenario where we have our critical infrastructure which could be an electric grid operator or transportation system or water supply or any financial institution. In this case, we took this AS number, which is basically an ASN of a financial institution And you can see many autonomous systems which have two types of relationship either customer to provider or peer provider And here you can see green boxes and green ellipses and red ellipses and green one shows that this autonomous systems implements ROV and red ones shows that it doesn't implement ROV And whenever SCI requests for its mail service, provider, which is 52.1 52.101.77.21, they it has two paths. One path is shown by a blue line and another part is path two Now consider the same situation as before where an s once zero zero comes and it starts to announce a more specific prefix Then in this case, there is a high sense that the traffic will be forwarded to this high hijacker the reason is that whenever 6453 sees a be forwarded to this hijacker. The reason is that whenever 6453 has to forward the traffic, then it will prefer for seven because it's a customer and customer route are preferred about peer route and the peer is 3257 so as you see in the figure this path to hole is vulnerable due to a single air"
  },
  {
    "startTime": "00:54:03",
    "text": "4755 and this is known as collateral damage. So for a CI, it is very important to have all ASS secure in order to have a secured path so in this oracle we basically outlined two research questions. So first one is finding the number of fully and partially ROB protected paths from a CI to its cloud provider And the second one is identify the effect of SCI's opposition provider implementing ROB on the number of ROB protected paths To answer this question, we have these four steps. The first one is path collecting and I will explain each steps in the next slides So I am merging this two steps, path collection and stitching together Here we used oblique route collectors from our right rice and route views. So what I public route collectors from our right rice and route views. So what we did is that we have two types of paths. One is the path from one is the path from critical from a CI to route collector. And another path is from Microsoft ASN, which is 8075 and towards towards the route collector so in this case we have we identified two paths path one and path then we find the common node between these two paths which is basically a peer collector And in this case, 6453 is a common ASN between these two paths. Then we found a new path as shown in the figure now the next step is to sanitize those paths. Sanitize means finding valid paths out of those new stitched paths"
  },
  {
    "startTime": "00:56:02",
    "text": "In our definition, valid paths are those paths in which prefix of a CI can reach its destination as and vice versa. For this, we checked Gaur exports model of route export and value-free conditions So generally, route export policy defines that the route receipt from a model of route export and value-free conditions. So generally, route export policy defines that the route receipt from AS should be propagated to which of its BCP peers. So if any of the conditions are violated, then it is called valid. So in our case, valid paths are those paths which are value-free For example, this one was the new path that we formed from our method Then we checked the relationship between the autonomous system using Kaja S-Rank API and we see that these relationships does it violate pally-free conditions and which means that a prefix from a CIASN can reach to Microsoft ASN and vice versa hence it is a valid path Now, the next step is, how to score the security of the whole path. So for this we use ROB as a metric for security scoring The reason that we choose ROB is basically it is the only one ready to deploy a solution to prevent prefix hijacking in the internet For scoring ROB, we use an APA from Robista that Robista does daily data plane measurement to check that which ASNs are filtering RPKI invalid prefixes So using that Robista API first we identify ROB scores of all autonomous systems on a path. As soon in the figure, there are four paths. So when we look into the"
  },
  {
    "startTime": "00:58:02",
    "text": "first path, the first one is source CI and the source CI and the last one is the destination CI so there exists three autonomous systems in that path and we find that all these three ASS have scores of 100. So the path of the whole RAB scores becomes 100 whenever an score, whenever an AS has the lowest score, then we consider the ROB score of the path as the lowest score because as we already saw from an example and as having the lowest RAB score has a high impact on the security of the path and there exist in many cases where we couldn't find ROB scores of S has a high impact on the security of the path and there exist in many cases where we couldn't find ROB scores of ASS because Robista currently covers around 30,000 autonomous system So in that case, we identified RAB score of the whole path as ANA, not available Then we used our method to do a case study for four political infrastructures in the Netherlands which shows two banks, one, what supply company and one energy supply company And all these four CIs use Microsoft Mail for their daily operations and Microsoft is at least two hops away from them So when you look into the figure of this bank, one of those banks, the x-axis shows the number of airs hops between a source and a destination air AS which excludes source and destination and the y-axis shows the number of paths. As soon in the figure you can see that there exist many 0% rave paths, which means any one of those autonomies on the"
  },
  {
    "startTime": "01:00:02",
    "text": "path have rube score zero there exist many 0% ROB paths, which means any one of those autonomous on the path have ROB score 0. So which and this graphs shows that there exists a large number of unsecure paths from this bank towards the towards the Microsoft mail And any of this path could be chosen by the operator depending on their business policy now looking at this figure of an inert supplier company, we can see that there does not exist any secure paths, which is 100% RV paths And we see, we see that it's all autonomous system on it paths paths and we see that it's all autonomous system on its on its paths are 100% ROV except its immediate provider. It means if it immediate provider does ROV filtering then all of its valid paths becomes fully secured RAB paths Then we did another analysis to find that how many unique ASAs exist on those 100% secure paths So for example, if you look into this bank one, there exists 80% paths with different hops lanes having 100% RAB, which means secured paths, and there exists 15 units ages in those paths So it gives us insight that if these 15 ASAs is form a group to forward CI's traffic and they could offer such concerns as a value added service to their customers along with visualization So here are the key takeaways of our work. We developed a method to calculate this security status of a path in combination with path finding. The second one,"
  },
  {
    "startTime": "01:02:02",
    "text": "is we found that implementing RAB fully by upstream providers will increase the number of fully ROB protected paths So our future work considers investigating the effect on pathfinding using additionally geographically diverse route collectors. Thank you for your time and concentration I am hoping for your questions and any suggestions. Thank you very much Shiam. We have a question from Tobias Hey, Tobias, I wish, Max Planck Institute for Inframatics, and I was wondering whether you conducted any data plane measurements to validate that the paths you synthesized from the rod collection information were actually feasible or taken paths that's a good question yeah actually we couldn't do data plan management because first thing is that we need to have some something some probes We looked for at last props if some those cis have props to do data plan measurement we couldn't find then we couldn't verify that thing I see a question from Juan Camilo Camillo from NTT. First of all, this is very good work. You did what you could understand that you had no access to I Camilo. Camillo from NTT. First of all, this is very good work. You did what you could, understanding that you had no access to ISP data, right? And so you were resourceful and you gathered those paths on your own the thing that I'm missing and that may very ISP data, right? And so you were resourceful and you gathered those paths on your own. The thing that I'm missing and that may be very hard to get is you did mention a specific service for Microsoft and IP that was the the, the, the, the, the, the, the, the might be very hard to get is you did mention a specific service for Microsoft, an IP. That was the resource that it was hard to, that you truth protect. Have you thought about considering how you could recommend, and this is hard recommend the companies to find those kind of those type of service"
  },
  {
    "startTime": "01:04:03",
    "text": "how you look at work that finds that because if you cannot find the fragile services then the rest of the work will be useless so have you thought about that how to automatize that work? Oh, you mean like finding the IP address of the critical services like that? IP addresses, it might be DNS names Okay, yeah, that's a good point. Actually, in our case, first we try to find the IP addresses of the mail servers that are being used by C CIs. Then we found the covering preference that Microsoft announces for their mail server IP addresses but yeah answering your questions we haven't thought about any method to detect those things Maybe DNS things can help Very well, thank you very much. Let's thank the speaker Elias, can you come to the microphone? So you can say, so you're over it so you're aware of it. Room is yours Okay Hello, welcome to protocol fixes for keychap vulnerabilities My name is Elias Heftrish and I'm one of the authors of this and I work for Athenia Research Center for Applied Cybersecurity and I'm here with my colleague, Nicholas Vogel, who's at the audience So my talk will be structured as follows First, we'll have a short recap on the key trap attacks followed by explanation on exploration of the problems with the shorter mitigations So don't get me wrong with shorter mitigations, I go good and well, they have their purpose"
  },
  {
    "startTime": "01:06:00",
    "text": "But they're not suitable as long-term fixes for the vulnerabilities And so the point followed thereafter is suggestion of protocols fixes for long-term fixes for this vulnerability And we would like to spark a discussion with you either here in the questions time which will be limited, I suppose but obviously also in the hallways and wherever you want to have it So what are key trap attacks? So what were they? So key trap is, have a look um look at these slides. So key trap is dost by the Nessai validation and there are several attack factors to this and in its most potent form it had a pretty high impact. So we managed to store resolvers for up to 16 hours with just a single response and it also required ridiculously little resources So essentially all on the attacker had to do was host a militia domain and several malicious zone file, a tractor resolve to resolve the name and that name. And yeah, thank you required ridiculously little resources. So essentially on an attacker had to do was host a malicious domain and server malicious zone file, attract a resolver to resolve the name and that name, and yeah, things would get downhill Also, all the tested DENessex implementations were found vulnerable This encompasses Resolver libraries, debugging tools resolvers themselves themselves, obviously And to be specific, we couldn't find any DNSic implementation which was not vulnerable Which also makes it sound to assume that there were quite an abundance of vulnerable networks worldwide and also made patching key trap a pretty critical thing So that required tight cornered with multi-vendor task force Doesn't go work Keytrap exploits the genetic protocol design. We call that the EGA validation approach, which is meant to ensure robust against validation errors, which is first try all possible DNS keys for signature record"
  },
  {
    "startTime": "01:08:00",
    "text": "until one works. And second, try all possible signatures from record set until one works One is a must requirement in the RFCs the other should requirement, but I'll iterate on that later and yes by the combination of these two requirements for example the specification essentially implied complex algorithm of expensive public key cryptography and thereby allowed CPU resource exhaustion attacks on the NSAC validators vulnerable requirements in the specification are actually pretty old, dating back to the 90s, and we found first vulnerable implementations in 2000 and 2004 by code review So that the actual fundamental problem, which was exposed by key trap ranges beyond ego validation So essentially a CPU resource exhaustion has never properly been addressed in the NSSEC So essentially a CPU resource exhaustion has never properly been addressed in the Nessac. There were points here and there which pointed into some directions, some addressments of some issues, but the whole overall problem was never been properly addressed until KeyTrap More specifically, the openness of DNS and the purpose of DNSSEG, openness of the protocols semantics allow for just a plethora of attack vectors which are key trap like. For instance, exploitation of the hashing, not only a signature validation and also exploitation of valid signatures, which were covering the different records that conforming to protocol semantics and different attack vectors So the short-term fixes which were engineered by the task force address these attack vectors but they also introduced so far unmatched complexity which has issues on its own and yeah, I'll come to that now So the first class of mitigations is architectural containment, foremost intermittent long run validations to allow other tasks in the pipeline to proceed and this works in preventing denial of service, but this also still allows waste of CPU"
  },
  {
    "startTime": "01:10:03",
    "text": "cycles i write low priority CPU cycles but we still should not allow an attacker to just rent up CPU for nothing. So there's an issue with that The second class and arguably more interesting class for us is limiting cryptographic operations The limits encompass a number of signatures try to validate a different record set. Number of the Neskes tried with a given signature, failed on attempted validations by message, and most interestingly, of all, the signatures and the S validations which are attempted or conducted per resolution meaning especially per client request these per client request resolution limit extend the requirements which were made for DNS resolvers back in the RSI 1035 specific already to the DNS setting But there are some problems with these Perez resolution limits. First and foremost, there was essentially, the vendors opted for implementing their own limits and the values themselves are either hard-coded or can be set by configuration file. So these limits are local to deployment, which is somewhat desirable you mean can imagine open reserver has different requirements and different vulnerabilities than some reservoir operated in a local network but it's also problematic at least in the absence of mechanism to signal and know these limits because for one, it introduced a factor of unreliability on the domain side Domain operator cannot really know, okay I've configured my domain correctly, but there may be resolvers out there which have tight limits so some validation can fail because the limits have been exceeded and on the domain side of things, you cannot really see it So this disincentivizes domain set use of the"
  },
  {
    "startTime": "01:12:00",
    "text": "NSSEC to some extent and domain side use of the NSAC has issues on its own it's been rolled up pretty slowly so this is something we don't want to have Second, having these limits chosen and not having some ways of communicating them also means that you cannot really adapt protocol development to that but I'll come to that later Also, limiting the NASSEC implies limiting DNS which is essentially a layer violation which on its own is maybe a bad design but it adds complexity to the already complex in ESSEC And by that hampers future, DNS protocol development. So managing validation complexity in the face of especially this pair resolution limit, limited validation budgets is challenging I brought to you some factors driving the complex of validation and therefore also the complexity of managing this validation limits and budget which is first the number of records sets requiring validation in responses For instance, we're going to introduce a new record type for delegation, which a delegate group is working on this would essentially double the computer of delegations, double the number of signatures required for delegation response to be validated which is yeah the openness of future DNS use case is being restricted here And there's also some elective validation cases, for instance infrastructure revalidation That was a presentation in DNS up recently, if you heard that and yeah obviously these key tech collisions, which allowed exploitation of multiple DNS keys, covering or being associated with a single signature these key tech collisions induce natural validation failures when they arise So not necessarily they make validation a matter of probability but when they arise"
  },
  {
    "startTime": "01:14:02",
    "text": "let's imagine you have Dineski collision and one zone with two keys which are colliding then the worst case validation complexity essentially doubles Secondly, these key tags don't necessarily necessarily follow a random uniform distribution We've seen also in previous work that we've seen also in previous work that the for instance RSA which is pictured here doesn't cover the complete key tech space and yeah, but that's just the matter of probability that a collision of course needs to be at least considered on the NSEC algorithm design signature algorithm design, but yeah, the key tech calculation idiosyncrasies are configured that it's not necessarily, yeah, predictable how frequently these key collisions are Then there's crypto agility to be considered specifically future algorithms might increase CPU load required from validation or implied by validation. And this may require reconsideration of global limits but these limits are set locally at the deployments as per now So this is an issue on its own and also different crypto libraries vary in CQV requirements just by switching out a library That could mean that configuration at a resolver which is meant to protect, is maybe not able to protect any longer the additional factor which promote this complexity we enumerated some in our paper and I think I'll just refer to that for the interest of media Okay, what do we suggest to make things better?"
  },
  {
    "startTime": "01:16:02",
    "text": "So first we suggest to introduce management of these validation budgets, but essentially if you limit per resolution, the value validation attempts that a resolver will perform you already introduce something like a validation budget So managing that by specification would be one approach to these things So we recommend to set a global minimum per resolution validation budget in the specification which reflects current operational insights and maybe updated over time Additionally to that, we recommend to introduce EDNS options to signal the current validation budget and the total validation budget from Reservoirs to name servers to make this apparent at the authoritative side And also this would support global monitoring of these validation budget which would then better facilitate protocol development also value validation budget depletion error code can be implemented via EDNS extended errors, which would signal such an error from resolvers to clients The next recommendation would be outlaw Keetail collisions. So just demanding key text to uniquely identified in Eskinosone would be blunt control tech collisions. So just demanding key text to uniquely identified in Eskina zone would be blunt confrontation to current semantics, which require the opposite And also just changing the current semantics of the current records would require well-proliferation coordination, which is pretty hard to enforce without breaking things So our solution to this is to introduce a new record type, which will be a security to Dean Eski. And we call that IDK key. ID key features a C 16-bid key ID, which is necessarily unique purpose be a successor to DNS key. And we call that IDK. ID key features a 16 bit key ID, which is necessarily unique per zone. So it's selected by the zone administrator in distributed set settings, multi-signer, for instance, the key text space can just be segmented for one provider and for the other provider And on the reserver side of things,"
  },
  {
    "startTime": "01:18:02",
    "text": "we require validators to insist on uniqueness DS records need to be replaced by analog IDDS records. This is required essentially for secure forward to insecure mode during transition. And we will not, we don't recommend individuals in new signature records, but the signature records can be repurposed by just selecting the key ID to, yeah to the key tag field and the signal records During a transition phase, domains would provide both the NS key and ID key and set the key ID ID key the ID key to conform to the key tech of the DNS key record record Obviously, in that case, they need to make sure that the DNS key tags are unique as well And IDDS and DS records are be provision required alongside Resolvers supporting IDK would query for both DNS key and ID key and either may be used for validation And by that measure, also the adoption can be easily monitored at domain and domains and inner transit nodes Response sizes are kept pretty small Only response sizes of delegation can double like this which is also only during transition So also note that during transition, actually, this could be streamlined with Deleg when that's specified So not necessarily the would not really collide with introduction of Dele or chemist streamlines to keep validation complex of delegations more Last but on least, we recommend to relax the absolute specification requirements specifically to degrade the master requirements from two RFCs shown here to Schult"
  },
  {
    "startTime": "01:20:00",
    "text": "And also we suggest to warn about 10 taking the SHUD requirements literally, because as we've seen Vikeetrap also shoot requirements could be exploited for this attack Okay, that was so far from all sides We brought you a few questions to be considered. We're also happy to answer your questions or if that takes longer, maybe to take it to the hall or mailing list. Thank you for your attention Thank you very much, Elias We have some questions in the queue. Well, we're getting many questions in the queue Mark, you're first mark andrews, I see IDK is not needed for this. It literally is not needed. We have already used key IDs, key algorithms IDs, to define semantics that are assigned to it. So the early keys did not know about insect three The current, all the current keys now know about insect three So the early keys did not know about NSEC 3. All the current keys now know about NSEC 3 and it's a future ongoing thing At this point, we could just say the next key idea to be defined has the new properties and the validators can depend upon So there's no need for a typecode role here and at work there's a couple of duplications of DNS algorithms so the operators can allude to can move to DNS algorithms which have the new semantics. Does that make sense? Yes, actually I think I saw this discussion on the NSOP mailing list. Yeah, I was hoping you'd seen it There's pros and cons to both approaches, actually"
  },
  {
    "startTime": "01:22:00",
    "text": "because when you introduce new algorithms so from my side, I think that has not been posted to the mailing list yet, it also makes specific So essentially it would require also a specification update because right now specification yeah you got to specification update either way, so. Yeah, right, right So I just the thing is that all the existing validators can very cheaply be moved to support the new algorithm because they're just an alias. Their existing code would support the new semantics Whether they've enforced it or not is another matter but they would validate against it They mayn't fail if the signers generate the wrong stuff but they will definitely come back with a secure answer if they're doing the right stuff. Does it make sense? Yeah, apart from if I, part, because my point is that there will certainly just on the implementation side of things if you just introduce new algorithms, it's not done by just introducing the algorithm, right? You're changing the semantics of the key text So there's certainly more than just introducing Yeah, there's definitely more than introducing it. But those which aren't particularly worried about key trap they can just do it as an alias. And for those that are worried about it, they can do the checks in the validator anyway. So I'm very sorry to be that horrible person that gets a good conversation but there is quite a few people in the queue and very few minutes left so if you can all make questions that are self-contained with self-contained answers that could be awesome thank you Wesherty and technically the IQ minutes left. So if you can all make questions that are self-contained with self-contained answers, that could be awesome. Thank you. Weser, USC, ISI, and technically the ICAN board, they would like to remind me that I'm not speaking for them Excellent work"
  },
  {
    "startTime": "01:24:00",
    "text": "Thank you, I think One thing that I would suggest is you have very detailed recommendations-based on this particular problem space. I'd love to see a higher level statement of when you are in an issue of any protocol that has multiple key signing stuff or multiple keys to select from what is a recommendation? You know, like if you have two conflicting keys that might be doing stuff, just stop, you know we should only be, you know, designing protocols with one key And then you can actually take that backwards and figure out how do you want to modify DNS sec with a should or the must in order to, you know, with one key. And then you can actually take that backwards and figure out how do you want to modify DNS sec with the should or the must in order to build a more generic platform for how to design protocols in the first place rather than just the specific issue Thank you. Very interesting approach approach Hi, Robert Kishakira, IpNCC. You mentioned a potential signaling of the global validation budget from resolvers to authoritative. I guess the answer depends on how you actually want to do that, but doesn't it also help? the attacker? I suppose in some ways, yes, but I think we're getting into the arm of security by obscurity if you want to prove that, knowing the validation budgets at the attacker side, right? Well, we are in that situation. Now you're giving more information to the bad guys as well yeah i think could be the downside of the approach legitimately but it could be bad anyways So thank you for the remark. Okay, thanks jim reid, just a random punter that's one of the off the street, speaking only for myself. This is very interesting work, and I think you've given us a lot of things to think about here but my concern is that you're trying to solve the problem in my view for Borrome wrong approach. I think adding more complexity and making more tweaks to the NSIC protocol is probably not the best way to try and address this particular issue"
  },
  {
    "startTime": "01:26:00",
    "text": "From my part of view, I think the simplest solution would be have some kind of implementation guidelines or configuration hooks and the validating resolvers to say that if you do too many attempts to validate and various things go wrong for various definitions of going wrong, just give up and return a hard fail. If we try to go further, down this path i think things get much more complicated very very quickly and it'll be very difficult to recover from that And I think the all-down idea of you suggested that about doing signaling to say this is the extent of my key capabilities, either from the resolve or to the authoritative server or vice versa, that introduced new forms of attack as well. So it's maybe best to try and steer away from that I do think there's something worthwhile looking further into here, but I think something much simpler is needed rather than trying to make changes to what is already a very brittle and very complicated protocol. Thanks. Thank you Hi, this is peter thomassen So let me look at my notes, actually So I think you said that limits add complexity and it's true. And it's also challenging to perhaps sometimes do validation of like a long-winded resolution within the limits. And that name servers would have to adjust the responses perhaps so that stuff fits in the limits. So I was wondering if there's an actual evidence that something's broke because limits were tightened up. I mean, they have been tightened up and I think the consensus is that the key trap vulnerability has been mostly addressed and things don't break anymore in that sense. And so my thinking is if we really need ID key for example that would only be justifiable if the limits are not sufficient and really cause problems. So is there evidence for that? And a related question is that you said, the limits cause complexity, but adding us signaling mechanism, of course, would be even more complexity"
  },
  {
    "startTime": "01:28:00",
    "text": "And it would be something that would support communicating the limits so it would actually be pro-limits which you just said are not cool and so yeah I'd like to look at the complexity trade-off between limits and signal. Hold on. It's just two questions, actually The complexity trade-off between limits and signal and whether having limits actually costs problems for the authoritative service. Okay thank you for your questions um so first regarding um the limits and complexity, so we essentially have introduced limits so the question is just how much do we want to manage them and all point is that these limits can cause bad things in a the long run with protocol development, especially if these limits are chosen locally at deployment site and you're going to do develop stuff, and then I don't know, C-name chains, referral, deep referrals, and then you could just break stuff by introducing new mechanisms For instance, select, which could double the delegation chain So it could be worthwhile adding this signaling bit of complexity, which is at least from my sense, not too come like an investment to actually manage this validation complexity or the implications on complexity from these validation budgets or limitations in the first place It was a second question. First question was evidence of two tie limits So actually, at the moment, no, but again, this is a matter of long time robustness of the protocol and protocol development process So at the moment, there's just one thing with we identified as maybe two tight limits which is maybe the patched in the meantime, I don't know but a bind nine approach was to just tolerate a single cryptographic validation failure And in case of a delegation, which we requires more than one signature validation because there's N-Sec 3 involved or whatever,"
  },
  {
    "startTime": "01:30:00",
    "text": "a DNS key collision could actually cause in the worst case such a thing to fire. I mean, collisions are not frequent that's clear, but they can happen and they do happen, and in that case, things can break I hope that answered your questions Thank you very much. Let's thank Alias for a great presentation And if the author of the next paper UNISAB are present, please manifest yourself. Yes Thank you Levine, you were worried me. I could not reach you, so the slides are there Have you passed the control? Sure, thank you you Okay, I can counter the slides. Good enough thank you. Okay, I can counter the slides. Good afternoon, Ivan. I'm the BNLU from Zhongguyen Lab Today, I will represent our co-officers to present our work, a unit style framework for Internet skills or so addressed by suggestivation First, let's review the necessity for social address validation A source address both things leads to very much malicious attacks as described in the FC 6959 and the representative attacks as a reflective DDOS. Therefore, network operators deploy social address validation devices in the networks to block the traffic with spoofing social IP addresses. Also, since 2014, the Mennel's initiative had been calling on the net operators to implement a site cell as close through the source as possible Various some mechanisms are proposed and become standards in IETF, the best current practice for sal, such as BCP-3H was first proposed in 2000"
  },
  {
    "startTime": "01:32:02",
    "text": "And IFC 3704 and the IFC 8704 was proposed in 20404 and 2020 respectively The source address validation architecture, IFC 5210, was published in 2008. Besides, IETF survey working group was formed in 2008, and M2 standardized access network cell magnetism that prevent nodes attached to the same IP link from spoofing each other's IP addresses And the IETF sound networking group was from founded in 2022 and aim to guide the development of new intro and interdomenational mechanism including their distributed protocols for SAW, such as DSOW, which was proposed in the IETF-113 meeting However, recent sound measurement studies through that the adoption of sal is worrying low on the internet This indicates that it remains a significant challenge to promote the wide deployment of SAW. This summarized, three fundamental reasons for this based on the survey. The first reason is lag or understanding for SAW Many network operators lack the technical knowledge, understanding, and practical experience. They do not know how SAW works or how to deploy or operator-specific sub-magnum which prevents they deploy some mechanisms in their network. The second one, is like open-source implementation There is very limited open-source effort on SAW As a result, it is difficult to form an acknowledged baseline standard leading to differences in understanding and the implementation of the same cell mechanism The third one is the performance concerns"
  },
  {
    "startTime": "01:34:02",
    "text": "Network operators cannot test and evaluate performance of different cell mechanisms due to the lack or publicly available test bed Without sufficient tests, network operators hesitated to deploy some mechanisms in their network So, to solve the problems, Union South provide an open platform to implement and emulate different sound mechanisms Before introducing the unit cell, let's first summarize existing cell mechanisms in terms of cell information sources and the sound rule format. In order to motivate the design a unified cell framework, The sound information sources consists of manual configurations third party a third database, such as RPKI ROE, and ASPA objects, and IRR data and the communication messages are between AI So rule format includes two types, prefix, and it's legitimate incoming interface, and prefix and its cryptographic key. In the table, we can see UIPF, survey and desal, rely on their communication messages between us and generate south rules with prefix and the legitimate income interfaces Passport, Epic, and Piscals are rely on the communication information between AI and generate south rules with prefix and the cryptographic key BASO relies on the third-party public database and their communication information between AS and generates south rules with prefix and the legitimate legitimate interface. Ingress filter relies on the manual"
  },
  {
    "startTime": "01:36:02",
    "text": "configurations and generate style rules with prefix and the legitimate interfaces In sum, this work, first of the revisits existing cell mechanisms from a high-level perspective and summarize a unified architecture that can cover all existing cell mechanisms and possible filter runs Besides, existing some mechanisms consist of their basic function for collecting cell information, generating cell rules, and executing cell filtering And they are different in cell information source or cell rule format Therefore, we propose Yonisal. Here is a figure shows the architecture of Unisal The cell agent is a call of unisal, which has three primary modules including the cell information collector, cell root generator, and cell enforcer and two data structures, such as the cell information base, and the cell table The sound information collector collects the sound information from one or more self information sources and consolidates the collected information into the cell information base abbreviated as save um on the site information base is a data structure that is stored the SIV information consolidated by the SIP of the information collector and the cell information collector. And the cell, and the cell route generated processes the cell information stored in the save and output a cell table according to the cell algorithms The cell table is a data structure that installs cell rules generally by the cell rule generator"
  },
  {
    "startTime": "01:38:02",
    "text": "generator And the cell enforcer performs cell on their data plan by using the cell table and the cell acutors, such as the ACL P4, and IP tables We have implemented nile max sound mechanisms, including eight existing some mechanisms and very new cell mechanisms The eight existing cell mechanisms include U URIPF-based cell mechanisms, such as LOSUR UIPF, strict UIPF, FPF, FPF, EFPUIPF with algorithm A and EFPU-RPU-IPF with algorithm B Bar-S, Passport, and D-Sau IPF with algorithm B, bas-sau, passport, and the D sound. The new sound mechanism is a chord in Enhanced D-South, abbreviated as EDC E-D-South E-D-sau makes the three improvements upon their desal. First, it reduces the side of the communicated information by you using the S-number to replace source preference the size of the communicated information by using the S number to replace source prefix of the corresponding S within the communicated message Second, it implements a neighbor discovery mechanism for building neighbor relationships And the third, it decoupled the control and the discipline channels Only the control channel reuses the BJ connection of the underlying router EDSI uses the control channel to exchange information to establish and maintain neighbor relationships, as well as to set up data channels with neighbors For external essence, EDSI uses a data channel. As showing in the right figure, along the proposal, of some messages are increased the contrapneling performance of ED cell decreases lower than DSA"
  },
  {
    "startTime": "01:40:02",
    "text": "DSF In order to evaluate the performance of their some mechanisms on top of UNICEF, we create the self-benchmark. We build the real world AS-level network topology according to this following step We collect the real BJP data from public route collector provided by route views and the ripe rivalries pass and extracts and extract AS paths attribute from the BJP data and obtaining the neighboring relation between ESS We create links for the neighboring AS to build the ASLOWER Internet topology and obtain the business relationship between AS, according to the data from CADA. Then we generated sub- subgraphs based on the four topology, which are the connected component of the fault topology Also, we assign the routing policy based on their business relationships and the very free principle. We also adopt a three classic scenarios, including symmetric routing, no export and direct server routine For the evaluation, set-ups, we use X-A6 server machine with 226 call co-cpcpOs, 250 gigabytes RAM and two 1 terabyte SSDs, and 1 terabyte SSDs, and 1 HDDs On the solar machine, we run UBN2 with kernel version 5.15, and use darker for each container to emulate an AS. Visualize our bird as their ESBorder and use IP tables to filter the packets on the data plate Based on the test pad, we evaluate the performance of the cyber mechanism in terms of sound accuracy,"
  },
  {
    "startTime": "01:42:02",
    "text": "control plane and data plan performance, and scalability And it's set for the scalability improvements, we use a network topology with 50 in our evaluation And we vary the deployment ratios of the sound mechanism from 10% to 100% to emulate different scenarios of partial deployment Here, the table shows the cell accuracy of different cell mechanisms implemented on top of unisome in the scenarios, including symmetric routing no export, and direct storage routine. We can see the table shows that the cell accuracy results of different cell mechanisms In symmetric routing scenario, both LOS UIP and the EFURP with algorithm B may improperly permit spoofing traffic. Innoisse ESPOT and the DSR scenario, both you UIPF and the EFPU IPF with our algorithm B may improperly permit spolping traffic Strict U.S.U.R.P.U.R.P IPF, ERPU IPF with Album A and Album B and FASI may properly block legitimate traffic Also, this is environmental results are the same as a theoretical analysis in the inter-domen sunlight problem statement draft Okay, here, the figure shows a concrepline performance for processing pure BJP message in terms of packets per second with varying proportions of sound messages over the total number of messages for cell and pure BJP. We can observe that both D-South and ED-SEL impact the efficiency of the control plane in dealing with pure BGP messages"
  },
  {
    "startTime": "01:44:02",
    "text": "But the underlying reasons are different For EDSI, its limitations arise from the other results constraints within each container in our emulation Instead, D-sign not only need to communicate more message, more sound messages additional resources for passing their delivered sound messages Then let's look at the data plan performance. The figure shows the data plane forward performance of their some mechanisms with varying deployment ratios Here, we employ IP table to acute their cell on the data plane, and we implement traffic generation two to generate packets with fail 1.5 kilobytes to evaluate the data plan for awarding performance in terms of packets per second Here, passport always has a more significant impact on data plan forwarding performance as we can observe than the other some mechanisms And the data plan forwarding performance of each cell mechanism decreases as the deployment ratio increases. This is because the size table with an HES increase with the increase of deployment ratio A larger south table relies in longer query time for each incoming packet And let's look at the Scala Bits of UNICEF. The figure shows the the experiment completion time of UNICEF, of course different network skills In this environment, we vary the network skills by increasing the number of AI"
  },
  {
    "startTime": "01:46:02",
    "text": "for the test-fed experiments. And then calculate their whole completion time of the experiment And the experiment combination time is a large longest time elapsed from launching their Docker environment, launching the emulation environment to generating complete south table on the data plan among all yes from launching the Docker environment, launching the emulation environment to generating complete south table on the data plan among OAS. In the environment, we've vary the AS numbers from 2040 to 200s. Also, we take D-S cell and the ED cell as the sound magnum, as the sound magnum Both experimental combination times for the desal and the ed cell increase along with the increase as numbers as we observed in the figure And compared to D-Sau, ED-SE shows a slower growth trends with the increase on netto size We analyzed that this is because a E.D.S. Converte is faster than the D.S D.S that this is because EDSI converges faster than the DSAR. Okay, here is a summary of the drafts which related to our work in IETF Thank you very much I'm happy to take your questions Thank you very much, Levin Are there questions? While people think about questions, can you talk a little bit about how would you see the deployment of UNISAB? How would you do the deployment? and you were showing the scalability of it? What incentives would people have to actually deploy UNISAP? i actually i i i i UNiSail is an open platform is an open software platform. People can use it to emulate their networking environments"
  },
  {
    "startTime": "01:48:02",
    "text": "and deploy different style mechanism to test their performance of some mechanisms in their target network environment. And then, based on the results, they can fall determine their appropriate sound mechanism for them. And also, they can further researchers they can also develop new cell mechanism based on unicell and compare the performance with other existing cell mechanisms to show their performance improvement or other design improvements yeah thank you very much Levine. If there are no other question, let's thank the speaker Thank you very much And see everybody after the break. Short break, I'm afraid So, thank you Okay. Six, six presentations Thank you eight on that. Okay, that's definitely something. Am I correct? You are probably correct I will check it. I, sorry, I really need to run to it. I'm afraid But don't worry"
  }
]
