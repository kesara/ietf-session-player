[
  {
    "startTime": "00:01:04",
    "text": "It's not showing here, but you'll see it, it'll come back here. Alright. So, note well, You probably have already seen this before, but if not, please take a note of it. And while you do that, note well, not very well. Make sure that, we feed each other with respect and professional professionally. So everything that you need to know is up there. Alright. With that, these are some of the meeting tips. I'm not going to go through all of these continuing on some of the resources that are available if you have anything that you need to look at, And, of course, Welcome, Chris. And with that, I think, Kevin, I'm gonna give it to you and, you can walk through some of the slides here."
  },
  {
    "startTime": "00:02:00",
    "text": "Sure. Thanks, Sanjay. Hopefully, everyone noticed that RFC 9538 was published This is the Acme, TLS delegation draft, or or the ACME draft We also have the TLS sub search draft for delegation as well, but We've been working on this draft for a long time. And so I appreciate all the hard work that the authors put into this and congratulations on. Pushing it over the line. It was, It was hopefully relatively painless, though. It did take us a long time to get there. So congratulations and and good work to everyone. Next slide. So we have 2 other draft that we did send out last calls for. One is, as I mentioned, the delegate credentials draft, thank you to Kristoff for putting in all the work there, and thank you to Mike, our sector early reviewer I think we we made a couple revisions and, we've made some good updates a security perspective. So, hopefully, it will be well received by the AESG The last call will end in the in a week from today. And then I will put in the the shepherd right up and we'll take it from there. Similarly, We have the capability capability advertisement extension draft. Sanjay sent out the last call for that. We have 2 weeks on that one. But, I think that one's in pretty good shape as well. There's nothing we we got through all the contentious pieces. It's it's a pretty straightforward draft. So, hopefully, that one will go easily. As always, please go and read the drafts. If you have any comments, questions, concerns, or support, please just respond to the list to that last call. Otherwise, We will once those close, we will we will move forward with submitting them to the ISG. Sanjay. Next slide. These are our existing milestones. As you can see, we missed the"
  },
  {
    "startTime": "00:04:01",
    "text": "we missed our dates for the TLS Subsearch draft that's okay. We got the early review. There's a reason we were working through those comments. I think that one's fine. We did finally get out the last call for the capacity draft So those should be taking care of themselves here shortly. We have the other drafts on the list that triggers interface which we'll get an update on. I think we're close. We had some good reviews last time around And so, hopefully, we can push that over the line I know we had to had to do some switch up with the editors on that draft, but and there's still a couple of questions outstanding from Alan, but but we'll address that today. And then we have the new drafts that that we we did the adoption for last time around. Next slide. So we have a lot on the agenda for today. We have 3 pages of agenda. I won't go through it all, but, obviously, we're gonna discuss the drafts that we have milestones for. We have a bunch of new drafts that have come in And so, I think we should probably just get right onto it. Yeah. Yeah. I think this is all the list of a draft that we're gonna be discussing today. But with that said, given we have a pretty packed agenda, why don't we get started with the first set of slides here. And Let's see if I can bring up the next step here. I assume you can see and hear me now. Oh, I'm not up yet. 0. Yeah. So we're we're gonna start with, with Jay. To give an update on the, on the triggers, 2nd edition version. I think as Kevin said, we are getting very close, to the finish line. But that said, I'll let Jay take it over from here and then walk us through. And, let us, you know, give us an update as to where you are and what needs to be done."
  },
  {
    "startTime": "00:06:00",
    "text": "Okay. Thank you, Sanjay. I'm Jay Robertson, and today. I will be presenting the updates for the second edition of the CDNI control interface and triggers. So just a little bit about me. My my work with Cash Purge over a quilt prompted my interest in this. And I'm, the newest member of the team that is helping out with uh-uh this particular and I appreciate near and Sanjay extending me this opportunity to contribute toward the development. So, So without further ado, we'll move on to the next slide. what has changed from B9, at IETF Sanjay presented the changes in V Nine. Now, we'll cover, what has changed since that version? The current version is V11. So we have had 2 versions since IETF 118. V 10 had no real consequent consequential changes. It was only made, to keep the draft from expiring on January 31st However, the 11 did and was mainly a cleanup draft. I effectively ran read the entire thing, last year and developed a set of grammatical and spelling, a list of grammatical and spelling errors. Kevin Mah did much the same thing and I combine the two lists together and effectively did an update to the specification to incorporate all of that feedback there. Additionally I responded to the three issues raised by, by Eona, which we're, which recovered in, IETF 118. These were the 3 knits that were messed that were messaged during that that were mentioned during that particular period of time,"
  },
  {
    "startTime": "00:08:02",
    "text": "mostly with regard to the, registries in section 11, a clarification with the language regarding subregistries versus registries and registries versus registry groups. As well as the language with regard to, the word use of the word repeat versus reusing definitions for trigger to So all of that has now been incorporated. And probably the most consequential change of all of these is fact that, per suggestion from Kevin Ma, we switch from, PCRE to, posix Section 9 extended regular expressions, which makes very, very con consistent with the other specifications of this type over here. So nevertheless, the diff is available at the link below over here in case anyone is interested in all the changes, but, effectively. It's a lot of cleanups that have been done in order to correct a lot of spelling in grammatical areas more than anything. Alright. Next slide. So the next issue that we'll cover is an an issue with regard to date local time versus ISO 8601. This is one of the, issues that Kevin Ma raised in his review of the specification as he noted, it seems like we're, spending a lot of effort to get a date time with no time zone. Can we just to use ISO 8601 local time and, I'll We spent a little bit of time contemplating, some of the reasons why we had gone, down this particular road over here. I think the, just for a little bit of background, here. This is this pertains to section 8 point 2.3 of the"
  },
  {
    "startTime": "00:10:00",
    "text": "section 2 of the, second edition of the spec. And it follows section 43 2 of ISO 8601 as a complete date and time extended representation. So it is compatible with that. But it has both the time zone and the decimal seconds, elided in this particular case here. The reason that we had, aimed to do something a little bit different was mainly towards, simplicity. We what didn't wanna go and burden the API developers with having to go and support some of the things that ISO 8601 supported with regard to, exact accuracy decimal fractions of the of the second as well as leapseconds and, and effectively how some of that goes and translates into some of the strange time, representations. So nevertheless, because we did not need this level of accuracy, we formed the state local time to make it a little simpler for uh-uh developers who are implementing this to go and parts, parse a, a little bit different a little bit, simpler time spec, but basically to make this happen, we ended up having to put in a slightly more restrictive syntax as well as the ABNF in order to specify that. So mainly that was the the reason why on this, I will note that as we were reviewing this, we've found that, despite the multiple reviews, we all missed the fact that all of the examples in section 8.2, had a times, but vacation that did not, did not go and fit with the ABNF that was specified in here. So we will be fixing that. In the follow on draft, of this"
  },
  {
    "startTime": "00:12:01",
    "text": "of the specification here, but we wanted to present this and see if anyone had any, concerns, or questions with regard to the sum of this, design choice that were made here. Well, everyone is good. We can move on to the next slide here. So that leaves what's outstanding. So, so the first thing is that with regard to the PCRE to positive change in section 7 point 4.1. Unfortunately, we introduced a small little, issue. Unfortunately, autocorrect got got to the best of me on this one and changed, locale to locate. So we will be fixing this, in the specification. There'll be a a real simple change on that. As mentioned in the time policy, extension, what we were just talking about. We'll drop the decimal seconds from all of the examples, which will go incorrect that section there, section 8.2. Will fit fix any other knits and knits as, as indicated, by others, and, we will also be reviewing Alan's changes. So in 3, So in 3, 3 sessions from now, Alan will be proposing a list of, proposed changes for, RFC 8007 which we'll all be taking a look at here. So I see that there's some questions from the audience over here. So, I guess, Rajeev, do you, you appeared to be 1st in line here. Do you have a question? My question was regarding the previous slide, you know, where we are choosing not to use any,"
  },
  {
    "startTime": "00:14:02",
    "text": "time zone specification as part of the time stamp. Have we considered, the effects of this? In certain edge cases where we may be specifying certain times as part of triggers in time zones where there are going to be time shifts. Like, you know, we all wish for the day when daylight savings time is no longer or daylight saving time changes are no longer a thing. But, in cases where you do have a time shift either forward or back what's happening. You know, and What if the time that you specified as a you know, a time in the trigger, either get skipped over or you know you know you know you know you know you know you know or maybe lands up getting hit twice and you don't have a time zone specifier, which allows you to end of pin it down to a specific variant. In that particular local time area. how do we handle that? So So that's a good question. So with regard to how this date local time is used, So, one of the big, use cases for this is to be able to go and run a particular uh-uh run a particular trigger at a specific range of time. So, if you look at the specification, there's typically a start time and an in time, and the downstream CDN has leeway and terms of when it wants to go and schedule it within that particular time, that time period right there. The idea of that was to be able to go and purge in the local time zone of wherever the cash was located on that. So,"
  },
  {
    "startTime": "00:16:00",
    "text": "So, effectively, if we were trying to target a purge to happen during for instance, non prime time hours, in the middle of night in each particular region, we could end up doing that and, in which case, the time zone is not needed because it is whatever time zone, that is in whatever area that the cash is in and the downstream CDN on that. So, I think if you look at this Yeah. That's the specific point that I was making that, what if the downstream case the casing node happens to be in the time zone. That has the shift happening And one of these trigger, you know, boundary points is inside one of those shift points. Right. Say say, like, for example, I have an end window for a case, pre fill operation, which is set to execute. And the endpoint is happening to be inside that window of time. Which is gonna repeat twice. Okay. Do I stop my case fill at the first occurrence or at the second occurrence of the same time point. So I I Nina has also joined the queue. So maybe I I've you know, given some of the historical perspective, I wonder if Ned has response here. Yeah. I'm not fully, was fully, on board when this this entire discussion was formulating, and this extension was defined in the earliest worked with Ori and Sanjay. But I believe that based aligned with the what explained And with those principles, a Changing the times, changing the mo moving from changing time zone, not time zone. A day at 7 time. If you look at it from the"
  },
  {
    "startTime": "00:18:00",
    "text": "point of view of peak hours or low hours and things like that, It will the the usage of of this time. Even if if it repeats, Oh, A passed, but you shorten the time It aligns with it may actually align with what The user won't. Because those are the low hours, those are the peak hours. Usually, the the the chances will be off in the off peak hours. A I agree that there is a sent affinity a of something missing of a of even the The cavity we're at, that that may may need to be need to be addressed. It's possibly we're here. Yeah. We can we can we can we can maybe specify, some behavior around those areas, And we have to keep in mind that those extensions are all the the The concept of extensions is that we can add more and more based on the different use cases that we want. It might be wise to In the future, based on the needs we identified to to define another, extensions that AMO fits with the the with time zones and things like that. A I I I believe you. I'm some fine with the I'm fine with there being caveat. Here. Like, you Yeah. So I think I think that the clarification here would be in place I would not change The Yeah. I would just cry for the definition and would not change it."
  },
  {
    "startTime": "00:20:04",
    "text": "And that's it. And I would like to take the opportunity Thanks to 2. Thanks, Jay. For a joining us and working so hard on those things. I really appreciate it. And it's really this this this Bring this thing forward. Thank you. Yeah. So I I I entirely agree with, near that if we have just a little bit, maybe a line or 2 of text in here, calling out this potential edge case and giving some clarity to implement us. Even if the clarity is as simple as in these kind of scenarios, it's the decision of the you know, cashing node to decide which you know, time point to apply and, just keep that in mind when creating your triggers, that's fine. It's just that it if you have that little sentence in there to clarify, it makes it so much easier for anyone who's looking to implement this to say, okay. That means I'm I have the ability to handle this. And it's not an undefined area. Thank you. Yeah. Yep. Thanks. In just keeping in in mind with the time that we have, I just moved on to the next slide. I think this is the last slide. You have, Jay, you wanna cover this, and then we'll move on to Ben, Indeed. Yes. So, so in terms of next steps, we determined that we would create a version 12, which just has these last few cleanup, edits on it with regard to the, extension and the, one small knit for, pos, the possex, regular expressions, and, then we will re reserve version 13 for Alan's proposed suggestions, which he'll be presenting, shortly about. And then hopefully at that point, once we have that"
  },
  {
    "startTime": "00:22:03",
    "text": "reviewed. We'll be requesting review and, and last call as well. So, and that should be it. Great. Thank you. Ben, your next is slides. I pull up your In logging. Yeah. Alright. Alright. Great. You know, are you talking about capacity and logging? Combine them both into one deck here because, capacity will be pretty short. Go to the next slide. So capacity is at, you know, nearing the finish line here, Last call was just opened. And, even prior to this, the last revision only had a couple of knits that were fixed. So I haven't seen any substantial comments major changes in quite a long time now. So we'll see what happens. If you have any comments, you know, announce the time. And I think we'll do, we'll do one more revision because there was, I think Sanjay, you had a a couple of other comments after, my last revision, But, again, I think everything left is pretty minor. So, so that should get in. If you're not familiar with capacity by now, please read the draft. You know, again, now's the time. The basic concept is to allow the downstream to provide information to the upstream to make traffic delegation decisions based off of limits around well defined values like egressbitsperseconds, session count and things like that all defined in the specification. Along with referencing both either an external telemetry source"
  },
  {
    "startTime": "00:24:01",
    "text": "provide real time feedback on the current levels or to include those levels in line in the advertisement. And, so that that's the status of capacity. Hopefully, we'll We'll get this on its way soon. Alright. So logging is a a draft that we introduced just prior to 118, I believe. So even though it it's a a new introduction. It's something that's been worked on for quite a while. So There's a whole long list of people that have been working on this thing for well over a year now. And, I think it's finally gotten to a state where, it's cohesive. So we'll we'll talk about all the things in this list here as we go through the slides in a little more detail. But the basic concept is that the existing CDNI logging specification and 7937 only provides a a single, way to build log files. It's a an elf derived format, with a sparse list of fields, And, the only way to retrieve it is via references from an atom based index. With no explicit transport mechanisms. So this draft addresses all of those gaps and, and add some additional things as you'll see. Next slide, please. So the first thing that we tackled was the the set of fields that we want to cover. And right now, this draft specifically addresses access log fields on edge cache nodes, you know, there are It's possible that logging could cover other use cases such as intermediary caching, origin requests or potentially other"
  },
  {
    "startTime": "00:26:04",
    "text": "services as well. Right now, it's focused on edge caches. There have been only a couple of changes in the field definitions from the previous draft that I uploaded, and those are there. But this this set of fields was arrived that, via a long process with participation of, I think, 12 or 13 different companies, that are heavily active in the CDN space. And it was a a back and forth until we narrowed it down to the current list. So there's a lot of overlap with 7937. There's also a lot of new fields. And they're organized into, us into 3 different groupings with a minimal standard and extended set to cover different use cases. Next slide, please. So to expand upon the the one format specified in 7937 We've added a whole new slate of things, including JSON CSV white space and protobuff, as you can see here, This we've we've kind of created a separation between the format of the individual records that go into an access log and the format of the overall file, which we call a container. So we can go to the next slide, you'll see that JSON is, again, this list because you could have a JSON container, you know, with a bunch of metadata surrounding an array that contains log entries that are also JSON objects. Or you could have a JSON container with with log entries that are in a CSV string format, for example. So it's kind of a a a mix and match to cover all the use cases. Addition to just the raw File formats, we also wanted to support tarballs. So, you can also"
  },
  {
    "startTime": "00:28:00",
    "text": "Group log files into archives at this point. Tar is the only supported format. That could be extended. But you know, rather than treating files as individually, we can also pack them together, next slide, please. So once you have a file, you don't necessarily know what that file covers or have been applied to that file. Because if you were to pull back the instantaneous advertisement, from the downstream that says, you know, here's the here's the FCI object specifying the login configuration, that's only a a point in time response. So you'd know this is what the configuration is right now. That doesn't really help you if you are looking at historical log files. So we've introduced this concept of container metadata which covers all of that information packaged alongside the the logs themselves. So in the case of a JSON container that can be inside of the log file. For other formats, you can have a separate JSON file that lives alongside the log file with all of this information. And this, you know, would allow you to decode the individual log records. Next slide, please. So we had this question of, you know, now you've produced all these log files. How do you get them to their destination. So 7937 says look at the Adam index and follow those references and retrieve them. It doesn't also doesn't really tell you how do you get to the index in the first place. So there's a small gap that's not really sufficient. For, you know, current modern production CDN use cases. So we've divided these these transport mechanisms into 2 different modes."
  },
  {
    "startTime": "00:30:01",
    "text": "One is pull. So in addition to 7937, we can also support SFTP and s s 3. And if you go to the next slide, please. There's also a push mode. So the DCDN can deliver the logs to an upstream endpoint. So to do that, the upstream needs to specify the details of where those logs should be pushed, and that's where we come to about new MI objects, to support that So if we can go to the next slide, please. So here's just a a simple example of an upstream telling the downstream to push logs to an s 3 bucket. And you'll see this is also making use of another draft that was recently adopt adopted by the working group for protected secret. Metadata. For that access key secret. So, there's a couple other features that are are shown here in this little example, things like the name template which allows the upstream to specify how the log should be named and the interval, which defines over, you know, what time period, a log should cover before rolling over to a new one. It can get a bit more complicated than that. That's what the specification is for. Next slide, please. So one of the other things we had to think about was, we have all this data and logs and a lot of it can be considered sensitive in some way or another. Things like PII, might need to be obfuscated for regulatory reasons, other things, might need to be obfuscated because they're sensitive to your particular implementation, things like encryption keys, So we've provided a set of transform operations that you can use to, redact sensitive information or protect it,"
  },
  {
    "startTime": "00:32:02",
    "text": "in, various ways. So you can remove things from URLs. You can truncate strings. You can do a reg x. Search replace against the string, you can do IP masking, both 4v4andv6. You know, or you can, encrypt the field And, you know, provide that that encryption key via the protected secrets metadata, metadata, Yep. So, if we go to the next slide, This is the last one. So I I feel that this draft is in a pretty good place right now. So I'm asking for working group adoption of the draft. It obviously is pretty big. It needs a lot of feedback. And I haven't seen any comments about it so far since I brought it, to the last IETF. So I'm hoping we'll, we'll get some participation now. On the list, even though I know a lot of people on this call have already worked on this draft outside of this working group And there is a there is a bunch of other stuff that the draft doesn't cover right now. Which doesn't necessarily need to be addressed as part of this draft specifically could be an additional extension to cover this other functionality. But, you know, things like filtering and sampling or just something that that we didn't get to yet because the scope is already pretty large. So I'll I'll leave that there for anyone who has questions. They wanna get into the the queue or, I'll leave it to Kevin, Sanjay and Chris, if you can maybe elaborate on how we proceed here from, that, you know, my request for a working group adoption. Yeah, I'll go first. So, yes, this Jeff was submitted back in 118. And, chairs have been just overwhelmed."
  },
  {
    "startTime": "00:34:01",
    "text": "With, a lot of documents that are coming in, and we have not done a good job of trying to stay on top of each of these documents. There's there's a lot of work, actually. But the long and short, this document seems like it's within the within the bounds of CDNI. I'll certainly read this, and I think I don't see a problem in in getting this document adapters, the working group, draft. Yeah. I think we we do need more discussion. We do need more reviewers if we could if if folks could please go and read the draft and post your comments to the list be helpful. I agree that the content makes sense to to adopt I think we just need to to make sure that we have the the support for it. Thanks, Ben. Okay. That's it for me then. Thank you, Ben. Okay. Next is Ellen. Let's see here. Yep. Alright. So, the presentation we'll talk about, propose scope. So it's not something that is, kinda necessarily has been find that, this person they've served me doesn't talk about every mile kind of, kind of detail, that, com proposing to to introduce to the draft, but more kind of of our future scope. And also I think to solve us a feedback and awareness. So, I can go to the next slide. So, and, as Jay said, the plan is I guess, the tentative plan election to introduce us into, kind of subsequent that's I'm not 100% sure that makes really could that separated the 12 for the 15 or"
  },
  {
    "startTime": "00:36:00",
    "text": "just work, together on the kind of V12 of V13 or but we we can figure this out, what's what's the best way to move it forward. The motivation for this proposed scope, right, comes from, and I've mentioned in this forum before, comment extends from the cash management interface work that have carried on and, to SMTA for last 2 years. And that has been sort of, within SVT A's body work has come coming to kind of, rectification. Out of that and this interface, heavily relies on and extends CAT E V2, framework. So, the reason for introducing this those changes now, are that the several core features that were kind of we think that belonged in the core as opposed to others, kind of, subsequently, we'll be introducing additional drafts that can be separate because they, utilize, the extensions mechanism introduced in CHTV to draft. So we don't have to introduce all extensions in in the contrast. We can actually submit them separately, or it makes sense. But some core functionality, kind of does need to be addressed. In in the spec itself and the draft some hence, these proposals. Additional driver is that kind of, again, CDNI and Open Cash and don't necessarily adherence one and same. So the goal is to achieve interoperability between the two frameworks. So some of the concept that what kind of I've been a long addressed within CDNI chat, not addressed in Open Cashion. So we're looking at several where several minor points of things that currently, are featuring as mandatory within the draft and maybe within open caching now open cash and derived implementations"
  },
  {
    "startTime": "00:38:03",
    "text": "don't have to be necessarily. So that there was that issue, and I'll I'll talk about that later slide, And last but not least, the goal of the cash management interface effort is to convert that interface into workable open open API, restful schema, and that drives some proposals that in, in, in the scope that, will allow us to streamline the spec and also make it more restful. Next slide. So, Although the list of topics, so there's a thing about 6, 7 issues that, want to introduce in that and then in that kind of major dropped update. So first is object list. Right? So so the calendar aft addresses here we rightfully so a forward full conference playlist spec. Right? So which is ability for UCDM, to use existing manifests as a way to to kind of drive cash balance operation. Scastron from is all about the efficiency and then streaming with through length, ops, and just, creating list of URLs where manifest exists as really a useful thing. So we want to take that further to to address not just video content for formats out there today because right now, support such less dash and, snooze. But that doesn't cover necessarily a content that is not fully video. Mix on video, non video content, for example, there are some, formats and use in the industry that include manifests plus some non video formats of, video files are related, for example, some mails, so subtitles So we want to have a way to kind of basically to feed into DCD, DCDN, those more complex file structures that are not necessarily are built in in,"
  },
  {
    "startTime": "00:40:01",
    "text": "in the video, manifest format, and then allow UCDM to actually use existing kind of, descriptors, right, of of this. And, Additionally, and and do that in at least in two different ways, different formats. Is JSON and plain text. Point text, which is like, a list of URLs line by line, and Jason will enable us to x actually include some of the metadata object in the list as well. So if I'm feeding, for example, the use case that I'm feeding very large list of files, and I want create some policy extensions applied to only some of the objects in the same trigger with Jason and Former, we will allow me to do that. There's been sort of in our group, and there'll be some advocacy for for for such extension. We could we should examine also additional format that would be kind of generic form of languages. So XML, and, and Yamal should be also considered. So if you kind of if you have ready files that list files of describe the list of files, you should be able to use them. Think that, but we need to see kind of, kind of where to get right now, CDNI metadata objects are serializing JSON only. So I would certainly, would look to JSON as a method for serialization of metadata after skips that's kind of left side of the slide. The right side is kind of so that's kind of feeding objectless to this DCDN, right? The UCDN can take existing this kind of list of objects and freedom but then, right, which is a great simplification. It simplifies processes, makes life easier, kind of, and so on. But also makes life also complex for all the troubleshooting because our"
  },
  {
    "startTime": "00:42:01",
    "text": "you gave me a used new CDN. You gave me a list of files. And many fast which could be updated, could be different versioning of that. I went in and as DCDN process them, and some of that And so, minimally, I need to be able to to report back and said, he gave me manifest This is a list of objects I've derived. Is it what you meant? Right? So we can kind of look at versioning and look at kind of if that was the intention of that was the correct version. Also, and then we can use adjacent format to do that. So that's kind of ability to to kind of trade structured list of objects in in in this between UCDN and ECDNN. Look at natural, kind of extension of that playlist support that is currently in the graph today. Next likely, So, additionally, kind of one major top topic came up as we were discussing kind of how to use kind of triggers for for task completion and execution, which is what triggers really describes this kind of queuing process and, job management. One thing was gonna seem like the last thing is that right now, there's really kind of a hand waving around the way DCDMC process triggers. You know, there there are some limits that are there sort of the the time policy kinda when the process should at least start and it should complete by certain time. Optionally. But kind of how triggers are chosen for execution in which order are they processed and acted upon immediately or can be done in batch? This is all kind of really useful for the area right now. This is one area that kind of we want to address in an extension, but we think it's in a core sort of like a time policy which will address, rules and provide UCDM more options"
  },
  {
    "startTime": "00:44:01",
    "text": "your processing in more detail an order of vacation, which comes first, which comes seconds, dependency do this, then do that. Concurrency and so on. That's first. And secondary also, if you are looking at really doing that, scale. If we are looking at a potential dealing with not 1, 2, 3 triggers, but maybe dozens or 100 of them. Should be better toolset for monitoring processing. Kind of right now, this, there's something called trigger status collection. That's kind of not really provides for a very easy way to to monitor structures. You have to miss quick what you get is one API call. Which returns a list of URLs and you need to go and kind of iterate by 1 on 1. Kind of if you have 2, you don't mind, but if you have 50, that's already kind of not what you want as as kind of as newly a job kind of, queue queuing and monitoring support and reintegrated. So also the needs, I'll I'll talk about kind of how we want to address that next So, first use case is priority. Right? So so that, we want to enable using the end to say, on which, which priority trigger should be processed and, to not kind of do this allow it to use also provide very simple kind of a priority mechanism. So, like, integer from 1 to 100 So not to kind of create to to to split too many triggers when you can help use cases are like urgent trigger, like, you've you have sets a figure's motion and something very urgent comes comes and you want it to be kind of, and kind of, come ahead on the line, like, urgent virgin and validation, should be able to set, like, a lower priority or higher priority in the queue and that he has picked up next. Or, actually intentional staggering of triggers. So you proposition the triggers and you wanna indicate more urgent kind of counted first than others. So priorities. I think. Put your free quote."
  },
  {
    "startTime": "00:46:00",
    "text": "Next line. Then, similarly, priority is good, but that's sort of not enough because in terms of It's it defines what gets picked up next, but doesn't say when. So maybe next can be, like, example, this again can implement a batch processing. Like, hey, I'm doing burgers, but I'm doing that at midnight. Kind of come up to me in 12 hours, and that's urgent. It has to be done now. And it's not. CDN wants to hear about it. So, proposed extension kind of, again, a part of the one mention that, we have specified, it will come in the draft hold that institution policy extension, will say, hey, this is high priority and also do it right away. If you can't, fail it, I'll know that it's not happening. So, that can happen for, for example, maybe you said it's urgent, but you did you provided dependencies are there may be a conflict with different attributes There's a number of reasons why it can be denied But, certainly, we've asked an explicit ask, which should be explicitly approved or denied by the CEM. Like, do it right away. Next slide. Independently. Right? So so that's a third and lost kind of some kind of, pillar of this of this extension, it's ability to basically define a dependency, between triggers and, so that the trigger is only processed, those back to state after, the other two triggers it depends on have been processed in full. Process in full can be one of three things, but so it's kind of in the nearest term, I love that it's called terminal state. Right? Or if it's no longer pending or active, it's either complete, success, sailed, failed or or in that council. So it's no longer executing. Whenever that's reached then the dependent triggers can start. And use cases are kind of fair, but I'm sure there are additional ones So purge content then proposition a new one."
  },
  {
    "startTime": "00:48:01",
    "text": "But don't preposition before it purchase completes. You don't actually don't don't don't create a collision cancel preposition, and retried only after it had been canceled because cancellation would be as notorious I wanna know when it's really complete. Only then I wanted to try. And and and change it, right, the same as that like, priority, but also, only do this when I'm after you've done that. Next slide. Now, one to one enhancement that came out of this and that will allow us actually to to help monitoring and also provide better tools with managing, again, a large number of triggers because I think you have that what we're seeking to do is that it's providing better tools to monitor I mentioned today, the API interface is really just a list of URLs that you you need to go and call and there's no way that should separate them. For example, if you are UCDN, and you have separate triggers unrelated you don't have a way to actually manage that. And unrelated can be that you have triggers that deal with particular set of content. You want to separate, I don't know, VGR from audio, for example, or or to be also a safe, the different content publisher. So different different areas of content that are con completely unrelated. So I want to be able to mark and deal with them separately. The create triggers and also monitoring that. And that's that's one drive and another is that we can provide an easy way to look at the trigger without looking at all of it. But triggers with extensions have become very complex objects. That's kind of just a spec and action and what you do and and extensions a large object. If I want just to abbreviate in a way, of, kind of quickly what is my cue and what's happening What's priority? What is things that I just spoke about,"
  },
  {
    "startTime": "00:50:00",
    "text": "So it's a label provides a good way to do this. It's kinda ability to, first, I'll mark the trigger as we create a trigger to label that, I can I wanna we need to label a trigger for video so then I can look at all my video triggers? Or whatever brand new thing happens to me. Like, the publisher may be a project. So whatever is subset. So I can then filter triggers not just by state as a dumb today, but also by label. Right? And also, label would be at the trigger spec. So a trigger level I can actually label a trigger, and I can also extend every extension can also provide a set of labels that would be exposed. So let's say extension says priority, So I can provide priority as key value label that can be easily seen, and the, at the trigger level. I'm looking at the trigger. I'll see its video, and its priority is So then I can make easily And and that can done the 1 in one API call as opposed to iterating through the list. So UCDM gets a gets a statures, what happens and can make decisions on what to do next. Introduce anyone kind of trigger council do all the things that that needs to be done with jobs. That's right. That's a minor one, but kind of important So, today, there are it triggers. There are several ways you can actually specify, kind of recalled, trigger spec and there are several types all but one really use URL in some form. Is it that it kind of list of URLs. His regular expression is that, playlist, right, their use of URLs is very happy, and that's natural. The only the only spec that is not using URL is, CCID is content kept your negative identifier the the wrinkle here is that, DCDM and UCDM,"
  },
  {
    "startTime": "00:52:01",
    "text": "may use 2 different types of URLs at least that's what we see today. One is a published euro and the other is cash euro. Publish URL implies, and that should be the default. Right? It implies that for whenever trigger is accepted for a particular URL, It has to match published configuration, and we have to say it. I don't think it says today. So if, for example, if, if you ask me for position, object for published URL or that you didn't publish, there should be an error. Right? Additionally, when that's a published URL, that should invoke triggering of related metadata objects. Whatever it needs to be kind of, whatever it needs to do, when acquiring content from the origin, similar to it, I'm prepositioning I'm going through the origin and I need some kind of, I should be able to to use what's already there. So, Unless, we are saying that this is a published, this is a cached URL, then all of that logic should not, should not apply. So the proposal is to add, attributes to these trigger facts of new URL and to indicate which URL type is is, implied and default being published So and that means the things that I just mentioned check configuration, then you can actually satisfy the trigger. And if not, if Kashi then, those those rules, do not a plot. Kevin, I see you have you wanna raise it now or Yeah. I wanna I wanna mention that we're over time for this topic already I know you're only you're only halfway through the slides. You you have 10 extra minutes to talk about the footprints draft You you can keep going. I just wanted to give you that time check we're running behind. Thanks. Appreciate it. Yeah. So, let let me, I think we're heading towards on here. So, kind of from, And the next slide,"
  },
  {
    "startTime": "00:54:05",
    "text": "rest of API, important, important change again, I'm under and, again, this is just scope. So, again, we don't have to cover every new details here. Will be draft coming up. We'll have everything inside. But, we want to streamline, the use of rest in a way that, will obviate the need in 2 different commands. Today, there are 2 different commands in the in the graph for cancellation and creation. Using kind of, post and some JSON instead of that come up with a way to actually use restful semantics. And, cancellation would be done to update our field called the liar state. And is this way, trigger can be created as desired state pending. So here was a trigger done process it yet. Put in pending, I'll tell you when I actually want to activate that. Complete. I want it to run. So it's kind of fully active trigger. Or cancel, which means that know, I wanna cancel, cancel the trigger And then, when it's in terminal state, we can use delete as today. So this will LA enable us really to use standard restful semantics. I think there's some difference of opinion should we use post changeional triggers. I'll patch an update, but I think that that's that's secondary. The the idea is that really align the trigger is fully, with rest restful cement. Next slide. Kind of, that's important. CIT triggers today, kinda sees monolithic use of figures for both content, and metadata. In reality, we have 2 different interfaces dealing with with 1 with configuration. And met the data and other wiz wiz content. The clashes in FCI FCI kind of right now publishes capabilities for for triggers for both we need to have a way to actually an FCS put it and say, 8"
  },
  {
    "startTime": "00:56:01",
    "text": "poor, trigger subject, metadata, use this endpoint, and it it will support these versions. 1, the 2, kind of bulldogs. And 4 content use the different endpoint with the cash management face that will support this function. There there is a proposal to do with that in FCI. Next and a CPM path. That's that's a big topic. I think we've we've got some discussion on the list, so I'm not kind of make too much of that. Kinda right now, the the draft mandates use of CDN CGM identifiers, which are kinda right now. The only way it's done is it's using, ASM numbers, which is really not good way to identify CDMs and and in in, you know, in the wild anymore. So we need to proposal is to, a, make it optional. So we don't actually force implementers just feed, focus data. And b come up with a better way to identify CDMs and probably, come up with some some method that will provide different multiple methods. So maybe there is identify a provider and some and maybe there's a simple method, but the thing we need basically that needs work. And I'm not sure if actually that's work that has to be in the triggers or triggers should like, kind of reference some other effort that will address that. That we also need a flexibility screw it into an authentication and connection manager. Next slide. Yeah. So that was, that was triggers. Right? And again, pro Proposal the follow-up on this to actually put up in writing and specify all these things. And, kind of, I wanna cover name footprints, and I know I'm short on time, but that should that would be great. So Sanjay, Yeah. Just one quick question. On the"
  },
  {
    "startTime": "00:58:01",
    "text": "on what you have covered so far for the UCDN to be able to specify priorities on managing the queue the question is that what, for any reason, the the DCDN is is not able to honor honor that. So is there a way for DCDN to simply deny? The request. 1st and foremost, so so the answer is to to fall. 1st, the supported that's part of us, the, extension that our support that should be part of FCI. We are today. Right? So if if this extension, a distribution policy extension, is not supported in full, so there's no kind of, no support whatsoever. So that will be denied because it will be actually not advertised. Subsequently, also, I think there is a way for, there's an actual return code that enables, DCM to to refuse refuse not supported, sub fuel. So you, for example, we support dependency, but, you know, support priority and there is no way to say it fully in FCI, there's too much you can just deny a request and say, may not support it. Okay. So let's start it off with Prince quickly. Kevin? I'd like to ask one one question as well. This is more from a chair perspective. So Alan, you're going to work with Jay to talk about whether or not we can incorporate these. Do you guys have a timeline for for when you're going to have that discussion or or know, when we can make a decision on whether we should move forward with the current set of changes in the, 07 biz or whether we wanna wait for these additional changes. Not opposed necessarily to the things that you've proposed, but I think it does imply we're gonna have bit more work to do on the draft. Jane year, well, Yeah. Uh-uh."
  },
  {
    "startTime": "01:00:00",
    "text": "There are several there are several things that are in the credits, Alan, since there are several in the data curve, the implementation, like the desired states. For example, For extensions, we will need to choose a pair extension, whether it's, in this class, or not, also for the content playlist, the additions Also, there we have FCI to support us. So so we don't have to push everything at once. We will I think we should aim to get something by something relatively closed. For the next, IETF meeting. But this is is I'm Won't be the one that pushing it forward. I I cannot commit and sign any checks, and on behalf of, Aaron and did the great job here. Thank you. And the MJ. So So we're thinking that we will have a proposal at IETF 121 see, as to how to move forward as not necessarily a a proposed text, a proposed draft, but a proposed plan. I think we should be able to get it into Graph. Again, this is derived. It's the, bay is a speck of based upon. It's really detailed and written up So we should not take a lot of time. I should convert that into into into draft. And I would even propose not to a speaker in necessarily be 12 and be 15. But maybe look at this, look at the scope going to the next graph altogether. We already have some significant disc discussion of that. Alan didn't get into all the details because the time limits So Yeah. And I think I'm I think the time of my life I was supporting the draft on this. So but, yeah. Go ahead. Thanks. Yeah, the only thing I would add is that Timeline wise, it would really be best to try to to narrow down"
  },
  {
    "startTime": "01:02:03",
    "text": "to basically come to the consensus of what would go in version 13. That should happen soon, rather than later. And in fact, if there's an agreement, then, a new draft should be submitted well before IITF 120, and then hopefully we we're really looking at working group last call and that kind of stuff towards 120. So I think It probably behooves upon us to try and get this done in the next couple of months. To have a good draft ready in ready for reviews. I think it's feasible. I'm kind of good. So, again, we've done at least 3, 4 reviews already. So that's that's scroll well established, because I think it's, you know, at that, within couple of months, we can have a graph. Sounds good. We're looking forward to seeing the draft. Thank you. I'll be really quick on through footprints because it's really a recall, kind of wanna see your answer. So Well, let me just kind of the the minimal justice to the footprints and we could spend a lot of time talking about triggers Oh, yeah. Well, I Just I actually wanted to talk about triggers for just a second. Okay. Yeah. Okay. As we do get into the next round of work in the SVTA on what we're calling the work ration API, you know, you touched on it a little bit we will rely on triggers for asynchronous notification to a to a to a UCDN can know when it's published metadata to a DCDN. It's conceivable as we flush that out, we may want to pile on some more minor changes or extensions to triggers. I know you've already thought about it a bunch, but just, you know, that's out there and that may not be work that we get to for a few more So we'll have to see how the timing of all of all of this is. But it's conceivable we may wanna change a few things. Just that really to worry than that, kind of, I'm I'm I'm aware and actually reach out to you actually to to make sure that you call you in on this."
  },
  {
    "startTime": "01:04:03",
    "text": "Kind of we have a mechanism of extensions that we can actually, we'll lay on. Example, one of the things that are kind of beyond this graph is actual WACCO support. Actually, we get real updates asynchronously. When something is completed, if you don't have that today, But that's something we can actually work on in a separate draft as basically using the existing framework that near important place around extensions. So you can actually extend things that that reach functionality without changing the core. What we're trying to do now is what we really have to do in the core that cannot go into an extension. Sounds good? Yeah. And I think I think as shares, we would encourage you that we don't want to drag this on for for months months months a So what we really if you can focus in on what we really need in the core, that would be great. Thanks. Yep. So, now influence, the topic that I can so that's a second so, go over the same draft kind of a previous draft if they didn't get enough, attention from the group and got expired. Near, gave me very useful comments and some notes on on on the syntax. So I submitted a new draft before IETF 119 kind of again, that's a recap of what we've discussed there is a need for a more advanced footprint capabilities going forward also, I think that's the tool we'd like to build and use for any form of sub CDM slicing. So things like do something within CDN, but only, in some region and geography be able to triggers application policy which is not using footprints. So my thinking is that footprints would become this tool tool that everything would rely on kind of for any sub CDN functionality, yeah, I'm not gonna cover all these use cases been presented kind of why we need that. Just wanna talk about what's new in the in in the draft. And what's actually lacking what what needs to happen"
  },
  {
    "startTime": "01:06:00",
    "text": "the next next five, please. So, in the scope, again, we are gonna introducing, your key footprints so you can create kind of footprints and sub footprints and so on. And allow clients to cash for print objects. So there's something that UCB and can actually cash footprint definition, then kind of, basically become a separate separate and addressable or referenceable object that's separate from capabilities, because FCI is capabilities and footprints, but footprints isn't actually today's secondary citizen. So that's what changed out. And add, 2 the 2 types one will be just a footprint that's named So kind of use a script when that defined elsewhere. That that's a proposal. And also use of, metadata expression language for equipment definition will be expression footprint. We can help define more complex expressions, with more bullions, a bullion expressionist and so on. That was an old initial for a draft. There was no change, really. What's new in the current draft when they update? Next slide? So, there's synthetics fixes, I mean, near point of some some kind of, obvious errors that's, like, use of arrays was there in the text but not modern in the examples. Additional thing that that, happened there that it's as a new object that we're proposing is footprint source, to be better able to be able to better define the definition of hope and where it comes from. When you say when you say, certain geography, according to who different, providers in the market, for providing of geolocation information so you can actually reference that and provide that in a structured way, after that today, we're kind of implying that when you say US, it's everybody's reason reality, there are different definitions of what geography could be"
  },
  {
    "startTime": "01:08:00",
    "text": "or even soft though, So it's important to indicate kind of what what is the reference reference, sorts. And, new thing that came out of this as support for, something called self published gale gale feeds. A real problem that exists in the market is that kind of access provider we'll see its own information, reported incorrectly and we'll want to actually correct that and say, well, these IP addresses are really in kind of in, are in US kind of in up kind of maybe Max Mine doesn't think so, but there's correct information. So its ability, with this will be an option for DCDM that happens to be an access provider to also self published your information, refer that in the footprint. So kind of it's it's footprint is this geography according to this definition that I also happen to have published also happens to be extender for that. 8805 so, I think that's kind of very straightforward use case, for this. So if that's a change, again, 90% over text is saying I think there was a discussion about, a pushback on taking a year ago about this being maybe kind of, similar to Alto. This next slide and the last one just very quickly, the footprints are not out they they kind of not it's not the one the same. Alto is is a way to address and access providers to to to publish their own network topology. Footprints are a way for DCDN to indicate which geographies does it cover, and kind of and has, has coverage for and So they are really, complimentary. So I don't think that I think it is something that is within CDNI scope. But happy to have more discussion. We're gonna have more time on the list."
  },
  {
    "startTime": "01:10:00",
    "text": "Again, asking for kind of second time asking for working group adoption speak as pretty fundamental tool that actually a diff different, additional work, in our space relies on having this. So this will be asked. And that's it. Alright. Thank you. Thank you, Alan. And I think we're gonna move on to you, Glenn, you've got a whole set of slides here, Let me bring that up. Yeah. slide. Please go right to the next Yeah. So several of my co authors neglected to register in time ATF 119. for I actually almost oops on that registered yesterday and got in. So while I'm sort of the editor and, shepherdor of of of this pile of work. I can't necessarily speak to all of it. So, some questions may have to defer to the, list. And we're not gonna go into too much detail on all of these. Basically, down. This work here represents a set of a configuration metadata, you know, generally extensions to RFC 806 that we've been working on for several years now within S BTA. This whole pile of work, is about to be published through SBTA, it will be called configuration interface 2.0. We've broken it up into pieces here to move through CDNI. And now all the pieces are in place see the new ones here, for for this IITF. So we'll go through 1 by 1 and talk about of these, but 4 of them are brand new, the ones labeled, new individual drafts. So we'll kinda go through these one by one. So next slide, Great. The metadata expression language, just one minor change since"
  },
  {
    "startTime": "01:12:04",
    "text": "the last draft, just a clarification of some wording. This still needs, review and feedback from CDNI working group members like the entire set. It's all been reviewed many times an SVTA, but we certainly want to get some non SVTA eyeballs on there. And I think based on a follow-up we had with Sanjay and Kevin, within the last few months, it was determined this in fact, can and should be within CDNI and can stay here and fact, it's not only foundationable for many of the other drafts in this family of documents, will be used, I think, Alan had mentioned in the name footprint. He has some desires to use it, I think, within the logging and face. We wanna use it as well. So we wanna move to adopt this as a working group draft. And get some eyeballs on it. Next. Okay. Process sync stage as metadata, Since the last draft, the same thing came up With, Sanjay and Kevin, you know, that we're requesting whether or not this can or should stay within CDNI, and we determine determine that should. On the next slide, I'll get to into a second. We did some updates on a diagram Sanjay and Kevin had requested. Since the last draft, really, a series of minor changes, clarifications that I've it out here, and this is all it was posted within the working group. Documentation as well. I won't get too much into that, but Like, all the others, we'd like to get some non VTA eyeballs on this and also move to adopt it as a working group draft there is one more slide here. Let's go to the next slide. This is worth talking about. Yeah. So, Sanjay, we discussed this within the SVTA, and I think at what you guys had said that the box in the middle should be labeled TCDN for transit CDN."
  },
  {
    "startTime": "01:14:03",
    "text": "That's not really the spirit of what this thing is doing really in CDNI vocabulary. Client really is, in this case, typically, like a video player. It really is the client. The downstream CDN is this guy in the middle doing the edge caching and then the upstream CDN what we call source origin is really the function of an upstream CDN just as source is defined already in 8006. So we just kind of added that clarification in here. Is that, Kevin and and Sanjay, if you wanted to comment on that. Is that meet your requirement, Yes. Yes. I think it it's it's fine as to how you described it. And in spirit of, you know, the separation as you have here, client is just a video player. So I think that the the important point here was that in order for this to be really, part of the CDNI specification wanted to make sure that there's indeed, you know, interconnections between the DCDN and UCDN, and that some processing is is doing, which exactly, you know, falls into the interconnection bucket There was a The Yeah. The original version of diagram did not have DCDN and UCDN labels on there. And so I think this should help clarify it in the CDN icon text. I agree. I think that's okay. I I do think that more generically, if could still be a TCDN. It could be a transit CDN, and you could still apply transforms, and and it would work. Right? But Yes. True. True. For though. the purposes of the this discussion, I think that's fine, Thanks. Good. Okay. So, and so like the others we wanna move, this one we feel now is ready to move working group Adoption, Next. 1. Okay. This has received thorough review, really, nothing has changed."
  },
  {
    "startTime": "01:16:02",
    "text": "Oh, just a, again, a minor clarification since the last draft, nothing's of a significance. This one, we would like to get moved to working group last call. I think And the intro to all of the Sanjay, this was one of the ones on your list also, to go to last so that we're in agreement on that. Nothing new. Nothing new. We like that. Next. Okay. This is a big new draft Pankaj Chandra from Hulu Disney, submitted this. I did some of the on it. I'm a co author. Pankaj could join us, so I'll sort of speak to this a little bit. This basically takes, source metadata from 806 and greatly extends it with a whole suite of additions that is easier for me to speak to on the next slide. So let's go to that. I can talk to some of these additions. Good. So we have 2 data models here. We're showing the purple one is the relatively simple, data model that was in RFC 806. We have source metadata. Which is just a list of sources. And then each source very simply just had a list of endpoints you know, to to reach upstream and grab a source from an authentication method, and a protocol. That was it. That was it. What we've now, created the source metadata extended which has all the same properties that source had. But it's got many, many additions. There's additions to do a load balance here in the lower left corner of the slide. Where you can specify a load balancing algorithm to balance between multiple origins. There's a mechanism what we call source detention. Where you can do health checks and put sick origin endpoints into detention for a period of time where you don't hit them again until they they measure healthy. And there are several extensions in here beyond the basic metadata that allow you to specify things that we see typically in commercial CDN configurations."
  },
  {
    "startTime": "01:18:00",
    "text": "Origin host names, web routes, indications of whether redirect should be followed, a whole suite of source connection control algorithms and Misource connection control in and of itself has lots of settings about, you know, how to deal with time outs and fail and failures from from sources So this one, again, has got a densive review and feedback with an SVTA, but we very, very much would like, to get non SVTA eyeballs on this one. Next. Client access control. This is another new, draft all So, Pankaj from Hulu was the the main author, several other of us contributed on this. This one does. Let me just look at my notes. Give me a second here. Yeah. This has got some extended rules from our c 806, for example, had location Acl and time window, Acl. We've extended those here to allow you to use some of the rich capabilities with specified in the processing stages where if you deny, for example, for an apple, you can now specify a synthetic response to come back. Very common CDN use case. So There's some minor extensions there. There is some, ability to specify what certificates and encryption levels are required for for a client to access the content. And the thing I wanted to just talk about here real quick is the new protocol types. So RFC 8006 called for 2 protocol types, HTTP 11, and HTTPS 1.1. It was a happy world when all of that was all that existed. We now need, you know, ways to talk about H2 and H3. So because there was no real standard on people have already taken it upon themselves, to use a few different naming conventions. So what we put in here in the"
  },
  {
    "startTime": "01:20:05",
    "text": "request for, Eiana entry zone. By the way, since we posted this up, Kevin, I'm sure you're familiar with this. It was new to me. In a the Anna Operations manager very quickly hopped on some wording they didn't like. And how we propose these new registry entries, and they just suggested some minor wording changes, and so we I went ahead and did that resubmitted the draft. I yesterday, I don't know if we wanna talk about this now. If there's any comments on this, H2, we're we're we're requesting that it can be referred to as httptwohttps/ 2 or simply H2. Same for h 3. HTTP slash h 3 doesn't really make any sense. Attention to BS only. So we've put this in as part of the document requesting these protocol 5s any comments or questions on that, or does that seem reasonable I just wanted to get it out there that we're doing the I think it seems reasonable. Good. That's easy. Then just one other comment, in the client access control metadata, We have defined, tentatively, but we've set it aside for now. AMI object to configure the cap, the common access token that Chris Lemons is helping drive through in in CTA wave. And when that Dock. It's a little further along in CTA waves. Certainly when it gets a dock number over there, we'll go ahead and add that object in to a future draft of client access control metadata so we'd like, you know, we'd like this to be accepted as a working group as well. This brand new draft. Next, edge control metadata, Love it. 0 updates since IETF 118. This thing has been sitting around for a while. I think I may have don't think I did resubmit but I'm not sure. I"
  },
  {
    "startTime": "01:22:02",
    "text": "although it may expire at some point. This will also it up top. To be moved to working group last call. This thing's been pretty stable for a while. Did receive, thorough review and feedback from Kevin a while back. Next, Okay. Another new draft. This one is a pretty simple one, there's just a few MI objects in here, describing some rules from of date of delivery. Among other things, it does dip into the world of open cashing. And allows you to set some preferences for request routing. There's an object that specifies whether you'd prefer, request routing to be DNS based or HTTP based, for example, some rules on how you select open caching nodes, sort of a grab bag of a couple of delivery really a pretty small draft, not a lot there. But we moved for that to be accepted as a working group draft. Next. We're almost done. K. Private features metadata. This is a new draft as well. Articulating a mechanism that we've come up within the SVTA, for, DCDNs to advertise private features that they provide. And for an stream CDN to configure those private features Of course, one can just start making up MI objects. All they want in a new generic meta objects, objects is just a mechanism to put a little bit of structure on that. Go next. Yeah. Unfortunately, I skipped a slide here by accident, but here's an example of a generic metadata on the left here type, my private feature list, and then I have a list of private feature and then we have a structured to find. In this example, Broadpeak is the provider of this private feature. They have a, you know, within their CDM, they have a feature called S 4 streaming, and these are the various parameters that S4 Streaming needs."
  },
  {
    "startTime": "01:24:00",
    "text": "Footprint it runs in, some various, you know, property specific to S4 Streaming, there's an FCI object that goes with this as well that allows downstream CDN to advertise what type of private features they support. And there's an alternate naming convention here where you can define the private feature, right there as a generic as a full MII which allows, allows you to get around some problems in it core structure of RFC 806 where you might have an heritance and override, and you don't want you may need to call out the private features individually as opposed to a private feature that they don't get overridden by that by a path match overriding host match for example, to it's a subtlety, but just wanted to get that in there. That's and then there's one final one in this family of drafts This one was authored by Ben. This is the protected secrets, and then you want to turn over to even speak to this one. Been still with us? draft Yeah. Sure. This this the only changes were, I think a couple of spelling errors, the minor language changes and adding sequence diagrams for all of those, workflow steps that big section at the end of the doc when when Yeah. And this we're not really saying this is ready yet for last call, though. Right? You still want some review on that? No. Definitely. It needs some feedback. Because I'm not I'm not sure anyone outside of SPTA has reviewed this draft debt. So I'd like to, you know, get at least some feedback before we We ask for, last call for sure. Yeah. So, Sanjay, you wanna just go back up to the very first slide. We can see the list of all of these to kind of put it all in context Yeah. That one. Yeah. But one one more. One after that. There you go. Yeah. So like I said, this oh, po. Kurant to"
  },
  {
    "startTime": "01:26:03",
    "text": "There you go. Yeah. So you know, this should really be the end of us tossing on a new docs to the file for the for new MI objects. You know, of course, I'm sure they'll minor additions and changes, but this is now a pretty stable set of work that we wanna finalize. There may be other things we put in on some related things around the APIs, but but for now, we wanna kinda really just put put a bow around this and get finished up. So we need eyeballs, on all of these. And, you know, please put comments in the, in the working group distribution. That's it. Any comments? Oh, we got Kevin. Yeah. I had a question about the private features 1. Is this just To It it's creating a generic generic metadata object that you can put anything Is that Sort of. Yeah. And then I I I wish, out Arnon was on to to speak to it better because he came up with a lot of this It is a way to have some structure around it, though. And with an FCI object to be able to declare that you can, support it. But, yeah, essentially, it is a generic generic metadata object. I'm I have concerns with that. I haven't read the draft. So so I can't articulate it well, but it just seems like I'm not sure why we wouldn't just make generic metadata objects for you know, the specific things, but And you could do that. This doesn't preclude you from just Making up MI objects as you as you as you wish This just puts a little bit of structure on them because they had it's almost like a registry of them in a way, but I I hear what you're saying. But we already have a registry. And and the registry doesn't require you to create an RFC requires should have a spec. Right. Non IETF specs can be registered in in the IANA registry. So Yeah. Yeah. I I think it's worth you challenging this a little bit in the"
  },
  {
    "startTime": "01:28:02",
    "text": "on the mailing list. So if we read the draft first, but yeah. Right. I mean, the other three didn't make sense to me. Yes. We should support HTTP 2. We should support HTTP 3. You know, that sort of thing. And and specific metadata object for open cashing Certainly, that's the whole point is so that you can create your own just don't know why we need to go that extra step for the private futures, but Yeah. No. It's it's a good point. Yep. Okay. Thanks, Sanjay. I'll let Francheska go, and then I'll come back. Hello, Francesca Raparomini. As AD. I just wanted to be careful of the charter scope that is Like, I see that there is a lot of new individual drafts that are being brought to the working group, and I I understand that according to the authors, these are small, Necessary updates to existing work so that they should definitely fit in the charter. But our charter is pretty clear on the list of deliverables, So I We should probably consider that if there is that's maybe some of these work might need the rechartering to, like, to, like, to, like, to, like, to, like, to, like, make sure that it fits. So just, I'm not saying that it definitely does. But we might want to consider that for the new work we've had discussions with the chairs about, the, the first two drafts that were updates from IPF 118. And, I checked the other individual drugs, as Sanjay, as Sanjay said, that that fits in the charter as well, the the logging one. Logging extension. But, yeah, we should have that conversation before definitely going for adoption. Yeah. I'm,"
  },
  {
    "startTime": "01:30:03",
    "text": "I agree. In fact, you know, I was kinda going in that direction. So so thanks Francesca for bringing that up. Flow, I think Looking at the list here, I have couple of you know, thoughts, one, exactly what, Francesca said So maybe as as chairs, We, take up the responsibility to reviewing these drafts to make an from the point of view that Are they within the scope of the charter? Or does any of the work requires rechartering and what that would be. I think that's something we can take The The second part is that if if let's assume that we determine that all of these are within the scope. So that's fine. Now, What? What chairs would like to see is actually, review done by others. So it would be great, really literally if blend, if there was another column here, and you had a name of a review assigned to that document who will do a thorough review of the document, and I think that will really help speed up the process and and not the chairs become the bottleneck of reviewing. We will certainly review it but I think We also need, given just the the sheer number of documents we have, it'll be really great to have at least, an official name assigned to it that would be responsible for at least giving a review and others, of course, need to review as well. And we wanna also know, you know, who is implementing it and where that is getting implemented. So I think that would be really helpful from from within the CDNI perspective, within IT of that, yes, you know, their support in in the IETF community. There are members that are participating in the working group actually going to be implementing it. And I and I assume the the qualification at least one qualification for being a reviewer is you're not one of the authors. Is that fair to say Yes. Yes. It's it's kind of like picking a shepherd and and we traditionally have done pre shepherd reviews and and"
  },
  {
    "startTime": "01:32:02",
    "text": "which have been an in-depth review. And so far, it's been shares doing that. But I think we we need to expand that in order to be able to scale And, you know, So a a shepherd can be anyone. A shepherd just has to not be an author and, you know, have the requisite technical See, now I would have turned to Chris Lemons to some of this for us, but is he now disqualified because he's a chair? Not necessarily. I mean, no. No. Just can't be only. I've agreed to do some of this. Okay. Yeah. So we can go ahead and and do this and on the maybe on the on the distro list, I can kinda list these all out and start getting some consensus on, should do this, particularly, we're talking really for the 4 new drafts, right, because the other ones are really pretty down the road. I I think, actually, the the existing ones, the cash control, the edge control, you know, those are pretty close. We're going to need to pick out separates for those anyway if we're going to do a last call. So we should address those first, but, yes, for the new drafts, I think we definitely should get more eyes on. I don't think people have necessarily had a chance to to review them yet. I know I haven't. But, hopefully, we can see where that goes. I just wanted to address Francesca's comment, yes, we should have a discussion as chairs on what our charter is. I think in hindsight, it makes sense that we didn't think about HTB 2 and HD if you didn't exist, it makes sense for us to support it, but but we should discuss whether that requires recharging. Yeah. From from the description of the documents, it really felt like, oh, this is really more maintenance of the current document rather than new deliverables. And and we Yeah. We might 1, 2, have as more rechartering to cover that. More clear. That shouldn't be contentious, I think. Right? And The the other ones, Yeah. We can discuss A glen, you're muted if you're speaking."
  },
  {
    "startTime": "01:34:03",
    "text": "Thanks. Yeah. A lot of what we have here are, you know, definitions of families of MI objects that you know, could have been piled on into 8006, but don't need to be 8006. And so we originally actually called this extensions to 8006, but then Kevin pointed out, no, we're really not extending it. We're just making new objects that are that fit into that oh, definition already. So I think it it fits in naturally as follow ons to 8006 but not really extensions to it or modifications. Know if that's helpful. 806 was designed to be extensible so that you could create new metadata objects I think, is But Francesca correctly points out we as chair should just you know, go through the due diligence process and make sure that We are. Making sure that we are fitting within our charger and recharging when necessary. Okay. So what's the word I wanna use for asking for somebody to review these? A shepherd. Is that the right Title, title, I I think as chairs, we will we we can assign Shepherd that is our duty. And and so we will for the We should we should all review I encourage everybody to go and review these documents and and post the list about it. You don't have to be a shepherd or an assigned reviewer, but we will begin assigning reviewers, to to help move things along. Yeah. Yeah. I think particularly on these new drafts, we really need auto eyeballs on those. Alright. That's it for me. Thank you. Alright. I think so. That really was the last set of slides and, Glenn, you did, you know, speed up pretty well. So I think that's good. And I'm just making sure that we're not forgetting anybody Looks like we have covered all the topics. So I think In terms of the presentations, we've have completed everything, but,"
  },
  {
    "startTime": "01:36:00",
    "text": "anything anybody has in the room that they wanna comment on? Anybody in the remote anybody that's remote wants to comment And if not, then we conclude the meeting. Alright. Thanks, everybody. And, 120. Safe travels all. Thank you. Thanks. Alright. Yes. Play that's"
  }
]
