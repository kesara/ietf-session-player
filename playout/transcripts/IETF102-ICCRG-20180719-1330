[
  {
    "startTime": "00:01:49",
    "text": "alright folks let\u0027s get started can somebody get the doors in the back so this is welcome to the ICC RG session in ITF one or two if ICC Rd is not the meeting you wanna be for then you may still want to be in the room it\u0027s fun I promise you very quickly the blue sheets are out please put your name down note well I\u0027m assuming everybody knows what the note fellas I\u0027m not going to read through this but if you haven\u0027t you should definitely read this before you participate before you open your mouth here we have several presentations today and it\u0027s it\u0027s kind of a it\u0027s a full agenda so I\u0027m going to try and hold everybody to their times a lot at times please bear that in mind as you\u0027re presenting I\u0027m going to be very tight on the presentation times before we get going though I need two volunteers as usual job escribe I need a volunteer Thank You Maria minutes Thank You Stuart excellent we can move forward so we have many exciting things on the agenda today I believe all the speakers are in the room not all of them but we have you\u0027re gonna start off with we\u0027re going to start off it not slow start Bob and Hakeem are going to present based chirping as a new way of doing startup Roland bless is going to talk about policy-oriented aqm steering Neil and Ian are going to talk about VBR first an update and then maybe a startup behavior and then ilpo is going to give us a brief overview of fasts or and an congestion control that may have relevance this is actually it\u0027s not yet a working group draft but it\u0027s something that\u0027s happening in core that info wanted us to want it two weeks to share with the ICC our G community because it basically is about congestion control or at least has intersections with it and then if we have time Brian will have the floor to talk about how to make things better and how to kill some other things but that\u0027s let\u0027s get started any any questions before I actually get started any questions on the agenda wonderful okay take it away Bob let me get this yep both Bob use this in "
  },
  {
    "startTime": "00:05:03",
    "text": "right right come in yep hi my name is yogge mitten I completed my master\u0027s degree at the University of also in June and my thesis was was about paste chirping is it possible to adjust his mic because that\u0027s okay right just um I think everyone in this room knows that there\u0027s this dilemma the more you try and accelerate the flow at the start the greater the over see you get shoot you get one round-trip time before you discover that you got it yeah that\u0027s sort of the received wisdom of slow start in TCP and not just TCP as a general dilemma for any flow trying to start in the internet without explicit signaling which is a feature of the Internet not a bug the lack of explicit signaling that is not not the UM overshoot so it you can sense you\u0027re getting delay and stop earlier and that\u0027s what something like every slow start does but then you get slower convergence and so what we\u0027re hoping to present today is a chirping which escapes this dilemma and I think that photomontage many years ago and it\u0027s actually extremely applicable to this talk where where you\u0027re trying to accelerate up across the length of a street ball you\u0027re screwing up everyone else and you\u0027re also going to hit the end I didn\u0027t hear the question okay okay so we\u0027re gonna start with some results that your team did just to give you an incentive to listen to the rest of the talk these flow completion time results on the left and cue delay results CD f\u0027s of cue delay on the right cumulative distribution functions and we try and quickly explain what we access on the left ear the left plots first you\u0027ve got flow completion time on on the left and sorry on the vertical axis and file size on a log scale in kilobytes on the bottom and the three levels of plot the intensity which I can find the red button there we go intensity that 1000 there means 1000 milliseconds average between each flow "
  },
  {
    "startTime": "00:08:03",
    "text": "start so it\u0027s getting more intense as you go down this is 100 milliseconds but between each flow star you know you know it\u0027s a practiced UV crater distributed file size and exponentially distributed time between the flows I think was out 4.9 distribution awkward fuck okay right oh yeah cuz that\u0027s right yeah we we found a paper that uncharacterized current internet since HTTP to flow sizes so we try to match the distribution to that alright so the oranges if you can\u0027t read this is with pace chirping and the greenish thing is data center TCP which has a very good low queuing delay this is the CDF of the queuing game delay in the same experiment and the final thing I need to explain is the difference between the two columns in each experiment one with no background flow and ones with one background flow just a infinite running flow and so probably more realistically you\u0027re dealing with the zero but sometimes you\u0027ve got a long download you\u0027ll be dealing with the one so you can see paced chirping gives you significantly faster flow completion time the green line by the way is the sort of ideal the maximum or the minimum flow completion time you you have you other gear to have that bit right so we\u0027re getting pretty close to it it\u0027s data density to be is um pretty far off so next oh oh I know I\u0027ve covered everything there so before we get into how it works I just wanted to talk a bit about applicability because because I\u0027m at the mic people probably assume this is about ACN right and it isn\u0027t we\u0027re just using delay based and that was because for a flow start you don\u0027t actually know what the capability of the bottleneck is you don\u0027t know whether it\u0027s got ACN and until it\u0027s too late oh and this is the same with any that there are there are myriad of different attempts to solve this problem with some clever new signaling from from the bottleneck but the trouble is on the internet are you going to hit that bottleneck or are you going to hit just a dumb bottleneck tailed rock you know so we wanted to just stick with something that was common to all queues but of course it\u0027s still not applicable to places "
  },
  {
    "startTime": "00:11:04",
    "text": "right but we this is research so having said that because we\u0027re getting a very low additional queuing delay where we\u0027re adding about 2 or 3 milliseconds typically on a 20 millisecond path as a footnote that says exactly what that means but that means we\u0027re most interested in cases where our bottleneck is already getting us a low queuing delay and so that\u0027s particularly interesting in something like a data center with data center TCP holding the queuing delay low to start with because there\u0027s not much point making sure your additional queuing delay is low when you\u0027re general queuing delay is high right and the other case is with with the f-4s your queue coupled aqm with the similar data center CTP like traffic over the public internet so in other words where you\u0027re you\u0027re isolated from TTP\u0027s general high queuing delay and that\u0027s there the environments we\u0027re more interested in and the that\u0027s what I refused these data center TCP in these experiments and things but with open source the code because we realized that this is now so it does seem to be applicable to the general internet because it\u0027s only the lay-by so if you wanted to try it in in your own environment you could but you won\u0027t see that in this particular talk right I guess the other point about applicability that needs to be made is that when you\u0027re getting the additional queuing delay very low it does mean that you can have shallow buffered switches without getting lost and I prefer to say it that way rather than if you\u0027ve got shallow buffered switches you won\u0027t get lost because I would have thought you put in the sizes of buffer you need to rather than be all the way around that so I think summary of this whole side would be general applicability now we come to the caveat as I said this is research that\u0027s why it\u0027s in IC crg but the reason we\u0027re coming to IC c RG is that i think it prompts it\u0027s very tempting it\u0027s got very good results and so it prompts some discussion on what do we need to do to make this happen it\u0027s certainly not production-ready we\u0027ve only focused on proving the concept at this stage over the period of yokum\u0027s master\u0027s thesis which was a one-year project and there are many issues they listed at the end but the two main ones just to save you come to the mic and pointing them out to us is the late acts because when we\u0027re we\u0027re "
  },
  {
    "startTime": "00:14:07",
    "text": "measuring differences in t\u0027lie for each packet delay backs mess us up so we try and for these experiments we\u0027ve just turned them off but obviously on the Internet you\u0027d have to coordinate with the other end somehow to do that and the other one is bursty Max and bursty schedulers which we haven\u0027t dealt where we\u0027ve only dealt with simple ethernet type links at this stage we have tried it over Wi-Fi and over the general internet and it seems to work but we haven\u0027t done any formal experiments with these sort of max all right so this stage on the pass over to yoki mean he\u0027ll tell you right work first I will add just a mic that\u0027s a pretty good height yeah so I introduced myself earlier and now now I will try to explain how page chirping works and it\u0027s it\u0027s really simple but it might take some time to understand so the basis for page chirping is package chirps and here we see a plot of the inter package grades of the chirp so a chirp is basically a sequence of packets sent at an increasing rate so in this example we have a 16 packet chirp and there\u0027s a geometry which it which is a variable that controls the span of the rates in the chirp and the goal with these package herbs is to continually pulse the queue by a few packets and then relax and then use the behavior and the information in guests from the packets and the acts to measure the available capacity of the bottleneck and with a chirpy can actually measure both the available capacity and the maximum capacity and the ultimate goal of this is to maximize the ratio of capacity information to harm in and in this case this is queuing the line so you want to get as much information with as little queuing delay as possible so if you have any questions if I\u0027m not being clear please come up to the mic and ask me so that was the basis and this shows how the chirps interact with the bottleneck "
  },
  {
    "startTime": "00:17:08",
    "text": "and how the sender and the receiver observe the packets in the chirps so in this case we have multiple chirps sent after one another and from the sender side you can see that the chirps has has increasing an increasing rate but once they traverse the bottleneck and arrive at the receiver and the rate is limited by the ball tonight so you can use this to to measure the available capacity at the bottom link and this is this is not exactly how to do it in in paste sharing this just demonstrates how we can use chirps and how they interact with the bottleneck so let\u0027s go to page chirping but all the chirps together so the basic idea is that we have multiple chirps distributed over a round-trip and we will maintain a herd rate or average gap using a WMA so that\u0027s basically a WMA of each measurement from each chirp and it chirps gives you some information but that information is very noisy so you have to have multiple chirps to actually catch useful information so the first three rounds consists of two chirps or bursts and then in the second round yet instead a marketing packet and you have another two - chirps and in the third round you have another two chirps and from the fourth round the algorithm runs itself and I\u0027ll come back to the number of chirps the next slide one thing to note here is that the queuing delay caused by the chirps is solely dependent on chirp geometry and I also want to note here that the chirp length reduction in the plot is due to a or it demonstrates what happened when the estimated rate increases from one room to another so the growth in the number of chirps per our TT depends on the game marble so every RTT we increase the number of chirps with a gain variable and this can be used to accelerate either faster or slower so actually in the in the previous plots "
  },
  {
    "startTime": "00:20:09",
    "text": "when we went from the third to the fourth round we have a gain of T which meant that we doubled the number of chirps and this also shows that it\u0027s possible to have a gain of 2.5 which creates five sharps in so forth so the more stable the measurements are the higher the gain or you can have a high again the more confident you are in the measurements another thing about pastry ring is that we try to push in a little harder than what the Oh with cushion a little harder than the available capacity grows and there are two reasons for this the first one is to make other flows yield and in the future I\u0027m going to do more experiments we would like to make activity activity trigger links catalytics expand the capacity and again the queuing delay solely depends on the geometry not the gate we shift from page chirping to congestion avoidance and knocking when the chirps feel around so and and this is something we we will continue working on it\u0027s it\u0027s not perfect yet but this is what we currently do so this demonstrates a small short flow using page chirping where each vertical line is either chirp or burst the two first world lines are two bursts the y-axis shows the packet gap and the top one is the inter scent gap at this server and the bottom one is stating to receive gap at the client you can see that the inter packet gap at the client is bounded by the capacity so all the packets that are sent and faster than the bottleneck will get cute and this can be detected using the returning acts from the from the client so what I get using this approach you get fast virgins and lower queuing delay this plot shows the tribute and the queuing delay of four flows started after one another and if you look at the first flow it reaches the capacity with virtually no queuing delay the only queuing delay is from the chirps and as you can see there\u0027s almost no queuing delay the second third and fourth Oh has to push the other files back so naturally it creates more queuing delay but it\u0027s not a bad with final things of "
  },
  {
    "startTime": "00:23:13",
    "text": "queueing life oh right sorry about that so in the TripIt plot you have it\u0027s a 115 megabit link and the x-axis is actually in round 3 times and not in time so yeah only on the bottom bottom plot you have queue size in milliseconds it goes from 0 to 6 and on the x-axis you have Rodri times spin the other plots that\u0027s not clear please ask questions I miss Katie sorry just clarification question on these results from a simulation or examined or it\u0027s a real test bed so we have a physical test by the way of five machines Neil Cardwell Google clear clarification clarification question what\u0027s the you "
  },
  {
    "startTime": "00:51:14",
    "text": "it\u0027s a module that uses that right and I guess making that into timing wheels and all that it would be well possible that there\u0027s advice in the thesis on how to disable Colonel pacing rate calculation how to disable responding to see marks and things like that which was actually quite a difficult part of it because there are assumptions built into the kernel that you of how slow start works because it\u0027s been around for so long and that\u0027s it thank you so much Bob so I yeah I mean we\u0027re sorry on your first presentation that\u0027s let\u0027s give let\u0027s give it up for walking so if the remote participants can hear me can somebody well if the remote participants can hear me I know I don\u0027t know how to bring it back it was but then it went away and I have no idea how to bring it back right which is so hopefully they can hear me that\u0027s wonderful to know they probably know by now but we had a power outage here and we are back can we see the slides does anybody know if they can see the slides well not yet for the next but anyways hopefully you can see the slides if not hopefully we will be able to or they can see the sides wonderful so they can\u0027t see our faces which is probably a good thing in it anyways lovely okay moving on Roland sorry before you start rollin we might extend the session about ten months into the break I hope you understand and that happens and I\u0027ll still try to keep this tight but let\u0027s more yeah hi my name is Ron bless from Katy I\u0027m presenting policy-oriented a QM serum which is joint work with Mario Martino this was presented this year\u0027s networking ensues so the motivation is basically starting with aqm so we have newer acclaims like pi and also GSP may try to be parameterless and they work reasonably well over a wide range of correct situations though they have one crucial parameter remaining which we will call for now the target delay set points or color for example as typically default value or 5 milliseconds of the target Elaine and the achievable performance basically depends on the current traffic situation so and varies with a number of flows and also there are duties and so on potential outcome is um depending on the traffic situation is either you have unnecessarily large d-line with a fixed set point or underutilization and so I am steering tries to actually improve "
  },
  {
    "startTime": "00:54:16",
    "text": "that edge GM performance by providing an external control loop around the existing AGM so on the right hand side we have the mobile HQ M with its our control and then it interacts with the TCP congestion control and a QM steering is rather meant to be an external um control outside the the ATM so what it does is basically it gets current values like the observed delay or throughput or events like packet drops for example and then it calculates a new target set point a given specific policy which basically comprises utilization desired utilization and the maximum delay comes with it later so on the idea is basically to adjust their target delay set point of the ATM instead of doing some optimization inside the ATM so it\u0027s basically applicable to these different nature arms that I mentioned in the beginning so on what is the target in a set point so just to get in rough idea that the new ATMs are try to keep the queuing delay around the specific target and so we have this visualization of the bottleneck buffer or at the port like that for the amount of in-flight data is on the y-axis and usually if there\u0027s excess data so you\u0027re sending more data than your bottleneck will be able to process the pickles will be put into the buffer and so the HTM then observes queue length or sojourn time have you and this is basically here the target is a set point so that is exceeded the AKM will start dropping packets or marking using ECM for example so on an example for this a key M would be the global synchronization protection which was submitted as draft and there\u0027s a paper but it\u0027s not has been standardized so far it simply drops packets if the target delay set point is exceeded and so I then tries to dynamically find a suitable dropping rate so the basic idea of a kam stirring is now to just adjust this target set point so in this case here we have a situation where we have underutilization so if you have only a few flows ATMs usually lead to under utilization because there we have you have too much synchronization so to say or we have only one flow so the back office may be too high so the current you used "
  },
  {
    "startTime": "00:57:18",
    "text": "congestion controls mainly to under utilization in this case and so what I am doing now can do is basically find a better set point by increasing that set point to a higher value and then we basically get full throughput again and avoid the initialization on the other hand if we have lots of flows and we have goods lost this synchronization then maybe the set point is in too high so we can basically lower the set point and then it achieves the same same throughput at lower so coming back to the policies basically we have that trader of throughput where this delay with a given fixed set point and then you can choose either one so the idea of policy oriented APM steering is that we have then easy to grasp policies like you can you find lower utilization target and maximum are queuing delay that you\u0027re willing to accept optionally we have also defined our utilization targets so if you want to have ultra low delay meaning you\u0027re basically below 100% utilization you can also specify that and so you have meaningful parameters like how much delay am I willing to trade for higher throughput or which would I am not willing to trade delay anymore so ATM steering tries to find best inputs delay trade-off within these given policy bounds so the challenges that we were facing is basically when should the setpoint be assessed or adjusted and which well you should be setpoint be changed the third point I have no time unfortunately to talk about this how to achieve these I follow agencies basically we\u0027re using some kind of which your queue for that so challenge one when to assess or just the set point so hey I\u0027m steering observes basically be interplay between hm congestion control and on the left-hand side basically we have in this situation no bottleneck so when there\u0027s nothing to do on packet drops and in on the right hand side you know in a situation where the current queuing delay is way above the target and the aqm needs to do its work and so we have to wait until ATM steering will do anything so it has not converted yet and so we are waiting until it\u0027s more or less converged so if it\u0027s the actual delay is fluctuating around the target then we you think more or less we are in a situation where ATM steering then can do "
  },
  {
    "startTime": "01:00:18",
    "text": "its adjustment if necessary so how do we determine the new set point that we need basically different strategies for increase and decrease so decrease is much easier because we can basically calculate the amount that we can lower the target set point we need some smoothing everything and so on but that\u0027s basically very easy to do the increase is so raising the set point is much harder because it basically depends on the PDP or RT key which is not known by the AQ so we just use an iterative Lian mechanism or iterative mechanism in order to try to find the right set point yeah so we need some kind of probing we implemented that and evaluated that and a small test bed here so we used that GSP aqm because it\u0027s fairly easy to understand how it works and um yeah I\u0027ll put the aqm steering is control loop around that the implementation used the DPD k base switch which is our you\u0027re also the bottleneck we have a sender and the receiver we have several times ten gigabit per second in ten gigabit per second bottleneck link and a 50 millisecond round-trip time we compared basically the GS pas with lower utilization with an utilization target of lower utilization target of 99% and the maximum allowed delay or 30 milliseconds then we compare this through our coral and GSP with a fixed set point of 2.5 milliseconds and also with a small tete tete dure buffer the same size and a large data buffer of 30 milliseconds so I present just briefly two experiments um one is just showing for steady-state longleaf Louis it\u0027s working as expected so GSB is able to try find the right setting according to the policy while the regular ecom\u0027s or their performance depends on the traffic situation and for tear drop it basically depends on buffer size but then you have either high-throughput all or delay button on both at the same time and experimenter is is about the transition behavior so um you\u0027re in experiment one we have on the left hand side in the throughput and on the right hand side the queuing delay and we varied the number of flows present at the bottleneck and um yeah the leftmost bar here is the small tailor up offer on the "
  },
  {
    "startTime": "01:03:20",
    "text": "rightmost bar is the large teardrop buffer and then we have our coral and GSP and the dark green bar here is GS pas and as you can see with basically two flows GS pas achieves higher utilization than the static a QMS and needs to invest there a little bit of queuing delay here you know lower than 10 musics so we have a low number of flows and so we can only have low lost in synchronization so with nine flows your situation is better so the normal static a QMS perform also quite well and GS pas adjust also to a lower value here so that works is expected while the tear drop buffer still suffers from under utilization and 4:36 flows we have a high amount of lost in synchronization let us achieve and in that case GS pas or ATM steering can even lower the delay so I\u0027ll zoom in to that that can be seen a bit better so the other aesthetic a QMS have still there human delay around 2.5 million seconds y GS pas then lowest on the queuing delay ch1 music in this case um so when just to given a brief overview of the transition behaviors so we have a situation here where we changed the number of flows from 2 to 36 and then back to 2 and so we\u0027re starting with two flows our set point is around 11 milliseconds for the target set point and then as the number of flows increases we smoothly adjust and lower the target set point because we can achieve basically the the same throughput with lower queuing delay and then we have a situation where there\u0027s not much adjustment here then our so it\u0027s quite okay to keep that then once the 34 flow is disappear our cubic here basically needs some time in order to reclaim the free bandwidth and so on the aqm steering kicks in as soon as the aqm becomes active and in this case we see a stepwise increment here in order to find the right wing today which is thin or a target set points or which is then around 10 milliseconds in order to achieve the lower utilization goal right so um yeah this basically also concludes "
  },
  {
    "startTime": "01:06:20",
    "text": "my talk so I am steering tries to improve existing a QMS and tries to find the best rate of throughput for this delay under a given policy it\u0027s basically operating as external control loop around an existing eqm and we did some evaluation and physical ice be testbed are using several 10 gigabit per second and yeah there are much more details in the page so if you\u0027re interested or invited to read that and I\u0027m here for questions we have time for maybe a couple of questions just a couple of minutes Neil Cardwell Google do you have any evaluations with a FQ caudill instead of single queue Caudill no okay I would if if you have time at any point I would recommend using FQ coddle the the cutoff authors recommend against using single queue coddle and recommend FQ Connell as the preferred solution because of the you know ice the way it isolates different flows into different queues bonus will be interesting to know how it looks in that case thank you yeah we just want to show for pure wake um so it works and yeah thanks yeah I\u0027m Bob Brisco I\u0027m wondering whether there\u0027s something we can do inside the control of the aqm to limit the amount that the outer loop has to do for instance there\u0027s this this idea of having to have a fixed delay target you can work out you don\u0027t need to know the number of flows if you like that the higher that the congestion level is the you can make that the light at the later I get higher or more conversely the lower the condition level is you can make that the later at lower in other words it can be a function of P rather than being constant and that that feel free your number of flows it doesn\u0027t deal with your round-trip time variation but at least part of the aqm can be made yeah we actually um so our implementation was basically integrated into the aqueous or you can do that but the the general idea was to have and control which is on a much larger period of time instead of I mean otherwise you may get oscillations right in order so we want to wait until the ACM actually did its job right and so then we are observing on larger timescales I\u0027m doing what I\u0027m trying to argue is that a fixed delay target may be the wrong thing anyway for a queen to do its job okay given given the whole point of what you\u0027re trying to trying to do is at that something that I\u0027m just saying part of it maybe we can improve in the internal algorithm before we but they\u0027re "
  },
  {
    "startTime": "01:09:20",
    "text": "the round-trip time and ability dependence we probably can\u0027t but because we all know the round-trip time but the things like the number of flows is is represented by the congestion level so that you cannot support to do negative Thank You Roland Neil you\u0027re up next hi my name is Neil Cardwell and I\u0027m gonna give a quick update on recent research involving VBR at Google this is joint work with my colleagues at Google you Chung and Stephen and Sohail the quick PPR folks Ian and Jenna and Victor I guess Jana\u0027s not alumni at fastly actually and PR and use it I don\u0027t know possibly well alumni of the Google BB our team who\u0027s moved on to greener pastures this is a work also done with PR and music and Kevin and the TCP team and then our senior researchers Matt Mathis and Jacobson so I\u0027m just gonna start with a quick overview and status of bbr at Google and then try to spend the majority of my time on a update up talking about research goals and focus areas for our group talking about some design rationales for the current prototype v2 code and and shows some lab results to sort of illustrate the behavior of the current pre-release code that we\u0027re experimenting with so as far as PBR version 1 it\u0027s something we talked about at the ITF the last couple sessions it\u0027s used for TCP and quick on google.com and YouTube it\u0027s also used for internal traffic between Google\u0027s lands internally it\u0027s available for Linux TCP and quick as open source there\u0027s also active work at Netflix going on for a TCP VBR annotation for FreeBSD and then we\u0027ve written some drafts documenting the version one algorithm and talked about it at previous IDs and there\u0027s a little paper in the cesium with an overview so our current research is focused on a couple things first and foremost is improving coexistence and fairness with loss based congestion control such as we know in cubic in the main technique we\u0027re looking at as is adapting B be ours bandwidth probing time scale to for a better coexistence we\u0027re also looking at just generally reducing Q pressure "
  },
  {
    "startTime": "01:12:21",
    "text": "meaning packet loss and queuing delay and there were primarily looking at incorporating loss and ecn signals for two different mechanisms one is a new model for estimating a safe range for in-flight data and one is an estimator for exiting startup faster and then we\u0027re also looking at speeding up the convergence of the men ITT estimate by making our probes more frequent and reducing throughput variance by making our our TT probes less drastic so if I had to summarize the design principles that were trying to adhere to for the for the v2 design here\u0027s a here\u0027s a quick list I would say first we want to try to leave some Headroom for other traffic so that there are queue slots or pipe slots for entering flows to grab weather are intermittent traffic chore flows long flows we\u0027d like to let them come into the path and get some capacity without hopefully causing too much queue and delay or loss we\u0027d also like to react quickly when there is a an apparent congestion signal such as loss or Sen to adapt to the delivery process that we\u0027re seeing right now to try to maintain flow balance in cases where we might be congested another and I should say that I\u0027ve highlighted here in bold the principles that are new and v2 and the others are sort of carryovers from the v2 design so as in v1 we want to make sure we try not to overreact to loss and ecn signals we don\u0027t necessarily want to do a multiplicative decrease on every round-trip time where we get just even one of those signals a new principle we\u0027re putting into play here is to try to probe deferentially you might say we\u0027d like to do our bandwidth probing on a time scale that will hopefully allow decent coexistence with Reno and Kubik but we still want to continue to probe robustly you might say to make sure that we have a nice robust probe beyond at or beyond our existing maximum bandwidth or volume estimate before we sort of fall back to and cut our estimate of those parameters we also want to try to avoid overshooting so when we\u0027re doing our bandwidth probing we want to start at an inflight level that\u0027s previously been measured to be tolerable in terms of loss or ecn signals and finally we want to make sure that we continue to grow scale ibly so "
  },
  {
    "startTime": "01:15:21",
    "text": "here that a new aspect is that we want to try to start our probing gently with just one extra packet that as we can grow in fly apparently without seeing congestion we want to sort of accelerate and grow actually to use any free newly free capacity as efficiently and quickly as we can so you may have noticed that a sort of one thing that all of these new design principles have in mind have in common is that they essentially need an explicit and independent model for a safe range for in-flight data and in bbr version 2 right now the the model is sort of composed of three different parameters first we want to make sure that we try to mostly spend our the life of a PBR flow cruising at an in-flight operating point that mate tries to maintain flow balance and also leave Headroom for other flows and to do that we have a parameter that we call in-flight low which is essentially a conservative in fly bound that\u0027s based on any recent loss or easy on signals we may have received and then of course like any congestion control you eventually need to probe to see if you can get more capacity and so we want to periodically probe beyond the level of flow balance to probe to see if you can get a higher volume of in-flight data a higher bandwidth etcetera and to do that we have a parameter called in-flight high which is essentially a max volume of in-flight data that the flow had recently in flight before it started seeing signals of congestion such as loss or ECM and then finally while we\u0027re probing for bandwidth we want to do that in a sensible way so if if while we\u0027re probing we we don\u0027t trigger apparent laaser or ecn signals we\u0027d like to grow our probing rapidly so the in-flight probe parameter sort of controls the incremental amount of extra data we put in the network as we\u0027re probing to start small and grow over time so just a quick run-through of the typical life of a bbr flow and you know you\u0027ll notice that a lot of these states are that are similar or the same as VB our version one at a high level but some of the details of the mechanisms have changed and the new stuff I\u0027ve highlighted in bold here so so as in V 1 and V 2 a flow starts in a mode that we call startup which is very much like traditional slow start in the sense that the flow is going to be doubling at sending rate and in-flight each round-trip the new piece here is that we do try to set our "
  },
  {
    "startTime": "01:18:21",
    "text": "in-flight high to an estimated Max safe in volume of in-flight data if we do see one of two signals either the filtered loss rate current loss rate is too high or the ECN rate mark rate is too high based on configured parameter so I\u0027ll talk about later and we\u0027re going to exit that mode when either we set in fly high using those mechanisms or there\u0027s the existing mechanism from version 1 where if we see a plateau in the delivery rate from the bottleneck we estimate that we\u0027ve filled a pipe so after that just as in version 1 we enter a mode called drain where we essentially pays a lower sending rate to gradually drain any excess in flight that\u0027s beyond our estimated VDP until we\u0027ve hit our target in flight that\u0027s our estimated bdp and enough point we enter the what we call a probe bandwidth mode which has a couple different phases that sort of go in a cycle over and over and in v2 in probe bandwidth were essentially trying to cruise at an in-flight operating point that\u0027s respecting a couple of different constraints so number one we want to keep the in-flight right around the estimated bdp if we can and that\u0027s sort of our default mode and our starting point but number two as I said before one of our goals is to leave some Headroom for other traffic so if if we have seen some loss or ACN signal that gives us an estimate of a Mac you know an in-flight high estimate for our ceiling then essentially we um we calculate we want to leave 15% Headroom and we can talk about the parameter in the QA people are curious but so that we try to keep our in-flight 15% below in flight high and then third we we want to continuously adapt our in-flight as we see lost or ecn signals as we\u0027re cruising along keeping our in-flight below inflate low and that\u0027s a dot adapted downward on either loss in which case we respect packet conservation to try to maintain flow balance by decrementing the in-flight low by the number of packets lost essentially or if we see an EC on signal then we cut in flight as well and here the current thinking is is to do something like the exponentially weighted moving average of ECM mark rate which is similar to DC tcp the current "
  },
  {
    "startTime": "01:21:24",
    "text": "thinking is that we will probably want some sort of hysteresis so because we\u0027d like bbr to work well over a wide area networks and imagine that if a LAN flow has a an RTT of 100 milliseconds say in a healthy Network there will probably be some local area flows sharing that bottleneck that might be causing the bottleneck to go through a cycle of some ecn marks a while with no ACN marks and it could go through several of those cycles over a single round-trip time and in that case the LAN flow might see that every single round-trip time has ecn marks but I think in those cases we won\u0027t necessarily always want to slow down because there might be free capacity so I think there probably needs to be some sort of hysteresis we wanted this to work over Y areas so that\u0027s why there\u0027s that hysteresis term there so when the ACN mark Reeth is above that hysteresis we want to cut our in-flight in some fashion that\u0027s proportional to the mark rate similar to something like DC TCP okay so that\u0027s the sort of steady state or a phase in which a flow spends most of its time and then of course we need to probe for bandwidth periodically and also of course finally probe to see if we can put more packets in flight so the idea here is that when we\u0027re doing that probing you want to grow beyond in-flight high slowly at first and then increasingly rapidly if we seem to have more capacity so in this mode we set an in-flight target at each point in time that is our current in-flight highest net plus this imply probe parameter that\u0027s going to govern the acceleration of the probing and that imply probe is going to start small one packet and then increase exponentially over time until such point as we see one of the termination conditions so one of them is that if we see a loss rate that\u0027s too high of whatever configured taller tolerance for loss is settled upon ultimately right now we\u0027re using 1% packet loss rate if if we see a packet loss rate go above 1% where the filtered ecn rate is too high then we say ok this is our ceiling this is we\u0027re going to set in flight high to this and then we\u0027re going to terminate our probing phase and also if if the level another the mechanism that\u0027s carried over from v1 is if our employee gets to be far enough above our estimated BGP that we estimate that we\u0027re building Q that also causes us to terminate the probate just like in v1 yeah garna fires quick clarification question what means - hi EJ right so this is that "
  },
  {
    "startTime": "01:24:27",
    "text": "would be a parameter that would be to be determined but yeah and it\u0027s but yeah let\u0027s see next slide so and then finally once we once we\u0027ve terminated our probing we think we\u0027ve got we or typically have a bit more in flight than we would like so we spend some time in a phase that we called the down phase of the program with mode where we hold the pacing rate below the estimated bandwidth to drain that extra data out of the network and you\u0027ll probably notice that\u0027s very similar to the drain mode I mentioned and in fact currently in v1 the only thing that\u0027s different about those is the particular gain that\u0027s used for calculating the pacing rate so actually the current plan is to probably unify those and use the same gain in each case and I think Ian\u0027s going to talk a bit more about that actually all right so that\u0027s the lifetime that\u0027s the sort of life cycle of a bbr flow and just to give you a sense of how it behaves here\u0027s a quick little example so this example has 6 PBR flows in an emulated path with 100 and 100 megabit bandwidth 100 millisecond round-trip time and a buffer that\u0027s 5% of the bdp and it\u0027s sort of illustrating that you get some reasonable fairness convergence and a reasonably low we transmit rate and you can sort of see in the any stacked or cumulative rate graph there at the bottom the below the hundred megabit line there is a little bit of headroom there to the left there essentially fur to keep loss and human low and to allow room for entering new flows so the the next piece of the puzzle I think is interesting and worth talking about is there\u0027s a question of how to coexist with lost base congestion control and here the way we\u0027re sort of thinking about it now is that we want to have goal of ensuring that Reno and cubic can continue to work well in contexts where they do work well today and specifically we want to make sure that they continue to work well in intra data center or a land environments you know regular Ethernet spanning the range of values and use today and also of course the internet last mile and here we want to make sure that we support the common workloads where it\u0027s used today certainly up to and including say 25 megabit 4k video at common broadband Artie T\u0027s let\u0027s say 40 milliseconds for example and the challenge here if we "
  },
  {
    "startTime": "01:27:27",
    "text": "want to coexist with these algorithms is that Reno and Kubik need pretty long periods without packet loss in order to utilize high B dps and maintain those bandwidth levels so the strategy that we\u0027re taking is basically to as basically sort of a a two-part strategy so the first part of the strategy is to say mmm my primary goal is to estimate the bandwidth that\u0027s available to my flow and then once I\u0027ve done that then the second thing I want to make sure I\u0027m doing is to adapt the frequency at which I\u0027m probing within some reasonable bounds because any sort of probing that I do is may well cause packet loss right and we want to adapt the frequency at which we risk that packet loss so that we\u0027re allowing Reno and cubic flows that might be on the path to reach a bandwidth that is equal to ours so that\u0027s the basic high-level idea and how that\u0027s implemented is a sort of dual time scale model including a sort of reno pseudo emulation which is essentially the approach some very similar to the approach taken by cubic which has to solve a similar problem of reno coexistence right so so the the basic idea here is that there are two different time scales that we keep in mind one is the what you might call a BB our native time scale on which it would like to probe and we\u0027re experimenting with a couple different approaches here one is a sort of time range of two to five seconds as calculated as an increasing log like function of our bandwidth another approach we\u0027re taking is is a more randomized approach on the similar time window perhaps a little tighter say two to three seconds and then the the second time scale is a sort of reno coexistence or reno emulation time scale and the nice thing about reno is it\u0027s very simple in its behavior so you know it\u0027s got this sawtooth where you can sort of get a pretty decent estimate of the time scale that reno flow with your RTT would need to maintain your VDP and that\u0027s essentially just your bdp times your RTT so that gives you a time scale that you need to wait before probing again because if you want other Reno\u0027s that might be on your flow on your path to be able to achieve your bandwidth you just need to wait that long you do at least make sure you wait that long before probably or perhaps causing them packet loss but we do place a bound on that which is where the that cap comes in there because we can\u0027t wait forever for our Reno I mean if if it\u0027s a 10 gigabit 100 millisecond bdp then you\u0027re going to "
  },
  {
    "startTime": "01:30:29",
    "text": "be waiting an hour before you get to probe again and so that\u0027s why we place a bound on it which is you know a manual so I had which RTT is a particulate or a smooth round trip time or the last round trip time right so the the approach so I experimented with a couple different ones the one that seems to work best is literally just counting the Hacket time two round-trip times because of course there can be wide variation in round-trip times and when you start out for example Europe you\u0027re often at the end of a probing moment when there\u0027s a big queue and that can all go away because PBR is able to drain all of that so what it literally does is actually just count the number of round-trip time experienced round-trip times since it last probe which you know is essentially what Reno is going to be doing as well so that I think that works reasonably well um me yeah go on just very quickly you have five minutes okay all right so that we take the min of those two time frames and that\u0027s our time frame for probing and then to go along with this we we need to co.design the bandwidth estimator filters time scale and to work with this basically you just need to make sure you cover always have a bandwidth sample that covers bandwidth probing period to be robust and the simplest way to do that is just to have the window be literally the last two cycles which makes it very simple it\u0027s just two 32-bit integers done all right so what does that do so this is an example you can consult the details later on but basically the here\u0027s a reno flow and a bbr flow in this sort of broad banded scenario they reach a nice convergence with VB are taking about 48 percent of the bandwidth this is and having nice low we transmit rate and this is definitely an improvement from vb our version one which were taken 92 percent of the bandwidth and here we take a hair under 50% you can here\u0027s another example just illustrate there are two cubics to be PRS they reach a reasonably fair convergence with cubics taking a hair more than the BB are with low we transmit rate pretty stable and and again this is a big improvement over v1 so another thing that we\u0027re working on is exiting startup faster because you know I described startup earlier you\u0027re sort of doubling your rate you can have a lot of packets in flight so that means if once you start to cause loss than you you can cause any loss pace congestion control to back off quite a bit so you want to exit startup quickly both for reducing loss and for fairness so the v1 had this Plateau criteria the v2 prototype has both a loss based heuristic looking at whether the loss rate is both greater "
  },
  {
    "startTime": "01:33:30",
    "text": "than the configured ceiling we\u0027re using 1% now and also there are enough gaps in lost gaps and around that it looks like signal rather than noise and in and the ACN heuristic is basically just looking at the ACN mark rate and whether it exceeds some configurated threshold so I\u0027ll share some it lists some illustrations of how this how this looks so here\u0027s a single DVR flow going into one hundred minute megabit bottleneck and it\u0027s the starting up smoothly it\u0027s the bottleneck sees a clear signal exits after one round trip time it\u0027s spent that around trip time of losses in impacted conservation to not create any further losses by the end of the round-trip time it\u0027s pretty sure it\u0027s filled so it exits startup this is a couple this is an example of a third flow entering in a series of flows entering and again you can sort of see it once it ceases a what it looks like a strong signal then it goes ahead and exits here at 30 megabits which is pretty close to its fair share of a third of the bandwidth by the way so we\u0027ve got also got some improvements for probe RTT that we\u0027re working on because of this convergence can be slow because it probes only rarely only every 10 seconds and then there\u0027s a risk of high tail latency because it cuts in fly quite low and the approach we\u0027re taking as well to make the probing less drastic instead of cutting down to four packets say a cut to 75% of the estimated bdp and then second we want to make the probing more frequent say every 2.5 seconds so the this to give you a sense of how that behaves you can see over on the Left we\u0027ve got the b1 behavior where they these big drops every 10 seconds in b2 the current prototype basically has much more moderate drops every two and a half seconds from or stable behavior and when we\u0027re going against cubic that the behavior is an even more significant improvement you can see with the old the wrong behavior you can get some weird oscillation as it\u0027s settling in but with v2 it\u0027s much more stable so in conclusion PBR version one is deployed at cool and were actively at work on v2 focusing on responding to loss and ecn signals and on coexistence with loss based congestion control and we\u0027re always happy to see patches or hear about test results etc and they\u0027re always happy to collaborate with folks thank you Thank You Neil it\u0027s actually I have a thought let\u0027s hold off on questions Ian you\u0027re next with the startup why don\u0027t we do that and then if we have time we\u0027ll do questions together "
  },
  {
    "startTime": "01:36:40",
    "text": "all right imean sweat from Google I\u0027m only going to talk about VBR startup all the things I\u0027m going to talk about our applicable to the quick code and the experiments that I\u0027m going to talk about are all in the quick code if you want to look at how the code actually works and it\u0027s all open source and chromium which has a link that\u0027s the implementation link it\u0027s also really mostly complementary to what Neal is working on Neal\u0027s working on a lot of awesome stuff but the rest of the PBR team but I have a few other use cases that I\u0027m also kind of working on that are a little bit separate so I look at a lot of web flows in particular so the very short lived flows and a huge portion just never exit startup right you send 20 packets why would you have to startup or even worse you\u0027re just constantly a plummeted so maybe it is a long live flow but you\u0027re sending 24 you 50 packets at once and it\u0027s never enough to to get out of start-up flows on police connections can experience pretty brutal losses so if you hit a packet policer and you start seeing really nice bandwidth and then suddenly the police are kicks in the loss rates can easily exceed 50 percent for at least shorter periods of time this is not great for continuous delivery and it\u0027s generally ugly for lack of a better word and also there\u0027s another issue for web closes we\u0027ve seen a few cases where AK aggregation is so severe that we actually exit startup prematurely because we\u0027re not provisioning enough Sealand and the aggregation is kind of on a time scale that\u0027sthat\u0027s so long it\u0027s not quite as common but it does occasionally occur on the court system web proxy if we\u0027re quick and on the court side they\u0027re extremely sensitive to Bend with variation of any sort and one of the comments that they gave to us is our encoder can handle a bandwidth fluctuation of up to a factor of two and anything more than that is very painful so a lot of the things Neal is working on like making probar TT less severe you know are would be hugely helpful there but another issue we\u0027ll talk about later is like draining and other things so the first idea is just make it more sensitive to walls as Neil said it already reacts to loss in this case I kind of took a different approach from what Neil did I looked early on it trying to exit startup early to the loss like you know if you\u0027ve seen loss and it\u0027s been at least an RTT and you haven\u0027t seen a bandwidth increase or other heuristics like that and those just generally did not did not work I haven\u0027t actually tried Neil\u0027s proposal so I\u0027m curious to implement that as well but the thing that I have that has worked quite well both in traces and based on the kind of application metrics I have is reducing the pacing weight so the idea of just reducing the pacing rate as soon as you have seen a loss to 1.5 X instead of two point eight nine you know in a huge portion of "
  },
  {
    "startTime": "01:39:41",
    "text": "circumstances seems to mitigate the worst behavior there is a slight regression in certain edge cases I\u0027m still kind of trying to trace it down like literally by looking at traces and one source of the issue is definitely a spurious retransmits so if you accidentally declare loss and then realize are those dispraise we transmit you don\u0027t go back up to the high gain in the current code and that\u0027s partially due to the fact that bv r just doesn\u0027t know about spurious retransmits and the quick code so I\u0027m hoping that once I fix that this will be completely neutral from an application metric perspective saves over 5% retransmit rate globally and it\u0027ll be a nice improvement but so the other idea which is a little more recent I haven\u0027t tested out as much is the idea of reducing the congestion window gain when in start-up so normally PBR uses a congestion window gain of two during Pro bar 2 error sorry probe PW and the idea here is also used to in start-up the logic is based on the idea of read riving kind of what congestion when do you actually need in order to double every RTD and then theory it\u0027s only two now that\u0027s assuming a continuous delivery model which is not reality in any way shape or form and so there are some you know there\u0027s a proposal to try to deal with mitigation but ideally this would give you 40% less queuing delay when exiting startup which would be particularly helpful for applications like web RTC yeah so and Neil also recently read arrived the peace and gain in PBR and those are linked there to his derivation and this is fairly similar from quite a bit simpler yeah so to deal with the fact that we\u0027re assuming a continuous delivery model you know we might need to deal with the aggregation of acts which you know at the very least we\u0027re going to see two MSS acknowledged at once very often and we may very well see quite a bit more and so we don\u0027t want to slow our gain and slow our growth just because we\u0027re not providing enough congestion window early on and startup but on the other hand we don\u0027t want to provide a ton of extra congestion window and then just like completely run out oh so I forgot to whinny one point early which is my mistake PBR is designed to be bandwidth based and when it\u0027s been death based it\u0027s awesome and this is where like if you look at traces it works really well it tends to fail pretty severely any case it goes any time it goes into a situation where it\u0027s congestion window based because like just weird weird things happen that are awful for lack of a better description and start up an exiting startup is one of the cases where at least currently you are always congestion window based because by definition you\u0027re sending it over two times the actual measured bandwidth so one of two things is happening either "
  },
  {
    "startTime": "01:42:41",
    "text": "your bandwidth is continually increasing or eventually it tops out and when it tops out you build this queue and there\u0027s no way to avoid gulping this cube except for actually experiencing hospitals so you basically they\u0027re experiencing a lot of loss or build up a lot of queue those are the only would two ways you can exit startup and so that was one of the motivations of like if we can reach the ceiling gain that actually might be even more effective than reducing say of the pacing game in terms of limiting the damage when exiting startup so that was the thought process let\u0027s go back to a co-creation yeah so as I said the goal here really is to make sure that we have plenty of Sealand early on in Stara and we\u0027re not limiting by that but as we get closer and closer to the plateau we\u0027re getting closer and closer to having only a to Xu and gain and we don\u0027t have to drain any extra packets so the the proposal is I think Neil maybe in the the Singapore ITF and I have yes and I see crg there he talked about this excess act approach where you try to do a CAG regression compensation by measuring periods where you see a delivery rate that\u0027s higher than the max bandwidth you\u0027ve ever seen and of a short-term delivery rate and saying okay well sometimes I\u0027ve seen basically stretch acts or aggregated acts and you add that excess delivered to your congestion window and the idea here is also add that to your Kadesh in window but instead of in the general case in Pro BW we\u0027re actually putting a windowed filter on it so you don\u0027t remember you don\u0027t forget sorry what the access was from a few are two t\u0027s ago in this case the proposal is just to add it to the instantaneous congestion window once when you see it and that way it will expire very quickly because as bbr gets faster and faster the access act is going to naturally get smaller and smaller so your your actual bandwidth and and your short-term bandwidth are gonna get closer and closer though and so hopefully this kind of asymptotes to two or three packets and then you have have a nice stable exit and you don\u0027t experience a lot a lot of sapone exit the simulations look really good you both exit startup faster and you experience either West queuing delay or less loss depending on kind of how you configure your buffer but I don\u0027t have any experimental data yet so it\u0027s it\u0027s a promising direction that hopefully is complementary to some of the things Neil\u0027s working on the Watts proposal is to increase the drain pacing gain back to 0.75 part of the motivation for this is that by kind of reducing the overhang hopefully by reducing the ceiling game - - you know we can afford to take about the same amount of time and drain but drain a "
  },
  {
    "startTime": "01:45:41",
    "text": "little less severely as I said earlier in the talk the courts folks are what see folks particularly hit by this point three four pacing game that PBR is currently using during rain and so they kind of needed it increased for their application anyway I imagine web traffic doesn\u0027t you know love being drastically slowed down on their placing rate either so it may be a general purpose improvement as Neil alluded it actually also might allow us to unify two mechanisms that we have right now which is drain and drained a target during probe BW and so basically it might allow us to just enter the low gain phase of PW get rid of the drain phase entirely and kind of just meld those two things so that would be nice yeah so conclusion in real world Lois we low startup is a huge problem for both of Reno and cubic and nbb are real-time applications are even more sensitive to startup and I think there are a lot of good ways to improve bbr a startup and you know I\u0027m kind of hopeful but maybe there is some of them are applicable to other congestion controllers maybe you know PCC and other congestion colors that people are working on and if you want to experiment with any of these I\u0027ve listed the connection options that\u0027s kind of the standard quick way of running experiments you throw them in the what\u0027s the equivalent of transport grams but they\u0027re called connection options and in the Google quick version and you can turn these on and off so and yeah thanks for all the help and all the support from from the greater bbr team including yo and each other in everyone so and questions for myself for Neil thanks Ian you have a couple of minutes down in questions Neil do you wanna come on alright but where\u0027s God it\u0027s not really oh go for the groin yeah hey I have a few questions a couple of questions - Neil so in terms of so the draft reflects we won is there any plan to update the draft if like V - the second question is has we - been deployed in production and do you have any experimental results here apart from like the graphs you present right so v2 has not been deployed in production and I think once we have deployed v2 in production and have some real-world experience under our belt then I think we will go ahead and update the draft to catch up with v2 we definitely do want "
  },
  {
    "startTime": "01:48:42",
    "text": "to make sure that the draft reflects what\u0027s being used out there in the field yeah I have another follow-up question so in terms of being friendly to Reno and Kubik there was a paper called copán and SDI which has a mode to detect that there are other flows of you know which are buffer filling have you explored any of that each - maybe enter a more friendly more when you detect that there are other flows like these right yeah so that is an intriguing mechanism in the paper we haven\u0027t done any experiments well I guess early on in the BB our development process we did do some lab experiments that were trying to do similar things but we haven\u0027t tried out their mechanism it\u0027s it\u0027s my gut says it\u0027s probably a very hard problem to make it work well in the real world so yeah it sounds very challenging but it does seem like an interesting area for research yeah for the notes can you repeat the name of the paper I didn\u0027t get that I believe it was coat I was a copper was that the name of the it\u0027s cool yes the most recent in SDI from paper from MIT okay I\u0027m really got a question it was just a point that in you with saying you know the WebRTC folk definitely want to avoid these spikes and I just wanted to make a point that when you\u0027re looking at coexistence is not just with cubic and really know you know it\u0027s with WebEx and and hangouts and and all the other real-time things not just web RTC as well you know that it\u0027s all that that has to work as well definitely yeah use your motivation for working with them is the idea that you know we can improve bbr for everyone by like learning from real-time use cases that are a little more latency sensitive I\u0027ll add a quick note the question that probably narced I think this is relevant the chart reflects v1 and I think there\u0027s a lot of detail in the slides for sure but it\u0027d be a lot of the rationale and a lot of bits are missing obviously cuz they weren\u0027t on the slides we can talk offline but we should think about some way of of sharing this in a way that\u0027s more draft like or some written up form so that people can see so that as thinking is evolving people have someplace to read and understand what\u0027s going on perhaps before coming to the meeting but thank you for presenting it here all right they\u0027re gonna move on to the next presentation Brian I suspect we won\u0027t have time for your the thing we\u0027ll bill unfortunately hi so on your "
  },
  {
    "startTime": "01:51:46",
    "text": "period opening I am going to talk about retransmission time outs so and the seeming contradictory goal of making audio fast you two random losses and slow enough due to congestion so next slide this so currently the first law RTO it\u0027s like well faso yeah is the design design done specify for cob but you should not feel that this is just for co-op it\u0027s just a environment found it very useful so such environments are subject to random losses and small expenses are very common and congestion typically occurs because of large number of the devices rather than large payloads this is obvious that eventually man billion so forty devices are going to be deployed and then we will hit some hotspots in density where every many devices parallel to each other and one important thing is that there is just one message in fly per flow so so the RTO stone in Mecca if you lose the pocket and RTO you need to wait for the audio to trigger and like I said first of a smart general or so this could be applied to some other party or make another protocol stew and the corn corn cotton conscious you won\u0027t report proposals have some issues so traditional corn ardent has had a sexual assault back up and besides that we just get this RTO respect of RTO until the and the Ryoka occur acquire a treaty sample which is unambiguous but this co-op connotation control events differ from that in the way that they do not recur in the fact of our key and we have shown the congestion collapse collapse will occur when the art it is large enough so there are some conditions I will not go through them to save time and since they are specific to toss out good so but anyway when you have RTT long enough the RTL triggers and this is persistent condition under some conditions making the next exchange will also do the same thing that the RTO is too small but the thing thing which is also also important for this presentation is that when these others don\u0027t retain the RTO they perform obviously better in the cases of random loss where you would want to have the RTO to trigger as soon as possible so "
  },
  {
    "startTime": "01:54:48",
    "text": "this is parity phosphor comes in so we try to find a pool pool middle ground between these contradictory clothes so on the other hand we want to improve random lost but on the same time we need to really hundred handle come just and say if we saw that we don\u0027t cause these collapse and in faster there are two ways to calculate RTO there\u0027s this faster here which is similar to normal then then this new slow RTO and some back of logic didn\u0027t select which RTO to use so first RTO is similar to what PCP use is for RTO computation expertly we have done the small small change that in it allows it to a smaller value in order to improve the RTO for sort exchanges but this is but not minor foreign type engine and then knitting is this flower our key of this is our traditional roasted corn sagar so so economists others to keeps the RTO and then it gets there on a bigger sample and we must do this slightly different way so so when we get an unambiguous sample we measure the time we took from the origin transmitted so it\u0027s the worst case time that they bucket could have been in transit we don\u0027t know know whether it was the RTP must from the origin of some of the retransmissions but we just take the worst case then apply some additional factor to this so this is more conservative done con algún that I actually believe that this is actually what France Algar this suit should have done in this case so it should measure actual RTT that the worst case RTD so I think that consultant is not conservative enough necessarily and then we had this novel back of logic so in concert that we had you have to state RTO logic but we add an additional state so so we have this fast slow fast state in between so if I start from the first test of this fast is the normal state where your network state is not Publius you are getting acknowledgment without any retransmissions and then then if you get some a lot your trigger so so you you don\u0027t know know which mid-segment you\u0027ve got the acknowledgement you go to this next state it is this fast slow fast and we measure at a slow RTO and then instead of using a slow activity directly write like Cornish algorithmic Roach would would use we have this passed out of your first to probe probe with this no "
  },
  {
    "startTime": "01:57:51",
    "text": "probe quickly so so that if there is random losses we are able to retransmit quite quite soon and discovers pretty high error rates quite well and only as the second step we apply the slow RTO if it\u0027s needed at all if we get the rest response back quickly we don\u0027t need to apply the slower T at all because we already caught a valid RTD measurement from this X J\u0027s and if the slow RTO is not successful we know that we have waited long enough the train unnecessary transmissions that we possibly have made any continuity first RTO best backup service in that state and since this first law state is such that it can still in the case of unnecessary transmissions course this ambiguity problem so we gonna measure at the RTT we need this last state so that we start with slower T also that we get with high probability valid RT t sample which is unambiguous I will skip this because I spoke most so then either so somewhere else was from a tester this is quite typical or not typical but at least you would see something along these numbers in the ILP deployments so lower bandwidth and RTP is quite high in our discussion and we have this concept of flow so we if there are not number of power flows on each of these flows perform perform in total if the request response exchange system and to simulate the small small devices which you access on a small amount of data we invest at the congestion control state every now and then so one to ten messages randomized and each of these flows flow to us this ecstasies under the 50 in total was done and the best ones payroll which is protocol one of the direction is 60 bytes and I\u0027ve also to test scenario so we had this heavy congestion with buffer world we have tested at Excel up to 400 close but I\u0027m on the sewing this largest 400 flows and the various sizes including in fact in a finite buffer which basically loses no bucket so every packet you sent to the network it to be honest thoroughly it once we finish with the public the gate coffee and without any unnecessary transmissions the RTT with this 400 clients and infinite buffer in this case is around 10 seconds link is of course "
  },
  {
    "startTime": "02:00:51",
    "text": "error-free to keep the congestion level up and then on other cases for this random velocity so we have 10 parallel port of congestion link is enough to satisfy those and error parameters are shown there we have shot the arrow straight and slightly longer better state and this alternate so here are some reversals heavy congestion and buffer flow so so here we see the congestion collapse with the development Cocotte a big make for this to make this 50x racist a they do up to 250 unnecessary transmission so 80% of the load is actually actually or the traffic is of this unnecessary transmission and all these increase your so it\u0027s very above this 10 10 seconds it\u0027s not so another slide but if you want to move from the back up you can see see that it goes much larger than 10 10 seconds and we transfer we can have quite quite highly reduction in the Indiana stern and openness in retransmissions because because the new flows always start with this small RTO so the initial RTO small we will keep making some money sir we transition but this situation does not persist with phosphoryl our kids it does with these other competition control approaches and as a result flow completion time will be much better with buzzer then wrestles with random loss so so here the first part you make pro-peace able to cover most of the most of the losses quite well and enables faster to finish much faster faster and we have been remembered that these two variants which we compare against are unsafe to congestion if they would handle call conversation properly they could get much worse performance than what we see in this test and further astray that the lower the RTO successfully in contrast what this other congestion controls or develop does not even have a concept of RTT measurement so it\u0027s just keeping this initial archaeal starting back up from there do you have some question related to this or general comment I think II just have one more slide yeah so I haven\u0027t just Congress so many of the facilities not balance between "
  },
  {
    "startTime": "02:03:51",
    "text": "handling random wealthy society and also responsive congestion it\u0027s slightly more aggressive than the traditional argument based on call but we try to try to make it in a sense a safely yes so this is all gory first and thanks for doing this work because and I think the first time we did court we kind of didn\u0027t really think about it and we\u0027re now working on it so it\u0027s good to see research in this space so thanks my question is simple and how much memory per endpoint do you need to keep to run your algorithm because they always ask this do you have an idea absolutely well of course for the ocular estimation and you need need some some memory which is common to any other whose estimates are do and in addition the Unity State but the logic itself is what parts it also so it\u0027s just these statements on whether you\u0027ll receive the on unambiguous our teeth example or on because sample you decide the next state and and then of course the back of logic but you need to anyway have this back of logic so doesn\u0027t increase complexity from thanks I\u0027m I guess we can talk about this in other places thank you yep this is a thank you for bringing the presentation here this is an interesting so very different type of condition control question that we typically do not see in IC crg but there\u0027s work around this happening at the IETF and this is the right venue for discussing approaches that people are presenting to condition control in IOT like environments so thank you for bringing it there any questions talk to l4 yeah I guess that\u0027s going to be the end of ICC RJ thank you so much for tolerating the power outage and being the being good through the whole thing see you in Bangkok oh did you yeah.he 36 I don\u0027t want to keep people yeah "
  }
]