[
  {
    "startTime": "00:01:03",
    "text": "[Music] all right [Music] [Music] okay so just to sanity check I\u0027m not wrong that this this was supposed to start three minutes ago right all right um we\u0027re gonna have to get started we\u0027ve got a ton of stuff to jam into the hour and a half time we have so so fight find your places assign the blue sheets they\u0027re circulating from either side here welcome everyone this is map RG the measurement and analysis from protocols Research Group I\u0027m Dave Wonka my co-chair is Miriah Colwyn who I\u0027ll expect to be here any minute and this is the the IPR statement for the IRT F and the note well and does anyone know how to fix that Thanks "
  },
  {
    "startTime": "00:04:03",
    "text": "yeah I was when I was using a clicker it was it just advancing a part of a slide each time all right thanks all right so so what we have on deck for today is what we\u0027ll well here\u0027s the links to the things everything map RG if you\u0027ve not been to map RG before it\u0027s the measurement and protocols of measurement analysis for protocols research group what we do is do measurements that inform the engineering or operation of IETF protocols so everything you see here should be attached to some IDF protocol or the operation of it we have a mailing list today\u0027s slides are up at that link there\u0027s the etherpad the audio to meet echo the jabber room i\u0027ll link there so the way we run mapper G is we put out a call for contributions this this time we the same thing On January 28th we put out a call for contributions to four presentations that have measurement results here in mapper G and we received about ten proposed contributions a couple of them are out of scope seven ohm were invited to present here today of which I think you\u0027ll see six and because one of them was presented a couple a day or two ago at the quick working group thanks for the proposals we got some really strong stuff unfortunately this time because of the way they changed the the the the week around and had the the free time during Wednesday uh we couldn\u0027t get a two hour time slot so we\u0027re in an hour and a half so I think it\u0027ll go pretty quick one of the things we do in mapper G if you have a project that has a tool or your results aren\u0027t particularly mature yet or you just want to tell something about an upcoming measurement conference or symposium we\u0027ll run ads for you and you\u0027ll see some of those here so that the tail end of the intro slides will be those ads the other thing that we did new this time was map of participated in the hackathon so we had a measurement analysis for protocols table at the hackathon and we had I think six of us at the table and we\u0027ll give a short rundown on what happened there but basically we did the call for participants and projects there we had three projects there and two people responded in advance two more showed up there and with Miri and I that was the six of us that spent Saturday and Sunday preparing content for the the four today so the agenda for today is I will go through this intro and overview and then I\u0027ll give a rundown on what happened at the hackathon then Ola fit in is going to come up and give up what we call a heads up talk where we give you a short amount of time to introduce people to a project that you want the community to know about and then we\u0027ll go into our longer measurement results presentations proper which will have of five five of them in a row there which is well I will just go up one at one at a time and then lastly if there\u0027s time remaining we\u0027ll invite sherry from Google to come up and do the quick presentation otherwise we\u0027ll just send you the link to the presentation that you did in the quick working group as for advertisements Ian Lehrer month it has a draft about safe internet measurements and it\u0027s certainly pertinent to this group it\u0027s about what would be essentially like best current practices "
  },
  {
    "startTime": "00:07:03",
    "text": "for how you would do measurements in the first place and then perhaps also how you would catalog the results and what sort of things should and shouldn\u0027t be in those results so there\u0027s a link to that draft there please review it contribute to help me in with that one of the proposed presentations for today that is just a in an early state and we asked Luke to come back when he had some measurement results is this project it sounds like he\u0027s doing some really neat stuff with line rate up to hundreds of gigabits per second in impecca capture and so Luke\u0027s got his email address there if you\u0027re interested in a product project or that software get in touch with Luke the advanced network research workshop for next year the dates have been set so let students in and and potential researchers that would bring work there let them know about that you can find it on the ITF website and it\u0027ll be in this slide set there\u0027s a quick quick interoperability and performance a workshop coming up next year the dates are here as well that\u0027s the second venue for that kind of work after that then we have we have here\u0027s Miriah and then we also have the dates up for the internet measurement workshop um very often we get to scoop the internet measurement workshop by finding out what they accepted and presenting the summertime before them so certainly one of my favorite venues that that\u0027s up also the internet measurements workshop is running a shadow PC again so especially if you\u0027re a graduate student and you want to learn how to be on a PC they\u0027ll run it on in parallel with the with the regular reviews really neat experience and of course fascinating when they pick different things than the real PC would pick so I get involved in that for you or your students if you have them and that\u0027s the end of the advertisements um could you switch to the next slide so I\u0027m gonna spend I\u0027m gonna spend on maybe five or eight minutes going through what we did at the hackathon so this was this past Saturday and Sunday and so um the-the-the weren\u0027t the project that I proposed was to work on privacy and security issues in ipv6 deployment if you were at map rge last meeting tobias fee big presented a proposal about we\u0027re basically we\u0027re exploring the way that v6 deployments are being put out in the world today and what we can discover that we think might be privacy sensitive or security sensitive to collect it as a set of measurement results and perhaps give it back to the v6 IETF community to say is there something that should be different in terms of how the deployments are going there "
  },
  {
    "startTime": "00:10:03",
    "text": "so we\u0027re meaning to write an internet draft and it would be probably the first one from map RG report reporting that set of measurements and Tia Oliver Goss gasser from tu Munich offered to come and work on this with me so when we first conceived of mapper G one of the things that Mary and I had as a goal was could we get these private measurements that sometimes happen in industry where they can\u0027t quote it disclose the details because it might be about customers or users on the Internet could we take some of those private measurements and work together with people in map RG in conversations and then digest the results and come up with some sort of summary results so that\u0027s what Oliver and I gave ourselves as a goal we would take his ipv6 survey studies from T Munich and some of my studies from inside Akamai and compare apples to apples and use the same tools to analyze them and try to produce a combined result that\u0027s more complete than either of us could offer you on our own so what do we get done at the at the hackathon when we considered what would be best current practice for privacy and security in hit lists right now to ipv6 internet is largely surveyed based on seed addresses or hit lists that you use to do trace routes or reach ability tests and there\u0027s a link there to the TU Munich hit list which was introduced in IMC last year so the nice IMC paper about how they developed the the v6 hit list and how they mean and then we updated and released a couple analysis tools that I\u0027ve used over some years to do v6 analyses and so we\u0027re taking essentially what we think is the largest publicly known data set of ipv6 topology and reach ability measurements and as far as I know again the largest private one from what I do is inside Akamai and bring them together so what did we learn well the first thing we learned is between our two sets just accidentally doing these topology and reach ability studies we found 1.2 million UI 64 addressed ipv6 routers so these are routers that we know the manufacturer of them ostensibly many of them are reachable on the internet and we think this could be potentially present a vulnerability in the v6 Internet if any of these classes advices are known to have have weaknesses the discovery of these was completely accidental we were doing a topology study to discover the core of the v6 internet he was doing a reach ability study and then we decided as a side effect there were these router addresses in there we also found that the public data set the results that team Munich prepared and our set inside they were focused on different parts the v6 internet again sort of accidentally so that the results were complementary and uh you know something like forty percent came from one of the set and sixty percent came from the other another thing that was surprising that we found is in one of the studies I was using uh more than a year old ipv6 hit list and we found it discovered more results sometimes than the current hit list so you can imagine these currently these hit lists are our curated over time and you have two distinct decisions about do you add it with what you hadn\u0027t removed from them and some in some addresses some sieve addresses hidden culled from the the hit list and last year and it turned out those were actually useful to find more of these v6 routers of interest and then "
  },
  {
    "startTime": "00:13:05",
    "text": "so I\u0027m gonna I\u0027m gonna go through quickly a couple the the similar results but basically the the middle column here is a device count and this is a GUI sixty UI 64 numbered devices that have you know I believe MAC addresses embedded in them in in various asns so we found an ASN that had something like 300 thousand of those at the top there there were 571 unique a ascends that we found a uie 64 number v6 routers in their deployment we found in that of the top ten vendors we found two and thirty-five unique vendors but the top 10 you can see here make up something like 99% of it so there are particular brands of equipment that that networks are choosing to deploy in their v6 deployments 235 and a different unique vendors in total and what we decided to do so I said there was there\u0027s privacy concerns with the data so what what Oliver and I did is we anonymize the ASNs here and we anonymize the vendors so I took a bunch of fictitious company names from movies and television like Wonka Ollivanders wand shop those are the manufacturers of the devices and then we made classes of them where we paired up the ASN and that particular brand of equipment so here 23.5% of the UI 64 numbered v6 routers were manufactured by wonka industries and we\u0027re in a si and what you can see here is for instance here\u0027s two different ASNs that use the same ostensibly popular brand in this case we\u0027ve substituted acting Corp brand equipment in there and that represented some twenty plus percent of it and then you can see that same brand is used by ASG down there as well so just trying to give you an idea and then we also see things like here\u0027s an a s that uses has a high population to two different brands of this equipment now there\u0027s many many of these brands so let\u0027s try to dive in and visually in a couple of these so what Oliver did is over the over about the past six months he did a time series plot here of the the five asns in these five different colors again anonymized that but this is a on the vertical axis the number of devices that were numbered using UI 64 addresses and the top of it is of five hundred thousand there so so this this blue line at the top saying over the last half a year there was a decrease in the number of UI UI c64 number robberies there but and we saw some anomalies like for instance where a number of ASNs the number seemed to drop and we unfortunately found that that\u0027s just an artifact of the way the measurements were done what we were seeing instead is over the last half here there\u0027s a number of a ascends that the number of eui-64 numbered routers which is a legacy addressing technique they\u0027re actually increasing so we think that current v6 deployments our numbering routers this way and a lot of them are customer premise equipment so there may be potentially PII concerns there another way we visualized it is on the horizontal axis here I have the top 20 a a sense that had the most the most you I 64 numbered routers discovered from the this private Hawk my data set and what I want to show you is just a couple of the the modes of "
  },
  {
    "startTime": "00:16:05",
    "text": "results we see here so here the green line there at about 20,000 is the number of target addresses that we traced towards in these networks and you can think of these as web clients that the touch for instance Akamai hosted properties and by tracing towards 20,000 hosts we discovered 110,000 eui-64 numbered routers that\u0027s the only data point in that particular TSN where you trace only 220 20,000 endpoints but you discover more routers well what does that mean it means that they must be using you i-64 when they\u0027re numbering things in the distribution layer of their network that we see multiple eui-64 hops on a path towards a web client the other thing we see is here where the the reticle is the number of you is 64 numbered routers we the red line is the more Bui 64 routers that we discovered the green is the the number of addresses that we probe towards well here they matched up exactly that means when we trace towards a client in that network we pick up one eui-64 numbered router ostensibly the one that\u0027s in the customer premise and then then there\u0027s also these bimodal results where we here we scan less than we scan towards about 40,000 and we call about 10,000 you i-64 routers so there\u0027s quite a distance not every not every path towards when the client seems to have that kind of router numbered router but then there\u0027s other ones where the red and the green are really close together so we almost harvest in in survey one eui-64 numbered router for every target we traced towards and then so that those are basically the early results we got from the this v6 so security and privacy study and basically what we\u0027re finding is that because things were all over the place with different AAS ends we really need to do a more focus study that tries to figure out how many of these are there they\u0027re really just being accidentally discovered so we\u0027re gonna set ourselves up for some follow-on work and we\u0027ll bring that to metallurgy when we have it another project was al Morton at a project at the same table where I\u0027ll just leave the slide here but you contact and contact him about what he\u0027s doing it\u0027s basically new ways to do performance measurements at at the Highline rates that are in the last mile in modern internet and then Miriah and Marian ended a path spider survey as well so we set ourselves up with a what wired internet access at the table and you can do some active measurements we can so if I come to the the meeting in Montreal I would love to have another session for for map for the hackathon so I\u0027ll get in touch with us if you\u0027re interested in doing that as well and those are the folks that were at the hackathon run late so we\u0027re going to switch into the next presentations and then if we have some time at the end you can have scanning all things about the ones along the way Ola where are you "
  },
  {
    "startTime": "00:19:06",
    "text": "[Music] thank you well I\u0027ve I\u0027ve been using the internet for a while you can see my my here my hair is white and I was you know I had all these sessions you know my bed blossom freezes my video session hangs my companions fell out on the video and I had you know always thought well it\u0027s my shitty operating system so rebooted my PC I I restarted my browser and whatever but then and a niche that they could be problems in the networks I started this project ten years ago to do a let\u0027s say a global measurement the inter domain dependability and we had just collected a few sites these are mostly research networks at universities and so so on and you know the measurement was quite simple we just sent hundred packets a second between the UDP packets and then they analyzed the results afterwards to see all the gaps in the in receiving the packets and we develops and some metrics try to understand what happens each each time you lose a packet like what are the cues at the moment when when you lose well just one more thing initially we set this up as a ratio project and we had people at university that\u0027s you know run a couple of PhDs and some thesis on this and I was helping out oh no the MSC is Monday and I discovered that the share engineering numbers out of this was huge I discovered that you know each week there was around 1,000 outages in this you know mesh of 10 notes we lost let\u0027s say in the over hours every week in time just waiting for routing or anything to come back up so we we started to investigate in let\u0027s say each let\u0027s say event and we found quite clear some you know short engineering measures to to to solve some other problems it turned to be mostly these these kind of problems related to BGP boundary crossings so you know we had the passive mode which appear to our customers that\u0027s resulted "
  },
  {
    "startTime": "00:22:08",
    "text": "in two minutes oth when you have the redundant collection you had been miss miss miss done with rhetoric rates when we started you know evading the reads from the routiers before being bitten you don\u0027t see any outages so I also saw that a shear fiber cut or or instability costed 79 seconds and that is probably due to bgp or whatever the riveter rewriting the forwarding table with the fuel bgp routing sec so so you really can\u0027t afford you know afford even a small change to the routing table so they are in image in mitigation so that it\u0027s for example the the PSE the protocol independence convergence saying kid you know it\u0027s faster but definitely we don\u0027t afford can\u0027t afford one minute outage a everything change so as you can see here here\u0027s a distribution all the entities and you know a lot of oddities in in the air over 50 milliseconds or less and that\u0027s probably let\u0027s say randomly be scarred so whatever so that\u0027s probably a that\u0027s a fact network solution care too much about seven but you can see that the violet figure shows that the most of time goes to the bigger entities which in is in the order of 50 seconds or more so minutes and that is to me looks like we\u0027ll be lets say BD related configuration related problems so I guess we need to do all of us do better which be to tuning and configuration we need to have a faster better hardware for PSC or stuff like that faster convergence yeah so as you can see I have a limited number unknowns and if anyone are interested to join or help this project with let\u0027s say bye power and also some hardware I just need an UNIX account so so let me know and I hope there\u0027s a lot of stuff to be discovered here about how the internet actually behaves thank you thanks a lot you\u0027re so next up we have your Dortmund presenting about satellite internet\u0027s yeah hello everybody my name is your coachman this is my first talk at an idea thank you "
  },
  {
    "startTime": "00:25:09",
    "text": "and the topic is satellite internet which was also on the agenda at the previous map Archy so what\u0027s it about it\u0027s about you stationary satellites we have five propagation delays so our duties above 600 milliseconds and that\u0027s why we deployed performance enhancement proxies to do split ECP unfortunately we\u0027ve encrypted transport layer headers we cannot use split tcp anymore and at the previous memory nickelodeon already presented the principal problem of this he compared HTTP to too quick and our finals complement his results we agreed with him and yeah if you look have a look at this so there are three major operators across europe sadly none of them has ipv6 support which is not that important for our performance measurements but i still wanted to have imagined so what we did is black box testing for all three operators we act with very simple active measurements which were one-way UDP delays auto transfer which i\u0027m not going to show here and page load times with common web protocols ok let\u0027s start off the one way you repeat delays we send one body DP packets in one second intervals we use the same physical house for sending and receiving packets which simplifies clock synchronization and of course we have the impact of the backbone network which we are seem to be neglected for now these are the results the result the delays in the return link are in general higher than the forward link operator Steve shows very stable delays whereas operator a and B suffer from UDP Cheever we have no insight into the system so the reason might be suddenly a true channel access mechanisms are some some impact in the access network of operator but let\u0027s keep these delays in mind next or the page load times so we had a look at different HTTP flavors we considered Google quick ref to different implementations for all tests we also set up a openvpn UDP tunnel which just allows us to disable the performance enhancing properties and we designed two static web sites are small one with a total size of 1.4 megabyte and a large one with total size of 10 megabyte or HTTP server did not have any features like sariputra stream prioritization so here are the results and as expected the TCP connections optimized for the PAP provide lowest page load times in contrast to the ones going through the UDP channel again for all experiments operator C shows the most stable results "
  },
  {
    "startTime": "00:28:12",
    "text": "and even gets more rigorous when looking at quick so let\u0027s have a look at chromium quick with chromium quick operator a and P suffer from I would say is their insanely large variations whereas chromium quick with operators C works quite well even better than the TCP connections going through the VPN tunnel but still if you compare HTTP 2 to chromium quick for operator C you see a the page load time is roughly twice and this is exactly the same which we also saw at the previous map Archie for quick go the variation is not that high but also the total Pedro time for when using operator C is not that good for large large website the key messages are all the same therefore I won\u0027t go into detail here but I think it\u0027s still remarkable that the order of magnitude for for some experiments is like really large ok as one of the last slides I want to show you some work in progress in the meantime we have moved to ITF draft 17 we are requesting 1 megabyte object within a single stream and each figure shows like 10 traces Florida\u0027s gray and the black line is one trace picked by hand so the first row is all reference hep-2 optimized by peps you can see that the object is more or less received at line rate and the second row is in PNG TCP again the operators from left to right or a PC like before so aunty TCP you see some retransmission bursts which is not the case for the third row which is quickly quickly works quite well for operator C but somehow fails completely with operator P we do not have an explanation for this yet and I have to emphasize that of course all the quick implementations are work in progress and they\u0027re all great so this is meant not meant to rate the implementations it\u0027s just like first impression what awaits us with quick over satellite and these results match the previous results and also match the results from the previous map arch imaging so as a conclusion I would say TCP traffic benefits from perhaps no surprise the performance wise among "
  },
  {
    "startTime": "00:31:12",
    "text": "operators also no surprise however the performance differences were at least for me surprisingly large and I think it is an indication that some operators have not tuned their network for a quick yet and there seems to be a correlation with the UDP liberation if I had one wish it would be that in future performance studies not only high bandwidth delay product links are considered but explicitly satellite links and as a future work I would say a lot that\u0027s all right thanks your um and you helped us get back on schedule thank you so we have time for a couple questions paper so roughly we\u0027ve used okay the questions like what was the setup of the operators like did we yeah for this comparable setups and the answers we have chosen some tariffs for the operator so say again it just went similar tariffs so like similar products the download dollar rates were all between 20 and 30 megabit per second and so I would say they are comparable with a dedicated channels or was there sharing it\u0027s a wireless channel so wireless cellular network with multiple users so we have no insight how to layer two channel access is done by the operator which is of course important sorry you said a cellular network I thought it was sadly wireless network with multiple users so we have no idea of the layer two mechanisms for it and this might of course be a good explanation for the differences Microsoft well one question I had was so is there any way to do the TCP measurement without going through the pipeline without split TCP is there a way to force that and measure that not really not as an end user I would say "
  },
  {
    "startTime": "00:34:12",
    "text": "that\u0027s why we use the Open VPN tunnels of course the Open VPN tunnels on UDP UDP suffer from the UTM place so there\u0027s no way to measure these people without electricity not for us do you have an S at the end thanks for a presentation very interesting for the measurements point of view I mean either I usually don\u0027t do satellites measurements because I don\u0027t have an access to satellite network do you know if there\u0027s any like public measurement platforms if ripe Atlas for example is there any probes in Atlas that actually have a satellite connection or any operators in your own if it\u0027s not a case would like to host one at least we have the right probe of our satellite links so oh and it\u0027s I can use that from thanks that\u0027s great thanks probe ID can you send to the list the mapper G list sure is there a textile right of course we disabled the probe during the measurement so it was offline for quite some time [Music] in Korea Montenegro so you mentioned the so there are some some quick and different populations of quick testing there\u0027s also been some work and I don\u0027t Iike rock units involved in those those that work to make quick quote unquote aware of satellite lanes you don\u0027t increasing BP or increasing market here or whatever do you implement any of those mechanisms experiment with overall or it\u0027s just pure quick off-the-shelf what can we do for certain learnings I thought I think that was already discussed last time yesterday there was a interesting site meeting about localized optimization of Eggman\u0027s which may might help to it\u0027s a satellite link more robust but also you have to touch the end the implementation parameters for quick like large initially window and so on okay thanks thanks a lot you\u0027re so next up we have Pablo from ski and Oliver Gaza we\u0027re going to talk about the DNS observatory we\u0027re gonna switch to this presentation okay yes so hi my name is Pavel this is Oliver we are here to present you a new research project of our site security DNS observatory the goal is to gain insight into the global DNS space using kind of a telescope and eventually to give access to data to researchers because we found that it\u0027s not so often that researchers have such access to such data to begin with we start by analyzing a large stream of passive DNS observations from recursive observers around the world that is we "
  },
  {
    "startTime": "00:37:15",
    "text": "analyze the traffic between the recursive resolvers to alternative name servers that is what happens above the recursive resolvers for those of you who have heard about far site security dns DB that\u0027s completely new machinery and we track only the big guys the top and objects for that research we focus on top 10,000 name servers but also other objects I will describe we focus on the data collected in the first three months of this year that is marking on three don\u0027t cache misses and we are just starting we just want to preview the data and ask you what would you like to see in the final work so as to the galaxies we track using this telescope we tracked DNS objects we aggregate the DNS traffic that is the query response traffic and by few kinds of keys formost the server IP address the IP address of the out relative name server but also effective TLDs SLD SFPD ends q types but also the or I\u0027ve already before or v6 address we have seen in DNS responses to a quad-a or any queries every minute we dump a list of top ten thousand ten thousand objects ranked by the number of queries we have seen and we aggregate these media files into larger files like one I\u0027ll revise daily and monthly and so on each tract object for instance each authoritative nameserver or each TLD is characterized using furtive features of few kinds first of all counters like the number of DNS exile responses we have seen from a particular name server also cardinality estimates which from us is the hyper lock lock algorithm like the number of distinct fqd ends we have seen from particular s LD and histogram estimates which allows us to get median response delay including the quartiles top three TTLs we have seen four particular records or all records we have seen in responses and so on so the very first result we\u0027d like to show you this graph this graph presents you the rest four top 10,000 name servers seen during the first quartile is the distribution of traffic on the x-axis you see the server rank so the big guys are the left these were the root name servers and TLD name servers live on the right hand side you are more likely to find his own authorities for SLDS and so on and we found that 60% of the queries we have analyzed are handled by just top 10 top 1000 name servers and when you filter by the response code for instance "
  },
  {
    "startTime": "00:40:17",
    "text": "an X domain which is visible in the red curve and traffic is more shifted to the left which kinds of story that no propeller name servers are the first line of defense from the garbage traffic I mean queries for non-existent domains they would servers we get a lot of of garbage queries whereas we are more likely to find the answer right hand side - let\u0027s put our name servers we also analyze how DNS traffic from recursive resolvers the authoritative name servers are distributed around names of networks that is we aggregate by name indeed a s we put the names aside because there is no particular reason to blame this organization organizations for wilt rather want to ask question is the NS really distributed I mean just to start discussion and just to start research we found that just 8 organizations and 53% of DNS traffic we analyzed here we showed the median response delay so on the x-axis you now see the response delayed by axis is the CDF although 50% of the server\u0027s server IPs we have analyzed respondent less than 25 milliseconds but still 1/5 needed more than 100 milliseconds to respond which suggests many recursive need to cross an ocean to get to as an authority we of course disregard the health of the name server but while there\u0027s clearly space for improvement and it may motivate replicating some zones closer to recursively solvers obviously and now time for oliveira alright okay so we also would like to point out the dns Observatory can identify significant events and what we see here is the the second-level domain of a large hardware vendor in the dot-com zone which basically reduced the TTL of the risk of the resource record to zero which you can see on the 19th of March which is the the blue line basically goes down to zero and what happens as a result is the green and the red lines start increasing so the traffic volume and also the response the delay starts increasing as a result because the authoritative nameserver of this hardware vendor gets hammered with queries this hardware vendor then also came up with the interesting idea to add more servers to cope with this problem and but finally they fix the problem by setting the TTL back to a sane level which then in turn results in decreasing values for response delay as well as traffic volume on on this slide "
  },
  {
    "startTime": "00:43:18",
    "text": "we show the introduction of a new gTLD and what you see here is as the green curve shows the traffic volume again which shows significant events when for example the the the gTLD was made publicly and you could by TLDs you could buy domains under this TLD and what we also see is the slight increase in registered domain and also the number of servers which are authoritative and within this gTLDs what to note however is that the overall traffic is not significantly increasing over time this means that and no it\u0027s a large service yet its operating with under this new gTLD now to another interesting effect that we saw namely the combination of the heavy eyeballs Algar with negative caching which leads to some interesting effects so happy I was as you all know is if you have a v6 connectivity as well as IP before you will send a and quad-a queries to resolve an fqdn and negative caching on the other hand allows you to cache negative responses or allows to allows the recursive resolver to attach negative responses in order not to hammer the authoritative all the time so and we found two interesting effects here the first one being that some ISP resolvers do not catch empty quad-a responses at all which of course leads to a lot of unnecessary quadrate queries and the second one being that some fgd ends have a very short negative caching TTL which then results in a large quad-a query volume and we\u0027re going to into some details in this graph so on the x-axis you see the top 200 fqd ends ranked by traffic and on the y-axis you see the share of empty quad-a responses for each of these specific fqdn so they hire these spikes are the more empty responses for quality records you get and we want to show you three examples here so the first one is a CDN for which is used for operating system updates which has a record a TTL of one hour but a net negative caching TTL of only 15 minutes and this results in almost 90% of all responses being quality no data the second example that we look at is actually two specific FTD ends which belong to the to a large at serving network where we see a regular TTL of five minutes by the negative caching TTL of only one minute which then again results in to two-thirds of all responses being empty quality responses and the last one that I want "
  },
  {
    "startTime": "00:46:18",
    "text": "to show you is two specific FGD ends again but this time for an OS time service and here we have the spastic effect I would say so we have a regular TTL of 15 minutes but the negative catching TTL which is 60 times lower namely 15 seconds only which results in 90% of all responses being quality no data so this example show you that the effect that if you have the happy eyeballs algorithm in play with short- caching TTLs can lead to a large number of basically let\u0027s say useless queries okay so you have probably heard of this DNS hijacking at X right if not please download these slides and click on these links the short story is that DNS second rpki properly deployed should make these attacks much more difficult and this is why we analyzed in a sec an RPG I support for in the top 10,000 name service on the Internet let\u0027s start with the red line which is DNS SEC support I mean percentage of DNS responses with the SEC signed DNS SEC signatures and in order to make this plot sane we represent servers in by averages for groups of 100 servers that are adjacent on the trees because otherwise you would see 10,000 points and that would be unreadable so the red circle shows you where Dina\u0027s sack is most likely to be deployed I mean for the most rural guys that is usually the named the root name servers and deal.this which is I think common knowledge nowadays however when you look at the rest the guys it\u0027s not so well and so when you measure the adoption by server IP it\u0027s just 4% but when you measure by traffic its 16% which kinds of alliance with the evening starts which are linked below for RPI for some reason the guys in the middle are more likely to sit in prefixes signs using RPI we also analyzed well what if a server is secured using DNS SEC or RPI I mean I wrote the two or both and that\u0027s the black line so intuitively the distance between the blue and the black tells you how these two techniques complement each other if the distance high or in they are deployed at the same time so sorry clarification question here yeah on DNS SEC or rpki I mean that\u0027s that that\u0027s the set Union here yes let\u0027s didn\u0027t even look at intersection yes that does the Union not the intersection so when you "
  },
  {
    "startTime": "00:49:18",
    "text": "average by server IP is 44% and by the traffic 49% so I think for now that\u0027s it just to summarize the observatory\u0027s any project provides aggregated view in time and we found the different sector might need more work in performance security as I said we are just starting and we\u0027d like to hear a feedback for most what evolution would you like to see in our final work what did we do well or not and if you are interested in playing with the data please do the wrap us an email we are trying open to shorten a go with researchers thank you all right thanks guys we\u0027re just barely on schedule so let\u0027s take the questions comments that are at the mic right now and then we\u0027ll move on to the next so yep no I\u0027m trying to make it quick Alex may over from Nikita DT what I would great work what I would really like to see is a function that actually allows me to estimate the traffic change depending on a TTL change so that would be I suppose it\u0027s something like this yeah if you can get that function out of the data that you have and that would be a greater estimated to recommend customers and also to compensate for measurements between domains with different details yes exactly thank you very much this already planned thank you hello beta from the like PI or dienes you mentioned that do you have a boss cause a lot extra queries that result in no data right now in the DNS the negative TTL is used both for both for no data and X domain would you agree that it might make sense if server split those numbers ah boy okay yes okay sorry what does it answer your question yes okay that was the fastest questions ever anyone anyone last question I\u0027ll bring mine back because it\u0027s a little bit long and we might have to take it offline so hi brain panel yeah so I\u0027m a little confused I would be interested to see the intersection as well as the Union because I mean like so there\u0027s a there\u0027s a an understanding that they use like one or the other is enough but there are a few attacks for both would be really useful to sees all right I suspect that number that\u0027s a little bit lower and I\u0027d like to see it thanks sure could you go back to the first graph on a black graph and I\u0027m less hurt occur si so I don\u0027t understand your conclusion from this one because the response delay doesn\u0027t go up well the DNS traffic goes up and you were I mean there\u0027s a shift between the green and the red line and and you your statement was sounded like a conclusion that they were aligned so yes and so you\u0027re right so it doesn\u0027t go up in parallel that\u0027s what you\u0027re the sort question is about "
  },
  {
    "startTime": "00:52:19",
    "text": "okay yes you stated that the dropping the TTL increased the delay and it didn\u0027t for like the first day as far as I can tell it didn\u0027t the first day that\u0027s correct so as we also mentioned so this is preliminary work we don\u0027t have all the knowledge that plays into the DNS DNS is a very complex system as you all know the it could also play in that the people at this company added new servers and those servers were maybe configured strangely they could be very low performance servers so we don\u0027t know that so we don\u0027t know all the details but of course we will investigate closer thank you thank you all right so this response delay in this graph you\u0027re actually measuring traffic from resolver to authoritative right so I don\u0027t think would be influenced by TTL because if there will be any cached orbino queries for to start with you know what I mean if it\u0027s in a cache you\u0027re not gonna see the queries coming from the resolver story straight to the client gonna get an answer right you\u0027re measuring the response time from resolver to authoritative from sizing the TTL would not influence the the rtt if you\u0027re measuring resolver tative yes so we are assuming that what actually made the response will be higher was right of the traffic not the TTL because TTL of 0 increases traffic and we assumed that it was so high that response did I went up I don\u0027t know okay okay and anyway the the point of the graph on our plots is rather to show that we have software for finding the trends in automatic way not ready to drill down into these problems because it\u0027s premier network all right but thanks for interesting work thank you thanks so much guys so next up we have Jason Sanchez talking to us about a DNS compliant status of resolvers if you\u0027re a first time I so welcome yeah there\u0027s my first a IDF and thanks for giving me the opportunity to present here my name is Jason Sanchez I\u0027m a master\u0027s student from the University of Chile in the electrical engineering department and I\u0027m going to present the Iranians compliance data so in different results before they eat DNS flag they on after the NS like the well some background first what is it ENS are the extension mechanisms and to the play new futures in the DNS protocol what kind of futures may extra data space for additional flags and also a response code also to send message larger than 512 byte and among other features what is the problem with a DNS there is some in the operational world there is some work around as you know and also "
  },
  {
    "startTime": "00:55:19",
    "text": "there is a prolific walls that block invalid traffic and some authoritative servers block the response or answer with a gram packet and what that means and the EDA the result birds have to send again the query away from a timeout and all this is by the body inflammation inflammation of DNS that not following the standards the solution was provided by the DNS provided the most common providers around the world this day is to remove all the workaround was to remote all the workaround was the 1st of February of this year and I\u0027m going to show you some results before the Status Bay before the DNS Flag Day and after the deer slug day ok I\u0027m going to show you the energy status of 19 and 19,000 and 61 resolvers we develop an algorithm to test design in forest age this is was made by dick tool and also on tool developed by human hook called by dick my dick is a tool when you can send some option codes to test the different extensions it\u0027s test was performing 3 child before the dns flag day and two times after readiness like day two as mode the network failures like timeout and others the test number one and second some of the tests in number one and second stage was base it is based on the DNS up draft a common operational problems in DNS servers that is and an advanced version well the first stage we\u0027d get some information about of the DNS version of the servers also with so bad as the ethernet support this is the results of the server version the most common is the pine inflammation it by an implementation also we can see here the ideon is our this is the comment made by dick and you can see that the most common response codes the final by the RFC 10 and 35 you can see that the most common response code here is no Aurora CO our code also we can see that the OP code according to the draft the most they recommended a week we can we have to see the option code in the additional section and the IDNs version number 0 in the in the answer according to the algorithm classification we can see that the most of the resolvers has content with the alienness version 0 but not complete is d2 for instance we don\u0027t see the option code in additional section but also we can see that almost 9,000 or soldiers has a complete compliance with the ad with the draft also we can see "
  },
  {
    "startTime": "00:58:22",
    "text": "the average of all servers by time out here and the second stage we can see they did a test for the ideon is version number one and also that is basic text based on DNA SEC DNS esic the first test here was Adina\u0027s number one we expected an bad version according to the draft and also we expect an option code in additional section and the version number zero well not our role of the results are according to the draft as you can see here the bad version only 3,500 for soldier responds according to the draft also we test the DNS SEC this is a simple command to send here but all not at all of the resolvers has the flag do DNS SEC okay and also we expect this flag if they are are sick are in the answer this is the classification of the organ that we will up almost 10,000 has okay the response with the the old number one flag and almost 400 thousand four thousand result God has they know to flag here the terror stage was the to taste different extensions I\u0027m going to present two of that for the time one of these is a chain Corinne DNF in DNS that is how experimental RFC think worry is today is to deploy the DNS like a client validation and you can test by the pipe tool we expect here no error code and also another flags this is made by a simple or one TCP session or two supposition according to the implementation as you can see almost the result bars are no compliance with this extension is so new also with this the client subnet is the result for this extension and the digital solvers are chilean resolver you can see that it is not content with the most common extensions but the main idea is made a comparison between the D\u0026S flag day before after the DNS like the complains and after the DNS like they this is a comparison of the minimal it in ears after shoulders we can\u0027t see that then no error code increase in some of these servers and I believe that is so important because most of the result worse has a increase of according to the draft that I talked before and I believe "
  },
  {
    "startTime": "01:01:24",
    "text": "that is a barrier well we can check here that almost 10,000 of the servers increase the network or here in the chain query the before the Green Line is there before the DNS like the dark line is there after that NSLog day also this is a for the client subnet and also on to finally is they recognize the server recognize the opcodes this is a comparison before the DNS logon and after almost all the servers increase the number of the recognitions and this is a classification for the algorithms and this stain almost all the resolvers has warnings to recognize the opcode and all the queries that we sent and thinks this is some of the testing this repository and that this is my email thanks thanks a lot I expect from the ikat power deenis three things the first few slides I see you testing with plus no rank that might explain a big chunk of the refused you\u0027re seeing which means you\u0027re not testing it enos there but testing that\u0027s a resolver is not allowing you to snoop the cache way us second the crazy did after that there\u0027s the chain query and subnet etc out refused you see make me very sad okay thirteen I have a big stack of DNS flagged a survivor ticket stickers anybody that want them I\u0027ll be over there ok thank you john reed ACMA i noticed early on the slide for example you identified various resolver versions to the extent that they answered was there any further breakdown you can do of correlating this type of behavior either with specific resolver vendors or with specific other cohorts that might appear such as consumer ISPs business ISPs things you know any further correlation or breakdown you can do with that yeah we could with we\u0027d get some information about it on all these about I showed some results here we recover all all the resolvers by by a ACN or by the some kind of version we have we get some information basic information about that but we we treat all in in in the same group with on we don\u0027t split the test here okay other raw data available there or anywhere else yeah okay yeah I think I tell one thing about the refused answers also unfortunately the DNS is kind of overloaded what the meaning refused is and like in the case of clients oven that it was originally specified to specifically say we\u0027re not gonna do client seven that with you but feel free to come back with a you know an online subnet query which is a real "
  },
  {
    "startTime": "01:04:26",
    "text": "downside of way ECS is implemented but there\u0027s a lot you really can\u0027t tell about refuse is currently given and on a similar note I don\u0027t think and somebody might correct me but I don\u0027t think any of the major ease all over I need a major resolver vendors currently implement change weary it\u0027s been passed as an RFC but actually doesn\u0027t have working code out there and then I had one final remark Andrews have you looked at Mark Andrews work on testing also eg MS compliance oh okay maybe bring an update to the mating des store at some point if you have updated data yeah yeah I know having their repository yet but if you want to you can touch it with me and I content they are data you can also use our mailing list if you want to provide some more information so thank you okay thinking Thank You Mira okay next is lunar for two presentations actually we start with Stifler so [Music] myself Rona Barrett from University of Oslo I\u0027ll be presenting about how the dscp impact on latency due to under using the Red Cloud box still next okay so here we see that wave at is you would like to use the DHCP code points and we need to know whether it will work or not there first we need to investigate whether the DHCP code point survives in the internet so these previous stories okay so the previous study shows that yes the DHCP could point survives in the internet most often so to do the tester we use the code points yes 1f 42 and EF so here is the plot box so our hypothesis hypothesis is the DHCP supporting routers the DHCP the supporting routers okay sorry the DHCP supporting routers would have impact on the latency during congestion so so first we need to create congestion on the internet path so before that we need to measure how many hops the DHCP cold point survives so here it is heads so we need to measure that first after that we say a pro packets we send a set of per UDP packets larger sized packets with the CS is 0 and we said the TTL hits plus 2 and all the packet will be destined to that it will never end up in the destination IP address so will not "
  },
  {
    "startTime": "01:07:27",
    "text": "get any complain that will be fine then the next after this probe we sent says set up monitoring packets those monitoring packets take their TTL limited and take cs0 and other da good point and all at once so such that we can see the impact then we follow the procedure a few times to create enough congestion and consistent results so let me tell you how this works here how we get the impact if the router support DHCP code points so let us say the router r1 and r2 o in the these TTL limited packets will erase r1 they will fire the TTL so they will generate a time exceeded messages so same thing goes for router r2 then using this formula like basically what we are doing is that take the so we are sending UDP packets with a good point or cs0 and in return we are getting ICMP time exceeded message and we are computing the RTD from that and we take the difference between the two terms and with respect to their code points with the basis yes zero and we compute the link delay due to the difficult point okay so to do a bigger large-scale internet measurements so we took ec2 my sins six in situ machine in different countries then we had twelve non eight nodes in different countries like China Korea Germany and a nine in Norway so these our Ventus points to Ron and we have nearly 1 Gbps link speed provided so to decide the destination the targets so we took 50 mm IP addresses and 1 IP 4 s these SS mostly covered transit or s access SS content providers enterprise and classified by the Qaeda so from these 52,000 IPs we don\u0027t applaud box with respect to 10000 we don\u0027t want to create enough congestion for a long time so we randomly took 10,000 and we did them as from this measurement we got nearly hundred thousand-plus unique links and picked it to a 50,000 our non links because we can see that we can see that they are not conjugated and with this measurement you Travis 200,000 Internet unique internet paths ok let me tell you the bad thing where we see so only 1% of the links what we see has a negative impact that means they increase the "
  },
  {
    "startTime": "01:10:27",
    "text": "delay due to the code points and these lines these are the so ok this is these are mostly covered that means it could be like the those SS where the links are present they may not like your code point so that\u0027s why they have they may have a negative impact and we can a difference it and because they are overlapping but what is the good thing is we see 2% latins improvement so and it clearly says is a source that EF so basically this is the distribution with a more than one millisecond and these these links are the median delays so what the distribution of EF will actually disappear ight so according to the wave RPC Draft we can see also from these results that EF performs better than f42 and better than CS 1 next so yes then we focus more towards the EF links EF improvement links so what we see is that over you can see nearly 30 to 40% of the links here we saw EF gives at least one millisecond improvement but to CS 1 and F 42 almost give zero improvement so yeah next so after that what we did is that these reactive links then we studied their SS which SS they belong to so then we plotted the number of dscp affecting links for SS what we can see is that at least 60% of the aass where at least one or they have only one dscp your reactive links they improve the latency of due to the core point yes and we can see at least nearly 10% top-10 at least is six reactive links next so then we studied different aces and analyzed so we classified these links as whether their ingress link or egress link or in the network of that AAS so we mean ingress link means so from another a s2 that router on the traffic comes then that router connected to that link and in the apex it improves the latency so that is the ingress link then egress when both routers of the link belongs to different SS so what do we see then what we did is that then we classified under and we see that will it is a China net backbone we see maximum number of EF reactive links and the important point of this slide is to saw that we can see even due to the asipi code points we see an improvement "
  },
  {
    "startTime": "01:13:28",
    "text": "of 54 millisecond so in global tell to me telecom and we can see for example china china net backbone we have zero ingress improvement but rest we see some improvement in the latency next so this result is recent and the 1.5 years before we had another measurement using our platform playing platform where we are trying to see we observe their DHCP policy from different ESS so there we observe that so basically that\u0027s why we compared these results what we see in that the previous results is that nearly in a level 3 has 40% remarking policy and Tahlia 60% codes and almost all it a rematch the DHCP and little happens and ingress there is a 0 in the egress and in network and in the broad net nearly elite list remarking and we can see we can confirm can see that with the star mark represents where the DHCP policy we observed and these are the latins improvement that we see we can see in the quotient so we can see in the improvement in ingress but not in the ingress and egress and that is all about DHCP measurements and what do we what we can see is that yes there is the opportunity opportunity stick benefit from the DHCP code points that way but is it could get yes thank you John linka I just not sure how you define links between I asses because if two routers belong to different ISS and it\u0027s direct period you might see both routers have an IP addresses from one of the is right because the using address space belong into one of them how to address that pit in pink and you don\u0027t know if it\u0027s going to be the next is of previous ayahs in the past yes we see some cases like that nah so basically I see the link that could be either from internet access points or just are different yeah but I if twice I\u0027ll just have a private Piron can see really hard to tell from my experience if it\u0027s this link number from address space of the is number one OS number two so you don\u0027t know if it\u0027s a lost Lean Cuisine is or the first link outside of it okay go quick question you talked about ca0 on the first slide and then cs1 later did "
  },
  {
    "startTime": "01:16:29",
    "text": "you do any other test with cs0 default no yes you said zero so all the Latin see improvement is compared with respect to CSS 0 so you get improvement for using CS 1 over C s0 yes that\u0027s fun maybe that\u0027s why we\u0027re defining in la PHP so that\u0027s an interesting data point thank you all right so we\u0027re ready to switch to her in a second presentation and we just got 13 minutes left in the session okay so I\u0027ll be talking about so what I did is this is the the dog is all about the different can we use the native DHCP sorry native protocols so yeah so here what we are doing is that we are studying SCT PDC CPU repo light and we use their transport checksum to see how the nut boxes home nuts react to them and for HTTP we use also v-tach so what it is is that we call we had a local test web sorry so of self equipment test then we studied about how the net filter in the Leno\u0027s not functionality then we studied IP APF and IPFW in freebsd for the notching then what we learnt from this study so these devices we collected from different places in Norway mostly in Oslo from their home net like from the colleagues so what do we see you see is that these are different kinds of network vendors and we see check there persons of OS like supporting Lee North\u0027s kernel from 2.4 to 4.4 some not devices of put red X and px works and the some devices contain open wrt so so here is the test measurements that we did we tried to test to it the UDP with a zero checksum and DC CPI CT PUD polite with your known protocols and in the right side we have observations so what we see is that UDP 0 checksum remains 0 takes on there is no computation of checksum so that is your efficient one we just mention them then another is observation can be some devices could do simply nothing IP level acting no touching of transport layer and or song did no nothing if they see your non protocols some simply drop the protocol packet if they don\u0027t understand the protocol so what we see is that you dip u20s exempt remains intact that\u0027s a good thing BX walks drops the new protocol packets "
  },
  {
    "startTime": "01:19:32",
    "text": "if you send SCTP packets it will drop it Jennsen top comments on dealing devices do know nothing if it is unknown protocol and so SCTP is a success because it can get it it does not have to change in the transport layer so it is simply does nothing so that is a benefit for SCTP but not for DC CP and UDP light who need to change the checksum in the transport layer so here is the net filter in Linux we started Linux percent 3.18 in the tp-link using open wrt so so what we see is that the nut module in Linux it has two components contract root 2x and not proto x this is the design part of that so X is could be a CT PDC CP or you\u0027d applied so the contract pro 2x it is responsible for nothing and then transport head or verification but it can update the transport layer and it can it fails if there is a put Collies and it does not know how to handle the transport layer but it can decide the entry in the buffer but not proto is responsible for changing the proto put napping or updating the checksum so what we see is that Linux supports a City PD CCP and they work normally sorry this is venue to be they work normally but HTTP there is a problem so HTTP it should not change the transport layer put number but Linux not filter it all changes the put number if there is a collision and if there is another variable that NF contract checksum that is by default it is zero and it takes the checksum for incoming packets so let us say UDP within the wrong checksum and if the this variable is enabled maybe the packet will be dropped in day by the nut box so but the good thing is that it is variable zero because it in its extra computation so nobody wants to enable it maybe so the next thing is we studied FreeBSD and in previously we 11.2 and we studied these firewalls and we see that there is no support for DC CP and UDP light in the not module only IPFW analyzer layer supports SEF implementation so what do we see in the previous implementation UDP with so what we tried is that so the multiple clients try to use the same nut and observe their behavior so for example you dip with the zero checksum we see for all firewalls the same behavior so you repeat zero take some remains intact that\u0027s a good thing the second is this is a P and a UD polite so something it will be draw wrong is happening like in IPFW it handles like it it does for the "
  },
  {
    "startTime": "01:22:33",
    "text": "later clients in the IPFW does like here nothing but there is a star which says that the packet will be forward to the last client so that means when two clients are trying to use the DCP protocol so who in the rest so the first will Sox is succeed the second when the packet comes it will override the it will actually do it will override so once it will override the holder spawns for the pass packet later so luckily comes back then it will give to the second client later client because the entry has been updated so it is a bad thing so so these are different anomalies that we see in disappear or if it is on one protocol the wait handles but SCTP that is a good thing that it works well both two clients will succeed and it of this basically we tagged it does not change the put number so we can use actual SCTP not module for the SCTP deployment like any not supporting acid HTTP so we can use setp so so the coming to the point that we see we observe that IP label SE tepees IP level nothing so it can work at least for one client it will work but for new clients it will create a problem so to see that we those devices we hide we try to we did a simple measurement to find the idle time how much time the new client has to wait for a succeed in connection if the first client is not active so what do we see is that at least some of the device will give a smaller smaller for example less than 200 seconds some are giving like normally most of the device give 10 minutes which is a bad thing but at least this is the current scenario that we see from these and not the devices and what we can or what we learnt from this study in measurement studies that configuring the device so basically configuring so putting the device I think the checksum verification or new protocol so what it remains is that if you are using IPS you if you are using option TCP option that is better than using some other mechanism or new protocol so better to use options rather than new protocol so then emulate ud polite we can emulate you deep with the zero checksum for you replied so basically what you can do it that in the UDP you can put zero checksum and application header can put the checksum so we can handle that so then native protocols are not by "
  },
  {
    "startTime": "01:25:35",
    "text": "default so it is good to support because most of the protocol DC CP and people IDF is more interested of deploying these things so it is they are not deployed but sorry not enabled in the devices so it is better to enable them and then the pseudo header is a bad thing because it in any has a dependency over the IP layer so better to remove not to consider the pseudo header then to support multihoming individual connections should look like a single home connection so for example HTTP the Linux the way it is changes it is objects the put number so then individual connection of the same Association to different nuts can be can be considered independently so it will be okay if the client and server can handle both the sites all right correctly so that is the thing we learnt from this local measurement study and from the different and not modules thank you yes right Coco easy question on slide 8 what was the bottom value of that of that graph it didn\u0027t look like it was zero indexed Oh slide 8 no go for that one yes your 200 seconds and then what is DL 5 that ok these are the devices that I put in the faster slide and they are the time out from there so is that like a hundred seconds or four year so one time over year five it could be 30 or 40 I think yet I think 30 30 second around okay yeah thank you that\u0027s very important yeah got boyfriend\u0027s quick question on they check something I quite like pseudo header checksums did you try the CCO trick that was presented in mapache before to see if that helped your checks on traversal huge echo sorry so I did not test the CCO thanks but it\u0027s so study that if the sister that if I saw sorry yeah this variable I think in your last presentation last idea of some some devices that so that dscp packets are dropping with bad checks some basically these it by this variable who was enabled in those devices I\u0027ll Chuck offline thank you okay thank you very much Runa that makes it wrapping up just in time thank you for everybody who presented brought data and had discussion we used "
  },
  {
    "startTime": "01:28:37",
    "text": "to have another presentation that we try to squeeze in at the agenda about quick migration but this presentation already happened in quick - Jessica I believe so and like we didn\u0027t want to cut out other presentations in that sense but you can look at the recordings in the quick meeting the slides are also online on the mapper Qi agender and we put a link also in our wiki where we like note all the presentations so you have everything at once so please do that Sheree is also here if you have questions for her if you want to stand up and show your face say if you look at the slides and recording talk to her thank you everybody that\u0027s it we will see each other in Montrell again and most important we will also have a haircut to table in Montreal again so please join us there bring dater and thank you [Music] [Music] "
  }
]