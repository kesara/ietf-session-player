[
  {
    "startTime": "00:00:04",
    "text": "Okay. It's now the last session on Thursday in the twilight of IATF 117. Welcome to the HTTP session. we have just an hour today. We have a fairly tight agenda, so let's get into it. We have ascribed. Thank you, Travis. I'm not going to display the note well because it does strange things to my machine to share a screen, But, hopefully, by now, in this point of the week, you're all familiar with the IETF note well. which talks about things like what appropriate behavior is, harassment procedures, intellectual property procedures, and so forth. They are important policies. So if you're not familiar with them, please search for IETF Notewell your favorite Internet search engine. As some people I can see are doing, thank you very much please log in to this session so that we have a record of who's here. You can do that by scanning the QR code on the screens. And following the directions, you have to log in to the data tracker and so forth. We also should have a physical blue sheet that allowed you to do that, but I'm seeing it anywhere. Oh, there it is. If folks could pass that around, that'd be great. Thank you, Jonathan. I test. Excuse me. So our agenda for today, we are talking about things that are not yet adopted drafts in this script That's the entirety of the agenda. And first, we'll talk about secondary certificate authentication for HTTP servers, which is a topic we've we've had drafts actually active in the group of that before, but we've them. And this is shaving that down to build maybe a little more manageable, so we'll talk about that. We have a compression dictionary transport which, again, is a topic we've talked about before and we're returning to it. So we'll have a a discussion of that. And then we have a discussion of availability, hence, which is another tilt at a problem we've tried before. have a theme for the day I just realized. So we'll go through that."
  },
  {
    "startTime": "00:02:00",
    "text": "And finally, now there are at the end, 2 new things 2 new drafts, which have not been discussed before in any form at all So although one of them is actually quite never mind. you get the point. We do have one talked from last time that we didn't get to do we want do we want and sorry. Depending on time availability, we can bash that in. I I think so. Yeah. Let's try let's try to aim for that. first of all, she Shivana, Yes. It's easier. Oh. Oh, Good. Good. Good. Lovely. Okay. We'll we'll try and do that then. Yep. Great. Alright. Stay on the line. get started. secondary certificates. Eric, please come up. and you can request permission to share slides. can approve it we Yep. You heard granted, dope. Which is the I I just, like, click something. Okay. Yep. Yep. Okay. just scroll down until you find yours. Great. Great. Thank you. Hello, everybody. Put your exported authenticator enthusiast hats on because Today, we are going to be talking about an individual draft authored by myself and Mike Bishop. Secondary certificate author authentication of ACP servers. So if we're some background, for those of you who have not read the draft. or maybe have, but This is based on an older draft that the working group has previously discussed. that being secondary certificate authentication for httP2. I will go over the main differences between this draft and the other one in a few minutes, but as a sort of summary of what this draft is"
  },
  {
    "startTime": "00:04:01",
    "text": "essentially discussing TLS export authenticators, which RC 9261 allow the ability to send and receive X Five Nine certificates at the application layer. The proposal that is being made for this draft is defining support for HP 2 and HP 3 servers. specifically to send secondary certificates to clients and make themselves authoritative for additional origins. and the way that essentially do that is a new frame type on stream 0 or the control stream that carries the exported authenticators. And I'll give the obligatory. This doesn't work for hvone, so we're just gonna pretend that Each one doesn't exist. So to better illustrate kind of how this works, I've but 2 diagrams that sort of show you know, the sort of flow that you can kind of expect using exported authenticators? in this way. In this case, we have a client connecting to server dot example. server dot example is this server is also authoritative for server 1 dot example and server 2 dot example. rather than including those origins in the initial certificate on the initial TLS handshake, we can see that know, after we make a get request, the server decides that it wants to send these unprompted certificates. out prior to sending the okay and then the client assuming that it trusts trust those certs. It can then make a get request out to of those organs. Jonathan. doesn't the server need to also send an origin frame? The so the ACP or the the origin spec would suggest that you would send an origin frame and one of the things that I probably should have done for these diagrams is actually include an origin framing sent. And, also, discuss in more detailed usage of the origin frame in the actual dressed, draft, So that is something that will be kept in mind."
  },
  {
    "startTime": "00:06:04",
    "text": "So, yeah, either include the origin frame or include the IP DNS track. I don't think it's the one Thank you. Mike. Mike Bishop. So technically, maybe not. there. get around the DNS check, you probably want something To although In the updated version? graphs of HCP, we say that authority is established by the strategic of it. certificate and Anything else is browser's choices to how to decide whether to trust But but Let's get the certificate first. K. I just so as a reminder, we're talking about adoption here. So let's not get too much into details because we are time constrained. Yep. I will. So This is a more interesting use case, which is maybe new. that I will get into Alright. I'd we'll explain here. you can do with this mechanism as well is that you can actually integrate this with forward proxy. So we kind of have, like, a hybrid proxy scenario where we have a proxy that is both a forward proxy and a reverse proxy. So in this case, We have a relay which forwards requests to server dot example, but caches or and or reverse proxy server 1 dot example and server 2 dot example. we can see we send a connect request. The relay sends a certificate for the for the origins that it can serve directly. And so when we make a request out out out out to one of those origins, we can actually send to get directly. So why do we want this? Well, connectionary use is important. This helps servers that host content for multiple origins. Obviously, instead of sending, like, gigantic cruise liner certificate, that continue to a whole boatload of Origins. They can much more granularly control. The"
  },
  {
    "startTime": "00:08:04",
    "text": "set the origins that they actually support. This is useful for reverse proxies as well, obviously. And sort of elaborating on this, we have this hybrid proxies example. where you can have forward proxies like mask, which can actually also reverse proxy a subset of origin for performance and load balancing benefits. There's also a privacy and security angle here. So one use case that can be imagined there is servers could make particular origins only accessible for certain users. You could combine this with client authentication mechanisms like you know, unprompted off, which is also you know, in the works with the working group to essentially know, make it so that origins are only accessible you or even visible to already authenticated users, like users who are in the know and not excluded. So that is that is a interesting, maybe, useful, maybe not use case, but So what has changed? Why are we talking about this again? A few things have changed. Firstly, the previous draft only defines sport for H2, and just understandable. h three wasn't entirely round. So this draft defined sport for H2 And H3. Importantly, this draft only includes support for unprompted server authentication, We've scoped down this draft pretty significantly based on implementation. to kind of reduce the sort of complexity My understanding with the old draft is that one of the reasons it wasn't able to make it through was because it ended up in overly complex. So It seems like we want to be careful about scoping this too far. If we wanted to standardize it, maybe split things out into multiple drafts or RRCs. if it was desired to standardize other mechanisms that were related to exported authenticators."
  },
  {
    "startTime": "00:10:04",
    "text": "export or export authenticators are also now an RC. They weren't before. back when the draft was last being worked on, which helps So it's more richer. And there's implementation interest Apple has recently been exploring uses for this mechanism as far as it pertains to relays in reverse proxies. Yep. How many more slides do you have? Shall I wait till the end? Probably. Okay. Okay. Because so was going to just kind of give a preview of some of the open issues here, and we can I'm run over these sort of relatively quickly. And if there are any that are interesting to discuss you know, then we can sort of discuss them afterward. But because this has not yet been formally adopted by the working group, I thought I would just run over them. there's been some debate about the settings parameter, whether it is needed, draft currently defines a settings parameter. to advertise support for this mechanism. the The new frame type would be drop by non supported clients as far as, like, if you know, there's not an necessarily an issue with sending a certificate to a client that didn't know about it. from that perspective. we've discussed also renaming the certificate to server certificate if that reduces confusion especially if client authentication ends up coming back into the fray, but maybe as a separate draft. there would be less confusion there. There is a good reason to have a setting, which is that servers might wanna behave differently for that are not known as working mechanism. There is this case where, you know, clients that use origin frames, the scope coalescing that don't know about certificate frames might drop them and then be looking at an origin frame that contains a name that they foreseen before. So"
  },
  {
    "startTime": "00:12:00",
    "text": "know, might not be a huge problem, but it is just something that you know, might be worth considering. another issue that has been discussed is client prompting of certificates from the server rather than just unprompted certificates from the server. This ends up adding a lot of complexity. There are some good reasons for it. Mainly that origin frames have, like, less overhead and more clear semantics, but it's likely miss, servers that would be doing this would be sending both an origin frame. and the secondary certificate. But The amount of complexity and, like, potential, like, privacy concerns that this introduces may make it better as a separate discussion and not to derail this work. And then the last sort of main open issue right now This is like less of a discussion point. is that the draft is currently h3centric most of the experimentation has been done with htp3 and as a result the frame size limits of h 2 kind of ended up as a sort of afterthought, but we will like, export to authenticators end up being large enough, that it would be necessary for over h two to send them in multiple frames. meaning that there does need to be a mechanism to sort of gather the authenticator fragments There are a number of different ways that that can be done one of them was to reintroduce a certificate ID. which, you know, you could use to sort of correlate multiple fragments coming in. this the cert ID could be useful for some other things, like, if there are weird schemes where the server and client both need to be in full agreement about which certificate is used for request response. that you could use the cert ID, but you know, just a just worth considering there."
  },
  {
    "startTime": "00:14:02",
    "text": "So closing up here, Having a clear focus on, I think, the unprompted server authentication is a good starting point to help us get the ball rolling. partially because now there is actual implementation interest and there are things clients and the servers, which are capable of doing this. thinking that as far as under like, managing the scope of work like this it would make more sense to kind of drive this based on implementation experience and to determine what works and what doesn't work. So with that, the final remark is that this work, is looking for adoption in each of these. So before, Jonathan, we our time constrained. We have about 2 minutes left on this. So let's focus discussion on adoption. not the details. Very good. Please. So I I'm obviously pushing in the other direction. I want to get this working for clients as well as So There's definitely interest junk to Northern Clapler. There's definitely interest from both sides as in doing client stuff and doing server stuff. I do agree that we should try and keep the scope simplest possible, But I I think clients getting rid of the client stuff would be annoying. Can we have the slides back again? Sorry. The you said something about a certificate ID. You Why doesn't that just link to the certificate request context or how you populate this ticket. Does this affected whether we adopt it, Yeah. Okay. Whether it's a good idea or not. And then Yeah. Okay. And then can you go back one slide again? We can revise that later. Yeah. The it's being adopted as a starting point. Not all the ideas. Yeah. I we I wanna do this, but only if we can do client side stuff. Thank you. Yeah. Okay. Alright."
  },
  {
    "startTime": "00:16:01",
    "text": "I will say I do think that there is an argument to be made to do client search in a separate document. just because there is no reason why these mechanisms cannot be separated between server and client Yeah. Authentication. Alessandro Goudini Cloudflare. We were interested last time around. We actually implemented them deployed their old drafts. and then nobody used it. But but we're still interested. And you know, there is the new mask sort of use case now, which is also sort of interesting for us as well. Mike Bishop with a process question for the chairs. we already adopted secondary certificates. It was moved to dead, but Does that mean it needs to be adopted again, or is this really just reviving the to draft. I think we'd want to see significant interest in the group before. trying again. Yeah. Watson? Watson led Akamai. I think this is a good idea. I implement, last time an older job, I think it's still good. Let's move it forward. Cool. to a home or just any list? We can do a quick show of hands, I think. Yeah. Lucas is also went into the queue. One sure thing. I allowed to speak or no? Yes. Yeah. Okay. I'm Lucas. Hi. I just put it let's do it. You know, I'd one of the questions we probably should think about is was this kind of level of interest before. but it failed to actually take off. We didn't get implementation, especially from clients. So we we've had interest from server side. If we got any interest from the browser side, Yes. That's the Good question. So Tommy has put in a a poll. If you're interested in adopting this as a starting point for a reviving secondary certificate, raise your hand, If you're not, please do not raise your hand."
  },
  {
    "startTime": "00:18:01",
    "text": "And if you have no opinion, you don't have to do anything. Okay. So we see significant numbers raising hands. we see a few saying no. If any of the folks who did not explicitly raise their hand there you'd like to say why you don't think we should adopt this, that would be helpful. Anyone? Okay. Martin. I didn't Go either way. but I would like to see a little more time spent Just just clarifying exactly what the use cases are. on this one and making sure that we're confident that we don't need something more And what's in here? So I think we should continue to -- A a little bit of discussion on that because there were there was a lot of back and forth about whether we have requests for these things, how they're linked into requests, and all of those sorts of other things. It may just be very, very simple. which would be great. I'd I'd like a just a tiny bit more clarity on that before we just jump into adoption. Okay. Thank you. Thanks. And for the notes, that was 20 and 20 raised and 4 did not. Right. Right. Right. Right. Thank you. Thank you, everyone. Patrick. You wanna share your own slides? Looking it here, I can just do next slide. Oh, yes. I I can graduate. Make a request. I don't know. Machine in the glasses. Great. Right. And I feel a little bit like Guaimi Coyote standing on the x, but Alrighty. So this is another attempt at using shared compression dictionaries history on a connection for compressing future requests."
  },
  {
    "startTime": "00:20:03",
    "text": "Previous attempts. We've tried this at least 2 or 3 times so far. They've all failed on the privacy and security boundaries and leaking information across privacy boundaries that mostly browsers cared about. But I think hopefully, we have a solution to that now. The main use cases that we're targeting right now are a previous version of a resource being used as a dictionary for a future version say the YouTube JS player, for example, it's a version JS file at some path. updates a few times a week. can be used as a dictionary if you've already downloaded it for a compressed version of another version of the file added new URL. But all negotiated with the client and the client cache for people that have already visited. And if you want to build a custom dictionary for your either HTML pages or JSAPI, that has all of the common template stuff in it. you can side load a dictionary as part of your navigation that can then be used in future requests for that type of content. So it has 2 parts This first part's kind of optional in the the actual compression scheme, but the the way a resource is advertised as being available to be used as a dick for future requests is it gets a user's dictionary response header on the initial request and it includes a few things as structured field params Probably, the most important one is a match that gives you an URL pattern for the types of requests that are same origin that the dictionary was served from that can use this dictionary for future requests. if the client so chooses. It also has a time to live for how long the dictionary is allowed to be used for independent cash lifetime of the resource. in case you want to have minimize the the variance"
  },
  {
    "startTime": "00:22:02",
    "text": "that are out there for your caches. And then there's some another optional one, which is the the hash algorithms that are supported by the origin for the the hashes that can be advertised for the dictionaries. the hashes and the reason will be coming up. The client so Once the client has the dictionary, that it thinks it can use for a given request either prenegotiated out of band if you have a native client or something like that or if you're just calling APIs. or if you had a user's dictionary request that came from a previous session, that you've got on disk somewhere. The client advertises the dictionary a single that it would like to use for the given request. in a sec available dictionary header, and it passes the the hash currently a shah 256 hash of the contents of the dick that it has available on the request so that if the server has a matching cache for the dictionary or a version of the resource that was already compressed. with that dictionary, you can just serve that one as a delta instead of serving the full resource. We currently have 2 encodings that we've extended for Broadly and Z Standard. We just you know, we're really creative. We added a dash d to the end of the the content encoding for each of those. It's not limited to those those are the only ones that we have that we know of right now. We're anticipating at some point in the future, code aware, WASM aware, compression schemes that can do better delta compression than just using Broadlier z Standard. And this is flexible because it just really negotiates the dictionary itself The compression is negotiated separately. so if the server has the hash either for a dynamic compression dictionary or a static resource already compressed. It just says, that uses the content encoding for the type of compression it used either broadly or Z Standard. And really importantly, it adds a very on the accepting coding in available dictionary to make sure that there's no"
  },
  {
    "startTime": "00:24:04",
    "text": "cash collisions for the resource and serving delta encoded resources to clients that can't understand it. The main things that we've sort of negotiated and worked around here is that The privacy protections are all client managed instead of trying to do it at support protocol. transparently. The client has to decide that it wants to use a dictionary for a given request. And in the browser case, at least all of the dictionaries are partitioned with caches and cookies. They're assumed to be private information, that belongs to the the session and the partition that the browser currently navigating within. They're cleared whenever cache or cookies are cleared. And probably most importantly, they're only used for non opaque requests. We are assuming that an oracle attack will be able to reveal the content something that's delta encoded. So we're just going to completely that and say, we're only gonna use it when you can read the contents of the response anyway. which happens to work out in most of the cases that we want to use it for anyway. We can mostly do this entirely on the client's side where the client just won't bother requesting addiction generic compression. If it knows it's going to be an opaque request. one exception to that is, you know, sort of the bane of our existence, cores. had to leak cores into this a little bit. for servers that are concerned about maybe accidentally leaking some private information, the server must not send or use dictionary compression when it sees a course request and the access control allow origin response header either doesn't match the origin or is a wild card which means it's probably going to be an opaque request when it hits the client. We'll still do still do blocking on the client's side, and we'll fail the request. I'm"
  },
  {
    "startTime": "00:26:02",
    "text": "but there is a chance that a timing attack could still happen if the server doesn't fail these or just not use a dictionary in these cases. So I tried to keep it as clean from browser stuff as possible, and try and keep it as much protocol level as possible while still solving the security and privacy issues we've had before. practically, this is going in a form of this, we've I think we've used different content encodings into an origin trial in Chrome in 17 team to see the the developer feedback and see how well it works with developer workflow. We think we've got something that works relatively well. the JS deltas, they can just build that build time against prior versions of their JS libraries and have the artifacts already ready. The dynamic compression, we've got tools that can build dictionaries off of a wide selection of URLs. So, hopefully and we have a few internal clients. We're working on external clients as well. just to make sure. But, yes, largely We think it's something exciting. We think it belongs in this working group. We're hoping to get the group adopt it, to and then bike shed on the details. And Thank you. So we have talked about this before. We had reservations before. I guess the question is how does the group feel about this now? So Martin, you're in queue. Yeah. This looks like good work. we're we're supportive of at least finding out how how well it works with with developers and whether or not it fits in with workforce, I think the the delta thing seems pretty obviously easy to integrate into into various workflows. I'm I'm I'm you a little bit more nervous about the the other one, particularly with some of the details like patent matching and and what have you."
  },
  {
    "startTime": "00:28:01",
    "text": "I I don't really want patent matching in the stack. if at all possible. I think we did prefix matching in some in some places already, so maybe that's better. But we can talk about that in terms of details once we get into discussions in the working group I think the right way to proceed here is we can adopt this we see how the trials that you're running Tone out, turn And then at that point, we can make a a firmer decision as as to whether or not we're gonna continue on how we proceed with the But I I don't see any objection to to moving this in here. So so to be clear, you're not suggesting that we gate adoption on the trial the feedback from the trial. No. No. I I I think this is one of those ones where it it it it At least intuitively, there's there's at least something in here that will be very, very useful. there's the part that we're not sure about. wouldn't wanna ship anything until we were very certain. But, of course, that's not how we operate here. So I I think we're okay. Alright. And the reason we're doing an urgent trial is we can shut it down. get rid of anything that existed in case it didn't work. Allen from DelMETA. I'm relaying a comment from fila my colleague. He says, I just wanted to voice my support for the compression dictionary support and that I intend to implement it and that since the addition of wildcards make compatible with our URI scheme for static resources. It's not Alessandro Giddini Klotler. also interested in adopting and working on this. We've done some work in the past in this sort of general area. I guess, also, like, what Martin said, like, we need to figure out some, you know, but it's it's probably a good could, place to start. Lucas Lucas Pardo, I I've been"
  },
  {
    "startTime": "00:30:00",
    "text": "like, following this work, hasn't been presented here before. Like, the stuff you've been doing, and made some comments on the draft and the shape of the headers and all these kind of things. think the separation that you have from what's what's what belongs in this group and what is is not in this group is is correct. And if it if it isn't done here, then maybe you'll just go and try and do it elsewhere. And I think this group is the right place that has expertise are some thorny edges that we really should consider and yeah, it's I think this is Good. but I'll have some reservations about some aspects that are probably completely is resolvable if we give it the time and and focus that it needs. Yeah. I will say in the the main proposal in what WG, there are a few pieces that are like, HTML browser specific? Like, how do you trigger a side load of a a dictionary to be used for HTML and things like that that didn't feel appropriate for here. that we will do in, like, the fetch in HTML 6. that's very normal, I think. And but but what I like is it appears that this is usable by other uses of HTTP without having to rely upon that. So k. k. any any other comments, comments, comments, Okay. So I Thank you. Because I I know that a lot of people are very interested in the potential performance wins here, and it seems like you found a way to unblock the concerns that, you know, potentially like. we're we're we're in there. And we've gone through 2 or 3 internal privacy and security reviews, and they haven't crapped on it yet, which is kind of a first for one of these. doesn't mean no one will discover an attack that we haven't seen, but hoping that just by Ignoring the attackable content will Okay. Mhmm. Okay. we wanna do a a home or I mean, Okay? How do I do that? you Oh. Oh. Oh, well, that's that's Okay."
  },
  {
    "startTime": "00:32:01",
    "text": "say, well, while he's typing really quickly on the pattern matching we've seen cases where, like, the the version of resources are in the middle of the path and then you have, like, main dotjs or whatever at the end of the path. which is why we kinda needed at least a wildcard in the middle I'm trying to avoid going full Earl pattern, which is a browser thing that has regex and all of that kind of thing. So right now, the spec just has a wild cards on or asterisks is only in certain places and as simple as possible. Anyway, marks a quick typer. Yeah. It's a great typing class. Okay. We're we're seem to be getting good numbers that are raising steadily, and I don't see any do not raise hand that's notable. My inclination would be to take it to the list and doing a call for adoption. I thought we can talk about that and Awesome. Good night with you. Yeah. Thank you very much. 20 72 Yeah. 27zeron. minute exactly remaining. impressive, impressive, impressive, impressive, Okay. I don't wanna That stopped. Alright. Yep. share preloaded slides. do I stop the previous ones in this meeting? Oh, okay. I'm just gonna sit up here if you don't mind. This is me, not not chair. So i. So I'm gonna talk a bit about a draft that I had on the agenda time, but we ran out of time. availability hints. Okay. So I don't think this is a surprise. Very is one of the most lamented, hated, misunderstood of mechanisms in HTTP. I I maintain that as specified. It is"
  },
  {
    "startTime": "00:34:03",
    "text": "Not impossible to implement, but impractical to implement, but because it requires you to examine every stored response for a given URI to tell the right response to attorneys for a given request. And proxy cache is maybe a browser cache can do that. I don't know. Their performance is But yeah. But proxy caches, that's just no way. not gonna happen. So you have a lot of heuristics. You have a lot of invention and and a lot of in how different implementations handle vary. and that's not good, especially as we have things like client hints using very more. and we want to encourage use of variation more. So oh, I actually I can't press space. There we go. So our first try to address this was the key draft. and that was Roy and I. And we had this idea that could actually have a little language and a response header that explained how to create a cache key all responses with that URI. That went on for a little while. Excuse me. And what we kind of came to was the place that whilst this was very flexible and and had that kind of attractive you know, deterministic look. You just use this recipe to create a key. It was really complicated. It was really awkward, and it was kind of a foot gun for for people running websites because they had to get it right. And if they didn't get it right, It could break pretty spectacularly. Also, it was awkward because you had to fit this little language into all the different HTTP headers. Keeping in mind this was before structured headers. we didn't have that kind of or or even before retrofit. We didn't have that way to talk about this of headers that we might have now. Our second attempt was variance, which is now a parked or dead. I think we parked it. draft. And this took a different approach. This was saying that responses would describe the the different"
  },
  {
    "startTime": "00:36:00",
    "text": "axes that they varied on and what the different availabilities were that you could kinda construct by following an algorithm what the right thing to serve for any given request was based on its knowledge the the cache's knowledge of what was available. So This was much less foot gunny. I think that's a word. and and more declarative, which is kinda nice. But but know, the cache has to know about the individual header semantics, and you have to build into the cache. Oh, here's you know you know you know accept encoding. Here's, you know, all the different ways you can vary something, which there aren't that many. It's growing, so that's an questionable how how onerous that is. it still is kinda complex and nonintuitive, and you have to have these kinda weird variants and variant key headers. So Let me try again. Talk to people about this in the background a lot, and What's nice about this this new approach is availability hints is kind of extremely intuitive. It's it's just listing for each axes of of variation what your available representations are on the origin server. And then a cache can use that to figure out whether it needs to go back to the origin server work and serve something from cash. based on the client's preferences. And the nice thing is that information is actually useful for other things too. People have other use cases for describing what the different representations along a given access are. There there's one kind of of of degradation here as opposed to variants in that it doesn't contain information about relative preference between different axes. So, you know, if you have a gzip French versus a a a broad lead English, you know, it's hard to express a preference. between those two. But You do have a way to express things like what the default representation. if no preferences are set, which is nice. So I've talked to people about this. We talked to a little bit on the list. I still think it needs some development. It's it's it's still a bit hand wavy, but I think it's"
  },
  {
    "startTime": "00:38:00",
    "text": "more promising at this point than what we've talked about before, which to me says maybe it's time to know, make it our most recent attempt at this and hopefully third time's a charm. So what do fake folks think? I know that's that's inspiring confidence there right now. Nothing. I know. I know. It's not time yet. So I think this is I guess maybe the other question asked is I think this is a fairly important problem calls, Very is awful. The question is, you know, Is this some an interesting approach? I was under. Alesandro Gianni Cloudflare. Yeah. That that's what I was gonna say. Like, the problem is there, and we probably should solve it. It's just I I I haven't I'm not, like, a a, you know, HP expert about an So I don't know that this is the right solution. I guess, we can try and then see what happens. I don't know. that's that's kinda where I'm at. Yeah. Yeah. Yeah. Chris, Chris Levin's Comcast The yeah. This problem is really important. would be really good to solve it. the solution looks marginally better than the previous 2. I think it's probably technically possible to implement it. which is which is You know? You know? You know? You know? a good thing. A good thing. A good thing. would also like to see where this work goes. Right? No. Lomack? Okay. Sorry? same same thing. I I think my feelings are not necessarily different than those expressed. I think"
  },
  {
    "startTime": "00:40:02",
    "text": "I I'm kinda wanting to give it a try, but I'm not confident that the outcome is gonna be great. But, you know, if we don't try, it's, you know, never gonna get fixed. So Yeah. take it to the list? Yeah. Yeah. We can take you to the list. I'm happy to get a sense of the room. as well since we have time. So, yeah, raise your hand if you believe that third time's a charm or rather if we should adopt. availability hints as we should something discuss and work on him this group. give another 20 seconds that people Open Meet Echo on their phones. Yeah. Okay. Seems stable. 1, do you want raise hand? happy to hear from that person if they'd like to Speak Yeah. I'm just not very happy with the idea that we keep swinging and on this one. So you don't believe in 3rd time as a charm. I I don't know. Fun. I guess what I'm saying is that I'd like to see a little more evidence that this is this one's gonna work. If I can respond to that, I I agree. There wasn't it was pretty lukewarm here. Yeah. I agree. Right. Yeah. I I would It's easier to try and gather that evidence when you have a little bit of momentum, not a lot, but a little bit. adoption is one way to do that. I I there's also I I've had and I apologize. I I didn't do my"
  },
  {
    "startTime": "00:42:01",
    "text": "homework for the slides, but I know some folks in the Chrome team have other use cases for this kind of information that if we standardize the headers, it's still useful to them. A clear signal support from a clear signal of support from a a browser or a client of some sort -- Sure. -- would be significant here. No. Cash. Yes. Right then. Not a client. Okay. Cash. Whatever. Yeah. Yes. there are lots of cashes out here. I didn't hear much from them. And I didn't hear much from the the the client side caching people either. So Yeah. I don't think we took 2 swings and misses. I I think we took 2 swings and learned. mean, yeah, I mean, if if you're shooting in the dark, and and you and you don't hear it. you go, well, I won't shoot I won't shoot there again. Boy, it's dark in there, isn't it? Thompson. Yeah. I like it I think we're getting closer, but I don't know. And I'd like to say just more enthusiasm. That's all. I mean, to that point, It seemed like people were interested in having this as a starting point from a document standpoint, but for implementations. I guess, like, if you are a client or cash, and you would be interested in experimenting with this. as we are developing it. Could you raise your hand? to Okay. You'll you'll try it, Mark. Okay? I mean, like like like I've been talking to the Yeah. I I've been talking to the cloudflare cache team about this. they're interested in it, but, you know, I do I have it on their implementation schedule yet? No. Yeah. So It's like 3, three people. interpret. I mean, I'd I'd the most optimistic thing I say right now is this is kind of in the same state that early hints was when Kazuko first talked about it. I was like, oh, yeah. That seems like a good idea, but I'm not gonna implement anytime soon. And Yeah. Oh, no. because Zoho is gonna talk to me wrong. But yeah. Yeah. Sure. Yeah. Yeah."
  },
  {
    "startTime": "00:44:00",
    "text": "Okay. Alright. That's good. Who's that? No. No. No. No. No. No. No. No. Yeah. Okay. I I I can make this quick. So I also wanted to talk about a a couple of caching spec. that I've been working on the background for a while. and talking to a lot of folks about. Now I can press space. So first, I wanna motivate this. and and all the work that I'm doing here. a large reason why I worked on things like Charter Cash Control And CDN Loop. and and and some other things was I believe CDNs and reverse proxy should be a targetable platforms, you know, so that people content folks and especially content management systems, origin servers, people running sites, can target them and know what they're how they're gonna interact with it so that for example, you know, Drupal or WordPress or any other kind of content management system, be configured to use a CDN without having to have a CDN specific plugin. and and and that creates a a nice symbiosis and and makes the system more viable, I think. And so I think the next steps in in doing that are a couple of different things. The first is this thing called Cash Groups. It's often useful to to to group stored responses in cash together for different purposes. One of them is invalidation. So, you know, if if more than one resource incorporates common information, It's useful to validate them together. You know? So, for example, the one I have here, When a famous person dies, you invalidate all the pages referring to them so that they get impression so you don't have stale information out in cash. and your reverse cashes. Or you want might wanna invalidate a lot of things that have share properties, but they don't share a con common URI prefix."
  },
  {
    "startTime": "00:46:03",
    "text": "That allows you to structure your site in in a more sensible way so you don't have to think that in validation when you're structuring your URIs. This is very common now. At least these 3 CDNs support this pattern. Cladfly uses the hashtag header Fastly uses the surrogate key header, and I I gotta love to use of that word surrogate from 2000, which kinda dog back then, that's great. Obviously, it come back. And Akamai has edge hashtag. And so the proposal is I'm sorry. And there's a new thing in the proposal as well, which is re invalidation by group. So Sometimes resources are so tightly bound together that's successfully revalidating 1. might imply that others are fresh too. So the the kind of use case here is maybe JavaScript Libraries You deploy them as a lot of resources. But when they get refreshed, they all get refreshed together. that saves you a lot of traffic. This is much more optimistic, I guess, or or tentative than the previous use case. The previous use case is much very much paving cow paths. This is, I think, something we'd need to sketch out and make sure we're comfortable with But it's interesting to think about other uses of grouping besides just invalidation. so that's just a new header, basically. And The second capability is an invalidation API. And so plenty of CDNs and reverse proxies excuse me, having validation APIs. and here's a list of some of them that I found just with a very brief search. It's just an HDB endpoint that you send a request to, and the cache gets invalidated. So, again, the proposal here is to just have one way to do that that we support a a variety of different selectors for how you how you tell the service what you're invalidating, but it create some commonality between behavior, between all those things. And and and that's you know, to me, the value here is not quite so much. in having a single"
  },
  {
    "startTime": "00:48:05",
    "text": "a syntax on the wire of how I invalidate things because, of course, you can write clients for for an invalidation APIs. The value here is is in aligning what invalidation means. and how it works and and the conceptual alignment between CDNs. is is the interesting outcome here. And third, this is currently in that second draft I think it might wanna be split off separately, is this idea of gateway description. and and and the use case here is that dd to use a CDN with a CMS, you have to tell the CMS certain things about how the CDN is behaving. And, of course, CDNs behave in different ways As much as we want to align them, they have different behaviors and different things that they port. And so the idea here is why don't we describe the CDN to the CMS? and come up with a file format that allows you to say, okay. Go to my CDN dashboard. pluck a file out of it, feed it into my CMS, and configure that CMS the CDN that I'm using and make that a more automated process. and it'll let it opens a new communication channel where we can get more fine grained about how the CDN is gonna treat the responses from that CMS. And so here's some examples. I think, hopefully, this is just starting point, but that's the idea there. And this is what it might look like if you're just using JSON, for example. And so this gives you a way, for example, targeted cash control. We allowed the creation of other target cash control headers like CD and specific ones, this would allow you to communicate back to the CMS which ones do you support and what their precedence is so we can take advantage of that. And so That's it. I've talked to a number of CDN folks about this both in my employer and and other CDNs. I I would characterize the responses is that looks interesting. We're not sure that we can implement it yet. It depends on customer know, demand. It depends on on on things like that."
  },
  {
    "startTime": "00:50:01",
    "text": "I've talked to folks at CMS's and they are very interested, and they're very skeptical that CDNs will adopt it. So I'm hoping to get customers and CMSs to pressure them into doing be frank, because I think it's good for the for the ecosystem. I think it's actually good for the CDNs. responses. Hey. Luke's Pardu. Maybe I had a Fox Mulder moment where I lost a minute or something. has was, like, one thing -- Sorry. Can you get a bit close with Mike? Yeah. Sorry. It was like one thing, and then suddenly at the end, it was a a structured way for describing some stuff at CDNs. Yes. But but those are 2 different things? Or -- This is 3 different things. Okay. what what's the adoption question? Like, some some of that Which ones are you interested in? Well, the last one seems really easy. Like, I don't kind of fancy people going object to putting some information in Jason and -- Don't give them ideas then. But So I -- Yeah. I I don't know. Like, there's too many things going on, and it's late in the week. So That's all that's all I got to say. I I don't know what you're asking me. but I got opinions on all of them. but none of them are big. I I think for the comments here in the interest of time, like, I I I don't think we're ask asking, like, you know, are we adopting these particular documents now? It's, What are you interested in working? We wanna just gauge the feeling tone of the rounder. for me, the the more interesting discussion now is is, you know, is this And idea of CDN's and reverse proxies as a targetable platform something that we as a community are interested And do we wanna push towards that? These are my ideas of how to get there. I'm sure there are others. Sorry. Can I just respond quickly then? Of course. I think the last thing is good. And and and and and separable from the other stuff and maybe something worth looking more into. tech. Yeah. These are 3 separate pieces to be clear. Yeah. Sorry for not making that more clear. Chris. So number 2, very, very interested. Although, it wasn't entirely clear to me"
  },
  {
    "startTime": "00:52:03",
    "text": "where that API is supposed to be, if if there's some sort of an expectation that it's supposed to be on every cash, whether it's supposed to be on some centralized system for the collection of intermediaries that was that was less clear to me than it could have been. Mhmm. Number 3 is also very interesting. and it is I'm gonna have to read it a lot lot more carefully to know for sure. but it looks very, very similar to some of the work that is coming out of the CD and I working group. And that probably doesn't completely surprise you. I don't know. But there there is some of that self describing, interop type things. but these are spelled very differently, and their goals may be sufficiently distinct. Bye. and for interest of time, I like to cut the queue here and just let's be quick on the comments and just Milley Baut. Interest levels. Rich Saul's Akamai, and your as you expect. Yeah. This is interesting. Did you speak to CDNs other than the ones you who have employed you? Yes. Okay. Cool. So alright. For purpose of time, I think number 1 is the most interesting and the easiest one because you just put an adapter in front of your existing APIs. So so Sure. Sure. And the others are okay too, but number one seems the most interesting and easy to do. Right. You you mean the invalid HAPI? Yeah. That's number 2. But okay. Okay. Yes. hoops. Oops. not Norris. like, Who is next? Mike Bishop, Akamai, I will echo the it sounds thing. Also, implementation, experience, We are gonna probably gonna need customers to ask for this."
  },
  {
    "startTime": "00:54:00",
    "text": "before it happens, So Yes. Please bring customers that wanna use it. I kinda see the last one as it like, the config file is telling you where to find common elements that you expect from each CDN, and the more common CDN things that have already been defined, the more valuable it becomes. I agree. I'd I'd just say that the other purpose of the config file is to allow CDNs to differ because they are gonna differ for a while, and so if we can describe that, that's useful. Right. So You've already got the targeted cash control headers. I think the more other thinking pieces that get defined. the more valuable that config file is going to come. we so we've had a bunch of back of my folks and no fastly folks can Oh, yeah. Pietchicoraveatrix. So first, the cash tax thing is fine. I think it's interesting and definitely should be adopted. the second and third thing it's almost like out of bound API. Right? So I'm not sure if that's http working group or maybe. every CD, NIO or whatever. Although I would also like to see a way to target gateways or other intermediaries, in protocol, in band, for for different use case. What? from the intracids. Eric Niger and Akamai, I think the like, the the conceptually at the highest level it's intriguing. I think while echo what others have said is, like so we really need customer demand, but I think there's an bigger issue I worry about is that since some of these we're looking at mapping thing kind of mapping concepts onto large multiple large systems that have been existing for a while, but that that's that"
  },
  {
    "startTime": "00:56:00",
    "text": "at the surface level, some of these things may be similar, but a lot of the doubles and the details are gonna be really different. Like, you take something like the the the cash groups. how people deal with what are those names based to, like, probably have very fundamentally different input input implementations. that that kinda ripple across or similarly when you actually do a URI I I purge of an individual URL what that might mean in terms of if you have configuration that does rewrites and other and flexible cache keys and dynamic config and edgelogic, can be is gonna be dramatically different. And some of those the complexity there might start getting past the edge of what we're gonna we may necessarily want to deal with, and it may be that Like, at the surface, it seems simple, and then you get into the details and everything on unravels and it becomes a lot harder than it seems. Yeah. I'll just I I'm super aware of that from talking to customers and and looking at the things and I started to try and accommodate that in the drafts. It has a lot further to go. You need to walk that line really carefully. I agree. Marco Munisaya. as an individual, and the user of CDNs. This seems great. Yeah. here. Yeah. Sure. Should try again? Sure. Good. Hey, Al. Shivan Saip. I work for Brave Browser. Cool. Who's working? Who's working? come. And, yeah, just there's a new HTTP response header that we were are shipping support for in the browser. So just we to get folks's opinion on whether there's interest and whether HTTPT working group is the right place for it. Yeah. So this is the threat model. Essentially, the attacker sorry. It's alright. The attacker we're trying to protect against is a local act attacker who has"
  },
  {
    "startTime": "00:58:00",
    "text": "authenticated access to the browser. because browsers collect so much information. So it's in academic literature. It's called a UI bound adversary. This is often used in intimate partner violence. like, discussion on that. So, yeah, just what's in scope is things like browsing history, URL, autocomplete cookies, not in scope is network sloping, spyware, stuff like that. And what you're what the attacker is trying to do is identify the victim has been to a sensitive website the victim is trying to hide that. And the sense sensitive here is defined as what the website, self report says. Next up next slide. Right. So but, I guess, essentially, as I mentioned, the two main use cases were Intimate partner violence because it's browsers are a pretty major attack vector. I know also the other one is, you know, law enforcement or someone coming up to you and saying, okay. Give over your phone so you don't have know, advanced knowledge that this might happen. but you're still trying to protect it. Next slide. That's essentially what it looks like. It's a structured field, headers sent by the website. And the client just indicating that the the server wants the client to not leave any trace of the website's interactions. around. Next slide. Yeah. This is just what it looks like in gray. We can keep this part Just there's a con consent, interstitial, and then you press yes or no, and then Then there's some UI indication that You're in off the record mode. And this is, yeah, just kinda like what we have right now. Next slide. We presented the said WebRSec as well. in W3C just to get a sense of we just get feedback. and the feedback that we got was You know, it's fairly simple, and we should keep the"
  },
  {
    "startTime": "01:00:03",
    "text": "actual considerations of what the client should do or what the user agent should do, to be considerations and not be super prescriptive about that. and to offer more, like, suggestions then then say like this is what you should do. Alright. Yeah. And this this comparison with existing approaches, like private browsing, that often overhights because you're in a session. and you're, like, getting rid of everything there. So that leaves telltale holds in your browsing history, It's also easy to forget to start and close. I won't go through all of these, but, essentially, just think of request of OTRS, like, an incognito mode for that website. and that website because the website is identify as identified as being the sensitive part. Next slide. Just last slide, I think. Yeah. I guess, like, essentially interested in Maybe one more slide after this. You can just skip to that. Just really interested folks have to think about this if there's interest in this, there's a 0 draft up up we're at time. So if we move to feedback, that would Yes. Yeah. and a Yeah. Just really interested in hearing feedback. And whether this is the right place for this. Dennis Jackson. missilla, this is interesting work. And and I know of time you skipped over the limitations slide. My primary concern is that this isn't as effective as private browsing. But because it's automated, might give users a false sense of security. So, you know, they go to Google, a domestic violence website. visit the site. They get this pop up. They hit accept. I think jumped in. Jumped in. But in reality, that Google search is still in their history, still in their local history. and it's strictly worse than going to private browsing first, and then carrying out sequence of of web sessions So so is there any way to actually If you avoid telling the user about this,"
  },
  {
    "startTime": "01:02:01",
    "text": "which avoids that confusion risk and keeps them focused on private browsing. then you've got websites that don't appear in your history. you know, without your your consent Mhmm. you've got this complicated user education issue. And I I don't really see how you can solve it. So I guess, you know, that that's my primary concern is that this is pitched as a privacy measure. that may not actually be effective may even be actively harmful. Yeah. That's fair. We were thinking about, like, 7 heuristics that we could use. Like, remove it if it's from remove it from, you know, n -1 if it's from the from a search engine or something. But, yeah, you're right. And we've kind of documented this Ryan Globus. I think this is a great idea. I mean, definitely an additional tooltip, an additional tooltip, an additional tooltip, an additional tooltip, an browsing, I think it's also great for like aad. Like aad. an email app or any other apps that open the browser it it won't go the incognito on its own unless you're super careful. Right. And I think if you're also looking for additional use cases, I think also, especially for queer folks and in unfriendly environments, this is a great use case. zele, excited about it. Cool. Thanks. Watson led, Akamai. I like the technology. I think it's a great application. just wonder why does this need to be a HTTP header as opposed to a meta tag or or yeah. We can do both. Right? Yeah. And just for me, I think this is an interesting idea. I'm just concerned personally not share head on that that, you know, the the issues here are mostly around privacy and and the integration into the browser. And those are things that that we We talk about privacy here. We have privacy implications and everything we do. But, you know, the the the most connected thing of this to to the HP working group is the fact that it's a header. and that's something that anybody can create. So that's why, to me, WebAppSec seems like you're gonna have more of the people in the room who have thought about for a much longer time. That's just an instinct, though. Yeah. Martin Thompson,"
  },
  {
    "startTime": "01:04:00",
    "text": "privacy CG chair, at the w three c. can probably take this. Cool. Thanks. Thank you everyone for a productive discussion. Yep. actually See you in Prague. like a dinner."
  }
]
