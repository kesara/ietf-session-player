[
  {
    "startTime": "00:01:10",
    "text": "the break before this. meeting. So We can be uncivil. We can. Okay. Carsten Carsten threatens to be uncivil sometimes, but I have yet to see that. 7777 Okay. Anyway, A quick thing on on dealing with with IETF meetings these days. We have a special version of Meet Echo that works on mobile devices. And now there's even a real full version of Meet Echo that works on mobile devices. You must use that and scan that code that's up there on the screen if you are or or just enter the meet echo from from one of the links in the agenda. If you are remote, you've already taken care of that"
  },
  {
    "startTime": "00:02:01",
    "text": "because you're on meet echo and you're already recorded. But this is how we record the blue sheets now. There's also supposed to be a little thing floating around, which I don't think we have that you that has that same QR code on it that you could scan. but just scan it from the screen upfront And That's also how you get in the queue to say something. So please don't just go up to a microphone, but use the Mead echo app to queue yourself up so we can have proper proper queuing, And, yes, if you want to control your your own slides, that's possible through Medica. So recommend that you do that, and that's it for that stuff. Here's the agenda. We have what we just did. You can't bash that part of it. But you can bash the rest of it. First, we're gonna talk a few minutes about the documents not gonna otherwise talk about today. Then we have a couple of active working group documents to discuss. Well, more than a couple. The EDN literals document, which will be a pretty short discussion. And then the the the set of 3 CDDL 2.0 documents. Following that, the it's individual document that are not working group products yet, but that are under discussion in the working group. DNS CBOR and the DCBOR work, the deterministic seabor. And then we'll just go over what the interim call schedule is between now 1 18 and then any other business if there is any other business. Any comments on the agenda, additions, changes, Seeing none, head straight into the documents not under discussion today part, Time tag, we the the latest updates changed it to standards track. from informational."
  },
  {
    "startTime": "00:04:00",
    "text": "And we did a brief final working group call for any objections to the most recent changes we got none. So it's now on my queue to finish up the shepherd right up and put it into publication requested, which I will do right after the the IETF meeting is done. packed Carson's note on that is please suggest alternative table setups And later when we talk about DNS CBOR, we'll talk about use case for PAC that Karsten, do you have any other words you wanna say about either of these two documents? I actually even have slides on them. Yeah. Sorry. You actually have slides on them. Slides on Okay. Well, let me go ahead and turn over the slides to you then. stop sharing. Grant. Okay. head for it. Okay. So you just mention time take. I I would just just add to this that the the date is still somewhere in the bubbles of the ISG right now, but I think our interface to that is stable so we can decouple from that now. And and, anyways, we are not quite as advanced as they are. So there is no problem. Yeah. And that even this one editorial, we we were still discussing, but we can fix this in the HF plus call. So my recommendation would be to to ship this now. On Pectra, We actually have a version dash 9 that has two ways of setting up a table, a common table setup and a split setup. Because when when I try to make it all common, I noticed that maybe that's not so right. and we we should have a tag for split setup"
  },
  {
    "startTime": "00:06:00",
    "text": "So both are in the right now, and there is new text about what happens when you import a table using some other mechanism, and that leaves unpopulated space. And and what do you do during unpacking? then so that there is a way of of doing undefined in in this case. So we we have a use case later today, but I think what we really want to to Collect now is implementation experience. So who else can interchange data? that that I impact format, that that should be done in the next month or so. And if that turns out positive, then I think we are done. And there is also a draft I would like to point 2 from Christian, packed by reference. that really complements the the table setup picture by having the other totally essential part, which is getting your table from its So please do read that one as well, and we probably should discuss this in in the next interim or so. that was packed. Okay. Well and you're next on the agenda, so I'll just let to keep going Yeah. So you said I I was going to talk quickly about the Indian digital document and a little bit longer about the CDDA documents, but maybe I should put in one slide for that, which is this year. really, Zebra comes with a number of languages. and it's good to be aware of them and and to be aware of where they differ So first of all, we have Cboe itself, which is the representation in interchange format that that's binary. that's concise. That's efficient. It's just very hard to talk about this on the whiteboard. So we we have various forms of actually talking about"
  },
  {
    "startTime": "00:08:04",
    "text": "seabor, and one is a simple hexdump with comments, which is called Cboe Priti in in the RFC editor, list of of formats that go into RFCs. That's useful, but it it gets very long in the tooth when you have more complicated structures. So we have 2 more. textual languages. 1 is the Seabor diagnostic notation. or usually call it extended diagnostic notation because it has been extended. once, and this is what we use for examples in our documents and for diagnostics. And this is what this first draft, the end of drill, is about. So this is a text form for a single instance. It's not describing multiple instances. It just describes a single instance can be a single data item or a sequence of single data items. and it's easy to convert back and forth between that, and and actual SieBO which is one of the functions that xevo.me does. xevo.me, of course, uses pretty much as as the other side of that the picture. It's important to keep in mind that EDN was derived from JSON. So it feels like Jason. Even if we maybe have it more useful for for humans And, of course, we had to add binary and tags and and things like that but fundamentally, fundamentally, it's still Jason. And then we have CDDL. which is a specification language and which we use for validation. in in programs. So in RFCs, we find specifications and and programs we find to validation. a quote, and that doesn't define a single instance, it defines a data model So a grammar for for c Siebel documents. And that is inspired by ABNF So it has a very, very different base syntax. from diagnostic"
  },
  {
    "startTime": "00:10:01",
    "text": "notation. even though they they both have parts that that look similar to each other, it's really something different. and it can work with anything that uses JSON's data model so it can work with Jason. It can work with Siebel And I even have a draft hole meant to make it work comma separated values of CSV. the files. So that that's really what you have to to keep in mind. it it's really easy to confuse diagnostic mutation at CDL, and it happens to me all the time, and and the compiler then showeds me that that tells me that that was the the wrong notation. Anyway, so for under dark facilitation, front, we had a a draft originally just was defining a little extension, EDN. that's why it's listed name. that was adopted in June, But pretty quickly, I think even during working with production, there was a comment that we really need an ABNF for diagnostic notation. So that was added in dash 01. I left one Easter egg in there, so I'm I'm still waiting for the first person to tell me this part is wrong. and the the the next thing that was added right before this meeting was an appendix that has essentially the content of the previous slide. So it it's helps you understand what the difference between diagnostic and CDDLs and actually provides examples for that. So this document is is kind of It's not urgent. we can use the things that are in there But, of course, it would be good to get the review and people actually to to do implementations, implementations,"
  },
  {
    "startTime": "00:12:03",
    "text": "using that ABNF. So I think that that's also up for for review. soon. And I hope the net result will be that we have these EDN literates, which are good a good extension. but also an ABNF that can we can put into all of our tools. So the the other part that I'm supposed to talk 20 minutes of water. I'm probably not going to be able to do that, but, of course, others may have questions. is the the pack of of 3 documents that will together be the the first fall of CDL 2.0. The first one is essentially fixing the grammar and Yeah. That that that is pretty much done, done, I think, again, more review would be good. just became a working group document. Then we have the more control. document, which is essentially the the second installment after RC 9165, which provided a set of physician control operators, including for ABNF, So that's something where where people should look at their specification work, and see if there's something that's that's difficult to write down. because we don't have the right control. operator. So for instance, we have a control operator right from the start of CDL to to put in A byte string with some keyboard data. with some encoded simulator in it. But to Excuse me. We didn't do the same thing for Jason. So if you want to write a spec where where somewhere there is a little bit of JSON code,"
  },
  {
    "startTime": "00:14:03",
    "text": "we currently have no way to define this in in city there. So that's what the more control does. And the third one is is the first part that really is CDL version 2, a module structure for CDL and that is implemented in CDLHC. Actually, all of these are implemented into the LC, but that's really something you should be playing around at the point in time and building your larger specifications out of snippets that that talk to each other using import or include. So Again, all three are implemented in at least one tool. we need to test these out in actual spec work. There are enough documents out there that that are using CDDA, so that shouldn't go hard. we need to get reviews. And, of course, we get need to get more implementation Rook. So, again, this is something that that weights for regions. Any questions on the CO2 pack? I I see. Hank, about to see I see no one joining the queue. And by the way, Carsten, you weren't expected who's to talk for 20 minutes. It's just that you had 20 minute slot on the agenda. So You don't need to use it all. Yeah. Hi, Kaston. This is Heng. we I think we have, like, 50 CDL fragments Coram, and I think we will use important food there. Quit? Wonderful. Wonderful. Wonderful. Wonderful. As soon as my coauthors read, supplimate in reality. They're gone for Mullins. But Yeah. soon. Yeah. Current was was an example I had in mind, of course, because that that really complicated. And yeah. Good."
  },
  {
    "startTime": "00:16:04",
    "text": "Kristen, I'm just I think that I'm giving good examples of how ACE and CWT documents are structured. will also benefit from using the module structure. Yeah. I think the that's so Assembly a few things. That's a very good point, and my next slide is kind of saying the same thing but on a different matter. So I think we actually have to sit down and write a few more examples, not necessarily in the context offer specific spec that we are trying to to get written up that, of course, also should contain examples, but a little bit larger examples that explain explain to people how these things fit together and can be used in in actual specification, but that would be a good thing. And I I think Ace and CWT are really good examples where we could do that. So Let's add that to to our to do this. So I wanted to steal 5 minutes from those 20 minutes to talk about 200 documents deterministic encoding, There's a lot has been said about deterministic encoding and and various venues, And it it's sometimes a little bit difficult to to explain things because 8 949 is pretty tar's, and it also tries to give applications enough rope to to actually do something that that specific application may need but there is something like like, well defined form of deterministic encoding that that some of us have implemented"
  },
  {
    "startTime": "00:18:01",
    "text": "And it turns out these implementations simply interoperate without us even talking about that. So I finally sat down and wrote a background on informational cement, about deterministic encoding. So you cannot make a lot of examples about deterministic encoding because it just happens. but what what what what you can explain things and then also explain how the application is not out of the picture, but actually has to do with share when when deterministic encoding is is required. So that's one document that I would like to develop. probably on the slow burner because it's useful as an Internet drafted. but it would be good to to hear from people who are gaining experience in the deterministic encoding space. And just to to yeah, benchmark check out this background document, I took the DCBO specification that we have been talking about. and essentially extracted everything that is is is can be said is is different from what And it turns out that, essentially, the the numeric type reduction is what's what's new, what's different about. DC board. So I simply wrote this up I think that that's not much more than a page of actual content. for this specification, and and it shows that if you have a good background to to work on, then then it's not very hard to actually describe these things in the terms you now have developed. So would like to know whether these documents are useful. on the DCB DC board document, we haven't really decided whether"
  },
  {
    "startTime": "00:20:02",
    "text": "we need this. But for me, it was a good exercise to actually first write it up So so we know what it actually is because right now, it's a very long the input document was a very long specification, and it repeated a lot of things from 8949 that don't need repeating and even even wrote a document about restatements That explains why that is a bad idea. So getting some initial reviews on this would would be a nice thing. So end of advertisement, block, block, Yeah. And and, finally, we have the some more active work, so we will talk about DNS Siebel in a moment, There is this draft about using CIDIA for CSV. I don't think any of us can capes CSV in the daily lives. So I think that that could be a draft that chopped purchase you, And so there's another draft that tries to track cddldatamodels from RFCs to make them more useful because if you just take your random r seat. then you might find some a router that needs to be applied. We you actually can use the specification text in there. And then there is the the draft numbers process document, which I think already has turned out to be the use forward that which certainly needs to be refined with experience. from using it So that's My Pot. there are no more questions, Should go to -- -- discussion. discussion discussion. Michael."
  },
  {
    "startTime": "00:22:07",
    "text": "Thanks, Carsten. Appreciated the DC board draft because that was very helpful to actually have it like, concise and down to kinda key adjustments changes. The one thing that jumped out of me was the reduction of you know, floating point zeros and energy integer zeros down to 0 and negative space and things like that. Was there anything else that we should be noting aside from like from negative0and0ornow0. You me noting in terms of Yeah. Were there anything that jumped out at you that would pause and implement or, like, pay very careful attention to this? Like, any particular segments there in your mind aside from the adjustments to handling of 0. Yeah. The the other part that is somewhat interesting is the handing of subnormal floating point numbers. So we haven't really talked about floating point very much in this working group because it started out with constraint systems and constrained systems don't often don't even have floating point. But the the observation is that what is a subnormal number of course, changes between float 4, float 632, and float 16. So even thinking about special processing for them, destroys this beautiful unified number of space. So you can't really do that. Yeah. That that's helpful. Thanks for calling that out because I'll pay closer to Anchen to that section because we've been using Jose on detached signatures on some machine learning things that obviously sometimes get a large number space. So that that's helpful. Thank you. On machine learning, I would"
  },
  {
    "startTime": "00:24:00",
    "text": "maybe you want to add that there there is a document in the making. that looks at the number format, at the floating point number formats that are becoming more popular for machine learning, like FPA and and before 16. And what's it called? I forgot the the number the name of the the search form. that that might help with those. Hi. Wolf McNally. Blockchain Commons. our original spec for DC War, a religious proposal for DC War, The subnormal number issue was called out as an issue for future work. it's not particularly relevant to our work at this time, although looking at the IEEE 754 specification it's clearly part of that specification. the numerical encodings in Seabor. inherit that. And so we wanted to make sure that, you know, would be responsible to call that out is something that while we're we weren't particularly going to apply energy towards that we realize that at some point, the community may need to. similarly with big nubs and things like that, which was in our first draft, but we took out our second draft because it's not irrelevant to our work, but we recognize that that you know, numerical reduction probably ought to apply those 2. But not something for our initial specification, which mostly dealt with integers and floating point values of machine size. and And so we want to kind of lock those down because those are relevant to our work. And I think that the the spec does a pretty good job as we had it. The the main thing that I think that I want to make sure that you're aware of Karsten when it comes to your draft you're currently writing. is that 8949, the section on deterministic coding, is stage those things, for example, like sorting map keys and so on as not necessarily required or that decoders of determining to succeed board are expecting seabore that's encoded"
  },
  {
    "startTime": "00:26:00",
    "text": "deterministic we should actually reject that as as as invalid. And so I think that anything going forward that basically says that you know, numerical reduction of 0, for instance, you know, yeah, there's Seabor has several. encodings of 0, and you minimal you know, minimizing that terms of the integer space is pre will accomplished by everybody, but minimizing that from floating point to integer is not, and we want to make sure that seaward decoders, have to reject floating point 0 if they find one because it's not a valid it's not deterministic. and And, again, how this applies to other number types, you know, rational numbers or whatever in big numbers or whatever. not part of our current work. but we recognize that it's for future works. And so I would just wanna make sure that what you're working on is actually strict. We're we're we're not presuming that postal's law applies here that We want to be strict in both what we read and write when it comes to determinism. That's my end point. Yeah. Thank you. Okay. Think I'm done. Okay. Next up is DNS seaborne. So do you want to run your slides? Okay? see. -- need to request I Same. Okay. Yep. Go ahead. Okay. Okay. Yeah. As already was introduced, I'm talking about DNS Seebo. 5 which is a concise binary object representation of DNS messages. And, yeah, I'm a team lenders. Sorry."
  },
  {
    "startTime": "00:28:02",
    "text": "Yeah. The motivation is basically that when we look at DNS traffic nowadays by consumer IoT devices, which is this somewhat wild graphic here. we see that we easily reach fragmentation for most of the names that we see, And so we need to compress DNS messages somehow. And, basically, that's the idea with application in a cbo, not heavily used compression, but also make the DNS messages more compact. Yeah. And we try to be concise by first of all, encoding DNS messages in seaborne and then also meeting redundant DNS fields in the the queries and responses. And we also allow for address and name compression using Packetseabor. as was hinted at by casting already before. Yeah. And we didn't do much since the last IETF, since we worked more on DNS Overcorp to be honest. But after IETF 117, we plan to work a lot more on this draft. So we only clarified that compression algorithm for the packed seabor is up to the implementation and discuss the format decisions on Pac seaborne And, also, we did some structural cleanups and synthetic spots in the examples. So another thing to report is that I implemented an decoder and encoder and Python during the hackathon this weekend. And Basically, we there have already an encoder encoder, we found a lip name, thanks to Marco. And And yeah. But in all the decoder is almost done. The pack"
  },
  {
    "startTime": "00:30:00",
    "text": "SQL support there is still missing. And, yeah, some lessons learned during implementing this is that some of the illusion we wrote into the draft mainly resyncing. For example, I ran into the case that I didn't really need the in the message. And But the draft basically says if there are these sections, then you need to have a question section. So, yeah, we need to maybe resync a little about that. And we also might need some dedicated specs for us Zudo resource records, for for example, options because they can take up as lot of space when we just use the byte encoding for that. Oh, the? Oh, yeah. Okay. And for the next steps, to what's the version 4 is that we provide and compare examples of the compression algorithms Then there was a DNS tier feedback that we need to provide that we provide a comparison that's basically going in the same direction And, of course, we need to address the lessons we learned from the hackathon. And, yeah, the next steps will be that We further work on the implementation and also do an in-depth evaluation of this And then there is also this potential of a global compression context and this morning on the mailing list. Maybe you some of you read it. Christian had an really nice idea to maybe use the packed argument for the media type somehow to say what compression contexts we you want to use for that. So, yeah, that is for my side. Are there any questions? or comments?"
  },
  {
    "startTime": "00:32:02",
    "text": "Discussion of DNS seabor. Yeah. We have someone we have someone going up to the microphone and then car your next. I can't figure out what's used. Yeah. Okay. That's fine. Good. So you have a -- Tell us who you are though, please. name is. Thank you. It's my first. right here. Nice. Nice. So you have a an encoder, but you don't have a decoder yet. So -- We we have a decoder, but it not capable of handling Paxibo yet. It's just something that we I didn't manage to due in time during the hackathon. So do you have dissect or for Warshai to look at what you're doing or We don't have a dissect there yet. I basically I I used some dumps of for testing from from pickup, but there is not a a dissect there yet. No. you need someone to help you with this? If you want to help with that, I'm totally happy, but as as as you might have seen, there might some changes happening to the draft for the next version. So maybe wait wait for the version before you start. Thank you. Thank you. Thank you. Carson? Yep. Yeah. My my main question was when are we going to discuss numbers? how much do we save which of the next interims can we plan to do that? I hope, as I told you and now also, the people, as that after the this IETF, I will have much more time to work on this. So then I will at probably then at the next interim, provide some numbers. But yeah. pence a little bit all of my time. Yeah. It's always very hard to look at these"
  },
  {
    "startTime": "00:34:04",
    "text": "Yeah. -- composites, if you don't see what they actually do to to actual DNS traffic, you you want to gets to compressed Sartoupec So that's why I'm asking. Yeah. Greg. Wonderful. Any more questions? Then then Okay? Thank you. Thanks. Alright. Let's Okay. Next thing on the agenda is DC board. I have no slides that anyone submitted for that. So I guess it will just be discussion Wolf, did you want a start any discussion on that? As he comes up to the microphone? Hi. Will McNelly Blockchain comes again. Yeah. Our original proposal for for DC board was motivated by the fact that we work with both blockchain and not blockchain companies that are interested in in determinism, which basically it means that different encoders, encoders, encoders of the same data should all converge onto the same bit sequence, essentially. And we identified CBOR as the best candidate for that, especially because it had a, a specification for determinism. Well, for example, including sorted map keys and so on. But as our clients, and we often mix various kinds of numeric types, including integers and floating point values, we realized that was something that seaboard didn't really address. And so I wrote an Internet draft after having written some code as well that did address it. And my length of it as"
  },
  {
    "startTime": "00:36:01",
    "text": "Carson has pointed out was primarily due to the fact that I think that determinism can't be fully accomplished at the encoder level. It needs to be something that the protocol designed on top of CBOR also needs to address that. And so our draft was intended maybe it was too big in scope, but it was intended to address both of those issues not not leave those stones unturned because I think they're important. And so Carsten is obviously more interested in the protocol level, and I think that's what his drafts are about. And, you know, I I concur that's important to kind of get that minimally specified. The obviously, the controversial thing that we talked about is what we're calling numerical reduction. which is that each numeric value should have exactly one representation if it can't. And so, you know, for example, 0 you know, there shouldn't be 0, 0.0. JSON has, you know, essentially, you can have an infinite number of ways of representing 0 in JSON. And that has to be canonicalized using things like the JSON canonicalization scheme and so on. well, our protocols were designed on top of CBOR. We want we we wanted to take care of as much of that dd, at the protocol level as possible. And so We have 3 implementations of Gordian DCBOR, which is what we're calling. Our our variant of this which is well formed Seabore, but it you know, you hand it off a 0 in any form. It always encodes as just by 0. And our we implementation is Swift Rust and TypeScript. And so, you know, if you're curious about that, I urge you to check that out. And aside benefit of this, aside from being more deterministic, is that languages that have week number typing systems like type like TypeScript and so on. you don't have to wrap your numbers in declaratory wrappers or used builders that specify the you know, whether it's an integer or a float because, you know, TypeScript doesn't care. Other languages that you care, you still just present the value as it is in the language, or you tell it what kind of value to extract. And if it can't extract it without losing new cuts. So that's a side benefit. of determinism. So"
  },
  {
    "startTime": "00:38:02",
    "text": "You know, there are, as I mentioned, some unaddressed issues that aren't be relevant to our work at this time, but we want to call out and make sure that the community understands them. And so, you know, I certainly welcome any questions you have about either our motivations or our execution of this so far. When we attended AEs, just just batch earlier this year, they they basically said, hey. Yeah. which should be a seabor workgroup item. So That's why he brought it to you. And, you know, we would like this to see this either, you know, become part of the seaborne specifications, strictly opt in. So As I've said, if you're if you're expect if you're writing or reading determine to succeed more. You can't just read or write any CBOR. But If you write 360 WER, it's perfectly fine C WER. Anybody should be able to read And that's, you know so But determinism basically, you know, does place the limitations on it. And but those are important for the for the domains we're operating I welcome any questions about that. Christopher. Chris Farrell and also with Blockchain Commons. So the three releases now are in what we call what we're calling community review. So we have one in Rust, which is what the the cryptographic community prefers to move things to. We wanna type script to demonstrate that We can handle a a less strongly typed language and we have one in Swift for mobile on on iPhone. we would love to see somebody do, say, a Python version because that's another one that's very popular in the various geographic communities and, you know, a lot of of I mean, things like HPKE, etcetera, are are know, described in in Python API functions. So if there if you are interested in porting one of these or being involved in the community review to to really pin these down and make sure there isn't any"
  },
  {
    "startTime": "00:40:01",
    "text": "security problems with these. We'd love to have your your eyes on the code. Thanks. Thanks. Okay, Michael? Guess this is probably a question for Chris fur and others on the other DC board draft is if the working group work who adopts Carsten's draft around determine seaboard, which I found very readable. would that prevent you from doing what you need to move forward, or could you contribute dot to the draft as it moves forward as part of the working group because that seemed like a very good starting place for saying, hey. If you need determinism, Here's a way to do it. and and know, go with that. Yeah. Wolf McNeil again. I would say that there you know, based on reading hearing Carsten's concerns and reading the drafts he's read, I'm ready to edit mine way back actually and remove a lot of the things that that I think we're out of scope. for for DCBOR. in terms of, you know, like, we had a number of proposals for, you know, people who specify APIs, not 1st API specifications, people who specify APIs with things they should consider. you know, as far as the protocol level is concerned, that's kinda I I admit that's out of scope. So But So so you know, whether the group as a whole chooses to go with Carson's draft that we amend or whether we do an 3 or a 4 and that moves closer, that's fine too. you know, it's not a matter of ego for us. It's a matter of just getting work done that we can actually build And so whatever the group thinks since I'm the new person here, I'm happy to take whatever advice the group has in terms of how to proceed moving this long track. I was just I was just gonna add. So one of the other questions is is it worthwhile? I mean, you know, a good man."
  },
  {
    "startTime": "00:42:02",
    "text": "IETF doesn't do APIs. and we basically are, you know, very much have a particular orientation for that, which is not appropriate for the Internet on DCBore. And so the question is, should we separate that as a that out as another work? you know, there are some interesting things in regards to how you approach being very safe with your representations So Where does that work go? and what you know, we'd love to have your advice as to Do we need to continue to revise our draft or know, focus on, you know, finishing up Carson's draft or reviewing whatever the mean it's pretty solid to start off with. So dot we're really looking for guidance. Thanks. Lawrence, Yeah. Lawrence Lundblight here. I I Carson's draft seems like a good starting point to me. It seems like it was kinda cut right down to the important stuff. I'm generally supportive of that. I'm it in implementing it in my supported Cutter and Cutter That's that's thing. my my main question, you know, that's that's that's stuck in my head here is how do you avoid subnormals? You you can't control the floating point hardware Alright? it's gonna put sub normals in there for you if you want it or not. if you do some calculation with them, is isn't it? I mean, maybe ABM missing something, but that seems like like that's a pretty sticky problem Seems that that at to try and solve that one. So So I am not a this is Wolf Magnolia again. not a numerical methods expert. My understanding of IEEE 754 is that implement all implementations of 754 should be bit for bit deterministic on various kinds"
  },
  {
    "startTime": "00:44:01",
    "text": "operations. So if something yields the subnormal value, which isn't a value we're close to 0, I believe. that it has forbidden coding. And the way we handle that in our numeric production code, is we attempt to reduce down to the next lowest level, and then scale up again. And if it doesn't match bit for bit, then it's not reducible. Therefore, We at every step, we're following the 754. We're not just, you know, twiddling our own and figuring out, oh, this is reducible. So if a subnormal value is produced, it should actually take whatever level of representation if it's, say, a double. to double go very close to normal, and you cannot scale that down to 32 bit float and then float up again and have the same value. won't be reduced. So that is my understanding of the default behavior. I'm very open to hearing from other people who have a lot more knowledge of numerical methods than me whether that's a valid assumption or not. But it's our current assumption going forward. and and, you know, obviously, I think that's why we flagged it as future work, not something that's directly relevant to our work. I answer your question? Okay. Lauren said maybe. If you Lawrence, if you have more to say go to the microphone, please. Maybe I can answer part of that. because I I implemented this in 2013 when when We stand at a Seebo. And Generally, the the what what we've just said, scale down and then up. And if it's still the same, then you could scale down. That's, of course, the implementation strategy that I also used in 2000 13, The the point really is if you cannot rely on your floating point unit. to be a full 7 54 unit, unit, then you really have to do which, I believe, Lawrence Stidge and do the the deterministic encoding, use seeing shifts and masks."
  },
  {
    "startTime": "00:46:01",
    "text": "because the floating point unit just would meet the specification of the deterministic encoding, and, therefore, you you are essentially forced to to implement this. by hand. I mean, it's not not not not a lot of work. It's, like, 7 lines of code, which actually in the the standard, so you can just copy them out and translate them. to your language, but I think that that's really the the less interesting. part. I mean, it it's you you need to put work in, but it it's not much work. So the the other part is these numbers actually come from an application that probably doesn't just copy floating point numbers in its lifetime. but actually tries to compete with And then, of course, you run into a problem with the and coding, because another application that might want to do exactly the same calculation might have a different floating point unit, And, actually, come out with with a different different answer. So if you have a reduced to 0, floating point unit that that doesn't deal in subnormals, but but just as puts all of them into a 0, then your your application level data at the end won't look like you were expecting them to do. So this this is really a problem And, of course, that's not a new problem floating point always has has had this problem, it was actually, in the language scheme, the the floating code numbers are called inexact because you cannot expect you can't get exact"
  },
  {
    "startTime": "00:48:01",
    "text": "answers. So this is really, I think, the more complicated part. But but I'm really interested in what what's you're missing from the the deterministic background, document is is something that that I haven't implemented because I have only implemented this both deterministic encoding and DCBO for for a language that's polymorphic by by nature. But if you have a strongly typed Language, language, that separates floating point and and integer. Then, of course, when you pick a number of of the wire, whether that is one type or the other type depends on its value. So good ways to actually convert back to to the application types as they may be required I I would be interested to to have some feedback on that so I can it into the back document. Okay. Yeah. I'll I'll address both issues. the I think it's true that that for example, our current implementation, our current draft, probably should say that we're relying on a stable implementation of IEEE 754, for it to actually be valid. And anybody who claims to support this draft. either needs to have that in hardware or software, but they need to have And I think I think that could probably called out very specifically. And The other issue is the 2nd issue rate in terms of what it was. I think the right type when when decoding things. I'm sorry. Repeat yourself, Carson. Sorry. getting the right type. So if you have an application -- Okay. Yes. I -- You know, I I I have it now. I have Right. Sorry about that. Yeah. So I've implemented DC board spec in 2 strongly numerical type languages, swift and rust. And"
  },
  {
    "startTime": "00:50:01",
    "text": "basically the approach I take, which I think should be the the regular approach is when something numerical type is encoded, You don't necessarily know how it was encoded. You could parse the seaboard yourself. In fact, you know, our our c BOR mutations let you look at it as c BOR structured as part c BOR structure and determine in is it is it uint? Is it a neg an int? Is it a flow? Is it whatever? You can look at it at that level if you want. but the base API just says, extract this as this type. So, for example, if I have What I think is a byte stream a stream of some kind of, like, array of of you and 8 it's encoded that way. And the and the protocol, built on this says it's built that way, then I should be able to iterate to their that array saying extract the next q and a the next q and a. Now a float has been thrown in there. Well, that will throw an exception in the API because it can't be extracted at the at at least the precision that that which it was recorded. Whereas, for example, if you go through the same array of integers of numeric values and and say, extract this is a double extract. This is double extract. This is double. It will obviously decode unless, for example, there's, let's say, a big number thrown in there. but we we're we were also reserving big enough for for future work. So we're not specifying how big dumpster to be handled. So The idea is that anybody who influences the DCBOR codec, can decide know, what's the maximum numerical precision they're going to support and support that. And they're and and, you know, so if you're not using floating points, you don't need to sport floating points. If you do support floating points, you need to support at least binary 16, which end end up to binary 64, which is the iterpleased Does that answer your your concerns? Yeah. I think the the the pointer to pull APIs, the the carry an expectation of what the the application type should be. That that's really my question. So in your experience, has have the flow you has the floating point implementations been"
  },
  {
    "startTime": "00:52:00",
    "text": "deterministic and what you expected to Yeah. Our use cases for floating point are mostly currently, actually. And because they're not near 0 at all, never get encoded subnormals. And because they're they tend to be you know, scaled up, like, you know, nanosecond values and so on, you know, delivered by the operating system, they tend not to have any of the strange edge cases that we've been discussing. So But other than that, I, you know, I wouldn't anticipate any problems. Obviously, we're not doing advanced mathematics on that at this time, and that's why my qualifications as as a not a numerical methods person, I think, are are satisfactory for now, but, again, future work. then one more thing that that I don't know if we've discussed it And that's the NANS and NAND payloads. I don't know if there's anything to deal with on that. And There's Quiet NAND and all that. So I I researched the the NAND issue before specking it the way I did. And what I realized is that there is no the quiet NANS versus signaling NANS is a processor level artifact that basically that of certain kinds of libraries in certain conditions, sometimes output signaling ends, meaning there should be a trap in the code. supposed to quiet lands, which are supposed to be just passed on in the in the algorithm. and that whether something is a quiet or signaling end generally has nothing to do with how it's encoded. Now encoding NAMS is important. But, also, 754 has an NAMS as which, you know, our supposedly a singular kind of value has a number of of of undefined what's called they call payload bits that have no semantic mapping to anything I was able determine. And so if there and since we're trying to create a subset of CBOR, that encodes certain that that has unambiguous encoding. If I wanna pass on an NAND, it shouldn't matter how it was generated. shouldn't matter whether it was signaling or not signaling because it's not gonna make a difference on the delivery And if I if show me once you show me a use case where it would make a difference, I'm happy to do that. I've already"
  },
  {
    "startTime": "00:54:02",
    "text": "change my stance about one thing on DCBore, which had to do with with no map values. I thought would be unnecessary, and I was convinced otherwise. So I'm, you know, so I'm happy to to look at use cases. But as far as I can tell, a nana's a nana. and that it has a 2 byte unambiguous encoding, which is the minimal form in seaborne. and that's how DC board should always perceive it. And any other encoding, whether it's payload's bit on, should be rejected. because it's not deterministic. It's still in that. I think NANS are interesting because you will find NAND payloads in the internal implementations of languages like like list list related languages will be and so on, but you will never actually see them leak to the programming model of that language because it's all happening within the interpreter within the the jet mechanism. So I think we are pretty safe with reducing NANS to to the single NAND value. But if we don't, then we still have to have the race to actually do something weird. Yeah. If I could actually, is there address one other thing that I'm not sure we reached consensus on yet is I made a choice as well to in my current draft too, Actually, it's an editor just draft. It's not in the last release draft. to restrict negative 65 bit integer which which Seaborg can represent which are well formed CBOR. but are out of machine size for most processors. And I think properly should be represented as big numbers. And so for to ease implementation, especially in constrained environment, when decoding DCBOR I basically made those invalid the patterns, declare them as invalid. So If you need negative 65 bit integers, you're out of luck in my current spec. Now if there's an argument you made for other otherwise, And, basically, that comes as almost no cost when compared. So So"
  },
  {
    "startTime": "00:56:02",
    "text": "know, that's the decision I made which, you know, is that fair. I think it's fair given the other concerns, and it's is definitely false in the realm of determinism. Yeah. You need them as 2 actually 2 big names. So Yeah. But I'll I'll leave it the flow to Christian. Yeah. Obviously, the representative by by big numbs or other forms, and I think that properly, you know, I feel like because they take you out of 64 bit scope, which is the common top for processors right now. because we want DC word to be easily implementable in a variety of constrained environments. that we we'd rather and validate those bit patterns. at this point. there's a really good argument to keep them. there there are good arguments. And if the background document Well, is not describing them, then Please send me a note, and I will expand that. Okay. I'll I will look at that more carefully. Thank you. Justin, I'm just gonna head off There is one note that I'd like to add on the topic of how do you deal with floating point units that produce calculations that are inconsistent? My understanding is that at least for the DCBO use case work that you have describe, The idea is that implementations don't arrive through this at these through computation, but they might be storing the floating point numbers in in some process and then reconstruct the Siebel. They won't be doing different calculations. They just store them and re encode them, and they are not they'll try not to store the encoder the the seaboard. but just the the past value. But as long as they don't calculate with them, I think they're they should be safe. and the note that It is like, it is because of that might help understand document better. Yeah. I'd say if if you're if you're creating a protocol that"
  },
  {
    "startTime": "00:58:04",
    "text": "where several actors need to perform comp floating point computation separately, they need it's out of scope for DCboard itself to to specify how they arrive at that. But then they and this is why again, I say you can't. all these problems at the codec level. But you should solve as many as possible at the codec level to remove cognitive burden from engineers worrying about how how things should be or will be encoded. it's unambiguous, when you're not gonna lose any precision and you can guarantee you're not gonna lose any precision. But how you arrive at those, how separate actors arrive at floating point values and then that they arrive at the same value. that's totally out of scope. That's something but something that should be called out is important if you're going after terminate as Okay. We're just about at the bottom of the hour, so we're just about done here. Just one more thing on is on the agenda to go over the dates for the interim calls. The dates that are up on the screen on the agenda are in the same cycle that we have been that we've coordinated with the core working group alternative weeks. So it's, again, Wednesdays at still at the same time 1400 UTC. on the dates on screen August 23rd, September 6th, September 20th, October 4th, and October 18th. does anybody have any reason we should not use those dates? Okay. I will post those to the mailing list just to make sure everyone knows about them, and then we will request them from the secretary. I think And that finishes us up. I'll I just wanna thank Marcos Ilocca for, again, taking notes. He's always willing, and we're always happy that he is. So Thank you so much. are adjourned. We Enjoy the rest of our ETF week. Thanks, everyone. Thank you all."
  }
]
