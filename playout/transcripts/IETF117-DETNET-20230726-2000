[
  {
    "startTime": "00:00:46",
    "text": "rules and policies, which are summarized in the node If you are not familiar with the the note button, please check It includes pointers to BCPs and RFCs. And I would like to remind you that by participating. you acknowledge that your contributions including verbal contributions, become a part of the permanent record. We also have contact guidelines So please follow appropriate behavior with your colleagues And I would like to make the same reminder Lumit that on-site. Also, please join the meter call for the electronic blue sheet and also for some polls at the end. so on. I will stop with you after talking and you get back the QR code. And I would like to ask everyone to join the note taking I gave the link to the chat too. It is a joint effort, and it's appreciated if you have"
  },
  {
    "startTime": "00:02:02",
    "text": "taking care of the notes. We have 2 sessions. This is the first one. Wednesday, this is the deadline session. And we also have another session tomorrow, join session between the row working loop and the the data working group in 1 stop later. Not not session 2, like, today, but session 3. So you are welcome to join tomorrow as well. Focusing on this online session. we have a very fact agenda. The link here is here. because at the usual page, please check it. It didn't make sense to copy it over. But I would like to ask the presenters that a working loop out to experience focus on on the changes compared to the last meeting. what are the plans to compare the document And for nonworking group contributions, documents, Please focus on but their aspects of the chart. They were addressing for new drops a summary of the new details and for your order of the comparison, what's new compared to previous version. We we will enforce the time This includes a discussion. Just an update of our on the working group status, we have a number of drafts for which publication is has been requested the young OEM framework, MPSOIM. And, actually, the IP OEM is getting there. the the next step before that. And the 2 new draft at that stage with the packet ordering function. and the MPLS over IPA queue. I would like to also mentioned the ongoing open working meetings on the analyst data plan, but there will be more details provided on that. We were copied on"
  },
  {
    "startTime": "00:04:01",
    "text": "really, as an exchange between active equal to those on end activities that we group to our team. This was just a carbon copy node action needed from our side. And then we would like to remind you our IVR policies and also that the work is primarily that pin on the list. That's why we build consensus please use the list we can have virtual meetings, like the regular meetings on the announce data plan, and we can set up additional meetings as needed for just don't hesitate to ask us Okay. And we can move to the next one, which is update from database. David Dial. Yeah. I'm Can you hear me? Yes. Excellent. Okay. This is an update from the open working meetings, on the enhanced Aetna data plane. Next slide. So we've had 7 meetings since I have 116 Yokohama. each meeting has a 2 part agenda. We've process oriented topics, requirements, draft, contents, evaluation stuff topics that will help move the work forward. and then in-depth presentation of a queuing scheduling mechanism. This two part structure has worked really well. The processor oriented outcomes, you've seen several revisions of scaling requirements draft, which we tried to use it. We discovered, oh, there's a few more things that ought to say or places where it wasn't as clear as we thought it was. There are also initial evaluations of TSN mechanisms against those requirements, and that's where a lot of us are just getting requirements draft came from. The initial evaluation our Texas slide deck. I'm not gonna present them here. as we are really short on time"
  },
  {
    "startTime": "00:06:02",
    "text": "is also an in-depth mechanism presentation at each meeting. that's worked very well, giving the folks have designed as a schedule queuing mechanism half an hour or more to explain how it really works as opposed to a really short 10 minute slot in one of these meetings has proven beneficial, I think, I think a lot of people have learned quite a bit about some some of what's going on. Next slide, So what are we gonna do between now and I get 118. Need to move evaluation of the TSN mechanisms to evaluation for post new mechanism. The expectation is the draft author for each new mechanism. We'll prepare initial evaluation, and then we'll have discussion of the evaluation. That should get us to a point of being able to talk about requirements coverage by mechanisms in meeting discussion which ought to start moving us towards what are the various sort of alternatives we could choose to meet the scaling requirements, Proposed logistics, the 8 AM US Eastern time slot seems to work better than the eight PM US eastern time. So I'm just gonna move to that. courtesy of my day job, we need to move we need to do that time slot Tuesdays rather than Wednesdays. Proposed first meeting is August 22nd, or they'll be happy to listen to requests and say, hey. People wanna take all much August off, wait till September. We do need to make sure that we don't try to meet on the Tuesday after the US holiday weekend the 1st weekend in September. And these meetings will be based on Meet Echo. We had an IET Webex failure went to meet echo and haven't looked back. And that's all I intend to present. Comments, questions, discussion, Did ketchup and rotten tomatoes? She's you sound go ahead."
  },
  {
    "startTime": "00:08:01",
    "text": "Yeah. Hi, David. This is from Huawei. Just many thanks to lead the discussion, I think, it helps a lot to make people know more about the mechanisms on the table. some proposals about the the next plans for your consideration. The first thing is that I I noticed that us the queuing mechanisms is really of our big or a very have a lot of variation about lot of chromosomes. So there are still are many new individual documents coming out to to show that there are a lot of interest to to introduce new mechanisms in the networking group. But a little concerned about maybe the the it will make the discussion more complex than the analysts. So a little proposal is that can we consider to class classify the existing human mechanisms. For example, the the a synchronized clear mechanisms, cycle based mechanisms, and queues based mechanisms like that. that maybe make the discussion more focused on the classification Because if we're looking at the academic contributions in the field, there are a lot of chemic mechanisms. So I think it's very very hard for the a working group to consider them all. That is the first proposal for your consideration. Hope it works. Second one is also related to the first observation because the queuing mechanism is really a lot. So I think the evaluation is really important As you mentioned, there will be you know, evaluation for each killing mechanisms to make people discuss whether it is valid or not. And maybe we can also consider to"
  },
  {
    "startTime": "00:10:02",
    "text": "let e each document authors do also provide the the detailed comparation between the the mechanisms they raised and the mechanisms of the existing of the existing work. For example, the IEEE you work, and also the academic work. I think that will make people more clear what has been being being worked to make it more scalable. or to more suitable for that net scenario. That is just two comments from my side. Thanks. Thank you. Classification, it certainly sounds interesting Is there an existing classification that we could use? I hesitate to spend a lot of time designing our own classification. David, you mean you have already have some classification -- No. I'm asking whether I'm asking whether you know of one. a proposal because I think if we ask the participants to the work group to devise 1, we'll have 5 competing classification structures in the Yeah. I understand. Sure. I like that idea. I I think maybe attribute worker can be a a clue. because there there is some, you know, attribute existing work And I think they face similar situation. So maybe we can seek suggestions from Yanosh or other people from whether they can give some clue about how to class classify these organisms. Please send me off the list. Mhmm. Yeah. Yeah. Sure. Yeah. Perhaps also, it's a something for discussion in the regular meeting is to ask the authors to suggest how they would classify themselves, and maybe they'll be some nice self synchronization. Mhmm. Yes."
  },
  {
    "startTime": "00:12:06",
    "text": "Okay. And she's on I let's see. I heard your your your other comment about asking authors to make comparisons and sounds interesting. I'll think about that. We can certainly discuss it further in in in the open meetings. David, anything else? I think you have some backup material know if it's worth just flipping through, not discussing it, just letting the working group know that there's the summary. Yeah. As if you if you want a quick quick flip through the backup slides, These are the slides that were prepared as an issue evaluation of the TSN mechanism. They're intended for reference only. We've talked to if we start talking about them, I can soon on behalf of Lou's Lou and Gianna's available meeting time, and I don't think they'll forgive me for that. But it's good to know that it's there. If you're interested in sort of a summary of all the more detailed discussions we're gonna have for the rest of the session, Here's a one slide summary on each of those topics. You can just go download it. And the other thing is you'll see across these slides as a common framework of how we're We're evaluating mechanisms against the requirements draft. This is what's going to be expected all of the new proposed mechanisms. Okay. Thank you. Thank you. Next up, we have Peng, who's, I believe, is going to be remote. just one more comment for the previous presentation. It's -- Go ahead. -- allow -- It's okay. It's okay. Just I think many thanks to to David. It it is really not a easy job. is 2 week discussion and a very long and a difficult"
  },
  {
    "startTime": "00:14:02",
    "text": "And I think David's job is really work is really useful and helpful. Thanks. And, Shushan, I wanna return the thanks as I think you are the person I remember the most for suggesting that we make time in the meetings for the in-depth presentations, which was a really good idea. And thank you for all all your contribution, David. She's on in all the their authors, Thank you. So we're gonna move forward. Can you hear me? Yes. Okay. Thank you. Calling from. and this requirement for scaling deterministic network. excise, please. So this document describes for comments, and the data plan, it has been through comments. It was updated to There are 3 version according to the discussion in the bioglay interim. and ready. Thanks. to everyone who contribute to that to this document, we add some new requirement with some changes of the structure and some analysis of the related test method the text. moreover some of the solution draft has evaluated themselves according to the requirements. Next slide, please. So here are the main updates for from their one version to the current version. in section 3.2. It has some words about the bandwidth utilizations. is about the QF. The growth propagation delay is part of the that time imposed in a cycle which impact the bandwidth utilization. and the section 3.4. we change the title to be scalable to the large number of flows and tolerate high utilization. and add some tech to describe the high utilization and"
  },
  {
    "startTime": "00:16:05",
    "text": "analyze the traffic class. for the high utilization of scaling than that, the traffic that requires then the service can significantly for your observe. facetate of a link or the portion of the link where is it dedicated to start traffic. for example, more than 75% and or up to near 100% utilization. The In this case, the overall provisioning of classity done the work, it is require scalable queuing solutions to improve the bandwidth utilization. and for the analysis of traffic class in QB it use track and class to divide the flows and number of it. It is usually 8 So that's a forwarding mechanisms. It Zelle isn't complex with a large number of flows or higher aggregation. However, when adding new flows, the get control list may be changed So the recirculation is more complex. there might be a method to simplify the calculation configuration, which that more work to handsets. And for section 3 point 5 and the 3.6 exchange the other of them because one of the potential reason of flat flow, flow fluctuation is traffic during caused by Hublots, double or 2 chance. So the main text to change in section 3.6and3.7. we removed from disturbing service of this section title and extract some point, which by the cause flow fluctuation"
  },
  {
    "startTime": "00:18:05",
    "text": "based on the text and the discussion to make it clear. the in section 3.7, we add a new requirements, basically getable to a large number of ops ways. Comcast, top logic. for most of the applications, the company that they can say is a consent those scalability of securing mechanisms is quite And for the last requirement in section 3, we change the title. to support multi in single demand, the multilayer considering other sections in this section are also for the queuing mechanisms. So we removed to embed themselves in the title. And in section, 4 of the data plan, wait. Wait. delayed section four points. 34.4 since there is no direct mapping items in separate three way. and there are some reference So next task, please. So here's overview of the technical in current version, and I won't talk too much here. for the potential discussion of the structure if there would be next last phase. So the last two slides show the main text change of the requirement 6 under requirements. 7. Here are five points. abstract data, the potential reason to cause flow fluctuation. and one is virus and massive traffic flows of different application. in scaling network is a record"
  },
  {
    "startTime": "00:20:01",
    "text": "more birthday traffic, and there will be more aggregation nodes which receives flows from more off train nodes adding the dumb, dumb, a mistake. delay of the packet treatment. and the First of flows can be accumulated as flow Trevor's joint and separate over hubs. and the loops formed in the network, toplogic increased maximaximaxim First of the floops, floats, and the note and the links videos are more common. a large network. So next slide, please And for requirement, 7 is scalable to a large number of hubs with complex developers. So the first paragraph is to see that the network top logic can also be complex. It is required to support, like, work with such virus type of holidays and large outcomes. the second proper paragraph is to clarify the scalable. Originally, we had 2 as bags while escaping the from the latency with Okay? variety of hops that's from another escaping going parameter with a variety of hops. which require a good scalability of doing mechanism however consider during the found the latency of the application is always a content we'd have to recommend first point Next slide, please. So the next step is to refine document and coordinate with a solution document for valuation. And I think one point that is not mentioned is about from data"
  },
  {
    "startTime": "00:22:02",
    "text": "they were talked in the last meeting and also related choose a clogged drip easier. we will add some text, but not sure to be a new requirement in step 3. since the necessary jitter and the packet loss as 3 basic indicators of of of their nets. So once to evaluate the solution, the other basic requirements. And thanks. Any comments? If you can scan in, please use the queue. That's better. But it's alright. You're at the mic go. but in same for others. I think we'll do it. You you you can you can go with just in general. Sorry. I'm not familiar with the new process here. Yeah. My name is Glenn Parsons. I'm with Ericsson. The the question I have on the requirements here, is I'm not seeing a use case. I mean, there's great requirements on how a large scale network, needs to be this big. It needs to have these requirements. But what's this for? I'm familiar with the activity in 802.1andtsn. where we're doing deterministic networking, for azb. for audio visual bridging. for industrial automation in a factory for aerospace, in an airplane. for automotive in in a vehicle. It's not clear to me what what at the application, that is driving the use requirements are. Thank you. Okay. Well, thank you. It has come out with Scirting Scirting and hops and scaling traffic and crossing the and your question about the use case, I think. In the"
  },
  {
    "startTime": "00:24:00",
    "text": "Scirting, And then that, we think there will be more use cases but that is not defined in this document. for instance, I think the some emerging use case like, yeah, we are under the cloud gaming. which also requests the 10 minutes take latency. I'd also suggest taking a look at the current document. I wouldn't call it a use cases document. It's farmage document, but it does talk a little bit to the use cases, a look, take See if you have comments, please send them to the list. Kiran? Hi. I'm Karen from Future Way. I have a question on your requirement 6. a clarification question that What do you mean by flow fluctuation? Is it just the dynamic entry and exit of the flows, or is it more than that. Because if it is just entry and exit of the flows, I I would prefer if you use a different terminology than fluctuation. Thanks. Well, thanks. I think it's one reason to cause flow of fluctuation. but there are also other reasons because the flow can't see So regular as the as local network. I think this is excuse me. I think this is a good topic also for the If you have some specific alternate wording you'd like to propose, I think that'd be great to see on the list and See see how it's received. Totally second. So the one thing as a coauthored, and we had this already just for for the notes here. relating to the application. I think the biggest differentiation between the different application that we see"
  },
  {
    "startTime": "00:26:01",
    "text": "is whether they need also low jitter or not, and we need to somehow escalate the jitter requirement that in our ad hoc evaluation, it shows up as a as a separate line item, which I think is a key for set of the applications. With respect to the application overall, yeah. I mean, we've been starting to use examples, which we don't have in our use case. RFC, and so I I would also raise the big question of how do we you know, progress with that. Right? So maybe have a base, let it stand around. And if people wanna contribute new chapters. I don't I mean, so this remote driving, of course, is my favorite pet topic. I'd I'd I'd be happy to write up something, but I'm not sure if we wanna do something on that front. So any suggestions from the group would be nice in that respect. I in mind, we're contribution driven. So if you think we need an update that cover that captures use cases. Do it as an update. I don't think we need to do abyss. Okay. We can just say here are some more. Okay. If you think that's useful and the group think it's useful, then we'll we'll progress it. Yeah. You know what? or the feedback anytime on the list or so from Ethan who has been had the trouble of steering that zoo and being the editor, I'd I'd probably wanna catch up with you afterwards on that. Thanks. And with that, why don't you come to the front because you're the next speaker, I believe. my odd one? We we had told you that. Would be giving that? Oh oh oh oh, Sure. So that they should get updated in the notes. She should be remote. Yep. Okay. No worries. So Hello. Yeah. This is Yidro speaking from the remote. Please go ahead. Oh, okay. Sure. So I'm going to have a very quick update under TZQF draft. So we have 2 version updated"
  },
  {
    "startTime": "00:28:01",
    "text": "The first one is from version numbers 02003. Basically, there there were some detailed presentation for the mechanisms for both of the drafts talking about the CQF variations at the inter definite interim meetings And thanks for Debbie. We with his suggestion. We are trying we were trying to merge both of the draft because they are they are trying they were trying to use the same queuing mechanisms. So the 03 version performed the merging for the text and the authors. And so that the new version of 3 gives a detailed explanation of evolution from their conventional CQF to the tag, the cycle ID tag CQF. And based on the discussion from the data in It said that that's an interim. Also, we made the Forwarding specifications, independent, of the encapsulations. And there are separate sessions now. We have their MPOS encapsulation and IP and IPV 6 with the DSCP as the as their what we call their short term short term goal for for working with the IP. And, also, we added a new IP basic Acher header for the TCQF based but further That's net IP data plane. right now, it's called the DIP option via 2 possible extension header. and also the INA considerations for it. Next slide, please. Yeah. This actually talks about something which is"
  },
  {
    "startTime": "00:30:00",
    "text": "independent from draft. In the interim, we show are a demo we had performed over the large scale over over the win, which is as shown by the right hand side picture. It was in Chinese But now it it's available in English version. So if I'm interested, you are welcome. to check this version. So this is the test bet for the high speed and the large scale network Next slide, please. Yeah. This slide talks about Another revision after the merging of the two documents This is from the 0 version 03204. We fall following the presentation of another CQF relevant proposal called a CSQF. It's it has, like, use the various use the similar concept similar concept for the for the cycle ID based but use the segment routing sys instead of per hop mapping. So also, another text. Another content revision is about after after another review for the CQF. Here, the secure means they're already public they published SQL which is 802.1qch. So we find out that CQF could support multiple in depend instance of cycle buffers. TCF can do the same, but we didn't specify in the in the documents We haven't found whether we really need it or not. But So if needed, we can add it So that's the that's that's the revision from 3 to"
  },
  {
    "startTime": "00:32:01",
    "text": "4. Next slide, please. Yeah. So we the the authors of those documents thinks the draft is quite complete. received as the review and the validation, both from the debt net interim meetings and and from mailings and off and offline. So there could be some other component that can work together with TCQF, I'm pretty much sure that Thomas will talk about more in later presentation regarding the frame interleaving. It is almost equivalent to the ingress policy I think. And But that one, we think there are there are 2 independent components from the from their architecture point of view, One is the TCPF, another is frame interleaving. So so we separate them. I think that's all from my side and in or suggestions on it? Okay. We can move to the next presenter Shafu? Hi. How I have a a a similar question. to teaser curve. it seems that the in order to improve, we saw utilization tsecure for you to be combined with flow internally when the idea of following any all originate to flow more tasintas"
  },
  {
    "startTime": "00:34:02",
    "text": "which utilize defining time is for the resources in network devices. While teaser code will be used at the queuing design considerable offer, enhanced that figure for which only contains a few crews such as a typical 3 buffer mode. to to make a flow even difficult to achieve significant, efficient, Sounds good. So we moved a little quickly. Sorry. I didn't catch that you wanted. I thought you were coming into queue talk not to respond not to comment on the prior graft. So I don't know if Torillist or If anyone would like to respond to previous graph authors, the comment? Okay. Okay. So Torla says he will cover it later. So he will hear your comment and or you've heard your comment, and we'll talk about it. when he talks next. So if you could Continue with your talk now. Thank you. Okay. Okay. Sounds good. Hi. This is a self open for a motibi. are we I'll represent the remote For please can do next week. Okay. For for This question will mainly contains the following updates. offers the other more courses interested in the collaborating with this topic and fully enable with the and the ETF scheme, IBM, OPS, another work, separate when the rate controlled and the sorted queue based on the solution for languages. And the temporary need they needed the content of untangle per hope."
  },
  {
    "startTime": "00:36:03",
    "text": "and give the definition of the the source with control plane extensions parts calculations, And the deployment considerations of robust combination that be happening in sample solution policies put this for next page. Okay. This proposal is to fund our potential query mechanism that the matching larger screening requirements based on the challenges of who existing marketers we introduce EDF for security to deviate the data play to uniform and equal wide bound to the delete data by in time all anti mobile Next page, please. So According to the research result of some of papers, an EDF schedule of which wall which selected to the partner with the shortest the the the line for transmission. it's an optimal scheduler for a bulk edit lasers. but the the the the product condition of of for ETF or to work appropriately that the traffic could offer any service of low master all the way to satisfy that given traffic could be in the focusing when it reaches certain EDF for scheduler. For example, if the sorted crew is used, the the condition is, like, you know, the formula. We need to go detail. And for the rotation part, all TQ is used, the the condition is, like, a 0 at Next page, please. So for the updated one, the solution of read controlledplusortedq It may use the recipient defined in RC2212"
  },
  {
    "startTime": "00:38:04",
    "text": "all the Internet with the shipping define the ATS and a single Python key. at because easy flow is read controllers. So workflows arrived at the e/nd of scheduler ineutility and the the scalability condition is the better. A pocket is put to the pay for according to arrival time out of the schedule, plus the allowable cooling delete. Next page, please. So for the update too, for the solution of already controlled plus RPQ, these are the rotation polarityQ, Here, the difference is that the independent use location, polyolecules with counted on time, the smaller the city, the higher the probability, It did. the the scrutability condition is a matter. A calculator is put to the article according to CT, it's less all equal than queue and the queue is less than cdplus80. next pay. For the solution of latency conversation plus solidq, the first is that the latency conversation is based on returns the deviation with templatedse, due to maintenance conversation, all flows arrived at EDF schedule which is your visibility all in inclusivity eliseability targets are one way to schedule the faster before the in in the CBD pathways But it shouldn't be noted that the the initiative package here also include those have a how how how we change the flow in in its ability 2 in the community after a"
  },
  {
    "startTime": "00:40:01",
    "text": "experience of the policy when the time A package is put to The Python according to arrival time at the schedulerplusq all according to algorithm at the incoming port, plus dpluse Next page, please. this new ETF scheduling is the concern, how well we can take the antimicrobial based on d and the untangle may be preferable only iegris. For the case of animal behavior where you can see the pockets that denied, But the word design the queue or location rules, all in include rules may why the under to under boundary, the exceeding one the data may be up to the delay level value. The benefit for this case, it's easy for buffer site design. into low cost accumulation. while for the pizza of on time only are equally in this view, you can see the progress within the night. worsell up to the delay level volume. due to concurrent arrivals, and this arrivals have single time to send. This is a difficult handle. I hope that the The amount of vulnerability flows facing the client side it's small. make the impact of the rainbow. Next street, it's The delay resource of a link can be represented as the corresponding buster and the bandwidth and resources pool for easy delay level According to the scalability condition, based on a neatly back of the constrained focus, Next, please. For the kids of latency conversation, with in time involved, it should take a care of past the combination. It is exactly, like, the bus releasing of a distributed priority"
  },
  {
    "startTime": "00:42:00",
    "text": "surgardening. Some of pay per gate and evaluation must settle for this bus of combination. it can be used for the tool design, the buff design. And let me just please Also, this table is the evaluation of the degree of matching between this proposal. and that they're not scanning requirements. We know that discuss in detail because there there's a note so many time. Next, please. So next step, we we improve antimicrobial function. and the and the we think that the ETF is the legacy logafin offer enhanced the display So would like to request the w c or dosing. equation that comments, frankly. Okay. Shisong. Sorry, Shaul. I haven't joined the discussion of open meeting for for this document. I have to raise my concern here. I really have a lot of concern for this document First is that, can you explain what have you down more than the reference you have listed in page. I think, page 2. It's I think the the reference you have list did that, which is published in 1996. of the EDF mechanisms has a really detailed description of the the work So could you explain what's more on the the reference or just some implementation description. Yeah. yes, the reference document is"
  },
  {
    "startTime": "00:44:01",
    "text": "pay document that that will very relaxed, way it depended on, the key difference that we think are costly Denise for example, Nathan's composition, that the robust to it. Thank you. Actually, that is another concern. If you read the the reference you have listed that it is a very very solid assumption that the the flow a constraint in our hope has been, you know, inside the the the the traffic specification that makes the every hop latency exact exact exact accepted or can be calculated. And every claim in that reference have a very detailed proof of how to calculate the the latency bond. But I noticed that in your the the calculation is highly simplified, and there is no proof And the the latency concentration, I think it is invalid because if the if the if the traffic specification cannot be reshipped in our hub, there will be possible conflict between different flows and the the latency ending more latency can just, you know, help when the latency is enough if the latency has already been, you know, over consumed, it is invalid. So I think maybe more detailed descriptions are really needed in this document. Thanks. Okay. We we we just give a tool for in the doctor Please take a look at it. Yes. I I have reviewed the document. I think it's it's"
  },
  {
    "startTime": "00:46:01",
    "text": "It's a fan far from enough. Maybe we can have offline discussion. Yeah. Please see the reference. That that is the real proof of the calculation. Thanks. Thanks. Okay. Okay. Alright. You're also the next speaker. Okay. Sentient. Okay. Okay. Please 10 to next stage. So for many reasons, at Yes. t as in TS, introduced a single and also TTM must be the duty cycle. The idea is that when the pockets of the schedule, the tough group, arrive at a certain node. know the way you turn on the green light for Our billing cycle contains multiple viable links to time slots each for specific flows. Oliver, TS require time synchronization and has gathered scalpidity issues calculation update and the installation. To meet the large scale requirement to destock the enhanced gears to introduce time through the tabs of resources to layer 3 are the related terms for the base the scaling mechanism and doesn't know the data plane. Next, please. So Keycloak, we can track the the time slot type for the sources for each link. for specific or just a patient care with the instance. the resource of the authorization period are opened and for the service and it is the learners. related to the service bus, the interval. However, if another work device instance data cleaning resources, according to the recent period with the they require a large buffer site For example, the number of long loading queues is large. So we decouple the ultrafusion period of normal, the screening period. the later matches the actual hardware capacity of the device."
  },
  {
    "startTime": "00:48:02",
    "text": "require only a few long term increase all our single paper clip. The past calculation is based on time scoring resource reservation. Initially, and obtain a flexible marketing relationship will be pinned incoming time slot and outgoing time slot in each node. under data plan map or classification period with the time slots 2 secure an impale with the family source. and access tunnel through the resource to these other conflicts. Next, please. The first step is to determine the ongoing sentence score that when the flow arrives at the dollar, did this is a necessary for the outgoing time slow diversification. took at an expected evaluation of a distance delay The following two figures is so that the how do I put in the and the colors in the thermostat and the header and the under faster than node. Assuming that the service flow PRU decony which is the network entry. after traffic and regulation, each is superfast of the has an ideal position relationship with and or treat a patient care meet. Based on this ideal position, we can't obtain the ongoing Standard Time's world for this server bus. Next, please. the controller may maintain our reservation sub task for each sub bus then maybe multiple subbodies of the service bus into and the Yixin based consume a different time slot. Firstly, and the current for each sub task allocate a fixed all the billing terms load the t0+00. according to the idea, a low position g 2 of the the server password, patient or or treatment period. then the fixed outgoing times load of the redundant node. we map to a fixed"
  },
  {
    "startTime": "00:50:03",
    "text": "ongoing terms load. and the and continue to allocate, I think it's the outgoing terms with the g 1plus01 until the Translindoza. And so on, tiered and the point there's a physical observation or assumption that the the the server cost that doesn't know the arrival randomly at any node that disease are preconditioned. for pharmaceutical resource reservation based on the ideal position. next sprint. How about the we arrive partitioning recent or choosing tier, they have some of the reason. part of the student being matched to the server bus's data, who's outgoing thermostat follows the clothes and it behind the the real arrival position. If the real allow opportunity is a company that didn't want them, it is difficult to allocate the time slot. In this case, the mobile case initially, the first option is that the are the incoming port of the network entry an explanation of the buffer can be placed before the figure of schedule the support pockets to to good at the fixed Alvaro position. Or the second option is to let us get in TI with equates the tool or children, period, period, to direct any store pockets in the cleaning server system. you any opportunities active delay may be introduced under control plane, doesn't know the awarded. Next, please. this is about the to contact the the time slot resource of specific ultra location pair with the instance of the link each link has its own time slot resource and information. which include the Thomas with the lens or should a patient peer with a lens submitting a peer with the lens for each tab is for the recent to the tuition peer with them. it continuously include maximum of the cost and the"
  },
  {
    "startTime": "00:52:00",
    "text": "and reserve the bus. A possible possible calculation was the maybe to maintain particular for each subtask and netted the summary of Waller Knows Western's evaluation. must then needed the total instance budget. Next, please. So in the data plan, the parties work with the time is through the for example, let's say, of or tuition period. mentioned before, each node is, essentially, the queen the source according to the skirting period. So it needed to mark the times with a z of ultrafiltration period to demonstrate the z single quarter of split in period for long loading to if the constraint old less than m is one way to follow the dual resolution twisting. The market rule is simple. that it's it's a single code equals for zmodem. For sorted q, a particle is put it to the queue, according to the arrival time of the square data, plus o, PIFO can apply in time hold on untimeable. So the under to under delay is, like, a falling formula. that the comprehensive delay of on time to under given for intents. Next, please. k, traditional traffic regulation per flow is a sphere placed on the English node. in a teaching. It turns data for each server bus optimal. Since different the server parts, then they can sum different the time slot. Next, please. So we have a global time slot study volume when volume nodes have the same time screwed lengths, the similar transcription period instance, and I don't want to maintain GPU much states per pops per past global time slot may be useful. the key feature is that the incoming times what I"
  },
  {
    "startTime": "00:54:00",
    "text": "always in the same time like, These outgoing comes with i. Next, please. So this table is the evaluation of ticker for much in the life screening requirements. what I will not what is the describe in detail. Next, please. So next to where we supplement multithreadingpifocalbase the the each use in camera mode to proloxinated TAS cuddles will be handled. qualishings under commands. Hi, Shuffle. This is Shusong. Actually, I noticed that in your previous slides, you also mentioned that you will enter the pie poll contents. Actually, I I noticed in your previous document you have already ended this part. But I said drones strongly suggest do not do that because the the playful as I know, there is no hardware in any router or switch can support it. or or our existing device can only support pifo. So the the the pifo is kind of I think, academic implementation So I think it's not really practical. You know, it's narrow. Yes. Okay. We just provide a solution for once. operators can select the based on the real sentiment. sucky. Yeah. That that is the first comment. And the second comment is that thank you for listing Says QF as the reference, but I have already mentioned in our"
  },
  {
    "startTime": "00:56:01",
    "text": "open dynamic meeting. I think the the difference between the the set RF, and then this document is still not very clear. So if you have And the more contents about today's part will be very helpful. Thanks. Thanks. Okay, Sandeep. I will send a more detailed the response in fairness at the unbooked language. So depends how you implement the data point. I am not proponent of using the big vendor data points because they don't give enough flexibility. A p 4 data plan is much more suitable. 4, activate.net activities then from some of the vendors. Last time I looked, people didn't have great queuing support or easy easily implemented queuing. So I'd be really interested I'd be really interested in if you could send a good reference to the list that that supports your statement. Yeah. I'm serious. Thank you. And would be great if we could have the discussion on one of the interns when we have more time because I'd love to Okay. Okay? Go. Alright. So this is another proposal from our site called GLBF for next slide. So we had a detailed presentation about this last week in the interim, and, yeah, the pointers here for the slides and the recording are there. So what I'm trying to do here in the working group slot is to focus on the questions that were raised. during that presentation and which is pretty much, you know, new slides that that just focus on that high level next next slide. Okay. So what is GLBF? GLBF is the tsnATS solution with dampers, as opposed to interlift regulators. And, again, high level. Right? So I'm not going to go into all the details, but"
  },
  {
    "startTime": "00:58:02",
    "text": "you're trying to have opinions in here that those are the type of things you'll have to read up on anyhow. So next slide, So This is basically TSNATS. 4 routers showing You have 2 stages there. 1 is just simple priority queuing FIFO with priority scheduling, that is the beautiful piece of UBSTS and ATS makes it extremely simple. to calculate the per hop bounded latency, but then to maintain that over multiple hops. There is an element called the per flow shaper, and that per flow shaper is basically ensuring that whenever on hops flows are colliding that they get, you know, correct it again that on every hop, you can calculate a very simple latency, which I'm showing there to be equal to or less than L1, L2, L3 for the 3 hop so you just add them up. Your controller becomes beautifully easy. And so we call this in time because when you don't have any competing traffic, the queuing latency per hop is 0 when you've got the maximum amount of competing traffic the queuing latency per hop is that guarantee precalculated maximuml1l2l3 and in time, of course, you can come as early as you like. but you'll never leave later than what's guaranteed. next slide. So in GLBF, we're now using the damper. And the damper, what it is doing is simply using the beauty of we can do packet headers, we can measure how long you actually stayed in the queue, and when you stayed charter than the maximum guaranteed latency, that delta of time is put in a packet as a header And on the next hop, it is being delayed so that you are actually from the start of the queuing up to the end of the then put on the next hop exactly being delayed by that guaranteed latent"
  },
  {
    "startTime": "01:00:01",
    "text": "And so that easily can be understood to have exactly and calculus as UBS only that now you're not in time anymore, you're not as early as possible, but you're on every hop. accurately the guaranteed pre calculated latency. We call that on time. Next slide. So why? Next slide. So this is kind of what you would have to have in a big service provider network to make this work. Right? So we have a controller that does all the good things. But then you have, on every hop, the necessity for every flow, to download provision. And then per packet processing update, the shaper state. Right? And this is the stuff that we hate. This is why we killed RSV PTE, and replaced it with segment routing to get rid of this per hop signaling and per hop perflow very expensive processing. Next slide. This is the detail. Right? So this is the wonderful state table and worse yet in comparison to, let's say, RSVPTE where you just per hop per flow or per tunnel, as they call it, their steering. You also have to have a per packet read calculate right update cycle at line rate. That's that's the hard part of the shaper. And that was a little bit too fast. Next slide. And we want this. and GLBF gives you this. Same controller, same calculation of latency, same admission mechanisms, but you don't need to do anything on the intervening hops. you're maintaining the calculated bandwidth that each of these flow takes and you don't have any per hop per flow state that you'd need to update from the control plane or where you need to have a cross packet high performance update of of memory state. Next slide. Okay?"
  },
  {
    "startTime": "01:02:00",
    "text": "Right. That's what I said. Cost feasibility, I said. So there are 2 problems. Right? The control plane in the high speed right cycle. Next slide. Okay. This is the second thing. Why is it so cool to be always on time as opposed to be as early as possible. This is the long explanation about the fact that in industrial setting and many other settings you have synchronous traffic And the fact that the network asynchronous reduces the need to have on the application devices complex things such as a p time synchronization engine or coupled with that play out buffer. so you make the application a lot easier when you have Synchronoss packet delivery in the network. Next slide. Right? So you don't need the clock synchronization on the application side. and you don't need that buffering, which depends side. We had a lot of equipment, which was working great, customers set up boxes. They brought them into a larger network, and they weren't working anymore because well, the payout buffer is too small. Why didn't tell us anybody that if we have a larger network, we need more buffer And why the heck do we need it? Why can't the network do the buffering for us? Next slide. So why don't we do dampers with any queuing and scheduling? Next slide. And that is pretty much if we don't have a standardized calculus, to calculate the per hop latency, we can't do things. can't build a standardized controller. And we also can't basically figure out how to do hardware fast, high speed, and low cost implementations. Next slide. Right. So that's what this says. Next slide. Right. So positioning And so go to the the the explanation. Right? The the the presentation slide has the example of how to convert this into high speed low cost implementations either with people, long term, or short term, we've gotta work around with FIFOs, which will work."
  },
  {
    "startTime": "01:04:01",
    "text": "which is not ideal, but which would work. But in general, We're positioning this as a long term solution. We think TCQF CSQF are perfect starting points. This can be a great, you know, 2nd generation replacement. It always takes longer to do packet header. We need packet header stuff. Anyhow, So this is not meant to compete in the same timeline as TCQF, CSQF, but succeeded and, basically, can emerge then the benefits what we get with this solution. the benefit of what we get from UBS Next slide. validation. Yep. So simple simulation. Right? We don't have high speed validation. We did the simple validation to actually show that just on a single hop, we have exactly that problem. If we don't do a shape or a damper, we will exceed the guaranteed easily calculate latency. And then if you insert the damper, then you will get it back Next slide. And that's now the upper one, The vertical bars that go across the horizontal line Those are the poor packets where kind of the burst aggregation across a single hop invalidated your mathematic simple simple model, and then you do the shaper. and then you see that through the queue, you're again staying within the bounds. So that was the simple validation. That's the high level pitch for GLBF. questions, Okay. There is one Yeah. Oh, there is a question. Yes. Hi, Paul. I'm sorry. Okay. helpful. So I just wanna ask a simple question that But the TLBF will require the worst case delay, for example, marked as defined in your document. the what's the credit they are for easy horrible to be the same? Otherwise, it's a way be necessary to cover multiple different"
  },
  {
    "startTime": "01:06:00",
    "text": "Most case, M in the pocket. sync. I'm not sure I fully understood it. But you can, for example, configure GLBF to have exactly the same timing as a TCQF by just making the q the FIFO q on every hop as large as your cycle time. for example, and and that's in the draft and the and the longer presentation. But I'm not sure if it's answer your question. Otherwise, let's take it to the list. Maybe it's easier there. Okay. I I will send the barely about this question, something Mhmm. Okay. I had 2 comments. The first one is when it comes to a particular situation while A lot of flows have to be released into the scheduler. So there's an x related t encouraged for the last flow When the latency is big enough, it has to be carried and the packets. But but but in your in in drastically, Maxq is about the latency from the up screen and kidney and agency rather than the the Dexter an agency incurred from the last So again, I I I hope I understood the question correctly. I think this is a problem that of course, we would be inheriting from UBS Right? And I think we also have solutions for that. in the frame, interleaving, I'll go into show first how we're planning to support that with TCQF and I'll get to the, I think, GLBF solution for that Next IPF. I didn't get everything done. Hi. Thank you. Mhmm. And then the second comment Actually, I I I want to make it a clarification for an Shuff's comments because maybe they they they Quality is not not good. It seems that we have to, as and assume"
  },
  {
    "startTime": "01:08:01",
    "text": "The worst case latency from each top each top is the same. Otherwise, the different worst case to latency has to be No. No. The the the big benefit of this solution over No. TCQF and all the other cycle mechanisms is that the per hop latency can be determined by the controller, by the sizing of the buffer as they're, like, independent on the other hop. That is one of the big benefits flexibility that we gain. Okay. Take Okay. We skip over 1. I think you've skipped maybe. Yeah. Yeah. I It's number 8. Yeah. Yeah. Hello hello, everyone. I'm Gino. Junu Zhou. This one is the new draft. called latency guarantee with stateless failed queuing. Actually, this draft was including in the another draft called a synchronous deterministic networking framework. but for the through the the working group discussion, it has extracted as a new draft. Next slide, please. Yeah. This is the contents I will talk about today. yeah, next slide, please. Yeah. This is the overview of the draft. It first introduced about the field king schedulers in general. And the originations made in this draft finally, the details of the work conserving stateless And Corefield King called c score is described in detail. The frame up and the end to end latency bound That is always covered in the Adrian frame of document."
  },
  {
    "startTime": "01:10:05",
    "text": "And the operational procedure in detail is described. But some of the subsections from street 3.1to3.4. has been already covered in the data plan open working meeting. in apronos. So I will just skip that part The the two consideration foot implementation has been edit in this draft. as a new item. Next slide, please. Yeah. This is the basic frame of the called the C score, which stand for while conserving stateless core fail k. Here, the finished quote finished time, f t plays the very important role, which plays as a service order packets in basic to smaller Feet gets value of your service. the entrance nodes, 0, the Feet has been has been is is calculated based on the flow state. But at the corner, those the Feet is updated by the d subh-1p, yeah, which is which is called delay factor. So in corner, the flow state doesn't need to be maintained. And the schedule works in work consuming manner whenever there are packets in queue the link never labels. And the The packets in the QR serve in the ascending order of DFT. these are the basic framework. And if if the delay factor function is page bite with that red letter, in the end of 10 latency is bounded with that occasion. That occasion will be covered later in more detail. Next slide, please. This is the graphical explanation"
  },
  {
    "startTime": "01:12:00",
    "text": "about about about about about about about about the framework just like mentioned. flow on the observation, the package from the flow on the observation travels from node 0 to 8, And The entrance node keeps the flow state. But the kronos are stateless. Next slide, please. And this operational procedural I'll let cover in the previous open meetings. So yeah, the details I won't go explain. Thank you. Next From here, this is a new slide. Yeah. tab, I want to elaborate to some extent. I I would like to make you understand the end to end light edge bound of the c score in more detail. In order to do that, I I would like to I I would like you to understand. The concept of the ideal flow isolation ideal flow isolation The is achieved by a scheduler, which serves the flow as if there's no other flowing an image in every link whose capacities equal to the allocated service rate are. to the flow. So in general, the the flows are contending to be served by a single server or schedule it. But in the ideal situation, the flow is isolated with the own its own link, but the link showing it to the a locating service rate. In this case, the latent shape or bound, these the function of the flows own parameters, And given us that the b here, large b is the maximum precise. The large l is to make sure I pick your size. and the small r is the allocate service rate to the floor. And if you will be better understand if the b is l. That is"
  },
  {
    "startTime": "01:14:02",
    "text": "there is no significant burst. So only the single packets comes in a while. then that d is and bound it by its own transmission delay only. Right? So this is clear. But for Neto is a ideal flow isolation scheduler, the latency bound of that network is not the sum of the latent edge bound of each node. You have to understand that. If you see that occasion, inequality, The large AAT is the number of the hopes. Sodis. is multiplied, the l divided by r, the transmission delay, it is multiplied by h. But the burst part. is not multiplied by h. the so called pavers to only one's property. Okay? Next slide, please. Yes. this becomes more complicated. But, anyway, If I go through this slide fast, the the figures at the right describe the worst case that can happen in a node. the The each flow are generated the next gen burst, at once. And the flow on the observation generic to burst, but with a slightly less slightly later than those other flows. In this case, if the fit for scheduler is used, As you can see, the blue Two package and merge it together. And you can see that the the length of the Largest to blue area becomes larger. that we called 1st accumulation. Okay? So the burst can be accumulated in FUPU scheduler. but but"
  },
  {
    "startTime": "01:16:02",
    "text": "with the tDNA or the failed king or c score, that we are using. We are proposing. doesn't have that bet attitude bad behavior. So the packet interleaves or dispersed easily. So in conclusion, the filtering or the c score isolate flow almost perfectly with efficiency robustness and cities tico multiplex in gain. And here, the the CSCORE's end to end latency bound, can be given like that. which is almost ideal But with the addition of lmex Dividedbylarger Elmex here. is still maximum packet length over all the flow sharing that link. and large AR is the link capacity. So This term is due to the non preemptive nature of the filtering scheduler. Next slide, please. So, yeah, I want to hear I have described ended latency bound given by the core, the the advantage of the c score. But anyhow, there are 2 considerations we have to take into account. First one is that the entrance node has to keep the flow state that the these are the disadvantage of this approach. But any but in some out, this can be mitigated. for example, stateful source itself can act as the entrance node. The so the node knows kind of packet it generates? So you can generate the finish time and mark them as market as metadata as in the packet and send into the network. Then it becomes the net network itself becomes stateless. Next slide, please. You're not gonna make it through all -- Yes."
  },
  {
    "startTime": "01:18:00",
    "text": "Next slide, please. And another consideration is time difference between, though. So the finish time has to be somewhat corporate it. but the network no one can have different class. but but but but you we can cooperate those time difference, including the proposition delay. can be compensated Next slide, please. Yeah. So this is the conclusion according to the recover document, I have made check and remark. As you can see, all I think all the requirements can be met, except there's 3.3. It is accommodate the high link speed. We have 2 use priority queue. Yes. Up until now, I think that 2 o'clock per 2 o'clock. in qndqoperation can be done with the so called pipelined hip Yeah. It can be seen in the next page. Thank you. yeah, this you you can skip. Yeah. next speaker is coming up, you can ask Thank you very much. While the a quick question. Right. So I think the the the one thing where we may misaligning in the comparison of the different mechanisms. be The latencies that we're calculating that we're interested in is the latency we can guarantee, not only when the flows that currently in the network are all sending traffic. But when all the Mac some number of possible flows that we even want to admit in the future, are all sending the maximum amount of traffic. Right? So discuss it on list that So we're gonna have to to And -- Yeah. Yeah. -- that we we I'm sorry. We're out of time. Okay. Okay. Just remember -- Offline. -- as your time is counting down, that includes time for questions. Sorry about that. -- speaker is Ruben."
  },
  {
    "startTime": "01:20:04",
    "text": "Hi. I'm a new face here. So case for giving me take a little time to introduce myself who is interested? just say, say, water, Huawei and the 3com year on the winter. now it's it's a old story and a long story. Nawa, New East Tracy is the same company obviously. So when now it's Tracy, it separate the array, for ADP I it is really giving me my of after the year of 27th in the fall. I have being working in industry, say, almost 20 years. I mainly focus on IP forwarding mpsil in switch research and development. That's all. Now we got to the the the the trapped. that that Jet Graph came to So the critical challenge Oh, of jet reduction in the scale data data minutes take network window in the in the in the scale"
  },
  {
    "startTime": "01:22:00",
    "text": "detaminates it at work we have a lot of problem will caught called the data That may be scaled in, traffic and the maintenance control, queuing and the forwarding mechanical Also, follow-up, Gregison, may get make generate data So The the the jitter will add up. bend the Apple flow spend multi data made data managed to take network. at So We next a lot. So we we propose the we what you clock are referring to plane? to provide the a a a a certain cure rate clock For you to document, to enhance the the control the the control plane. that provide thomasinkraerasing wasting each domain. Next Saturday. We we use the comperson node with which with then which to the listen who can Who can kick you all the the the packet"
  },
  {
    "startTime": "01:24:03",
    "text": "the packet who are transmitting from the from from the the asked we're trying to make things from the domain. So We We we we we composing the the the value we use the the 4th The mechanic calculate compensating value based on a reference delay. and that actually transmit it delayed. And conferencing for transmitting delay at the competency node, to efficiently reduce the data. Next slide key. The they're my canon have many Benefit. What's the one? the time synchronizing is not require we send you to detonate the domain it's not only between the data node domain. Second delay, the egress and the egress node. click the act truly transmitting delay in each domain So with no need. Emmy, any any any tool in the in the in the pin node. Staking certainly, the compensating transmission delay. itra, compasses in node. connected to the listing."
  },
  {
    "startTime": "01:26:04",
    "text": "Next slide. in closing, we the draft proposed a new method of package queuing and the scheduling scheduling of compressing to not could sacrifice for reduced data? Thank you for all the testing. Questions? Okay. you very much. Thank you very much. Ryllis, I think you're back again. Okay. So this was the additional thing that came to mind working on all these per hop mechanisms and trying to explain some of the things that that that we're missing and then also the TQF next slide. Sorry. This is not as nice I can, yes, as the previous slides, I'll I'll promise I'll I'll do better. slides for some intermediates. So in TSN, we have this thing called Gate. which at the edge of the network on ingress can be programmed to allow for whatever packets you classify, to let them in starting at exactly now Up to now. And then Meh. No. Again. No. Okay. So, basically, very accurate timing of this, and there are multiple use cases. Right? So It is actually the base mechanisms by which CQF in TSN is being built. Right? is not a novel mechanism. It's just a configuration of these gates. the interesting stuff for us, which we don't have as dedicated solutions for right now, in which I think is a missing building block in the"
  },
  {
    "startTime": "01:28:01",
    "text": "is what I would call gate as a function the same way as we do, you know, the the pre auth elements to enable flow interleaving. There may be other users the folks with t s n experience might jump in and say what it could be good for. We we probably wanna use exactly what t s n has we've been doing there and see how we adopted so that we don't need to reinvent the wheel So it shouldn't be that much work other than good document work to to define them. And so this draft is just meant to make the argument that we need architecture for the flow interleading. Next slide. So the The use cases are really want to maximize. possible dead net utilization in a network deadnet traffic utilization network while maintaining minimal end to end queuing latency. Right? And the benefits of this mechanism is proportional to the number of hub So the larger the network is, large rings in a metropolitan access area or so. and also then based on different type of speed links. And this works best with mechanisms that have on time delivery like TCQF, CSQF, And we act I I actually started to add this to those drafts, and then I figured it's much better if we have it really as an independent component. Next slide. Now the point is this is not depending on these mechanisms. It just works best with them. So this is the example use case. Metro metropolitan access ring. I've seen these to have as many as 20 or 30 routers. in in areas like India. So now you consider that you have these microsecond area cycles, but then you have flows which are in the order of milliseconds when they sent their packet. because they're, for example, based on remote driving, you have 50 frame per second video that you steer. So you need to send for every"
  },
  {
    "startTime": "01:30:02",
    "text": "frame that you have, let's say, one round trip to update all the status. So in the order of millisecond. And you have thousands of these flows, for, you know, all these cars that are driven around the city from people sitting at home. And now you ask yourself what latency can I guarantee? And when you look at the models, then it is basically The low rate doesn't help you. The problem is each of them may have a 1500 by packet In worst case, you're adding up per hop a queue that consists of 1500 bytes times 20,000. So it's going to be way too long. So the only way you can do that is You can call coordination of these flows, interleaving You're basically within, let's say, every 10 millisecond interval, Each of these flows need to be sent into the network at a particular offset within the 10 millisecond And then you can, you know, increase the capacity of dead net traffic that you can surf at the same latency by a factor of 100 or up to a 1000 versus the micro second that you have without gaining additional per hop latency. Right? So now this interleaving, next slide, And I was talking about this. Next slide. Okay. I think I'm missing a little bit where I was trying to to say the thing. I hate slides when they're not following what I want to say. Who wrote these slides? So there are really 2 interleaving cases of interest. Right? So the one interleaving case is you just have the same entry and exit point and you're just aggregating these flows. Right? So you have thousands of these flows, but they're all from here to there So on this point, you're aggregating these flows by giving each flow a particular offset in time when it's burst goes into the flow. And you have the aggregator flow. And this now has a not higher burst, but just a higher rate. any point in time, there is only one packet of 1500, but"
  },
  {
    "startTime": "01:32:01",
    "text": "the frequency at which they come is much higher now because you've aggregated 100 flows, So you have hundred times the the bit rate. Right? And you can do this aggregation with any of our per Hopper. forwarding mechanisms. Right? So it doesn't have to be TCQF or any of other our cool mechanisms. You can use the whatever worst thing, let's say, RFC 2212. Right? So insert from 1995 or This works for all of them. Now only when you get to the point of you've got some flow coming from here, and it goes along here, and it's the slides don't have the nice picture. Right? If you're interleaving these multiple flows somewhere in the network, then your per hop mechanism needs to work together with this. Right? So that You have the accurate on time for hop forwarding that allows you to pre calculate the interleaving within the network. Right? So now proposals that basically say we must have this flow interleaving gate on ingress be coupled with a per hop forwarding, ignore the fact that for the simple point to point case, It's completely independent. And I think that's an important case for us as well because that, for many deployments, Right? may already give you enough the detonate to make the gates worthwhile. And that is exactly what I'm trying to say here. We can use it with t s n a t s with any of these mechanisms just for the hub end to end point to point aggregation It's just not going to work well if we do the interleaving of different flows in the middle of the network. That's when we need the better on time mechanisms like PCQF. Next slide, Right. And so when we compare our TCQF and CSQF, That's also when we have a difference in how well they can be used in conjunction with this flow interleaving. And so there is a a chapter of that. It's it's quite complicated. where you can understand that CSQF can make it easier to avoid that you're overloading individual cycles"
  },
  {
    "startTime": "01:34:03",
    "text": "along the path because you can kind of allocate cycles adjacent to each other much more easily. Right? So how much worth is it? difficult to say, so we're still in the evaluation. But the reason of being able to distinguish performance of TCQF versus CSQF had me talk about that flow interleaving. Next slide. Yep. So pretty much, I think, think of Gates as another, whatever we want to call it, you know, interleaving function or pre of eye, or so that the total set of functions that we wanna have at some point in the network in this case on ingress, right, as a separate function. for flow interleaving as an orthogonal aspect, and I hope I'll of of of have better visual slides. There are all these wonderful slides that show how autonomous cars are interleaving so that you don't have to have red lights, and I couldn't find and I'm not even sure if you can show a video here. Totally different question, but it's it's so easy to see that in many other places, they're kind of looking into exactly this to get low latency with high utilization. Please. Yeah. I wanted to reflect to earlier statements and also here in the last summary slide that Does that the chunk state common architecture between the data and the working group and the into to the 20 s and Taslo. And on the PSS side, effort has been done to make the TS and QS features available for nonwish devices. I gave more more to the chat. That is an almost done or a project, the 802.1bc, Mhmm. whose goal is that folks do not need to open up the 2 1000 pages of 802.1q, but extract this QS feature and make it available for routers, firewalls, and so on. So we may rethink how to commonly use These QS features in that data and the DSM like, ecosystem."
  },
  {
    "startTime": "01:36:00",
    "text": "Yeah. That's And also being Yeah. And I was thinking maybe just that when we do our yang model, We also have the yang model so that you have a consistent provisioning or something like that, whatever the minimum is that we need to do so that you have a consistent dead net operations and provisioning experience. Dan McDonough. Could you please back out to your use case slide? you have the topology. Yep. just have a question about that one. More. More. More. It's just like More. this one. Mhmm. So you are saying that in a metro ring. You're expecting here to have, like, 30 hops I I've I've seen that in India. So usually in the comments. You can negotiate me rate me down to 20 No. So number 1, it's the what they will be doing, they have been they they don't make round robin, they have, like, no cross connects to shorten the times. And for the metro rings, usually, I don't see that you'll be doing IP hop by hop they will you know, usually, they'll provide an optical link between the endpoints within the So, I mean, that's that's always the question. Right? So What do you wanna think that we never, never ever need to look into when customer wanna build these networks. Right? I mean, I've I've been been dealing with the problem that even the second failover and SRP rings wasn't good enough because you couldn't build a ring between East And West Coast of the US, and you had to subdivide in 2 rings. and they were really annoyed about it. And so I'm I'm I'm rather sticking with flexible solutions for whatever network I'm getting, and I was trying just to get good. Look. We're just fine. So you're just making a worst case scenario. we have to wrap up. Well, by the way, that's a good discussion Shafu was in line Let's let's go to yeah. Yeah. I I suppose Okay. Oh, sorry. I have I have a question that okay. Based on that year of flow interleaving it seems that the the forwarding play definitely needed to to any certain"
  },
  {
    "startTime": "01:38:00",
    "text": "I could call to Yes. decoder to flow In the name of it. Yes. You you you you you you know that. You'd only need to to provide a single path The the the forwarding plane needs to allow you to predict the time when packets are at a particular hop in the network so that you can plan the interleaving. Yes. Sounds good. Okay. Thank you. Ruby. Again, my My name is Juugin from HCC. The the draft they have present on the last lost the meeting. So way I have pretend here again. we know in the in skill network didn't matter. No lightning topology will make queued result complicating problem more difficult to So Wing, It's we When conflict happened on the coming to pond, change the plan cycle of the head. and the perform cyclecalculate to it's a elemental route clay thing. Next slide. So we propose the It was So we proposed the our VPFC climbing information model"
  },
  {
    "startTime": "01:40:01",
    "text": "they try from the railway networker at Type table concept 2. bid which if he's a manager resource to solar complicate. The model installed a resource plan mode to meet the requirement awful, awful, See at Cuap. p s qapple all time slot. On VPF, we we what you priority forwarding path is the forwarding path that Sapoter cycling based forwarding, And have map of relationship between the schedule cycle of upstream and the downstream node. VPFC is its tablets a jet like a panel VPN, the And the it has a unique identity psychoinformasing, and the social sales in bandwidth right away thing. Next slide, please. ang so, There many can can provide many PanFate. what's the to ret realize the resource plan model and a part of who detonetecontopline. bits can meet requirement of CSQF. tftgraphtimestlot 2nd, delay. we only need the high node of VFP, schedule the date of the detonator flow, according to the cycle resource owner by"
  },
  {
    "startTime": "01:42:01",
    "text": "which did not follow along pronounce. redraw taking the suddenly, resource we're wasting cycling and the transparent for network node. So we make the implacay thing of the resource relevant reason for data plan device. as lightweight and the stateless as possible. So finally, with PFC is a like a tunnel. it can be set up by microfflow. Next slide. We have update from the last a meeting. we welcome the as a coaster. and we tend the the trap standard to information. we have tens is a part of our control plane. adds up we kept applying from the same model. to Year Harris your control plan. 3rd, we tend to some more introductory and redefine the forwarding paths and the modifying the define of way path of p, Next time. Our next step, it obtained the the the extend the the plan model to improve resource your utilize this thing efficiently. and the add this curve"
  },
  {
    "startTime": "01:44:03",
    "text": "to send all config information model for the select mode. That tall? your Thank you. Thank you. Thank you very much. Comments? Okay. We're gonna move on to the last speaker, speaker, And we do expect to poll at the end of this at the end of the slide. So please scan in and be ready for Hi, everyone. I'm Karen from Future Way. and I'm presenting the second version. I am. -- sure everyone's awake. It's good. Welcome. Well done. just going to hold it in my head. So this is the 2nd version of this proposal. Next slide, please. 1 of the bigger changes that we added a new author, Lewis, is a coauthor for this draft now. And a quick recap to our problems space. We are working on providing an interface from application to deterministic networks, and we felt that it was under specified as the dead net architecture. our current so my biggest motivation is how can we do remote process automation? So our focus is on applications that are sitting and some kind of a softwares and programmable domain and they can control your sensors and actuators on the other side on the factory floor. the main focus is from application to that net relay node because we consider that net relay node will the edge of the network. Next slide, please."
  },
  {
    "startTime": "01:46:02",
    "text": "And we are looking for some kind of programmable interface. So if you want to do automation the pace at which you want to move your machinery or some other parts of your factory floor, they would dynamically change having a programmable interface is really useful, And in order to do that, we use we make an assumption that applications are running ipv6 because it is pretty difficult called to extend ipv4 now. And so it makes lot of sense to use extension headers in and specifically hubbyhopextensionheader. And what that format will look like is presented here in the slides. So the way we go about it is that we identify set of traffic patterns that are relevant for process automation. and how they can be reflected through this extension header. So that's the general idea. I'm not going to go into the details of the extension header, we can have that discussion on the mailing list. Next slide, please. So those are the things we did not change, and these are the updates since we spoke in March, Lewis's not coauthor and he is interested from the effort that is done on 6ganddesire6g. And one of his major feedback was that instead of just looking at this traffic patterns, the request we are sending from application to dotnet, maybe we will need to add some more information or maybe we want to extend this extension header. I kind of paused on it that let's see how this trough works, and then we can start adding. other elements of requests that we need to send. Maybe And one thing is right now, I'm just talking about the request that will go from application to dotnet."
  },
  {
    "startTime": "01:48:01",
    "text": "There is also a possibility that they relay note could send inform some information back to application in order to express some error or some other state changes that happened in network and application may be interested in. and informally cross check. Yeah. So second point is quite interesting. I don't know about this room. but I've been shopping around and speaking to ipv66 birds, and I asked them that, hey. I'm using this hub by hub option. Do you think it's the right way to go about it, or I'm doing something wrong about it. And so far, I have got very positive feedback. Most of the people are telling me that it is an interesting use case, a very interesting use case how you are using extension headers for this work. And then there was a very comprehensive review from Florian. He actually went through line by line of document and provided me comments how this document can be improved. to the extent that it was very hard to provide all the answers in a comprehensive way. I created a Google Doc link, and we exchanged information through there. And I actually invite all of you to just go and comment on those on on on on Yeah. Just let's comment and resolve those issues on this Google Doc. or another approach I'm taking is picking up one topic and bringing it to the mailing list and discussing it with the team. Next slide, please. So these are the major changes that there was controller term was kind of confusing. because we are in IETF and network Normally, people confuse controller with network controllers. So I started using term process controller, but I spoke to someone"
  },
  {
    "startTime": "01:50:01",
    "text": "yesterday, and they are not satisfied with this term either. So things will evolve. And suggestion was that Maybe industrial controller is a better term than using process control. and there was a contention on the terminology cloud. So I tried to remove all the instances of cloud from the document because there is no way I was implying you would do a remote process automation from a public cloud. It could be your enterprise cloud on private cloud or maybe just your applications are removed from your factory floor. through a van or some other mechanism where detonate is deployed. So I took care of that part of terminology, and There was an interesting question from Florian that how do you perceive where your process controller is sitting with respect to applications. is your process controller part of application, or is it a separate entity. So I've tried to explain it by adding communication model in section 3dot do. And so The general idea is we consider it as a functionality. or a logical function point. And by doing that, we make things quite simpler and whether they should recite with applications or outside applications that should not be a concern to us. We are only providing an interface for operation and control functions. to the deterministic network. Since we discussed, right, there is yeah. There was I I since I was looking for all the traffic patterns and looking for end to end communication also I inserted a text on transport protocols that TCP UDP may not be suitable for for operation and control, but I found those things that"
  },
  {
    "startTime": "01:52:01",
    "text": "outside the scope of this document. And in next revision, I will remove them and clean that part up. then there are some editorial set of changes that will be very easy to see through the diffs. Next slide, please. So this is the generalized communication model will be added in section 3.2. So it might help us with some terminology also fusion between process controller, network controller, industrial controller. Let's say I have a debt net, and I just provide logical points that OC point is operation and control points. So In my mind, it covers a lot of scenarios where process automation in the commercial automation or any kind of OT related functions are involved. So we can call that OC point and things the traffic patterns that are related to sensors type of devices. they can be called sensor point and then similarly for actuators can call actuation points. The major differences between read, write, and the traffic patterns for sensors are more in level based and and like, pulses going out from the sensors and for situation, it is writing onto the devices with certain latency constraints. Next slide, please. Yes. So the next steps are we want to look into OCN options in a more comprehensive way. For example, this is something I want to get a feedback from the group. Would it be viable that we can have also communication back from the dead net relay node to the application in terms of if there are any errors that we want to indicate back And another thought that came to my mind was that we have the notion of service sublayer in dotnet. And that was supposed to be implemented on the detnet end stations."
  },
  {
    "startTime": "01:54:00",
    "text": "can we use this effort to define service sublayer in a more comprehensive way. But that's just a raw thought. I'm not too bound to it. Something that workgroup working group can think about. And do how yeah. So there is one part. So we we are using extension header to express applications requirement to the deterministic network. How delay node on the net will translate it. We haven't covered that part. It is more like a soft where an implementation issue. So that part is outside the scope of the document and I just wanted to highlight that. So more comments and feedback. This time, I got more time to talk about the internals of the document last time. I was just focused on the motivation part. and and I'm scared, but is workgroupadoption is it premature to ask for it, or Should I do that? Well, let let's see what the comments are. You're always free to ask. Okay. But it's let's let's We have Janos and Torlas in queue. Yes, please. Yeah. So thank you for the presentation. my question is if operations technology experts like industrial automation experts, this. are interested in both expressed this is a good way to go because or is it just networking as first time to do something without the involvement of industrial automation x experts. I have not seen industrial automation experts on the list. are in the now. this. Yeah. That's a good question. How do we get feedback from the experts because those guys are not sure. If you have any ideas to let me know, I I can tell you that Florian"
  },
  {
    "startTime": "01:56:00",
    "text": "is involved in some type of OT networking. and he did thorough review of this document so we can at least say that there's one person who's involved on the OT side has looked into it. I was just talking with somebody working with Industrial Networks, and he said the IETF was so lovely to them when they had a draft that said industrial Fubar, review us came back and said, hey. If it's industrial networks go away, the Internet go to an industrial forum. So it's but, yeah, talk to him. Yeah. see. I wanted to get to the one point about the packet header that you were saying. Right? So maybe as a general comment not only to you, but I think we have so many different of these proposals that ultimately will end up requiring something in the in in in the packet header, and maybe we you know, the coauthors of these different documents could come together and just, you know, do do something of this this goes nowhere, but this would be a draft to put all these things that we potentially from dead net from our proposed wanna have in the header so that we can engage with MPLS and 6 men and ask them what in general do they want us to to do so that when we actually, you know, probably much later. come to them with a with a real working group conclusion that we don't, at that point, time need to start from 0, the discussion with them about how to exactly do do it in the solution Yeah. That's a fair point. Only thing is you also mentioned MPLS And since I'm looking from the application side, I don't assume they would use MPLS directly from the end station. That's one of the reasons we want to introduce extension headers with ipv6. Oh, no. fine. I mean, we have these these big religious, you know, sites, one for ipv6 one for MPLS. I think the the MPLS side"
  },
  {
    "startTime": "01:58:00",
    "text": "there is MPLS in the Linux kernel. Right? So you can buy network function elements that directly do MPLS as well. So I'm trying from from the dead net head where we started with MPLS, and and hopefully doing more for SRV Six now in the work as well. I'd love stay neutral there. So what you are saying is very close to this point number too that should we specify service sub layer more comprehensively? I I also think that in something like the industrial stuff, I'd be surprised if MPLS takes a role, but I was saying, let's take the the header, you know, have a have a joint discussion between what we may wanna have from that net with the header folks that are responsible 6 men and MPLS. yipyipyip Okay. While you are always welcome to ask for adoption and many do, I think it's a little early. And, actually, Janesh agrees for adoption. But what we do wanna ask And this is a there's three ways to answer this question even though the raise hand tool that says raise or lower, We're interested in if the group is interested in hearing more on this topic. You know, is this something we should keep discussing and Raise your hands If you are and you've read the draft, Lower your hand, do not raise hand. you're interested and have not raised, If and have not read the draft, and come to the mic, Or join the queue if you'd like to object because we'd like to hear why. So if you participate in the poll, you're saying, yes. You wanna work on this or you would like to hear more. if you think you don't wanna hear more, don't participate or even better come to the mic and a, you know, Maybe we shouldn't be talking about industrial applications, although that's what that net's all about. you know, maybe maybe we shouldn't be talking about it here in the Yeah. So we're we're this is actually the reason we phrased it this way. We suspected there would be general interest."
  },
  {
    "startTime": "02:00:01",
    "text": "not a lot of people have read the draft. And that's, I think, what this is showing, although you know, the the numbers are less than a third of the people in the room. So we'd love to hear if anyone wants to join the queue. and object Well, that's great. It's nice to end without any objections. So there's definitely interest to keep hearing more. So please discuss on the list. Please read the document. use your opportunity and comment on the list. and we will see you all tomorrow. Thank you very much for a a good session. and all the good contributions. Thank you, Janos. We'll see you tomorrow. Thank you all."
  }
]
