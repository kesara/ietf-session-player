[
  {
    "startTime": "00:00:08",
    "text": "all right uh welcome to ippm this is the last session um i think we'll probably wait a couple minutes before starting to let some other people show up but i guess if you are remote uh maybe if someone can give a signal of if you can hear the audio just fine all right we have confirmation yep thank you yes so we're not in vienna even though the"
  },
  {
    "startTime": "00:02:00",
    "text": "slide sets up okay does it matter so in case you're wondering we're still in philadelphia just a quick audio check before we get going as hell works fine we hear you thank you so congratulations everyone you've made it to the last session of the week this is ippm and we'll get started very briefly here i can take the time to remind you all if you're in the room that you should please wear your masks except for when you're up talking then you have the opportunity to take your mask off otherwise we would like you to keep the masks on and please use an ffp2 kn95 or equivalent mask as well oh"
  },
  {
    "startTime": "00:04:05",
    "text": "okay we have now been teleported to philadelphia so let's continue so yeah welcome everyone this is ippm last session of the week i hope you're still awake and fresh and yeah let's get on with it i hope you're all familiar with this slide if you're not you should really try to read it by now um it it sort of it's important for whatever you try to say and what you contribute with there so meeting management uh we're running this as a hybrid meeting so you can join me deco on this link here um if you are on site and you want to join the queue please do so via meet echo either using the the on-site tool that you can have on your phone or via the meet echo session on your laptop we'll be using meet echo for sharing slides all the slides are loaded here we can advance the slides for you or you can request permission to to to advance the slides yourself notes can be fine here and we would like to have somebody volunteer to take notes for this session it's the last session of the week oh thank you very much do we have somebody who can take uh the role of jabber scribing as well just relaying stuff from the chat oh okay perfect perfect all good um so document status we have a couple of documents that are owned by the iusg right now there is a bunch of reviews of iom flags direct export rfc 8321 bis rfc 8889 bis lots of good reviews some revised ids"
  },
  {
    "startTime": "00:06:02",
    "text": "flying around and it looks like we're making good progress with this we also have iom conf state and ipv6 options that have been handed over to our ad and we have an ongoing or we just had a working group last call on explicit flow measurements and i see we have martin duke in the queue i see i'm still the delegate from vienna that's cool um uh yeah so comp state that's on me i've sat on that for like almost a month i'll get to that like next week um ipv6 options to be clear is with the well i mean i i have possession of it but the action item is on the authors um just to reiterate that in case they're listening and don't realize that and um 8321-8889 will probably be a while before i guess through isg i think we have to do another big scrub on the experimental language in there kind of lucy goosey you can try this or that stuff um and frankly input and help would be useful there uh if people aren't so inclined thanks thank you right next slide please so we have a quite packed agenda today uh lots of drafts uh so we will be focusing mainly on on drafts that we are working on that are adopted by the working group so uh we'll be starting with a presentation of iom data integrity and deployment it will be presented together and then we have the iom yang and stamp yang stamp srpm then we will have a bit longer presentation on the ip capacity protocol followed by a presentation on the explicit flow measurements which has just been through uh working group last call um then we have ippm responsiveness and encrypted pdm v2 these are the these"
  },
  {
    "startTime": "00:08:00",
    "text": "are the adopted drafts that we focus on this uh this meeting um are people fine with this order of presentations so it seems uh and then we have one more presentation uh it's about precision availability metrics uh which will be presented by greg so let's get started hi everyone so just a quick update on the iom integrity draft first okay so what we made as changes from previous version uh is pretty light so we can call on the document quite or pretty stable by now the changes include um that we replaced references to draft that became rfcs so you can find them on the slides we also rewarded some important parts and that led to actually merging the capsulating node and valid sorry validator roars and we also included some editorial changes so basically you can find the poor request by clicking that link and you will have each detail that you need and so a good news is that there is an ongoing implementation effort by the university of liege in belgium so let's look thank you and so we could actually already consider a working group last call maybe but we feel that we're not in a hurry so this is my question for you should we"
  },
  {
    "startTime": "00:10:01",
    "text": "wait for feedback on the implementation we should have it by let's say in september and then consider working with pascal that makes a lot of sense um is do you know if there's any plan for other implementations so we can test interrupt with this i don't know how much that right now this is only planned for vpp okay um i i think having the one implementation is probably sufficient so we can do working group last call reviews um it would be fantastic to see other implementations um you know as we are progressing the document so i don't know if we can try to rope people into doing hackathon at the next itf or something like that that could be useful this is worth mentioning that um the implementation is about the integrity of the trace because this is kind of a corner case in the in the integrity of iom so this is so that's all for the iom integrity i don't know if tal is already available should i take the presentation for him yeah because you had a conflict so so quite quick for this one too next slide please so no change from uh ietf 112 we published the new version right after uh vienna and basically it only includes references to beer and so that led us to consider maybe a working group last call xiaomi hi uh xiaomi from zte i have a comment on this document uh i suggest authors to add one more reference to iom com state"
  },
  {
    "startTime": "00:12:01",
    "text": "as you have seen on this chair slide that document is now passing working group last call and with our transporter ad so i suggest you to add that reference and add some description on the function of iom capabilities discovery thank you okay thank you so again i'm just a proxy of tal and other authors of the document but i'm sure they will consider your your remark thank you thank you for the notes should we get that down i think that's the conf state configuration state document that they want to reference too uh gentlemen was asking for a reference from the deployment document to the iom conf state thank you hey can you hear me this is it yes thank you and in this presentation i will report the major changes since last meeting and next um we adjust the comments from andy as the young doctor early review there are some major uh minor issues firstly use the derived from our self for the when statement using identities we followed this suggestion and modified the yam model and then use the interface with data type yes we align"
  },
  {
    "startTime": "00:14:02",
    "text": "and the use of plans string as a list key to adjust this we add length 1 to max to disallow empty strings and to cl uh clarify the use of order by user we checked the list and the usage there is actually no special user order so we removed this statement and is there any mandatory functionalities we add a description to clarify this there is no monetary type of profile in the list but at least one profile should be added and then there are several points in the model the description are very simple we added more detailed information here and there especially for the mentioned lines at last we cleaned some needs and that's all next the rfc 9197 is published this year model is already aligned stable and mature so we would like to ask for group last call thank you yeah it it seems like a good time to do that thank you very much"
  },
  {
    "startTime": "00:16:01",
    "text": "morgan yep okay hello um next slide please um so this is justin um give you a quick update we progressed the work and as discussed in our previous meeting so included the coverage of rfc 8972 stamp options extensions so next slide so now it's for their operational stamp information now it's a config false so it will be read only next slide and it includes the uh 8972 including a stamp uh session identifier that uh may be used for uh session demultiplexing as well as extra padding location uh tob time step information tv class of service that allows um testing uh treatment of different dhcp markings and uh in one way uh both directions what direct measurement was access report uh follow-up telemetry so to uh improve um accuracy of timing measurement in particular in a virtual environment nfv and authentication for extensions next slide"
  },
  {
    "startTime": "00:18:01",
    "text": "so next we'll continue working and we'll try to get uh it ready for the working group last call by the next meeting any questions so please uh take a look at this document and uh please send your comments and uh questions on the mailing list thank you um yeah thank you for this when you uh when we have a new version that we think addresses all of those early comments um that looks like i think that early review is done quite a long time ago yes 2018. um so i'm wondering if it would make sense um you can just let us know on the list and then we can kick off another young doctor's review uh we yeah and then we'll do last call yeah we follow by the list of comments that mahesh provided and addressing when we feel comfortable then we'll appreciate your help and we'll reach out so to have another thank you hmm uh hi everyone my name is raques gandhi and i'm presenting the stem extension for sr on behalf of the authors listed next night please so uh it's a very brief update on the revision that we posted recently uh just to highlight uh some work in other working groups and the next steps excellent so uh many thanks to footer for"
  },
  {
    "startTime": "00:20:03",
    "text": "rfc 8972 for the flags and the tlvs so the new flag we had defined verification flag uh it applies to all the stamp tlvs and including the two tlvs defined in this trap so some text added for that and there is a notion of symmetric packet size so this is also added for the the two tlus defined uh in this draft and this also allows us to transmit the tlv flags back to the sender so other than these two updates um we added experimental values for the tlvs in order to facilitate the interrupt testing and implementation uh but we also have made the request for um earlier allocation as well there are some few minor editorial changes and we don't have any open issues so many thanks to everyone who reviewed the document and provided comments as well and next slide please so we do have a few uh companion drafts in other working group this one in the spring for the srpm using a stamp there is also some for the enhanced srpm as well as one for the mpls pseudo wire next slide please and uh we are seeking your comments and uh suggestions uh at this point as well as the inaudible location thank you greg yeah comments hi thank you rakesh and uh i just wanted to um fyi uh we talked with the and footer about some other methods and"
  },
  {
    "startTime": "00:22:01",
    "text": "we started work on document ip udp encapsulation of stamp in mpos and using lsp team to bootstrap uh stamped session along with their controlling the path for reflected test packets so aiming this work for mpos working group but appreciate your reviews and comments to mpos and ittm working groups thank you okay thanks greg yeah we had a good meeting and there were some good discussions and we'll update the draft if there is any need for it as well thanks that's great to hear um so regarding the early allocations since that's something we can do is that are there any objections to doing those early allocations i guess you know greg we had gotten a lot of comments from you before are we do we think it's okay to go ahead and ask ayanna for that now um okay i haven't thought about it as objections i haven't thought of my comments as objections so it was comments and as rakesh pointed out so we had a meeting and we realized that there are many ways to um so okay the problem that this draft addresses is real operational problems so now we started to work on a little bit different approach to addressing the same problem so yeah it's perfectly fine to go and help with the implementations and get the early ayanna so to put it in a uh good footing great thank you thank you very much thank you very much we'll we'll continue with that then okay yeah okay thanks awesome"
  },
  {
    "startTime": "00:24:01",
    "text": "oh hi everybody um i apologize for my voice i've got a uh vocal cord problem and i'll uh i'll go to the video here briefly to uh uh slots for the media are already taken blah blah okay do you want to share your screen no i just wanna oh i i screwed up sorry let me cancel that i just wanted to just wanted to wave to everybody hi and now i'll stop the video to keep from bombing either my audio or my wife's uh class so um yeah hi uh we've um we've made some real uh substantial progress in the last uh interim period between meetings and that's uh thanks to tommy and marcus for initiating the sector review and thanks to brian weiss who um it was responded to us twice now and so uh this uh this first page is uh mostly about the uh the comments we've already resolved in uh in the zero two of the draft and then we got some more comments and we had some open issues so well we're going to go on with with that um as they know in the remaining slides but um let me say this that uh i think a lot of things have come out of the sector review that are really valuable for anybody designing an active measurement or a protocol to deliver that active measurement and um so i want people to think in those terms you know how can i apply this to what i'm doing uh it's certainly more applicable to anything beyond the capacity protocol and the truth is the capacity protocols more applicable to things beyond just measuring capacity we've been measuring uh loss latency reordering duplication we've been"
  },
  {
    "startTime": "00:26:01",
    "text": "measuring everything since day one so um those are the uh you know it's it's really lined up very well in udp based transport to measure what everybody needs to measure excuse me so there's there are two categories of changes uh the text clarifications alone and the text plus protocol modifications so the the text is uh clarified in o2 um we also clarified that we used a conventional communication setup with a well-known port at the server and the uh brian's observation that authentication mode can help us with uh uh protections for features like bit error checking um let's see so uh like i said uh all the all that sort of stuff is in touchdown in note 2. and we had this the original idea of four security modes for operation uh the unauthenticated and the password uh auth code it is implemented but uh authorizing uh for all the important messages and encrypt all the things we were looking for one more recommendation on item d here and that's uh that's been a topic of the um uh the other uh um messages we've exchanged recently so if we go on to slide three here brian or um not brian tommy or whoever's doing it who is it is it tommy okay thanks tommy i'm doing so well yeah cool thank you so the the thing the main things uh to keep in mind here this this is um the previous draft and the current draft described the protocol version nine and that looks like this we've got a setup exchange we've got a test activation exchange and those are both parts of the setup phase"
  },
  {
    "startTime": "00:28:00",
    "text": "and we look on those differently from the test phase in terms of their demands on the hosts and their um uh requirements and the information they expose and also uh exchange so these in red we were you know zero one of the draft we were adding the server admission control and also the load adjustment uh algorithm check so we we had the mod ability to modify the load adjustment algorithm already built into the protocol we'll get into that a little more and then the next slide so um we're actually further down so slide four there tommy thanks so now we're looking at the next version of the protocol the next version of the draft and we've still got the setup exchange still got the test activation exchange and what we're uh what brian's basically asking us to do is to uh add the the auth digest and processing on the reply in this test setup so that would make a complete authenticated exchange for the initial commands for things like the um the femoral report and the bandwidth admission check and so forth and then what he's also asking us to do is to add the authentication digest on the request and reply for the test activation exchange so that's a that's an important ad um we think we can do this obviously it means a protocol uh modification and updating the fields and so forth but that seems doable so then a little more controversy comes when we get to the load pdus and the feedback messages so let me let me talk about the feedback messages first we're sending these load pdus we're um"
  },
  {
    "startTime": "00:30:00",
    "text": "uh what the heck is this lies more lies in browser user agent strings from rich salts i don't know what the hell that means rich are you in this meeting i guess not okay so um uh so when the load pdus are flowing uh in the test phase we've got this feedback messages and 50 millisecond default and that's where we communicate the loss the delay uh the receive rates all the other parameters i mentioned like reordering and delay variation and um or um if the if the server is making these measurements then the server sends the new sending rate down to the client for an upstream test so that means sending the sending rate structure down and basically brian's question here is uh can we add the auth authentication digest and processing to the uh feedback messages and uh yeah so um so this is uh this is what we'd like to do is to probably do this uh it's probably going to be an additional option beyond authenticating the control phase the setup phase here and um uh but it seems worthwhile to uh to try to do that um i'll also note that we've got uh rates up to 40 gigabits per second now in the uh in the sending rate table uh of the running code and the uh um and allowed for it in the uh um you know and basically in the in the protocol so uh that's cool but uh the the place where we're having problems is in uh um basically adding the authentication digest and processing on the load pdus because obviously to"
  },
  {
    "startTime": "00:32:00",
    "text": "measure capacity we're sending a lot of them we're going to encounter hosts either at the client or the server or both uh that have processing limitations and so you know you're going to see us pushing back on the idea of of um authenticating every packet in the in the load pdu stream we've got so much other stuff to do all right so three things to keep in mind test setup test activation they're the control exchanges um they're the uh the things that we uh would probably be easily able to uh to authenticate also probably encrypt and um if we were to uh if you were to imply in encryption then we'd also need an additional pack to open an ephemeral port on the firewall brian really looked at our firewall uh operations and gave us some really good advice on that and this is the kind of stuff you know firewall at the server and this is the kind of stuff that would be really useful for anybody uh planning an active uh protocol and installing it in the network so these with all things to keep in mind you know as i go through the details now uh tommy you can switch to slide five uh we may need to go back to slide four here occasionally but uh let's try to just go ahead so the firewall operation um at the client uh the client basically initiates all the exchanges so we punch our own pin holes in the client and we're okay at the server though um our current practice is that we open an ephemeral port range so whatever whatever port range comes back from the client uh we uh we basically allow that and um but if we uh if we put that dummy packet in as i just showed uh that would open the pinhole on the client for"
  },
  {
    "startTime": "00:34:01",
    "text": "the client firewall for the um you know the two-way exchange and the test activation and that seems likely to work um whether we encrypt or whether we just authenticate so um um i think you know we i think we can probably handle that with at least one more uh dummy packet so that's again a protocol modification we'll be looking at so then moving on here we need to look at reorganizing the mode of operations uh same thing yeah yeah yeah so um basically based on brian's input we're looking at required authentication for the control modes messages um the test setup exchange and activation exchange they would both be authenticated in a required mode of operation so we could have optional authentication for the data messages and maybe only status as i mentioned optional encryption for the setup messages and um um maybe the activation messages too uh maybe use dtls for the with those exchanges and also maybe and this a big maybe maybe reuse the keen from the authentic uh from the dtls in the authentication aspects um then we'd also have this optional unauthenticated mode which obviously we've got working now so um you know those are uh that's that's our reading of the current uh requirements and and um and options for the various modes of operation i haven't got these lettered yet but that's what we're working with um next slide please tell me so yeah so going in deeper here we prefer not to add the um digest on the load pdu"
  },
  {
    "startTime": "00:36:01",
    "text": "uh the only real uh information there is um like the control information is the uh the bits that we flip uh to stop the test and um if and if an attacker clears the stop bits um the tests are going to stop anyway after a timeout with the test duration already specified that comes during test activation if an attacker adds the stop bits um you know jumping in on a message then a premature end of test you you'll see that but it's no threat to the internet just kind of annoying and um adding the shot 256 what digest that significantly increases the minimum packet sizes so basically we're we're trying to avoid that if we possibly can and and you know we think we can do that on the other hand with the status pdu um there's a skeptical stuff to protect here um integrity wise uh as i mentioned the new measurements or the sending rate command and the sampled rtt measurements those are all you know sort of important to uh to protect so it seems viable to protect those with the digest but we need to keep the um the fact that round-trip time measurements are are taking place on that on that 50 millisecond uh feedback so uh you know it's the old trade-off between accuracy and security protections got to keep that in mind uh next slide please tell me okay key management so this is a brand new thing that uh brian has raised it's good to know about uh we're currently using manually configured keys uh one per server um he suggested we look into our c20 7210"
  },
  {
    "startTime": "00:38:03",
    "text": "which we'll do if we add a key identifier that could help us uh to do the key management we don't have that we don't also have a config file for the key and the id so um you know these are things we could look into adding brian suggests or we could add a section uh just describing the orderly key rollover so there's lots of options there but this is a you know it's an excellent feedback and i'm really glad to get it so then the dtls for confidentiality in the setup phase um it adds re-transmission and order delivery uh those are good things but with the cost of uh you know like a fairly fairly significant uh dtls setup as i understand it so um you know i've been looking at the pictures i see tommy nodding there it's a it's a uh um you know it's something to keep in mind um if we were to do that um on both the uh both the client the test setup and test activation then we need a dummy packet from the server to open up the firewall at the server side uh dummy packet of the client to open up before the load pdus and that that's the packet dummy packet exchange so we need probably to wait for time for the dummy packets to go back and forth and then we'd be able to operate fairly safely on the on the new ephemeral port and the original ephemeral port from the from the client but um you know that all seems possible so uh you know we're going to look into doing that and then um again this i mentioned the deriving key from the dtls session um we'd be looking at that on the feedback messages and um if we're wondering if you've got the support necessary for that in the"
  },
  {
    "startTime": "00:40:01",
    "text": "open ssl from the dtls session it's a question so that's an open issue next slide there please tell me all right so um um we've got the topic of silent rejection during the senate setup phase um in authenticated mode we'd likely use silent projection now because we don't really know where the requests come from there's no authentication but um if we've got a successful validation of authentication then we could return the full rejection message with the area code we've uh sorted this out in the follow up and discussions with brian and uh if we have authenticated mode with failed authentication we could have the silent rejection again on the other hand uh compile time to help troubleshooting we could turn on uh um to reject turn on non-silent uh for troubleshooting that would be probably a good thing so um you know these are things we can uh look at doing also the client uh does not currently validate the server setup response i think i mentioned that in in the picture and so we need to be sure that the uh what digest uh checking lists and um expanding that would fix it uh to check on the uh in the setup response all sort of dual on my my mind um the authentics time is not a complete production of uh against replay attacks uh brian pointed that out um it's a you know kind of we could add in a record of previously received messages within that window and we could add an id uh which can't be replayed with the same hmac but the um you know the idea is this isn't"
  },
  {
    "startTime": "00:42:00",
    "text": "an infrequent diagnostic message we can't measure capacity all the time um and also we're not norad here so uh maybe uh we could not worry about this quite so much but the id might help so um you know those are things where we're thinking about ways to solve them and again thanks for the comments so next slide there brian tommy so the kind of the summary that came up in bryant's brian's uh uh mode d encrypted all the things this is the safe advice um the strong authentication uh for all methods is a good choice we've got that for well for all of them that we think we can do make it optional for a site to deploy that sounds good require it on the authentication of the setup messages so we're going to have a mode for that make it make authentication um optional on the data plane that seems defensible to brian and um because he understands the effect on the test accuracy the next uh sector error reviewer or the ids they might not understand that so you know we got to make that clear and um since uh orderly key rollover is a good thing to have so we'll look into adding the features uh to support that and also dtls and reusing the keys possibly um so that's a good summary i think of of all the kinds of things we're looking into but notice that that brian asked the question look um he said i haven't seen anywhere that full encryption is a requirement and i i took it from the wording of of uh privacy is the default in the um perversive monitoring is a an attack rfc and also from"
  },
  {
    "startTime": "00:44:03",
    "text": "ted hardy saying that you know encryption has to be on by default in itf protocols but that's not written down anywhere so so look you know if if the authentication and the encrypted setup is enough then you know i really like to hear that from the ads that are ultimately going to have to review this thing because we could do that very easily but um putting this all in a tunnel it's going to make the um it's going to have an impact on the measurements it's going to have an impact on the host that can perform it and note that it it's basically not in this list so so this is brian's advice he's not speaking for the sector he's not speaking for the ids but this is all very reasonable to us so you know or we can do this i think um and um you know that's where we're gonna go for now be great if we could hear we want you to do two more things too um so anyway let's uh let's keep bugging people about that next slide please tell me so we finally get to something fun to look at um i mentioned that we can have uh new types of algorithms uh supported by the protocol so here i've got a pod where we've got megabits per second um on a docsis downlink up to when i 1 000 megabits or one gigabit and i'm comparing on the left the type b algorithm which is the current default to a new algorithm which we're calling type c and which is implemented in the new running code so let's let's look at these um uh these measurements um basically it's it's a two ten second tests in series on the one gigabit downloading measurement it's uh the one it's a udp 7.5.0"
  },
  {
    "startTime": "00:46:00",
    "text": "in debug mode and we've got the uh 50 millisecond feedback measurement you can get that from the debug and um packet loss measurements are in blue so you see those counts and that b is fairly nice uh to the network we get some uh some bursts of loss here but not much when we're testing kind of in the steady state at the at the maximum now the big difference between type b and type c is that we're going to continue to retry a fast ramp up mode and the fast ramp up mode is very different instead of being linear at a factor of 10 it's now a multiplicative at a factor of 1.5 the current sending rate um by the way that's a good thing and now what we see here is the the air bit rate in the 50 millisecond and also the one second sub interval measurements you may recall that we take an average of the 50 millisecond measurements every second and this one one second rise to the gigabit range is very valuable in the mobile testing and if you want to have short intervals of testing like five seconds or things like that or if you're in mobile and you expect lots of variation in your uh maximum data rate because you're switching back and forth between 5g and and the other then um then type c algorithm is for you it's i note that the rt rtt variation it's kind of an underestimate um the servers and everything else here are kind of limited to one gigabit so you would expect if um if we had greater bandwidths than than the bottleneck on the docsis downlink we'd probably see the delay climbing up here and then we have a real"
  },
  {
    "startTime": "00:48:02",
    "text": "uh higher delay measurement of responsiveness here but the uh the key thing is that you know we can basically measure uh responsiveness when it matters and we do that in the same context as we call speed test we do it in the same context as uh the capacity tests and in the responsiveness metric as i'm reading it the big difference is that we're making the measurements at udp and not at the tcp or http so it's kind of a complimentary thing to the uh the other measurements that have been proposed so um um oh we have martin in the queue would you like to take his comment now or would you like to take a look yeah sure go ahead martin yeah let's give your voice your vocal cords a chance to relax um uh do you anticipate delivering this document to me in next six months or so yeah i hope so i hope we can um so like i mean you asked to refer to the like the previous conversation about like what will get through the isg i don't want to presume to speak for roman or paul um and i personally you know don't have a strong position on it um maybe it would be good to start like rather than just sort of guess we should just like start a dialogue with um with them and you know i'm happy to be included we can just start discussing like you know how much of a deal breaker is this for them what are the costs to this to the system if we do have to encrypt everything you mentioned them in your talk but just like kind of capture those trade-offs and kind of see what they say rather than um guess and guess wrong yeah especially thank you especially since we've got running code that we're trying to keep up to date here and that's yeah i don't want to throw away any code yeah it's not like i said the reason i asked about the six months is because of"
  },
  {
    "startTime": "00:50:00",
    "text": "course there'll be different ads um next yeah yeah that's a good point so um at least we'll keep one of them though right okay good thanks martin i'll uh i'll do that and um we can refer to the sector um reviews as well thank you um so um next steps reviews test experiences uh comments are welcome uh the implementations are happening um you can see this diagram here where it basically says oh look you know there's pins popping up looks like a road map to me that's because we've talked to lots of people who are doing this and um we're really looking forward to everybody's input and uh ideas on how to do this additionally what we can do with it and then the backup slides you'll see that we can implement alternative rates of rate programming to emulate applications so that's a really valuable feature of this protocol and this uh it's not just capacity it's um almost anything you can imagine that you can write the load adjustment um system for it can be static and it can be dynamic based on the measurement feedback so uh think about this i'm almost sorry i called it a capacity measurement protocol because we can do so much more than that so thanks everybody for your attention and um uh we can i guess we can just run on here now there is a comment there from uh will hawkins also have the same feedback and problem um in responsiveness uh the presence of encryption the"
  },
  {
    "startTime": "00:52:02",
    "text": "protocol means that it's more difficult to measure low power limit devices we think this problem is significant so will's basically agreeing with me um uh we got to let kind of let the measurement side of this uh go loose on encryption to get effectively the right answers thank you will and um edward says it looks like we lost the room uh i don't know if you can hear us from the room uh and everyone we have some network issues here so kind of medical is offline in this room so we'll we will try to get back to you as soon as we can um yes we have this video okay thanks i think i'm pretty much done though yeah i i think you hear us because i enabled my local microphone here but um we'll hopefully get this fixed as soon as possible do you do you want me to share some slides or something like that to keep going uh oh"
  },
  {
    "startTime": "00:54:00",
    "text": "yep all right it looks like we are coming back online over here al do you want to try talking and seeing if the room can hear you if you assuming you can't hear me okay yeah yeah all right yeah i don't want to take up any more time oh thanks everybody for your attention and feedback yeah um actually al um i i did have a couple comments myself that's fine so i mean in in general i think the the approach and sentiments you're expressing i i agree with a lot um when actually could we jump back to like slide five or one more six oh oh yeah yeah um so when we're talking about the authentication of the data the actual load packets which i would certainly agree that that doesn't seem like a great idea to try to add more authentication to those since it's a pretty minimal attack um i have a couple questions so we you mentioned the three second timeout here is that a negotiated uh timeout in the kind of initial test setup um such that that is just an example time or is that a fixed time in the document um it's a fixed time in the in the protocol it's a you know basically a pound define and um uh okay um and then i was also wondering you know like if we're getting if for some reason we get pushback on not having these authenticated kind of going into you know what do we actually think an attacker"
  },
  {
    "startTime": "00:56:00",
    "text": "could do with this would be good i mean um you mentioned they could you know stop the test prematurely um would there be something we could potentially do at the end either in a status feedback or it seems like like the the final message which would be authenticated to indicate if one side like thought it sent a stop or just essentially detect after the fact that something like a stop bit was set by an attacker so you could just confirm that like yes we ran this test and there were the number of stops that we expected to see or there was a mismatch and so we should throw this result out yeah um i think i think the client can send the client can send its own stop bit and that um uh that that would either be uh it would either be in the load if it's an upstream test or in the feedback if it's a if it's a downstream test and um and both the client and the server have the timeouts for ending the test okay thanks for clarifying all right yeah you wanna i think you have to stop sharing you go hello working great so i'm igor lubachev um i'm going to talk here about the explicit slow measurement that's a draft that's in the working group last call if you recall the uh if you've been following it uh it's been basically merged together from two different"
  },
  {
    "startTime": "00:58:01",
    "text": "drafts that explore slightly different techniques to solve a very similar problem uh that's why we have quite a list of others here um nothing has much changed since the uh were since the last call so we have the same version of the draft um we haven't received a whole ton of feedback in last call maybe because it's perfect but but so i'll take this opportunity to kind of reintroduce the problem and show you what we've come up with and then ask for hey if you do have some comments for during the last call i mean it's awesome time to to give them and to improve things so the problem is uh well network operators need to be able to troubleshoot problems such as loss and latency issues and i mean detect them first and to do that it's really best to actually be able to observe the problem because otherwise a response to a trouble call is we looked at a few at a few statistics here and there we saw nothing uh sorry um that might be just uh like well maybe you haven't looked enough maybe you should look at a few more so uh with protocols like tcp uh at the last resort you could pull out a wire shark and try to observe the problem as it's happening if somebody is saying that you have lost in your network and you can't find it so observe it um with encrypted protocols that's not going to work well because the transports are encrypting all the headers everything that would be useful for this purpose and they're doing it for a good reason i mean at least good enough reason number one concern is all that clear tx stuff could lead leak uh some information that could be"
  },
  {
    "startTime": "01:00:00",
    "text": "considered to disclose more stuff than endpoints wish to disclose so privacy risk and the second concern is that unpassed devices trying to be super helpful uh to be able to be very helpful need to understand what they're seeing and when they understand what they're seeing they don't understand any different anything different and may cause problems and as an effect you have protocol classification uh that basically you just can't do any innovations uh can any do any changes we're all familiar with that so that's like another reason why some of them looking at quick of course encrypted transports chose to encrypt as much information as possible um there are other uses for these techniques that are unrelated to encrypted transports so we're trying to find uh develop techniques that can be used with just a few bits like really really a few like one or two three um that would be enough to figure out quite a bit of information about any problem that's happening the advantage of having just a few techniques a few bits is that first of all fewer bits means it's easier to do any sort of privacy security analysis you're much less likely to leak stuff inadvertently especially if your bids are purpose-built and not built for some other purpose and second is that an explicit signal as opposed to implied signal from transport headers means that it's not integral uh to the operation of the transport which means you can just turn it off or you can only enable it selectively when you need it you can grease them um and that's that will help against protocolsification"
  },
  {
    "startTime": "01:02:00",
    "text": "next please um some of the prior art uh latency spin bit uh after quite a bit of debate in quick working group it's been added to quick version one and its purpose is to be able to measure um round trip latency now this draft in this draft we are the whole ton of additional bits that are designed to do particular measurements uh latency measurements and different kinds of loss measurements uh they can be used together in combinations uh next um some of them again the goal is not to read the eye chart and the like ever since this table is in the draft um but the idea is that we are discussing what different bits do we compare their performance in terms of fidelity versus latency of the measurement versus how how many measurements you can do on a particular flow so that's about latency next and similar for loss uh we have many different loss metrics you can derive from using different bits or their combinations again uh we have a bunch of analysis for comparing a different alternative what you can do with just two bits one bit um trade-off between again fidelity and um how quickly you can see loss after it's happened um do you see lost shape or do you see just"
  },
  {
    "startTime": "01:04:02",
    "text": "approximate average loss per connection round three perhaps anyway so a lot of different analysis here next sorry do you want to take a comment now for martin or yeah yeah i'll take that i'll take a comment hey gore martin duke google um square bit in particular uh seems to have a lot of overlap with 8321 um which is this is this is standard proposed standard uh informational okay so um like uh uh one like i think we should probably figure out where we're gonna like discuss and specify this square bit thing unless it's just referring to that draft so um it is referring to i mean it's square bit is not was not invented here okay it is just one of the signals so the purpose of the draft is not to specify bits on the wire that will be up to the protocols how to how they choose to implement it the purpose is to give you techniques uh to analyze techniques and to say and basically to say that's what you can do with as many bits as you want to spare yeah i mean 83-21 even though it's currently experimental and headed for standards i guess it's the same thing it is not attached to a protocol and i guess there are other draft other drafts that are instantiating that but um also interestingly um i think they've gone away from the like end packets to having like a time interval um so i don't know i think we should get our story straight on that uh but yeah i think that's exactly the kind of feedback that's totally awesome and we can we can discuss looks like giuseppe he's a co-author here uh maybe he wants to find something hello"
  },
  {
    "startTime": "01:06:00",
    "text": "yep yeah just to answer to martin point yeah uh the square bit is introduced also in the 8321 but yeah the difference as you know that in the 8321 in particular in the proposed standard document we focus only on fixed timer blocks while the square bit is on based on fixed number so in case of course these uh will be standard track i mean the explicit flow measurement this will need a an accurate uh detail as we do in the 83-21 yeah and i just want to clarify that i i technically have no dock in this fight um but uh like i think the community should decide the best way to do this because 83-21 had both and 83-21 this is down selecting to time so um like i said and that's fine yeah i mean whatever works best for the purpose right i mean if we decide time works fine fine but if we decide that we look at time and say it has problems for the purpose i mean what's important is that it's fit for the purpose yeah absolutely yeah i mean if the if the name clash is a problem we can obviously rename it well i mean i don't think it's a question of name class thing i think name clashing i think um like if you're going to use that if you're going to use a bit for loss detection i think communities to decide how that works um and whether it's this or whether it's packets or whether it's packets or time either one but we should decide thanks thank you all right uh next slide"
  },
  {
    "startTime": "01:08:00",
    "text": "just a comment on that um so does the current document where it defines the square bit talk about the time option or does it only describe the uh no it talks about uh packets yes i mean what i would suggest given that this is informational it's kind of laying out here's the zoo of all of the different bits this should just describe both say hey there's you could do it based on time you could do it based on number of packets this other document over here chose to do it based on time here's why you would want to do that but if in the future someone has a really good reason why they want to do it based on packets that also exists thank you sorry i just yeah i think that's reasonable um uh with the caveat that like if 83 21 business the pro standard we really have community consensus that that is the best way to go forward um then we should probably make sure that's clear in any informational document and yeah totally we should discuss it yeah thank you all right uh so this slide is basically talking about this being actually in use um there's a number of industry uh that's in the implementation so for example akamai orange implemented it and ran uh this for about a year uh in production we've made um go back we're a little bit short out of time okay very good so i'll just summarize a number of implementations from different operators uh a number of uh implementations from researchers um so that's been done and just like the last slide uh the quick history of it is that we've been running uh last call since july 6. uh we just received on the least substantive good feedback from marcus"
  },
  {
    "startTime": "01:10:01",
    "text": "about some hidden parts of the delay bits i just now feedback from martin um so let's uh looks like we need another revision of it uh let's discuss on the list uh probably we want to extend the last call and get another revision in yeah probably maybe get another revision in and make a new last call at a later point yeah okay thank you okay all right so next i think we have a responsiveness and uh christoph i think billy wanted to share his own uh slides oh yes i would like to share my screen that way you can see in animations which will make it easier to visualize the changes i want to show you screen yes let me know if it does not work we see it so you know you should see it in the presentation mode yes great so um [Music] so hello yes um this is responsiveness under working conditions um we submitted a zero one version um shortly before uh this meeting here the update from zero zero one to zero one is uh the mill is listed here we closed the set of get up issues we merged a few pr's got a few contributions and in terms of the significant changes there's first stuart cheshire added dns based service discovery for the network quality measurement or the responsiveness measurement that way it means that if i am on my network i can basically browse these dns based services that are"
  },
  {
    "startTime": "01:12:00",
    "text": "being and the the services that are being announced through dns i can discover them on my local network and just discover endpoints on my network that allow me to test the responsiveness in my local network so we hope that this is going to be a very useful addition we added server-side example configurations in the appendix so that if people want to deploy a responsiveness measurement endpoint they can simply simply take a look at those example configurations and we did a significant rework of the measurement algorithm and of course some wording changes minor minor minor fixes and so on so i want to double click on the significant rework of the measurement algorithm so how is the was the previous algorithm and so the previous algorithm its goal was to find the point of good put saturation the way it worked was the following we started with a set of connections by default four connections and at one point we reach the maximum good put with those four connections and it starts leveling off at this point the question is whether we would be able to achieve higher good put by adding more connections because the bdp of the path is is very large right and so the only point to do this is we were adding more connections right until we again we reached the maximum good put at which point it levels off again and so now the question becomes well did we actually reach the link capacity yes or no or do we still need to add more connections and to learn this we add yet more connections into the pool until we realize well okay the good but didn't changed so there's no change in good good put and so we declare saturation at which point we started the latency probes right we sent a set of probes on"
  },
  {
    "startTime": "01:14:02",
    "text": "set of probes on the separate connections so what are the problems um with this approach so first the problem is that we only have a very small sample size right because at the point where we reach a good put saturation they might already be very high buff upload and so sending those probes might take quite a long time and so we can only send a very limited number of those probes so the sample size is pretty small on extremely buffer bloated links that have several seconds of buffer load these latency probes have a tendency to time out so we may sometimes not even be able to get a measurement and finally these kind of one-shot measurement that are happening only at one point in time have the tendency to be impacted by short-term uh buffer occupancy variations right um we have seen cases where there is on these load generating connections there is an effect of what we call a synchronized packet loss where all of these connections get a packet loss at the same time so all of these at the same time reducing their congestion window and so they are all of these at the same time reducing the buffer occupancy and so sending these one-shot measurement probes has the impact of being has the result of being impacted but d by these short-term buffer occupancy fluctuations so these here are the again these the the draw the drawbacks of this algorithm so what we realize is the way we can actually solve most of these problems is by basically well we we keep on trying to reach capacity right but instead of waiting until saturation to start the probing we start probing right away and we probe every 100 milliseconds we probe on separate connections and we probe on the load generating connections"
  },
  {
    "startTime": "01:16:00",
    "text": "and we just keep on going through the algorithm and we continuously probe every 100 milliseconds we send one probe on the first load generating connection and one probe on the separate connection so what does this means as you can already see graphically right we have a lot of probes now right and so we get four data sets with this approach we get latency on the separate connections we get tcp handshake latency we get tls handshake latency and we get http rigorous respond latency under on the separate connections on the load generating connection which we call itself we get also a http request response latency and so those are now very large data sets right because we have sent lots of probes so how do we aggregate all of these four data sets into a single number first of all we take the 90th percentile right so that way we filter out those probes that were happening at the beginning of the test when the link was not yet buff uploaded then we need to weigh those four different numbers right and so our goal is to put equal weight on the load generating and on the separate connections now for the separate connections we get three data points right the tcp handshake latency the tls handshake latency and the http latency and so we weigh these in in the way we show it here in the in the slides we have one sixth for each of the uh separate data points and half for the on the low generating data points that way both are weighted equally now this is going to give us a number in terms of seconds and so as we want to express responsiveness in terms of round trips per minute we basically normalize it to rpm for those interested this is the final formula 60 000 divided by the p90 is of"
  },
  {
    "startTime": "01:18:02",
    "text": "the different values uh normal appropriately weighted the way we have described it so the advantages of this approach is first we have a very large sample size about 150 data points for 15 second test which is great which removes a lot of variance which avoids uh all of the issues that we described earlier we have much less timeout issues because the probing happens right from the start so even if you have a link that has huge buffer bloat we usually end up to get a quite a few samples of this very huge buffer scenario and there are no more short-term fluctuations because of the huge data size and the huge sample size we have this has been implemented in mac os ventura the upcoming macro is released and the network quality tool and the open source go responsiveness tool has it implemented as well now in terms of the remaining issues there are few more more remaining issues that we would like to address issue number 17 on the github is we would like to use a well-known uri for the json config that way any web service could basically expose this responsiveness measurement as a service and we could simply discover it by hitting the well-known uri to see if there's a json config available issue number 63 we need to explain the impact of congestion control it came up in the past but we haven't yet had the time to address this question and we want to write a section on how the different congestion control algorithms like cubic bbr and so on can affect the responsiveness issue number 55 we want to provide guidance on how we can provide an"
  },
  {
    "startTime": "01:20:01",
    "text": "estimate of how confident we are in the values of the results because there are different ways of how the algorithm can evolve and because we may hit timing issues sometimes the numbers may be more or less accurate because the goal is to provide a measurement of responsiveness within a reasonable time frame sometimes in order to provide a result within less than 30 seconds we have to give a lower confidence in the result issue number 66 what l also brought up is we need to allow non-tls measurements because on some low end devices if we want to allow for example that your router exposes the dns servers via the the service through dns space for discovery right low-end gateways usually don't have the performance to actually fill the link with tls traffic which self-generated tls traffic and so we want to allow for non-tls measurements and finally the most important one um search i filed an issue number 62 called the flaw in the working conditions algorithm and i want to double click on this one because to explain what is the problem and how we can solve it so if we look at what i explained earlier in terms of our algorithm right we have we are measuring we are trying to reach the maximum good put now good put is actually not what we are measuring we are trying to measure responsiveness which is the buffer bloat and so it means we are trying to measure the buffer occupancy so the bottleneck link that is driving the maximum good put has a certain size it has a maximum buffer occupancy and for sake of examples here we say let's say this size is 64 megabytes it's a very buffer bloated link right so as we go through the algorithm and as"
  },
  {
    "startTime": "01:22:00",
    "text": "we start with four connections right and often um one tcp connection has uh has frequently uh a limit in terms of how much data it could can put in flight and for someone typically this can be around four megabytes eight megabytes six megabytes for this example let's pick four megabytes right so we have four connections with a maximum of in-flight data of being four megabytes that would give us a maximum of in-flight data of 16 megabytes which means as we are on the top graph right we are we have created those four connections and we are leveling out in terms of good put we haven't however yet achieved the maximum good put which means the buffer occupancy will be zero right there's no buffer block yet happening now our algorithm decides to add more connections let's say we add eight connections now okay so eight connections times four megabyte means 16 megabytes so the um however we are still reaching to the good puts we haven't yet reached this point of inclination where we are actually creating buffer bloat now as we reach the maximum good put we actually start building a queue now and so now the queue starts filling up but as it's only eight connections which means 16 megabytes worth of buffer occupancy we haven't yet completely filled the link we are only at 64 megabytes uh sorry we are only at 16 megabytes on a 64 megabyte buffer right now our algorithm keeps on adding more connections right so now we are at 12 connections and the buffer occupancy keeps on increasing now at this point in time we realize okay we reached capacity for the good put and so we declare saturation and we terminate the test what does this mean well this means that this is the responsiveness that we measured but in"
  },
  {
    "startTime": "01:24:02",
    "text": "reality the responsiveness is much much worse right we haven't even yet filled the buffer completely so what is the solution to this problem right well the solution is we go through this algorithm but then we see that well actually the buffer the responsiveness is still evolving right we haven't leveled out the responsiveness yet and as we realize that okay with 12 connections we reach 48 megabytes of buffer occupancy and so we can say okay we we leveled out at 12 connections let's add more connections to see if we can push it even higher so we add more connections and we realize that the buffer occupancy is increasing now with 16 connections we had 64 megabytes worth of buffering and so we actually achieved the full buffer occupancy the only way for us to find out whether really we achieved 64 the full buffer occupancy is by have one more iteration of adding more connections and we realize by adding more connections we are not the good but is not changing nor is the buffer occupancy changing and so it means we can declare saturation and we can declare the final responsiveness result so this is the new algorithm so it means that we not only need to saturate good put but we also need to saturate responsiveness and once good put and responsiveness stop changing we declare saturation and so in in the i in the draft for the upcoming version what we need to change is in terms of the algorithm is that we are adding connections as long as either good but increases or the responsiveness is decreasing so this is going to be the new algorithm for the next for the upcoming version in terms of other news um the open source go go responsiveness implementation is"
  },
  {
    "startTime": "01:26:01",
    "text": "evolving rapidly and of course we would like people to try it out we invite everyone to to test it will here is on this call and would be very happy to get pull requests and github issues as well in terms of the other implementations uh ucla tests now started measuring load and latency as well we don't know details about how they do this but it's probably a very similar measurement methodology so the load latency in ookla is going to increase also the awareness on the issue of buffer bloat and responsiveness under working conditions so with that i'm at the end of the presentation and uh if there are any questions i'm i would i would be very happy to take them thank you very much christoph wow kristoff and will for your work on this um i think that the uh the uh one thing you're going to want to fix in the draft is the uh is the equation for responsiveness i was about to do something with it and i noticed that um instead of one-sixth you've got one-third in all the denominators there uh accepting the uh something the last one so i i was confused by that but i see now that you mean uh one-sixth and that sort of adds up to one with the half uh half-weighted uh aspect um i think that when you're also when you're reporting capacity uh it's you know it's based on it's based on a lot of connections and that's the uh um then i i tried it out on a couple of cases i saw 20 connections and so forth"
  },
  {
    "startTime": "01:28:00",
    "text": "and um you know those are you're going to get uh if matt mathis was probably here too you would say the same thing you're going to get opportunities for synchronization and the interaction between the the different tcp algorithms and so forth thanks for turning on your video so i can get some feedback i see you smiling i learned i really learned a lot of this from matt so i'm just i'm just uh proxying matt here when when i tell you my story but um uh i think i mean i think you're on the right track in uh monitoring the delay as well as the good put but um there's other factors here too that matter and uh you know we've been we've actually been we've actually been looking at um in our capacity measurement we've been looking at the uh the possibility to reduce the some of the factors we found that uh reordering and duplication um they happen they happen on 5g networks more prevalently than any place else and the truth is those are measurements that um or packets that are delivered that contribute to capacity so tcp isn't going to tell you about those you know it's going to discard those as as it forwards information up the stack but we can grab those in the udp measurements and uh uh you know like we end up we end up including the reordered and duplicate packets now we're thinking about making that the default especially for mobile testing so you know there's lots of uh there's lots of room for uh uh improvement in our algorithms here and you know i'm glad to keep exchanging ideas with you thanks sure yeah thanks for your feedback all right um i jumped and cue not chair head on just"
  },
  {
    "startTime": "01:30:02",
    "text": "to comment um one uh earlier you mentioned the the issue about using a well-known um and i looking at that i think there's some debate about you know what's in that is that just the config is that the actual test um overall within kind of http i mean there's certainly a sentiment that well known can be overused i think this is a decent use of it but um i think it'd be worth if you want to drop a line to mark nottingham who has to review all of those anyway to see you know is this going to be something that would get through the expert review for adding well-known um and any advice there that'd be good to get um regarding the new new algorithm i think everything's good one of the things that came up in the chat um i think there's questions about like why would you for example not just measure when the responsiveness starts decreasing um and i think the answer to that is you know the responsiveness won't actually decrease until there's enough load um i think then one edge case came up to me it's like are there any scenarios in which the responsiveness only starts decreasing further to the right such that you know like we could get to 16 um and you know our good put has flattened out but the responsiveness will only start going up starting at 20 or 24 and so we could actually stop the test too soon um is that something we should be concerned about i so on your first comment yes absolutely you're absolutely right unless we hit capacity responsiveness won't change at all right that's the party on the left um i don't think there's a case where we could hit"
  },
  {
    "startTime": "01:32:01",
    "text": "and only 20 flows farther down the road responsiveness would start changing i i don't think that's possible i think the moment we hit capacity responsiveness will start changing that's just um the way it is if that would not be the case i would love to see that it's a good point absolutely i i just i just cannot imagine how it could happen uh this is stuart joshua i'll say a few more words on that and actually in the last couple of weeks since the draft deadline christoph and i and randall have been discussing this internally but it was after the submission deadline one of the things that i'm starting to think is we might be able to simplify this algorithm instead of this current sort of staircase function where we add four connections and then wait a bit and then add four more it might have a more elegant simplicity about it if we just add new connections at a one second cadence while measuring application around trip time on those connections so we're gathering data and the the initial idea in my head to propose a termination condition for this is we add connections once a second and we cease the test when four seconds have gone by without either an increase in throughput or an increase in latency so we kind of keep a high score of the highest throughput we've seen and the highest latency we've seen and every time we break that record we record the new record after we've gone for four seconds without setting a new record for either of those things that means we've added four more connections we've put more data into the pipe and neither has changed and just to back up"
  },
  {
    "startTime": "01:34:00",
    "text": "a little bit to second what kristoff was saying uh as we have more data in flight initially we fill up the bdp of the pipe which is good because we're getting throughput and when we hit that limit additional data is now sitting in buffer somewhere we may not detect that instantaneously but i think over a four second window if we're continuing to add more data in flight then within four seconds we will have seen the delay go up and that will cause us to keep testing until delay stops going up so uh that's your answer tommy um i think if we didn't have that four second window there would be a risk of a premature exit but that four seconds i think is what makes it work thank you yes i think we need to move on a bit quicker a bit over time uh okay um so i'll just say the thing i came to the microphone originally to explain a bit more about christoph mentioned the the dns service discovery service type the motivation for that is i might i may run the network quality test and get a lousy score but as an engineer i want to know why so we've been talking to home gateway vendors and wi-fi access point vendors who actually want to host a test endpoint on their wi-fi access points or on the home gateway so you can eliminate the modem or the dsl from the equation and do a local test and see is it my wi-fi or is it my cable mode that's causing a problem so that's the reason we want that auto discovery and if there are any other home gateway vendors interested then talk to us about how to how to host one of those test endpoints locally hi could you hear me yep okay well um just a small comment about the them"
  },
  {
    "startTime": "01:36:02",
    "text": "this this technique to measure bandwidth um i i'm not sure how noisy it is because uh you know the inside the the network you you got some other traffic and perhaps i i'm not sure if you are just mentioning uh just uh i i don't know the the last smile or something like that or uh you are measuring something more more bigger than that but i imagine that there are the there is some some traffic more traffic and could be a little noisy um i mean the responsiveness is not like an stride line like you you you draw like there and therefore how you deal with this this variation because it could be really very very important when you you go from you go through several links thank you yeah thanks for your comment um so maybe i i didn't introduce that properly at the beginning what we are measuring here is end-to-end capacity and end-to-end responsiveness so from the client to the server right so it's not necessarily last mile it is wherever the bottleneck the bottleneck is up for this kind of communication right um so yes there may be other traffic and so we are measuring what at this point in time what is this the user's capacity that the fair share of its link usage that he can get or she can get okay okay then then you you got really a lot of uh noise sure i i mean the signal are very very okay great i will contact you later thank you that's good thanks"
  },
  {
    "startTime": "01:38:06",
    "text": "all right um let's move on uh nalini do you want to present so okay so this is um our pdm destination option it's an ipv6 destination header and basically what we do this is an end-to-end measurement it's put on at the source at the end use end client and we put in a sequence number and um timing the idea is to be able to very quickly separate server time from network time and uh the potential users are large um uh uh enterprises okay and so what have we done uh we had an early sector review and i'll go through that and we're working on implementation of this and we're also testing uh extension headers across the internet and i'll talk about that okay so this is the sector review and basically they said it wasn't ready because it's still very early and and there's a few things uh left to do uh basically on uh authentication authorization and so on and otherwise they think it's pretty good and so we will continue to work with them we feel pretty good because it doesn't look like there's huge amounts to change and we'll address whatever there is so that was that feels good okay next so the big thing that we did this last time"
  },
  {
    "startTime": "01:40:02",
    "text": "other than implementation is look and see can ipv6 extension editors can actually be used because if they can't we're wasting our time in defining this thing it doesn't matter encrypted or not it won't work so so that's what that's what we did and so we tested stuff so um so next and so what we did and so so what we did and this is a this is the start of the testing and yes we know we need to do a lot more and we're working on doing a lot more but basically what i think we showed is that from we did a patch to the freebsd kernel so that it sends out extension headers with every packet and we sent real ftps so i'll show you that and the other thing we did is we did it across um on a couple of different continents maybe yeah three different continents four different and a bunch of different cities the idea is is to see if it's going through the core of the internet um because if it's you know because we want to find out is it being stopped at the source is it being stopped at some transit network is it stopped at the destination if extension headers are not getting through then where are they and ideally why are they so next and and so it was real easy because pdm our mod to the kernel sends pdm with every packet um we can just do a very large ftp and that's what we did and this one happens to be toronto to mumbai and you can see it was a big old ftp and successfully transferred next please"
  },
  {
    "startTime": "01:42:02",
    "text": "and you can see our wonderful little pdm extension header right there in the packet trace next please and in fact turned out a lot of these things were fragmented and so we didn't even mean to test fragmentation header but it was fragmented and all those got through just fine uh as well next so you can see um it appeared to go all the way through for a bunch of different sites big old ftp going across okay next please um and so we also started doing curls and um this one was actually at the hackathon we started doing it to um i think we set up an apache server in warsaw with a bunch of junk data in it so it would create a bunch of um uh fragment headers and we've tested from the hackathon and here it is um we're doing a curl from the ietf network successfully next next yeah so we're going to do a lot more testing and to see kind of where things start and there's already a number of people who want to work with us we will continue on implementation of our encryption because ideally pdm will work to give us our wonderful data that we need and then if it actually works then we actually do need to encrypt it because the data can is is actually too good and can it can be the source of an attack so one does wish to um encrypt it so yeah there we are martin i'm sorry so i maybe i got confused by different so the you're finding the eh is making it"
  },
  {
    "startTime": "01:44:00",
    "text": "through just fine at least in the measurement point you have correct oh good great thanks correct that is indeed what we have found i again i'm i'm saying these are the points we tested these are the tests we did i'm not saying anything more than that hi paul briscoe um just say what i said to you when we were chatting in the hackathon yep um that all the advantage points are data centers essentially they're they're the the hosting services data centers and so nothing's going over a sort of consumer access network no no no no no no there's going over the public internet it is going over the public internet yeah yeah but not a consumer access network like a mobile network or a oh yeah or a dsl network or yep yep yep we will do mobile testing we'll do testing yes yes yes no you're you're very right there's lots of yeah yeah there's lots more testing lots of people you know anybody that wishes to help test knock yourself out we're setting up an email list i mean yeah yeah there's a lot more testing that needs to be done you're you're you could not be more correct yeah because because i think that's um that's where you're more likely to find problems but yeah hey let's just see one step at a time let's see where we got problems because you know the other thing too like like um let's find out where what's the situation where is it being dropped why is it because i tell you um just right in our testing um at the hackathon um we found one bug in a particular router implementation where the the hop by hop header just right there wasn't going out at the source and so of course the question is what happened if they fix their bug right"
  },
  {
    "startTime": "01:46:02",
    "text": "and like lo and behold you know so so and and now it's it's super interesting we're also talking to the free router people i just had the little young man uh write me back we'll we'll um we'll modify free router to send h-by-h i mean hbh because what i think is if we can bypass if we can control all the equipment if we can control the equipment control the end points and know exactly what it is we're testing i think we gotta we have some shot at figuring out what is the actual situation yeah any feedback or comments or anything that anyone has on you know what we're forgetting you should remember to test this you know please please let us know we happy to test yep we'll more results next time thank you very much greg okay next slide please okay so um this is updated just to remind you what we're trying to do is um there are services that are governed by multiple slos that are composing their service level agreement and they [Music] are expressing requirements and expectation of the service for the particular metrics in this work we're trying not to look at how each particular slo is complied with"
  },
  {
    "startTime": "01:48:01",
    "text": "but in overall the combination of multiple slo how it uh reflects uh their uh service uh and um as a whole and uh we express that as uh precision uh availability of the service that is characterized and constrained by uh multiple slos so if you look at this uh figure so you see that their period where their particular slow a particular metric is within acceptable range but then there are periods when it exceeds uh the critical threshold and these uh periods are can be considered as uh service and unavailability whereas there when it's acceptable that's uh acceptable and thus it's a service availability period next slide please so uh this is update we already presented it uh virtual meeting in vienna remotely we received a very detailed and uh helpful comments from mad work together and ned uh agreed to join us and continue working on this document so let's look what uh updates we have now uh for this meeting to share with you next slide please so we clarify the problem statement and um so basically what we are trying to uh solve what we are addressing and it's not only on particular values that uh the service experiences at the given time uh"
  },
  {
    "startTime": "01:50:00",
    "text": "at a given point in time but how it uh relative to the thresholds their slo that is set for their uh particular metric next slide so then we clarify the message of how tam can be used so it can be used to verify their compliance with the specified quality expectations and that can be used in multiple ways for example optimize service delivery or account for obviously operator to subscriber interaction next slide there was one metric that we missed so we added packets uh since last violated packet oh yeah next slide and with terminology so uh if you recall uh in the first version we were not yet decided uh whether referred to the metrics as uh errored time intervals and well violated so we just discussed and uh comments from that helped us us to settle on a violated term so now uh everything is referred to as violated intervals severe violated intervals or violation free interval next slide so as you see so this item is can be taken out there are some more work that we will be doing in working uh with this document and um still we have some plans for the future and we will appreciate your comments suggestion and"
  },
  {
    "startTime": "01:52:00",
    "text": "please uh think about joining the work there are a lot of work to be done next slide please so again uh we think that um since we merged this work and addressed the comments from ned uh the work matured enough that we appreciate your consideration for the working group adoption any questions i don't see any anybody on the queue okay then thank you thank you very much that was our last presentation we made it any closing remarks yeah no thank you for a good meeting thanks for getting through that agenda i think we had some good discussion here some good follow-ups and please you know continue those discussions on the list and in the github repos for the document script that's appropriate and yeah we'll see you next time and have a good uh and safe travel home everyone um because"
  },
  {
    "startTime": "01:54:18",
    "text": "okay so i can continue with the next step request for early allocations for which one like there were so many crap uh uh [Music] yes"
  },
  {
    "startTime": "01:56:09",
    "text": "is for the existing is you can't tell"
  }
]
