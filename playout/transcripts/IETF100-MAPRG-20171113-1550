[
  {
    "startTime": "00:00:05",
    "text": "yeah works with keynotes Oh let\u0027s see if it works with I can just I could just go to you next time but for so you\u0027re using keynote yes yeah for me for me doesn\u0027t work and the other people can just go next slide or they can use guess I can\u0027t have you seen the company that makes his water Larry there there I actually noticed that they were sort of dragging around his big cart full of boxes with doctor who stands on the side how are you exhausting yes because I already millions the week no I was so oh you were in Seoul yeah I was so was it good the equipment was pretty nice the location was so you you\u0027re Gangnam are you a machine and just started because there\u0027s time is it we\u0027re ready we should start already what\u0027s the matter I have to pick the slice she doesn\u0027t have that I\u0027m just I\u0027m the personal assistant to the chair no no no I see where you\u0027re going well I want to say yes that\u0027s what I\u0027m negotiating about yes well no not well so I work so hello Bea Brady I think the "
  },
  {
    "startTime": "00:03:17",
    "text": "session started already yeah so if you could concentrate a little bit here stop shedding that would be really nice I\u0027m a little bit disappointed that there are only so few people but they might also be in the break and we have to start so welcome to the memory section in Singapore we this time I only have a one and a half hour slot that\u0027s mainly because Jeff and I were really lazy so by the way I mean I could even this is not Dave this is just Roland helping me out because I wasn\u0027t able to bring the right adapter for my laptop and yeah I before we officially start we also have this note about intellectual property for the IRT F that you should be familiars we have like we have mailing list of course that you can subscribe to which is not doesn\u0027t have a lot of traffic but like you are all free to actually generate some traffic there if you want to announce your fancy measurement work or your some questions about measurements please use the mailing list we have some other logistics here about remote participation and the slights of course this link if you don\u0027t have the slides here doesn\u0027t help you but you can click on the link otherwise and that\u0027s our today\u0027s agenda so as I said we have only one and a half hours lot this time and Dave and I actually didn\u0027t put a lot of effort in to try to get various people from the research community community into the room so we are extremely happy that we have a really nice gender and that was many people who already know about my body and came to us we wanted to present their work say thank you very much next time in London we will definitely ask for a longer slot again and we will spread out in the world and try to get interesting things we\u0027ve seen somewhere into map ready so stay tuned there but anyway if you have something that you would like to present in London you can also announce us anytime about it so we can actually take you into account early on because I guess their gender and ilona will actually be crowded if you yeah also two announcements so we we had this earlier that we also give people five minutes lots of just want to announce their measurement work which is like somehow ongoing or if they have requests to the community that it\u0027s our first talk and then another thing that we would like to kind of keep up is inviting people to give updates on previous talk they gave so if you already have a talk have had a talk and never Chi and you just have some newer data because data gets old quite quickly you can easily get five minutes lots of just present an update and that\u0027s our second part here and then we have for longer presentations the "
  },
  {
    "startTime": "00:06:19",
    "text": "first one is on principles on measurement so it doesn\u0027t provide data but I think that\u0027s also hopefully good input for this community and the other three are about measurement data and that means if there are no other question of the no questions about the agender we start with the first talk okay Klaus [Music] just yeah sure yeah and and also thank you to Roland again for having me out but also to met for it for taking minutes okay thank you and good afternoon I\u0027m closely and from finished regulator okay closer to the mic okay I\u0027m posting then from finished regulator and it\u0027s nice to be here next slide I\u0027m going to talk you about the European measurement tool project so the the yeah well that\u0027s based on the year made neutrality regulation I\u0027m not actually going to talk that too much here but what does that mean basically for the end users that the entities are able to use and provide applications that the ISPs should basically treat all traffic equally with few exceptions and for us the regulators we need to of course monitor and supervise that believe obligation summit well if you\u0027re interested more about that with Everett there\u0027s another presentation you can check but I\u0027m actually going to the next slide to see about the deep well-being basic goal about my craft so there sir I deem available that describes also the basic principles of the regulation the goal is to give sufficient details for developing the actual measurement metrics it describes the different use cases basically for the the quality of service performance measurements but also how to detect the traffic management practices that may in practice affect the quality of service or the availability of certain applications so for example throttling or something like that blocking and it\u0027s really important also to be able to detect if it\u0027s the end-user environment that actually is making the impact or is it the ISP who we can blame so basically we again my craft is available and while next slide space now no to the main topic so the European regulators we have decided to develop a net neutrality measurement tool well we are just starting the the actual tender documents so we don\u0027t have yet anything I mean any code available but basically the idea is to have a open source tool "
  },
  {
    "startTime": "00:09:20",
    "text": "that\u0027s also possible to be able available for the whole industry and basically we are now targeting for well basically few mandatory measurements like the quality of service for speech delay maybe some neat neutrality measurements like the port blocking and of course it\u0027s a tender so we are hoping to get all so many additional measurements to be available in the first place so basically we\u0027re targeting to have a tool that\u0027s that\u0027s available for the end users they can run it via browser or app and detect the quality or DD how to say neutrality of the Internet access service so if they\u0027re say they own ISP is doing something nasty so basically the tender is going to be launched early next year the development process is basically next year and we are hoping to get the tool available early nineteen and erm this is something that I would expect that well also the industry would be happy to well contribute I mean that\u0027s our goal to have you to check what we are doing check that we are doing things right and hopefully the tool is also useful for other people than the European regulators I mean we try to spread the tool to the market but I would say that well also outside the Europe that\u0027s a tool that that hopefully is actually useful so if we can go to the slide and also some background information so I mean this presentation doesn\u0027t give you that much details so with the hey Brady net neutrality regulatory assessment methodology that basically tells how ideas about the how to do the measurements well lady that\u0027s the basis for for the some of the measurements we are now building it provides the the basically now our idea on how to measure speed that\u0027s a bit different than what we\u0027re seeing in the industry because it\u0027s really based on the well I\u0027d be packet de lodo basically including also the D haters but we are not we are talking about IP level speed not anything lower and there also some ideas on how to miss it in it neutrality we don\u0027t have that many how to say standardized measurements available in the market and of course well this is are used for who our tool then if you are interested in the tool development "
  },
  {
    "startTime": "00:12:21",
    "text": "we have another document regarding the deep how we have specified the tool it gives you more idea about the what we are talking for basically it should be available in mobile and fixed it should enable the end-users to make their own measurements and help you help the inner race to assess the the regulation and how it\u0027s implemented so basically if you have any comments questions sir please contact me well we are happy to share you some information so thank you do you have time for a question now a quick question so you\u0027re in Faulk Akamai so it looks like you\u0027re building this tool and you\u0027re going to share the tool but are you going to share the data that the tool collects well basically the the tool is really early face now but see hmm well maybe we can take that offline by I think it may take some time for me to jacket\u0027s you and good answer okay yes oh we draft read the documents talk to Klaus you can also use the MEP regime ingots to start a discussion there thank you okay thank you so next one is Kyra presenting for dave blanca basically giving an update on his talk on ipv6 hello I\u0027m Kyle rose I work at Akamai not with Dave typically but but I\u0027m interested in his work and he unfortunately is not able to make it so so I\u0027m giving the presentation for him you can see here that this is a a continuing study of the evolution of the ipv6 internet specifically looking at the use of the address space if you want to know more about the methodology involved in this study you can take a look at these two links next slide so you can see here this is a the blue line here represents the represents the observed ipv6 client addresses on a single day in in the Akamai network so across the 240 thousand some servers that we have that\u0027s the total number of observed addresses close to a billion next slide yeah that\u0027s right it\u0027s their animations yeah so so that close to a billion number is actually more more ipv6 addresses than active ipv4 addresses seen per day next slide and if you look at this over a week you can see that there are more next slide you can see that there are more active ipv6 addresses seen in a week than exists in the entire ipv4 address space so it suggests that people are using the ipv6 "
  },
  {
    "startTime": "00:15:22",
    "text": "address space in a much more liberal way which is which is good but it\u0027s also interesting and it reveals that there are that the v6 internet is being managed in a very different way next slide it\u0027s also the case that there are more active slash 64 fix\u0027s it seen in a week then there are total ipv4 addresses seen a week next slide so this is a count of the total number of a SNS that have v6 connectivity so it\u0027s been creeping upward for the last three years you can see that it\u0027s reached about 6500 now next slide so if we look at the if we look at the number of if we look at the number of clients in each of these networks and do a CDF where we\u0027re looking at the proportion of a SNS that have a certain number of clients that have at least a certain number of clients or then you can see if we restrict that to if we restrict that to networks that have a that using the ki P methodology we have determined have a lower bound of 32 independent clients you see that there are 824 SNS and so this graph and the next graph are limited to only that subset of a a sense that means that of the 6500 a SNS that have v6 connectivity only 824 of them actually have more than 32 clients simultaneous clients next slide so about of the the 824 about 40% had more than 1000 / 60 forest per day that\u0027s what the the solid blue line indicates next slide now if we zoom into the into the upper right of the previous chart sorry go back for one second if you zoom in to the upper into the upper right we\u0027re going to use a we\u0027re gonna use a CCDF to kind of look at the what\u0027s going on with the with the networks that have many many v6 addresses so what this chart essentially says is if you look down in the lower right where the where it\u0027s it\u0027s getting toward the right side of the graph those are the networks that have that have at least 10 million client addresses next slide so about one percent of of the asns in this study so about ten ASNs have more than 10 million daily total / 64 prefixes or addresses that says that there\u0027s a ton of concentration in the v6 address space at the moment but many networks have v6 connectivity but most of the clients are actually in a small proportion of those networks next slide so per ASN the maximum lower bound estimate of simultaneously assigned / 64 prefixes is about 20 "
  },
  {
    "startTime": "00:18:22",
    "text": "million now if you the way that this graph is is laid out this is what the green line is showing you is is the number of clients the number of what we believe to be simultaneously unique clients in in a single network well in order to get the total number of clients in in in the ipv6 internet simultaneously that\u0027s essentially the area under this curve under the green curve where which is a little bit confusing because the because the the y axis is logarithmic but if you were to do that integral you would get about 110 million clients simultaneously next slide so web wide simultaneously assigned such as before prefixes are about 110 million that that means that\u0027s based on the ki P methodology which allows us to put a lower bound on the number of simultaneous clients it could be higher than this but this has less bias than looking directly at directly at observed addresses because observed addresses there are many more of them potentially as clients can arbitrarily decide when to when to add a new privacy address next slide so some of the key takeaways from this are that despite the steady growth in the number of ipv6 capable networks many seem not to have v6 capable hosts and it seems like there\u0027s a very small proportion where all of the where most of the v6 clients are concentrated so the a small number of networks continue to dominate client counts so the top 1% of networks have more than 50% of the clients by the lower bound estimates of simultaneous clients and while most ipv6 clients use temporary privacy addresses there are millions of v6 clients and some networks that are not using privacy addresses and this wasn\u0027t this is evident from the study but not from what any of the previous slides if you go forward one to keep going yes stop there if you look at this slide you can see so the big purple block there is reliance Geo note how large that is this is the observed number of addresses pert seen per day now this includes privacy addresses next slide if you factor out the privacy addresses and look only at simultaneous client IPS that block it\u0027s much smaller which suggests that they\u0027re using that they\u0027re issuing privacy addresses to their clients where some of these other networks or not they become more prominent as they become a larger share of the total number of simultaneous clients so anyway that\u0027s all that I had any questions Lorenza "
  },
  {
    "startTime": "00:21:23",
    "text": "you can\u0027t issue privacy addresses sorry that they true they chose privacy addresses some some networks are using DHCP to assign addresses and therefore don\u0027t don\u0027t allow the clients to choose privacy addresses I misspoke there right but like realized viewers a mobile network doesn\u0027t use DHCP I am NOT trying to quibble with with with your explanation but I wonder if there\u0027s something in the data that you\u0027re not seeing that that\u0027s why because the Anwar line you\u0027d really expect to see these privacy extensions so like if you took Comcast for example you\u0027d expect to see a big thing there particularly on wireline or Wi-Fi networks where hosts join and leave frequently sometimes they might Rev their privacy addresses multiple times but on a mobile network at least implementations I\u0027m aware of they basically until you disconnect and reconnect they keep the same IP address and so it\u0027s surprising to me that when you look at the data you see different addresses it\u0027s interesting it\u0027s something I\u0027ll bring to Dave\u0027s attention West particular USC can you speak to why you chose slash 64 is your aggregation boundary because I think the current recommendation and my data might be out of date because I don\u0027t follow v6 closely enough but I think isn\u0027t a slash 56 a another reasonable choice to study the boundary between where network boundaries are because you may be aggregating multiple slash 64\u0027s we\u0027re actually in the same you know technically in the same network because the slash 56 is the recommended assignment to networks isn\u0027t that true I\u0027m not sure that I can answer that question I think I\u0027d have to I think I\u0027d have to pass that one to Dave okay yeah look in the latest RFC that recommends where the block boundaries are and I think slash 56 is the is the recommended and part of the ki P methodology is to is to look at is to essentially build blocks of build subnets that are large enough to have at least a certain number of IPs but I don\u0027t know how that fits in with this study but there may be there may be something there any other questions thank you [Music] so hi I\u0027m Brian Trammell I\u0027m we talking today about a paper that I wrote with Mark almond and Rob Beverly that was in a CMC CR this April um this is an adaptation of a presentation that Rob gave to Singh Kham focusing on slightly different primitives so I\u0027m "
  },
  {
    "startTime": "00:24:25",
    "text": "assuming everyone here cares a little bit about network measurement or they just want to be sitting in a cold room checking her email so this fundamental operations research protocol design policy development I mean this is you know you got a measure you got to measure the network in order to know how it works but there\u0027s basically no support in the stack there\u0027s one explicit measurement function in the IP stack it\u0027s ping ICMP echo request an echo reply otherwise what we do is we leverage unintended features like trace route or we use brittle hacks like looking at passive tcp loss and RT t so you have to figure out you know you have to figure out what um congestion control you\u0027re using in order to be able to figure out exactly how much loss you\u0027re seeing so on and so forth and then there\u0027s inference I mean like so there\u0027s a measurement conference that I just came from IMC that had a whole track on you know here are a cool little inferences you can use to take you know data series that\u0027s unrelated to the thing we care about and try to figure out you know try to tease out you know something about the performance of the network or something about the topology the network so on and so forth so this leads to a question right like so the result is is that like operationally relevant questions and research relevant questions are hard to answer right that\u0027s why we have a research group here because if it were easy there would be no reason for us all this if you\u0027re and say look at my wonderful results I just you know I ran a program and it dropped out the bottom yeah you can do this we don\u0027t need to have presentations about it so like you know how do you do routing you know it\u0027s solve problem PGP right um what\u0027s the capacity utilization of a link how did networks interconnect what a s operates a given router all of these are things that you have to go back to there\u0027s no there\u0027s no explicit functionality in the network for finding these things out even things that we think are simple or hard delay between two right like I\u0027m gonna ping this and I\u0027m going to I\u0027m gonna see what the RTT is I\u0027m gonna divide it by two that obviously works that\u0027s the one way delay no not at all there\u0027s path delay there\u0027s hose delay there\u0027s a symmetry there\u0027s um for protocol traffic differentiation so your ICMP might not even be going over the same path that the actual traffic is going over um we\u0027d like to know what the endpoints are on a communication as everybody knows because the IETF said that there\u0027s no such thing as an that there are no naps in the network so that\u0027s actually pretty easy how did what order did packets arrive in a row destination that they ordered where they modified where they mangled you know what path did they take how are they cued you once you send a packet you know you could put a destination address on it and it appears the other side or it doesn\u0027t it\u0027s really difficult to figure out how how it got there so we look at this situation got depressed about it and said well what what would happen if we rethought the internet protocols fast a chasm with measurability is a first-class component what if we said well the point of this stack is okay well we have the the network layer which is optical transport layer which is end-to-end and each of these things had facilities in it in order to be able to answer these questions what if answering these questions was as important as delivering packets so the approach that we took here was to define some first "
  },
  {
    "startTime": "00:27:28",
    "text": "principles for measurability and then to imagine that packets could carry this measurement information what you know what what would we put in them so what have a candidate primitives for this and then from the candidate primitives follow the measurement capabilities that we would get so the principles that we looked at where that measurement should be explicit in band that the consumer should be able to cost the measurement provider should retain control measurement should be visible and measurement should be cooperative a little bit more about these so the reason for explicitness other than um you know explicit is better than implicit from epital of Python is that it reduces the ambiguity of the measurement right like so by using IP TTLs for courteous route you\u0027re basically trying to do you know something hop by hop that isn\u0027t necessarily hop by hop you might not be seeing all the hops for example it also increases future proofing this because if the feature is explicit it\u0027s there for measurement we\u0027re not going to change it for some other thing and have the measurements break like so for example stretch acts or for TCP RTT it should be in band this is primarily to ensure that the measurement traffic is going to get treatment as close as possible to the real traffic that you\u0027re trying to measure the consumer should bear the cost so if you want to put a lot of boxes on path whose job it is to do measurement um and you know okay well now your throughput through those boxes goes down by 90% because you\u0027re measuring things nobody\u0027s going to deploy them but it\u0027s a lot easier to take that cost and shift it off to someone who\u0027s doing the later analysis so the information that comes out of this it\u0027s okay if it\u0027s a little bit inscrutable as long as as there are some algorithmic transform you can apply to it the provider should retain control this ensures that the users so the measurement provider here in this case is the entity that\u0027s putting the traffic on the wire that is that has the data in it that is measurable we\u0027d like them to know how many of all their traffic is right you\u0027d like to be able to opt out of it or opt into it measurement should be visible so if I can see the measurement and you can see the measurement then everyone can see the measurement um this increases the transparency the fact that there is measurement traffic on the wire and that there\u0027s the that this can be trusted and measurement should be cooperative this is just sort of a rephrasing of principles three and four and basically to sort of leverage the existing puzzle between the middle in the end so we have a set of candidate primitives have a look at the paper for those I\u0027ll be talking about timing an arrival and change a detection and cue delay so what we\u0027re not saying hey we should actually build all of these things into um you know IP version what what are we up to now seven or that we should build these things into TCP two or whatever we\u0027re saying you know this is a illustration of the principles we\u0027re using these to test the principles and see if that makes sense so for timing we want to be able to figure out you know what the round-trip time of a particularly "
  },
  {
    "startTime": "00:30:29",
    "text": "transport um flow is turns out that the time stamp option is almost right for this but it since it wasn\u0027t designed for passive measurement it was only designed to give information to the sender um it doesn\u0027t expose anything about the delay right so if you have a time stamp or a a packet that goes in one direction and an AK comes back you don\u0027t know how long it took generate that AK so the way that we fix this is we basically take time now in time echo these are basically constant rate clock things just like the STC PTO sopped and then the delta time which is this ticks in the same clock at the sender since we saw the echo so one thing to know here is that there\u0027s a resolution overhead trade-off right so with respect to to TCP timestamps you have to put them on every packet because they\u0027re used for for a wraparound detection for this you could basically say I\u0027m going to put this on 1 over N packets and I\u0027m going to eat the the Lawson resolution so this meets our second and fifth principles that it\u0027s in band is visible the fact that we have this Delta here is just for measurement that meets our first principle for explicitness and then the resolution overhead trade-off is basically a way to achieve the sender control so the way this works here\u0027s where it goes fast so oh okay I send a packet with I\u0027ll be the sender you\u0027ll be the receiver I send a packet with a time stamp on it the receiver sends a packet back but this time stamp and then it that goes back the thing and you can see there were there were no delays and I wait two ticks of my clock and I send the next thing and then we keep going and going so far so on and so forth and if you put an observer in the middle they can basically look at this and look at the rate of this interlock the rate of receivers clock compare that to a local clock I mean you have the clock drift issues that you have anywhere where you\u0027re trying to do network based measurement and you can essentially figure out what the rates the inter departure times are the inter arrival times you can get jitter out of this and the delay information gives you the correction that you don\u0027t quit with TCP timestamps um so a rival information this is basically in TCP done by looking at the the series of sequence acknowledgment numbers and attending to model how retransmission works a more explicit way to do this would be to make this wall stre order invisible and a transport an independent way we actually didn\u0027t invent this this is inspired by a Scott savage paper from 1999 where this was basically trying to get better information for TCP congestion control with a misbehaving receiver so the way that this works is you increment you have a randomly chosen of an increasing number for every packet sent you increment it by and are you increment it by a given increasing number so you can tell what the order of those increases are you maintain a running sum and then you echo the running sum so this meets the consumer cost primitive our consumer cost principle because so actually teasing this apart does actually require you to look at every packet and pull things apart so and in order to figure "
  },
  {
    "startTime": "00:33:29",
    "text": "out what the the order is it\u0027s also explicit and visible as the other one and up we only had three on that one so the way this works is you start with a starting number and you have an increment series and we can just go through these and in the general case it goes back and forth and back and forth and back and forth and you you just echo the last one that you saw each time you sent it um for loss detection let\u0027s go through this and let\u0027s say okay we lose that packet here an observer okay yeah so here you\u0027re going to basically echo back on the next packet well I saw the 200 plus the four I saw that I saw the two or four but I haven\u0027t seen the 211 so I\u0027m gonna go back to a four go again and then yeah so you\u0027re gonna see that the okay no so you\u0027re going to see that the that increment in the middle is just missing so the sender now also knows in any observer on the left side of the sender over here is going to actually see that that didn\u0027t get lost even though there\u0027s no indication anywhere in this that there was um a like there\u0027s no there\u0027s no flag on this and says hey this is a retransmit you\u0027re not looking at anything in the in the transport level sequence numbers so on and so forth the observer on the other side essentially makes different inferences by looking at the series of of increments this also works for reordering actually we can go through these very quickly so here you know the the first packet here actually got reordered and in was received after the second one and you can again see this in the sequence of echoes so yeah well yet so study study the numbers and do the math for yourself offline because I have what like eight minutes left at this point yes good okay um you can take this a little bit further there\u0027s a primitive in here called probabilistic and triggered stamping this is the second numbers of the sections in the paper so this is essentially a request for information to be added by a router so probabilistic stamping is you know every in packets are going to get information out of the packet a problem or critter of stamping is you essentially have something that looks very much like the TTL time or the the IP time exceeded message you say okay decrement this when you are the inter outer that understands this please put some information on the packet and the the two basic ones we will look at here were things like performance Diagnostics so okay I\u0027m gonna give you the time stamp for now my instantaneous queuing delay and my instantaneous queuing capacity and you can essentially use this to do to essentially build maps of where the hot cues are in the network you can also do this for technology discoveries this is essentially explicit trace route the addition that you get here is you have the AAS number some identifiers that the router chooses for itself so that if you know which a yes it is "
  },
  {
    "startTime": "00:36:31",
    "text": "you can actually go and and look that up in a table and then you have both the incoming and the outgoing um addresses so you lose a lot of the aliasing problems you have in trace route so yes we here we meet explicit visibility this is an example of sort of the co-operative principle the idea that here okay we actually now do you need to ask the routers to do things for us so in conclusion um measurements critical I think you know the the people in the room you\u0027re probably all agree on that this paper is kind of a position paper to discourage spur discussion debate and inform protocol development that we need better support from the network so really what we see is the as the contribution of this work is the set of principles so the idea that we you know we have these six basic ideas we can test that against measurement facilities that we look at adding not just to this this imaginary future internet wherein we actually can go back and say okay we\u0027re gonna change this back so all of these things are built into it but also any present sort of design for adding measurability for to a protocol or for building a measurement facility that is not directly integrated with a protocol paper demonstrates these candidate primitives that address long-standing and important trailwood measurement problems so this is basically sort of an add go read the paper please talk to us if you think these principles are interesting that\u0027s that\u0027s really sort of the takeaway that I want to get in that I\u0027d like you to get from this is that we think this is a good groundwork set of principles for designing measurability for the future internet so with that I think I do have a little time for questions thank you very much anyone all right cool thanks okay so we\u0027re ahead of time but maybe take a look at the paper and think more about it and come back to the my prejudiced nexus Roland hi what no no got a kicker hi my name is Ron Friedrich I work for a surface national research network in the Netherlands and I\u0027m also an assistant professor at the University of Twente this is joint work with all the organizations here on the slide and I\u0027m going to talk to you about the route Canary and specifically about the evolution of a measurement that we started earlier this year so first of all what is the goal of his talk I\u0027ve both I and my co-authors William from a Nelnet Labs and Moritz from SLE and I\u0027ve talked about route Canary before in various venues but I wanted to present this at the Met party as well because we learned stuff while we were evolving the measurement as the route KSK project sort of grass through time and got paused at some point which I\u0027m going to talk about later on in the presentation "
  },
  {
    "startTime": "00:39:32",
    "text": "and I want to give you some takeaway the stuff that we learned while we were setting up the measurement and while we were actually measuring and sort of adjusting what we were doing so first of all quick recap why did we start this project I hope everybody has noticed that I can start this project for replacing the root DNS key with a new one this year anybody not familiar with this project please raise your hand I don\u0027t believe you Jeff so the goal of this project was we wanted to track operational impacts of the root kxk roll over and specifically we wanted to see if any resolvers on the internet were experiencing problems because of this roll over process but he said various times during this project there were going to be stages where some resolvers might start showing failures because they were unable to pick up the new key or because they were unable to receive responses because they were unable to deal with fragmented responses coming back from the root and we wanted to do this for two reasons we want to be able to warn people if stuff starting going wrong on a large scale but we also wanted to measure this over the whole period so we could at the end of the process sit down analyze the results and check what if we learn from this process and how are we going to have to deal with future case get rollovers so our measurement methodology measures from four perspectives I\u0027m going to show you two of those just to explain why we think we need many perspectives actually three of those I\u0027m mistaken so we use write atlas probes which is one of the obvious choices we use loominatee which I\u0027m going to tell you a little bit more about in the next slide and we\u0027re hoping to use some of the data a penis collects at the end of the measurement so we can sort of compare notes and see what we observed from all of these different perspectives and then finally we also want to look at traffic recorded at some of the route letters hopefully most of them we can look at the day-in-the-life data for them to see what happened what was visible from the route while this case k roll over was going on and how resolvers were acting based on the various stages during the case k rollover so a little bit about loominatee i\u0027m anybody not familiar with ripe Atlas excellent so loominatee is an HTTP proxy service there\u0027s any has anybody heard of the whole unblocker service it\u0027s one of these netflix on blockers raise your hand if you have heard about it okay very few people so basically if you\u0027re in a in a country and you can\u0027t watch certain content content on a service like Netflix but there are other services that this applies to as well you can sort of go through some 30 VPN service to pretend you are in another country and then you can see the content that\u0027s made available in a country and this is what Halle does but if you click agree on their terms and services you actually agree to become an exit node for their VPN proxy service and academics love this kind of stuff because you can use this to do measurements from a residential perspective and this is actually something that the folks from Northeastern University are collaborating in this project have done "
  },
  {
    "startTime": "00:42:32",
    "text": "before so they\u0027ve used this for other measurement they had a paper on the TLS landscape and we had a paper at youstick security and IMC which is about DNS SEC validation and what\u0027s really nice about this is that you have millions of exits and exit nodes that are in residential networks so basically you\u0027re you\u0027re sort of observing the same thing that a penis is observing with their measurement based on ads but you have different visibility in the ecosystem so what we do is we have HTTP requests that trigger DNS queries and using that week we\u0027re able to cover about 15,000 a SS and the interesting thing is that 14,000 of das that we covered are not covered by wrap Atlas probe so we are getting different visibility into the problem space which is nice so what is our measurement methodology basically we have a set of signed records we have signed them bogus records for all of the signing algorithms are in use today we have almost all of the DS algorithms are used in the today so then you get a matrix of algorithms that you can do measurements for and if a resolver and we can measure one of three outcomes whether a resolver validates correctly whether it fails to validate so it gives us a surf feel when we\u0027re not expecting to see one or it doesn\u0027t validate right it regardless if it\u0027s assigned record but the signature is bogus it still gives us a response so it\u0027s not validating and of course there are some corner cases here but I\u0027m not going to go into detail about those so if you want to have a look at this project so the URL is at the bottom of the slide we actually have a couple of life results up if you know if you go to the website you can you can have a look at the live results this is for the most common signing algorithms and this shows you visibility into the atlas part of our measurement unfortunately that\u0027s the only one for which we have live results at the moment loominatee is a little bit trickier to get life\u0027s results for that but we\u0027re updating our results as they come in and what you can see here the green there so these are all pie charts the the green part of the pie chart shows you the fraction of resolver Pro Plus resolver pairs that are actually correctly validating that particular algorithm and signing algorithm and DSL rhythm and the orange part are resolvers that are not validating that so there are just normal resolver that don\u0027t do dns sick and one of the takeaways here is that you don\u0027t see any red so we don\u0027t see any resolvers failing to validate which is what we would expect to see if stuff starts going wrong during the case Kay rollover now if you click on one of these pie charts you actually get a more detailed view for that specific algorithm so this example shows you shot to five 60s with RSA sha-256 signing again you see the detail of which fraction of resolvers is validating and which fraction isn\u0027t but there is also a CDF and that shows you the time that the population of probes spent in that state and the takeaway from this particular CDF you\u0027ll see it if you click on all of the algorithms is that if resolver is validating it\u0027s "
  },
  {
    "startTime": "00:45:33",
    "text": "actually really stable it\u0027s validating all the time so it never flips from validating to not validating it validating all the time and that\u0027s actually a nice result because we didn\u0027t actually know this before we didn\u0027t know if resolvers were actually stable if they were validating or if they flip state very often and the resolution of this measurement is once every hour so we think we\u0027re getting quite good visibility into this so just to give you an idea of why it\u0027s important that we measure from different perspectives I guess everybody sort of assumes that right path assistance um somehow biased right it\u0027s it\u0027s people like us that pick up an atlas probe at an event like this plug it into our network we are not representative of the Internet residential users are sensitive of the internet right so if you compare the fraction of resolvers that validates that we observe with the Illuminati measurement in this case and we compare it to the right measurement you can already see a huge difference right of the 13 of a set of 13,000 vantage points on the luminosity measurement 7% were behind a validating resolver but if you look at the Atlas measurement then 42 percent of Atlas probes are behind validating resolve that\u0027s a huge difference and again the AP neck will give you a different number because I think it\u0027s what is it 15 60 percent behind validating resolver roughly yeah so sorry can I ask a quick question to the previous yes sure because it doesn\u0027t say in Turku it says I don\u0027t can\u0027t even read it no other so we didn\u0027t do the full analysis we just looked at how many are of it so the rest is either not validating or the order of a non validating which means that they\u0027re behind murmurs over that validates and one that doesn\u0027t but we didn\u0027t do the differentiation here you\u0027re jumping the queue anyway let\u0027s go back to why we started this measurement because we wanted to observe the root case k roll over process now the chart here shows you what that process initially looked like and I marked one point in red which is the first moment when we would expect that something might be going wrong and this is September 19 of this year at that point in time until that point in time we had three keys in the DNS key set for the root we had the new case K the old case k and one zetas k and on September 19 as SK roll over started and that means we introduced a new key into that set it grows in size and consequently this was the first point in time when responses from the root might actually get fragmented the specific key if they\u0027re transmitted over ipv6 because it would now exceed the minimum MTU for ipv6 so we wanted to know what happens well I\u0027m not even going to show you the Atlas probe results because nothing happened there and I\u0027m going to say something about that so these are the number of servile responses that we got back in our Atlas measurement you see one little spike a few days in from the actual introduction of the new satis k we looked in detail at that and that "
  },
  {
    "startTime": "00:48:33",
    "text": "was just one vantage point failing for a completely different reason that had nothing to do with the new Zetas getting introduced so the takeaway from this is nothing happened this is traffic to be route with saying thanks to West for giving more it\u0027s access to that the takeaway from this is again nothing happened because there was no noticeable increase in TCP traffic to the route which you would expect to see because people were getting truncated responses and falling back to TCP on the right-hand graph you can see there is also no increase in truncated responses so you wouldn\u0027t see TCP traffic because of that so nothing really happened so Wow summary of that nothing exciting happen okay and then so October 11 of this year the new KSK was supposed to go live and then I can decided to pause here boo so did we do all this work for nothing well no so the rest of the presentation I\u0027m going to talk to you about the sort of side effects that we had from this measurement and what we learned from that so spin-offs the first spin-off we had was an online algorithm test while we were designing this measurement I was having a conversation with William from an outlet labs and we and we said well can\u0027t we use all these dis matrix of algorithms to actually make an online test that allows you as a user to figure out which algorithms will my resolver actually validate and actually that turned out to be rather easy to do so if you go to the website you can actually click on the testing you you\u0027ll see the results and actually for the ITF network here so this is for my home and I hope you can see that all the way to the right my home resolver actually validates one of the newer algorithms EDD sa two five five one nine because I run a patched version of open SSL to get that working on network here you\u0027ll see that the left hand column RS am diva md5 isn\u0027t treated as insecure is treated as secure and that probably means that they\u0027re running bind on the network here as a resolver is there anybody who knows about whether that\u0027s true anyway I lost a NOC team the second spin office that we test algorithm support for all of those Atlas probes over time so actually if you go to monitor that route canario dorg you\u0027ll see a map and you\u0027ll see measurements popping up as they come in so you\u0027ll see a little green or orange dot popping up on the map you can click on that and that will show you the state for that resolver at that time and it will show you a lot more for instance a whole table for a s is the number of probes in those yeses with validating resolvers whether that went up or down over time so we get a really nice visibility into the resolver ecosystem behind atlas probes over time and we\u0027ve actually already seen that the number of validating resolvers increased over time and i\u0027m hoping that one day another resolver will pop up that actually validates some of the newer algorithms that we have for DNS SEC also anybody "
  },
  {
    "startTime": "00:51:34",
    "text": "have a guess which resolver these people are using you can see that it\u0027s one nine two one six six eight one one but it\u0027s forwarding its traffic to somebody else anybody guess okay it\u0027s Google yeah because Google somehow thinks RSA md5 should return a surf film go on go to the next why is it not can you press yeah oh thank you okay so we made a little mistake somewhere during a measurement and actually we forgot to resign our test domains and that means that the signatures were expiring so if the signatures are expiring what you expect to happen is that resolver will start to return surf fails to queries that they were validating before because the signature has expired next slide please but actually something weird is happening at the top of the graph because you see that a fraction of resolvers are still saying no error so they are validating this and they\u0027re saying that it\u0027s okay those expired signatures are still fine and that\u0027s actually an interesting side effect because most resolver implementations will have some sort of grace period next have some sort of grace period which is configurable within which they will treat an expired signature as okay because they\u0027re assuming that their clock is off and from this measurement we actually learned that that is sometimes configured to be quite extreme and we\u0027re she going to do another measurement later on to work out whether it\u0027s just set to a very long time or whether some resolvers just ignore the expiry time altogether which would be bad next slide please so the final spin-off that I want to talk to you about is the Swedish canary and I assume you\u0027re familiar with the Swedish Chef next slide please so Moritz from sa DN who is also part of this project presented this work at the NSO arc in San Jose in September I think and after the presentation the good folks from IAS came up to us and said well we\u0027re doing a KSK and algorithm roll over for the dots dot a CCC Tod and you can read more about a project on the URL that\u0027s on the slide slides are also on the meeting material website and they asked us if we would want to measure that and signal problems to them because they\u0027re kind of scared and the interesting thing about this is that it\u0027s on I would say a more agile time scale than the root case care will over for which we now have no deadline this is due to take place in less than two weeks so they\u0027re actually starting at the end of November and they hope to finish by the end of the first week of December so that means that we had to work really hard to get a measurement up and running but it also means we get our results pretty quickly next slide please so for this project we developed some new methodology that covers issues that are specific to algorithm rollovers so I\u0027m not going to go into details but just to give you one example one of the things that you need to do is you need to introduce signatures before you introduce new keys because the RFC specified that you could otherwise do a "
  },
  {
    "startTime": "00:54:36",
    "text": "downgrade attack and some resolvers will actually fill if you introduced the new key before you introduce new signatures because then we will say I\u0027m expecting to see signatures with this new algorithm and they\u0027re not there so I\u0027m gonna treat this as an attack and I\u0027m not gonna validate this anymore and because DOTA c was the first TLD to actually sign it zone in in 2005 and this was well before the route got signed there are still resolvers out there that have a trust anchor configured specifically for dota c and we already with this and there\u0027s his threat on the DNS of mailing lists about this because we discovered that most resolvers will actually prefer the the more specific trust anchor for dot to see and if that doesn\u0027t match up with the key they will return sir fill regardless of whether there is actually a chain of trust from the route all the way up until that signature that they\u0027re validating and that\u0027s a problem next slide please so to conclude and I think the main takeaway for me from from this project is that we started measuring this root case cable over sort of as an adduct project we had a discussion early in the year and it turned out that nobody was actually measuring this and then we said well we think this is important so we should start measuring it and those are thinking about the measurement evolve lots of spin-offs develop so we got lots of nice site results that will give us better understanding of how this protocol actually behaves in the wild um and it\u0027s also a case study of why rare events like the root case can roll over or like an algorithm roll over that they\u0027re doing in dota C really deserve our attention as a measurement community because if we don\u0027t measure these things then we\u0027re gonna keep making the same mistakes over and over again because for Algar and rollovers some of them have occurred in the past and almost all of them that I\u0027m aware of have gone wrong in some way that\u0027s why we need to start measuring this type of stuff so um I\u0027m gonna sort of say what the chairs probably also says measurements give you better understanding better protocols so fewer failures keep measuring people next light please most of the data that we collect is open data and so although the right measurements are stored in the in the Atlas API so you can actually extract all of those measurements if you want to have a look at them yourselves on the monitor that root canary site there is actually WebSocket you can connect to and it will stream live measurements to you as you connect to it and all of the data sets that we\u0027re collecting well of course when we publish papers about this will be released as open data and I think that\u0027s lust yep any questions right this is Aurelia my question is did you analyze another route service a lot of the be food server for the traffic to show for changes in traffic no we only had data from B roots at some point we "
  },
  {
    "startTime": "00:57:39",
    "text": "will want to look at the other route letters as well but for this particular event we only had access and we had to do is really quickly right because this is in September and the presentation was two weeks later so we had to sort of get work with what we got and I think you\u0027re now going to say that another root letter saw something different yeah so I know there was nothing happened at the September 19th that is a very good news for us so okay so you can concur that what we saw actually you saw the same thing good any other questions okay thank you very time thank you come back when you have more data about the sweet peas Giovanni yep no no you can just wave your alright so yeah this work we conducted together uh with twenty and is I just before I start how many DNS people are in the room here or the DDoS protection people right so you huh yeah that\u0027s what next place so in this talk this is a paper presented on I am see last week and the weather approach we have in this paper is the operators approach I work for our operator and the finger wanted to do is like how we can actually optimize and better engineer the deployments of the our name servers so this is came up from a production question and by optimizing better engineering I refer to reduce the latency to our clients because well time is money so there\u0027s many reports that show that if I go to that can find Amazon saying that searched times will higher latency loss of money and DNS is part of that as well so next with and this is the real setup we have for it\u0027s not even working for Dardanelle and if you\u0027re not into DNS let me break it down to see you so um we for the Dinan L the T ccTLD for the Netherlands we run eight different authoritative name servers they have this different color somewhere any case on my unicast like you use the same for any other domain just only for that NL and we use eight you can use different numbers is just for redundancy alright thank you and high-availability see if it works all right next please and when we have a client at home let\u0027s say you we want to resolve in the domain name like example that I now you go to your local resolver that you can find in a computer next please but if this resolver can actually make a choice to go to any of those eight name servers authoritative news services see "
  },
  {
    "startTime": "01:00:39",
    "text": "on top I think it\u0027s the same if you\u0027re on here on the streets if you wanna find an ATM and there are eight ATM around you can go to each of them someone gonna be further than you so no you closer to closer to you and we as an operator have no control on how resolvers Mac I should get to choose and the question that we want to answer is like how we as an operator can actually help them to get the best results like theirs shortest latency to the main service next please and of course some of the name servers they use any KS which means they\u0027re like this name service distributed across the globe assortment of various machines next please and this actually came out from observation ahead of the company so this is the same setup but the area of the graph show the actual number of machines that each authoritative name server has so Nath nods big dinners provider it provides services for us and you see like they have any caste of vehicle any case in any case cloud but when you look next to the actual traffic we see that the biggest name server in terms of machines that we have doesn\u0027t actually get the most traffic and that puzzle males like how I can better engineer that you actually see that the ns5 has a bunch of local any caskets a lot of traffic which was interesting so like how you can actually level out the same next place and for our unique asteroids they\u0027re out late locating the Netherlands we have 22% of our curious queries comes from the US are even though the other any case I\u0027d met not authoritative server has sites in the US so what\u0027s going on here so that when this work started next and this is happening because we as a side the end will only operate what so on circular both the recursive resolver this is this is actually cold run by created by different DNS software like a mother pine and the clients actually those clients are located at homes data centers or different people so we have a lot of different entities then they have different goals so we our ago is just as an operator try to serve that matter next and this kind of has been done before there was a work by the way no esos and the steam um but it wasn\u0027t in the live set up when our questions like we don\u0027t want to focus on specific software versions or vendors we want to see how this behaves in the wild because that\u0027s what we care and and that\u0027s what we did next so we have set up seven measurement setup actually more to the deltas measurements weight away the same more it\u0027s for the presentation and we have used Amazon data centers we have set up on each data center a different authoritative nameserver and we have activate some of them and activate at a certain time and next please so let\u0027s consider measurement we had only two an authoritative nameservers for our example domain and if it\u0027s a ripe ripe Atlas if you have only two two sides here one is on poly 101 in San Francisco this client at home can go to the resolver and this recursive resolver can choose what is what is the one who "
  },
  {
    "startTime": "01:03:40",
    "text": "want to send the query next please and we didn\u0027t have done that for all the orders so we have this combination see like how it actually varies as you add more name servers so the first question I had do actually ow there is overs choose all the authoritative name servers available like for that right now we have eight they actually go to the eight or they stick to one and the first thing we found is actually they use all of them they actually query all of them so let me get a measurement was his name true a it\u0027s like only have two sites and I don\u0027t recall by heart which is hideous but what matters is like very quick we have 96% of all the arrived at those resolvers like for now the probes after one query they take like less than five queers likely to queries to actually reach the two of them and if you have like four resolvers it would take like six queries after the first one for the clients for the clients to reach all the name servers which means like if you have one very badly name server authoritative name server running that means that if this is one\u0027s gonna traffic attract traffic from a lot of people because that\u0027s the way the recursos behave next place and another question we\u0027re headed so that we found out okay they go to authoritative but how a actually distributed the queries between captain query with these measurements were carried for every hour have a very short TTL so they would expire the cache and we will do that every two minutes so let me cuz you know next please let\u0027s go see there I set up that we had once fingers chewy we have two sites one in Sao Paulo and one in Japan and what I have and what happens is like the this is the first graph here shows that median art ETF for all the rap atlas probes towards sambal electrolytes Tokyo here and this graph shares a distribution if you have only to name servers available for this example that NL you see that like dr. take you through a similar latency they distribute they get like the similar number of queries so the resolvers like if you have more less the same agency i\u0027ll distribute more or less evenly next please but for sites since rathus has a lot of probes in europe you would expect the frankfurt would have a lot of lower ladies if it\u0027s the case and Sydney would have a higher one and if you that\u0027s the that you are possibilities that you can go to the name servers what\u0027s gonna happen is a distribution it\u0027s very a virtually proportional to that so with the mean sets Frankfurt we actually get a lot of queries and said we\u0027re gonna get less next ways so we that confirms the previous work but for the first time here we had actually don\u0027t have done that in the wild which is the fingering in your attitude next also to confirm it to have look not only to aggregated traffic but it\u0027s also important to look at individual resolvers how each of them behave it\u0027s a very beautiful picture but it\u0027s let me try to explain this to you each column here represents run resolver "
  },
  {
    "startTime": "01:06:40",
    "text": "a probe ripe at this probe and resolver and we have divided them into different continents so next please lets me focus first here in europe and this is the measurement we only had like two name servers available tokyo and some power next we see that and this is the the certain this is the number of resolvers and you see some of them only stick to some power here they never they never distributed queries the y axis here shows the distribution so if you have only one caller means like your only sentence to GRU here next it\u0027s some other resolvers you always stick mostly to japan here next some others they kind of like just reboot among the two of them next and you see like 69 percent of 65 percent of all the Rizzo which is the x-axis here they have a weak preference for one of the two available authoritative and first week is just a definition like this end between 60 or 90 percent between 69 percent of other queries they sent to one of those name service authoritative service next we also found that 37% of those alternatives and they have like what we call a stronger preference they sent like most of the traffic 90% of the traffic it\u0027s you only only of those two name servers next is this and some resolvers they just always chooses lowest ones available so it\u0027s it\u0027s you\u0027re measuring the wire that\u0027s kind of thing if you would expect to get next we also this will always done with ripe evidence but we also decides to look into like production and servers and we would like we\u0027ll look at the root nameservers use the detailed data how did the tails are in the paper and we got data there from ten of the name servers and what we and this is that a nail that we run we have data from four of them out of the eight and what do we see for that is the root the queer is six percent of the queries are distributed across 60 of those authoritative nameserver so the root with letters and each color here each band represents the server so you see like there\u0027s an e fluid distribution some of them are very weird here they stick to one and further analysis more less evil a distribution which means that the production operations of the roots to ten ladders and then a now confirms our finds of our test bed next please so what is the summary of this measurement resolvers will query all available track data the name servers and the distribution it\u0027s inversely proportional of the median RTT of the particular authoritative to the particular resolver and recurrence will prefer a fast responded status but cannot but I also would query and slow runs over time there are some additional findings I referred to you in the paper you can read that there next please and that\u0027s the point the most important point from your this presentation because I work for operator we interests and how to better engineering that and the biggest recommendation a we have here like the conclusions that is Louis authoritative nameservers they will lend "
  },
  {
    "startTime": "01:09:40",
    "text": "the response time of the DNS server that means like if you have four name servers then one is running in a very small machine in the very poor neck poorly connected site you\u0027re going to reduce the performance for everybody in if you\u0027re a global DNS provider like we have customers from all over the world you cannot deliver good performance with unicast so our recommendation here to use any cast in now of your name servers and why any cast Wesson\u0027s going to talk a lot about any case so you can also watch that um but any case you can use the same name server and distribute across the globe in different locations so you can provide locally better latency for all your clients so they have to have good peering and these entities work actually an impact in production we actually deploying a lot of we are placing our unique cases and that I know for any case next place next data sets paper questions yeah thank you Giovanni you can I there\u0027s a question okay you just take the Jeff Houston a penny I\u0027m kind of curious about your recommendation that everyone should use any cast all of the time for rooted for authoritative nameservers how do you reconcile the issues around ICMP v6 messaging and the ambiguity and loss of signal that any cast invariably produces surely that recommendation is actually not a good recommendation unconditionally and you need to be bloody careful when you deploy any cast in v6 because you will not ensure the integrity of any kind of ICMP packet to big signaling so I\u0027m sorry it\u0027s just not a good conclusion at this point it should be a lot more conditional than what you\u0027re pointing out so this work is only I think I\u0027m putting does like me but did not mention its we only have known that for IP before I have stood that for ipv6 but regardless the focus of this goal is to deliver better response times and I think our conclusions still hold for that and I say feel uh this is something we have to take a look into that but it\u0027s a part of saying our goal is to deliver better performance for our clients we want to reduce latency so I disagree with that so come back with the ipv6 measurements then yeah that\u0027s another way to see that well it\u0027s something we have to do okay thank you thank you and this is our last presentation from Wes okay so I\u0027m Wes Parker and I this is about ver clutter which I\u0027m not the native speaker of that language I\u0027m probably mispronouncing in a little bit close though so this is about a broad "
  },
  {
    "startTime": "01:12:42",
    "text": "and low to where any caste mapping study that we did in combination with the University of Twente and from I\u0027m from USC s in information Sciences Institute next clickers okay next okay good wait wait no go back so our goals for this project were to really develop a technique that did a few things like accurately map anycast catchments and then study the B roots anycast ipv4 catchments and then predict some load and advantage of those changes this talk today is not about those three those three we\u0027re discussed at the DNS or workshop a while ago and you can go watch the counter talk to this one because in at that talk I didn\u0027t talk about the study of any caste stability over time so today you\u0027re going to see sort of the flip side of that Todd so really quickly as a reminder any caste is you know a multiple sites that serve the same address or host the same address and typically for things like ripe Atlas or any sort of any caste measurement utility that\u0027s been done to date you have to have lots of measurement points you have to lots of lots of vantage points where you have all these devices around the net that are going to send queries here any caste catchment and then you\u0027re gonna watch the them come back and figure out where you got to so next first loader is sort of the inverse of that and so we\u0027re actually using the entire Internet as a vantage point so the way we did that is we collect responses to ICMP pings that we send out from the catchment so in the in the example on the screen there\u0027s a box on the right and you\u0027ll notice that that\u0027s within the anycast catchment that was a server that I ran we sent out five pings and you see them come back to different places and we noted the discrepancy of which ones went back to which place so it was very different way of doing any cast catchment and to do all those pings we used previous work that was based on an ipv4 hit list that\u0027s basically a long list of IP addresses that are likely to respond to an IP IP v4 ping then it gives you a big long list of of examples there that URLs later on the slides as well next so what did we see so we studied a couple of anycast networks the first of which is the DNS SB route and you can see that this is the coverage from ripe Atlas so there\u0027s lots of little circles on the diagram they\u0027re all kind of small so to make sure that they all fit but you can sort of see the coverage is you can see a lot of bubbles in Europe and of course some bubbles in the United States and if you next if you shift to our technique for doing that measurement that you\u0027ll find that the circles actually shift quite a bit and one thing to note is there is a huge scale difference so I\u0027m gonna ask you to flip back and forth a little bit between the two because it\u0027s pretty you\u0027ll notice that there\u0027s actually a fairly big scale difference the ones on ripe Atlas the the bigger circle sizes is 277 or more nodes and on veut flutter the biggest size is a hundred and eighty-five thousand nodes in one of those circles so that there\u0027s a huge scale difference but you can still see the the shift in circles as "
  },
  {
    "startTime": "01:15:42",
    "text": "they move from one map to the next okay next we also used the university of twente has a nine site anycast testbed that\u0027s sort of spread around the world and you can see there\u0027s all the sites that we used for our test note that there are some sites that actually sort of have the same upstream so there\u0027s a little bit of bias you know toward one upstream may actually be hiding two sites within it because of there\u0027s sort of a single upstream next so similarly when we studied you know that network in terms of what sites kind of excuse me what IP addresses out in the world were responding to different sites we saw sort of a similar kind of coverage where now we actually have a nine color map and if you go on to the next one you\u0027ll see that the two things happen one we get more spread as we did before but not only that the colors actually shift so if you toggle a little bit you\u0027ll see that there are some places where the color shift look at Australia in particular and note that the colors are shifting from one to the other meaning that ripe Atlas is measuring a different actually set of of catchments than Denver clutter which is arguably more accurate next so given the significant number of vantage points what can we study and learn about networks around the around the internet and so one of the things we asked is can we study traffic catchments within an AAS now that we have so many vantage points can we see what happens as the number of sites change and and you know what happens with the size of the prefixes as a larger and larger prefixes are announced as well as what happens when the size of the a s goes up do does that actually change visibility and how many sites are seen in an anycast Network next so to answer one this is a graph of on the the bottom is the number of announced prefixes and going up the the vertical site is the number of sites so you can clearly see that there is a relationship between the number of prefixes that are announced for a given a s is likely to mean that they are also better connected to a larger number of anycast sites so that gives you an indication that sort of the larger ISPs are more likely to actually deploy stuff are more likely to hit more catchment sites next this is a bar graph so I\u0027m going to have to walk through it cuz I\u0027m sure it\u0027s quite small back there but on the bottom right is slash 23 s and so what happens is is one thing that we find with respect to the size the bottom right and if you go along to the left it increases by one so it\u0027s a slash 23 on the bottom right a slash 23 on the one next to it 21 and it gets all the way up to a slash 8 in the upper left and what we see is that the number of sites that are seeing / Network block increases as the the prefix size goes up so specifically 80% of prefix is smaller "
  },
  {
    "startTime": "01:18:44",
    "text": "than a / 16 are more likely to hit a single site and the larger prefixes like / 8 / nines and / - yes mixed and then finally we also wanted to look into the stability of any caste you know what how how likely was it that things flip so we did a study or or university of twente really did the study of measuring their test bed across 48 hours so they sent these same measurements every 15 minutes across 24 hours not 48 my bed - they\u0027re tangled site and a couple of things about this graph is that the 2nr and the from and our graphs are in the middle and note that there\u0027s there\u0027s a definite break because the top line is 3.5 million on average and the bottom line is about four thousand we\u0027ll get to that in a second and in the middle is the measurements that went to not reachable or from not reachable so in other words in the middle of the study all of a sudden that site went offline and the pings went away or inversely all of a sudden that one came in next so a couple of things to note about this is that three point five four million VPS you know maintain catchments in other words over the 24-hour period we never saw them flip at least on fifteen minute interval 89,000 actually did change from either from responsive to non-responsive or vice versa and then 4600 vantage points which is 0.1% actually changed catchments so in other words these are people that you know started off in one catchment point and then at some point ended up at a different site the interesting thing to note though is that you could actually look at this and argue well is actually more likely that they\u0027re going to turn their computer off than they will actually switch catchments points right but you know the reality is is they probably turn their computer off intentionally and you know they probably didn\u0027t flip the classroom at points intentionally that\u0027s the network you know making fun of it but the end result is only about one and a thousand only about one in a thousand chance of a TCP connection lasting longer than 15 minutes will actually switch to a different Nek site next in all of that data so now if we just look at the ones that did flip it turns out that 63% of the ones that flipped came from five a s\u0027s and so these are the the top five flipping a s\u0027s and I\u0027m sure you\u0027ve noticed that the top one is 51 percent so China net alone accounted for 51 percent of the things that flipped and I think that\u0027s the way that they do their network and you know they have a special network over there so they they do some load balancing tricks that probably you know are the result of that of all the flips they\u0027re all located within 2809 guesses out of the total set next so really quickly sort of the summary of why we think that this technique is actually a better mechanism for "
  },
  {
    "startTime": "01:21:44",
    "text": "measuring a large quantity in DL to really study anycast network I\u0027ve given you some of the results but in in the end ver floater sees about four hundred and thirty times more network blocks than an Atlas but act interestingly enough Atlas still has some unique ones like somebody else was talking earlier about two different you know ways of measuring stuff Atlas still saw some unique blocks that we didn\u0027t see an example they saw 2079 that we didn\u0027t see and in similarly burp letter saw some that Atlas will you know will never see in certain countries that Atlas can\u0027t really be deployed in and everything we did by the way it was slash twenty fours because that was the sort of the the smallest routable address block that you typically see in ipv4 on the Internet in total burp litter once I go back in total vertical it or so about 3.8 million different you know slash twenty fours note that birth clutter doesn\u0027t have everything geolocate about there were some address blocks that we actually don\u0027t have geoip data for whereas Atlas because the probe IDs are actually you know encoded with the location that they they deployed at they actually 100 percent coverage so that\u0027s actually a better thing all right next and then finally this technique and the code and the data sets are all available so if you want to pick up the code and go if you have an any cast network and you know you\u0027re interested in using it please come talk to me because we\u0027d love to see this run on more stuff to see more data out in the wild and with that I\u0027m done questions Robert you\u0027re stuck that means you see can you go back on slide please Thanks could it be that ripe has 2,000 or so unique / 24 is because the target list the hit list that you had for pinging included an address in that 24 that was not being able so you thought that there is an IP address didn\u0027t respond so then you consider that you didn\u0027t get data from that yeah I mean the reality is is that in some places the firewall restrictions are very different right they don\u0027t allow any incoming reality is there yeah right so we we haven\u0027t been able to analyze exactly why there\u0027s that much difference but the reality is is that people can go plug those those devices in behind a network where maybe that entire country doesn\u0027t allow incoming pings right you know so there\u0027s boundary issues on on the flip side if initely somewhat related what stops you at least as an experiment to do a full ipv4 ping like John Heidemann in this case does that make every 24 hours or something so yes and and so he\u0027s a co-author on this - yeah the reason being is that with a hit list you don\u0027t need to flood the internet so you could you\u0027re absolutely right he does he floods the inside every 24 hours so you know what I\u0027m waiting right and we use his results in order to you know make it slightly easier for us to send out that many you\u0027re right you can only arguing that maybe doing it once "
  },
  {
    "startTime": "01:24:45",
    "text": "would give us somewhat better results and then if it turns out to be a useful thing then maybe repeat it but possibly I\u0027m not sure we want to do it for like the 24 hour 15 minute study because that we actually do rate we did rate Linda limit them going out so that actually doing the entire hit list that we did took me 15 minutes a run from when I was doing it at Butte because we didn\u0027t want to over saturate the network with you know that\u0027s that\u0027s absolutely fair I\u0027m not saying that you should over exaggerate draclyn but the tools are ready to do that no you\u0027re right it\u0027s possible okay any other questions one more Giovanni said Ian I was not involved in the study but I know the people but it\u0027s just not a question it\u0027s just a comment that I had so first time I said I saw the presentation and the paper I was like this is interesting how can actually as an operator is that and I was talking to doing about that later and I think as an operator the cool thing about this fair flute it\u0027s a actually if you want to design your own any caste system you can actually get your prefix your Anika\u0027s prefix and before put it in production you can actually say hey I\u0027m gonna have some sides here and there what\u0027s gonna be the load distribution so it\u0027s just a remark for people here you\u0027re designing a caste network if I sort of estimated where the traffic is gonna go to each sites you should use this tool and if you watch the DNS or talk that\u0027s exactly what we did is we actually able to predict loaded B route in advance of actually switching over so we could figure out that and the results were I forego we forget which one is which but we predicted either 82.6% would still go to LAX and the and the rest would go to Miami and the measured result was eighty one point four so they were either point one and point point four point six off I mean they were really really close so it was an accurate prediction mechanism but go watch the orc talk for that half great Waterton okay all right thank you so quick quick question are your data available yes the data on the next slide yep the data the data sets are all there the papers there and the you know as an IMC paper so it\u0027s just published a week ago or two weeks ago at AMC but and the software is there too so it\u0027s it\u0027s very easy software to use just say ping and it does give it a hit list okay thank you very much that means we\u0027re at the end of our agenda and we have four minutes left Wow say you can either leave the room or you can also ask additional questions or whatever I don\u0027t know thank you very much see you next time thank you are you ready chair on the chair [Music] "
  }
]