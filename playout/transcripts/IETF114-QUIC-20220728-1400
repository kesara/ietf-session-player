[
  {
    "startTime": "00:00:09",
    "text": "now oh um so welcome everyone to the ietf 114 meeting of the quick working group if that's not where you expect to be either physically or digitally then i would uh hurry on over to where you're supposed to be so some uh things you know it's thursday now so everyone's probably used to this but i remember the importance of the ietf note well this is a copy of it i'm sure you've seen it if you need to find references to it it is very easy to find both on the ituf website but also googling anything related to itif note well it's a fairly unique name uh so for some itf meeting tips um in-person"
  },
  {
    "startTime": "00:02:01",
    "text": "participants uh you probably used this by now but remember to use meat echo light when you were in the room this is useful for us chairs to keep track of the cue with both remote and in-person participants and also we want to stress that those there in person should be wearing a compliant mask at all times there are n95 masks available at registration uh please remember to keep your mask on and remote participants i mean the normal applies for making sure your audio and video are off unless you're you know presenting and we do recommend using a headset and or checking making sure your levels are good on your mics so in minnesota we have our tireless volunteer and robin marks who is who is uh volunteered to take notes and we thank robin yet again um the blue sheets this is again being handled through mid echo remote participants doesn't have to do anything uh those there again please use media collite for chat um we are of course using mudeko and zulip so um and for the cue the chairs despite our and arguably larger than life presence we will be running the queue with the assistance of zahed who is you can see up there in person thank you zod uh matt just to jump in quickly there sorry uh we did have brian volunteered to take notes as well that's great the more the merrier um if somebody in the room would just like to assist the remote notetakers that would be greatly appreciated um so do dive into the notes and contribute where"
  },
  {
    "startTime": "00:04:02",
    "text": "volunteer for javascribe though um would someone like like that job um just in case we need to augment anything to the bike one of us could probably do it too but um shall i do that jonathan that would be uh very helpful thank you all right thank you thank you all who are helping us run this meeting all right so agenda which can be bashed uh so we have chair updates as usual which you'll have to suffer through and then we have three working group items the first and longest is multipath followed by the acknowledgement frequency work and then quick lb after that we have other items uh including quick time stamps multicast quick and uh and talking about quick announcement tooling martin would you like to say something yes uh martin duke google um kind of an agenda bash but like rather than actually introduce another presentation i would just like to briefly say that um uh i think most people are aware that lucas and i have a draft to put quick versions in alt service and serve dns service b records as we just as we dissociated alpn from quick version um in http bis today they are likely to like dramatically strip down alt-service and not have parameters and become service b would be the like transport capability discovery mechanism the upshot of that for this working group is that we will likely accordingly strip down our draft to just service b and the place where service b stuff lives is in the protocol working group where it applies which will likely be bringing this draft too quick sometime between sometime in the next month or two so look for that and we'll probably call for adoption uh relatively early since it's a simple mechanism thanks"
  },
  {
    "startTime": "00:06:00",
    "text": "thank you for that martin um is there anyone else who would like to agenda bash or otherwise have a note of order here and if not i will move on to oh miriam okay mary is not ready to go okay uh update since last meeting uh uh as probably everyone hopefully noticed and if you didn't i have great news for you which is that http 3 became an rfc at long last so the and of course another document which is q-pac related of course is uh yes this is obviously a big milestone they lagged considerably behind the other quick documents for reasons that are not really worth going into but we no longer have those on our plate and they are rfc's where they can live in perpetuity so and then other updates the quick grease bit which is a very simple uh a relatively simple document but important one is currently in auth 48 and version negotiation and v2 which are sort of proceeding together as as was agreed are currently under ad evaluation so other things we had issued an adoption call and it completed for the quick careful resume uh this got some feedback that in about its potential suitability for other potential working groups specifically the discussions around uh congestion control's life at the itf so the chairs are going to take this under consideration uh qlog is not presenting today um and this is not due to a lack of progress but they are so and expect a new draft for uh before 115 and updates to be on the list"
  },
  {
    "startTime": "00:08:01",
    "text": "we also like to call out that l4s which you may have heard about at this itf work was done during the hackathon including interop that with quick stacks which is very exciting um because quick is you know participating and working with l4s um with the l4s work and i think that is it lucas do you have anything else that you would like to add to our boring chair updates uh nothing major just that you know matt and i were both really excited to be there in person this time around um to actually meet each other for the first time in however many long and meet all the participants as well um unfortunately we can't be there for various reasons uh so thank you very much for your patience and support to people who have been helping behind the scenes to make sure this session can run smoothly and i agree with that of course and now we will both stop talking and we will allow muria to take over as the first presenter and my meat echo is totally not allowing me to do anything so that's interesting okay i got i got it to work try it again ah no there we go okay hello everybody my name is mia coolerin i'm one of the editors of the um multi-pass extension for quick but probably the"
  },
  {
    "startTime": "00:10:00",
    "text": "least important editor but i'm the only one here unfortunately um yeah so we published a new version and this was actually quite a big update based on the discussion we had last time the biggest change is that we merged the pr which which has the what we call unified proposal we could discuss this last time and i have a slide later on this and there were some other a few changes that i want to update you on one is that also an issue we discussed last time that we now decided to have some mechanism to also signal that there might be or that you want to use one of the pass only for standby motors and we have a new frame for that there's um some clarifications about um pass closure or like denying a path which is also actually kind of new protocol work so i will go to that and then more guidance about x scheduling and delay calculations but that's also kind of still work in progress because that's some of the hard parts yeah we didn't solve or we didn't close all the open issues we still have three of the open issues that we discussed last time time and so these are uh need a little bit more work especially the first one um it's on my plate to look at what we actually decided uh or like why we didn't allow servers um to migrate and if that's still important for multiple so i will do some research and come back to that just didn't do it now and the other ones are also looking for a more input about like the actual need for these things and so for all of these three open issues here that we discussed last time already we have to figure out if this is something we want to do at all if this is something we want to do in the space draft or if this is an extension in the future and that's ongoing that discussion we have also a few new issues but i have to say these are a little bit more on the editorial side i think so like not fully but as i just said there is more work needed about explaining how to calculate act delay correctly and"
  },
  {
    "startTime": "00:12:01",
    "text": "maybe some of this is kind of um normative but um it's not clearly so that's ongoing that's the two new issues listed here yes so especially you know what's i wanted to explain the problem um quickly here but not sure if we will solve it completely or um finalize the discussion here so um the problem we have is if you and there's still a part about using single packet number space so if you use single packet number space then things like calculating your round trip time and delay correctly what you need for congestion control is actually not that easy and this is just giving an example here so you assume you have two passes and pass one is quicker or faster has less delay than past two and you decide somehow on your packet scheduling you send packet one to ten on pass one 11 to 20 on past two and then 21 to 30 on past one again um so what can happen is that you receive all the packets on pass one because that's a faster pass but then because depending on like how you you decide about your egg strategy you actually trigger an egg when you arrive when packet 20 arrives so that's the packet that triggers the egg but of course that egg will will acknowledge all of the packets so 1 to 30. and when you receive that egg actually you don't know um that packet 20 triggered the egg so you might not know how to calculate the round trip time or the delay of each of these passes correctly because packet 20 was sent on path two but like uh 2030 was sent on pass one but the delay you basically calculate based on the egg is the delay of past two so that's like usually what's happening is like you get the delay if you if you just do like whatever you do with what would do today with eye calculation you get the delay of the of the longest path and not the delay of the shorter pass so you have to be smarter about this you have to either you know add some more information to"
  },
  {
    "startTime": "00:14:00",
    "text": "the to the egg about saying like what which packet does this delay information belong to or you have to split up your ex in a smarter way or you have to schedule a packet in a smarter way or whatever and that's a discussion we're still having and my phone goes to sleep and now i cannot no no i can control it again okay um yeah so that was just to explain the problem and i think we should have the discussion actually on on github because it's not like it's not like we need to change something but i think we have to be clear about what the best way to do and then a quick um review about what we've merged already based on this question from last time so this is in the new um past status frame the past status frame has like this past status information which is the important part and what we specify is really you have two two choices here standby or available and what it does it just really gives a recommendation for the other site what to send on the pass it doesn't tell anything about what you're planning to do on your site right but you give some hints to the other side that it's preferred from your point of view to use one or the other path or to not use it and that's it and effectively it's also just a recommendation of course if there's a situation where you can use one of them you have to make a local decision okay i go on so the other thing we merged which is um a normative thing here is that we um other than for rfc 9000 where basically if you have a migration event and you don't want to have the new pass you just don't react we actually give an option here to um actively deny a new path by using the pass abandoned frame on the existing path um so this is just because we have to pass abandon frame we can make use of that and that can just like avoid some delay so it's like a little change but it can happen some situations to optimize"
  },
  {
    "startTime": "00:16:02",
    "text": "i go on we can come back to this okay and so this was the biggest merge we have in this this site is mostly recap slightly um modified recap but mostly recap from last time so um the question the big question we discussed last time is like using single not multiple packet number spaces and like um there are pros and cons on both sides right so the one of the biggest difference was like single packet number space does support zero length connection id and multi packet number space does not and that's the most of the discussion we had last time from a code complexity point of view there are kind of pros and cons on both sides like on the first look like single packet number space seems to be much less code and much easier for that point because you don't have to introduce a new packet number space but then if you look at this like delay calculation issues how to get condensed control right making sure you schedule the packet in a smart way that your excise doesn't grow and whatever it gets very complicated and it gets like um it's not like uh uh there is a normative way to handle it is like depends on the implication how to do this right so it's easy to get it wrong effectively so i think there was like a lot of support from from a complexity point of view to actually go to multiple packet number spaces because that makes the spec just more clear there is still a discussion about hardware offload because there's also um implications here so with multiple packet number spaces you actually have to change um the the crypto part how you decrypt the packet number because you have to consider the congestion the connection id but on the other hand if you have modern packet number spaces it's actually um easier for your hardware offload because then you know exactly you don't have to decrypt like uh you don't have to guess the packet number because you can much better figure out where you are and you don't have a lot of out of order packets effectively in this kind of thing so there are also pros and cons here so um so this is very much nearly the same"
  },
  {
    "startTime": "00:18:00",
    "text": "slide as i presented last time and we just merged this in now which means basically packet multiple packet number spaces is now mandatory so if you um if you negotiate multiples the multi-pass extension you have to support multiple player number spaces but it's optional to use zero length id connection id if you want to use zero length connection id you also have to implement all this other stuff and then it gets complicated and we put a lot of guidance and and discussion in the document about what how to do there and how to get your delay calculation your easy hand easy and handling your congestion control right but it's kind of optional if you don't support a [Music] single packet number space um and the other end requires you to use zero connection id basically you can only use one path even if you have negotiated multiples but like there's no obligation to implement it at least it took me that entire in sweat google it took me that entire time to get into the queue and get the pages load um can you go back to the last slide of it just curious question about hardware offload um is there available hardware offload that is uh for quick today because i've looked into this and i haven't actually found it and you might be right that it makes hardware upload more complex but it really depends on what the hardware offload api is and so like i'm not personally familiar with them i don't know maybe maybe this is much more widely available than i think and or maybe you're anticipating future hardware offload that will exist so um okay so yeah we should talk more in um that was my issue a while ago and it is speculative based on um conversations that that i've had with hardware vendors in the past um the issue 25 is kind of but no there's certainly no hardware offload that certainly i was aware of when i filed this issue it was just concerned that by messing with the crypto algorithm we're going to have this other mode for multipath which might not be a very viable thing"
  },
  {
    "startTime": "00:20:00",
    "text": "so the we so there is no hardware offload which is i think already deployed but we did some research and we had some work about trying like how to implement hardware offload and we only went for the multiple packet numbers based solution because you don't have a lot of packet reordering whatever which makes your offload engine actually easier it's different than what's today in rfc 9000 but it would make it easier if we have multiple packet number spaces and given it's not deployed yet that might actually be um a reason to go there as well yeah i mean i i like i don't think this is a showstopper at this point but it's something you can consider and like if we do everything where like you need special hardware to do multi-path that's probably bad for multitask no you don't need special hardware to do multipass it's just like you have to optimize you have to have um or you can do less with rfc 9000 and you need more if you want to support multiple packet number spaces but it's the same logic it's just like an extended logic right it's not like you have two different paths to go for okay thanks okay so yeah we matched it so i just wanted to point out chair hat off on the cardboard awful load thing um no uh yeah again there's no hardware offload the hardware offload that we have been working on um the vendors involved uh would probably not like multibath anyway just they just they typically are not happy about the hardware doing anything related to quick that's complicated um they don't like packet numbers either so i would i would say that it probably is true that the multiple pack and workspace is a little bit easier to support well but um it's also going to vary depending on the vendor you talk to like what their opinions are on what is hard and for their hardware because what what's interesting about this is"
  },
  {
    "startTime": "00:22:00",
    "text": "that different hardware uh can do different things easier than others when it comes to crypto offloads and uh quick in particular so it's it's probably a concern but i'm not sure that it's something that necessarily has to be designed into the protocols or taken as a major concern yeah so i mean um there is this issue in github and it's still open and needs more discussion but i don't think the decision is clear in which direction we should decide here christian yes regarding this offer it's really the encryption issue when we cannot have multiple numbers without injecting more than the packet sequence number in the encryption so that means that whatever offloads that has to be aware of not just the connection id in the incoming packet but the sequence number of that connection id so it's that's conceptually a really simple fix in the api but it has to be there this should not be i mean if people can actually do hardware offload and extract the sequence number and things like that the additional step to support multiple number space is not that high it's just that there are twos i mean you have to extract 42 bits of the sequence number of the connection id and you have to use 96 bit sequence number which is composed of this id and the 32 and the 64 bit of the packet so it does change api it doesn't change the complexity very much because you need the cid anyhow otherwise you're not able to find the context of the connection and if you can find the contents of the connection you can't find the keys"
  },
  {
    "startTime": "00:24:00",
    "text": "so i mean the complexity should not be used for down there jana hi um so i want to emphasize a couple of points that just both matt and christine said but uh one of the things when thinking about hardware offloaders people typically use the pcb model to think about it and which is why sequence number seems like an attractive thing to consider handing off to the offload engine it's not necessarily true and in fact true in the quick case for a number of reasons one of them being that we don't necessarily expect sequence numbers to be sequential we want to have gaps we want to be able to do various things with sequence numbers that's in fact part of the protocol i would say philosophy and even the design that packet our packet numbers are not basically stream numbers uh stream id and stream sequence number sure that's fine but not at the packet level we don't necessarily need it to be sequential we don't need it to be uh in any particular order the sender ought to be able to send it in whatever order it cares and uh the receiver is able to handle it um that's important and so i would say that uh going forward again to christian's point as well that something explicit needs to be handed down is very likely to need to be handed down to the offload engine from the quick sender and it is not something that you can simply offload as a task to the offroad engine so i don't think that sequence numbers should uh play a role in how we decide um hardware operation multiple or single packet number spaces so the problem is you want to decrypt the packet number and um and how you decrypt the packet number"
  },
  {
    "startTime": "00:26:00",
    "text": "depends on the packet number space right so that's the part you want to do an offloading uh martin duke google uh so uh at the risk of going down the hardware offload rabbit hole um uh like the encryption and decryption problems are quite different the encryption problem yes absolutely you just passed the packet number with the packet and i think it works pretty well on the decryption path the the hardware needs to keep some state obviously because it has to you know predict the the expanded packet number uh you know it has to expand it from the the truncated packet number so that's like a harder problem um and it becomes even harder with multi-path but uh right if you build the ap apis right it doesn't matter and just the question if vendors will find it worthwhile to implement the apis and i don't have an answer to that okay i go to the next slide which is my last slide i think um so we merged in this proposal again saying uh um multiple packet number space is mandatory single packet number space is optional to implement basically and there's some more editorial work here because like the way it's it's structured in the document doesn't make that fully cleared and but it's really just editorial work um that we will do um we have some remaining issues but as i said again for all of these issues we really need to decide if they should go should be addressed in this document or in an extension what's the part of the the base document that we want to um discuss um so it's like i don't think it's like hard to find a solution to this problem but it's just a decision of what we want to do here or later um and then the most important part is that we really want more implementation experience because you know if we all end up only implementing multiple packet number spaces maybe we don't need single map number packet space or if we actually think that zero length condition"
  },
  {
    "startTime": "00:28:01",
    "text": "connection id is not that important we don't need single number packet space um or maybe there's additional considerations for hardware offloading that actually changes our mind so i think these are the open questions and what we need is implementation experience uh eric kinnear apple just a really quick question about the path abandoned stuff is the expectation that the old path receive both a path challenge and a path abandon at the same time but we still want those to be separate frames so you try to open a second pass and start the past challenge process and if you don't want to accept that you don't have to reply to the past challenge and then there's no path right so that's what we already have today and just like to to avoid this this time where you like time out until you get like a challenge response back or not you can on the other path that's that's exist if you have multiple packet number spaces you can send a path abandon and just give like a explicit signal that you will not reply on the other path that's it but if it when you first initiate that that triggers a challenge to go down both paths right because it needs to be validated in both directions so the new path is going to be getting a path challenge saying hey is this actually working in this direction but the old path is also going to do so when it sees someone coming in from a new place you're saying the old past is also sending a past challenge yes that's my understanding uh that's sitting in the middle of a different section of our c9000 essentially it's for when the off path attacker forwards some packets you confirm that on the existing path okay so the reason i bring that up is a it's interesting to get both a challenge and an abandon at the same time the abandon refers to the new path so that seems okay it might also be possible to just like put in the challenge like hey i don't want this other one yeah so the abandon only works if you have um pass"
  },
  {
    "startTime": "00:30:01",
    "text": "identifiers yes and because you don't send it on the same path and like we should look up this part you're just citing because maybe that doesn't make well it may also have been overridden by some of the new texts so i should go double check okay thank you alessandra gadini cloudflare um we're starting work on multi-path right now and um my current impression is um on the single path versus multiple packet number spaces um question is that the single pocket number space doesn't really give us much benefit and um implementing both is kind of annoying so um we might end up just implementing the multiple packet number spaces um we might reconsider later on after we actually deploy the whole thing but um that's my current impression at least it's not super clear what the benefits of implementing both are and a quick implementation kind of needs to support the non-zero connection id um case anyway so you have to implement multiple packet number spaces and the single packet number space is just i don't know more complexity for not much benefit so this is exactly the point so like if you if you don't need zero length connection id that's probably additional overhead you wouldn't want but like we're looking for people who actually have a use case for zero length connection id and i mean one of the benefits is like you save a few bytes right so that for some use cases that might be really important um and like there are two options here i mean the one option is also we could try to treat the multiple packet number space solution to support at least zero connection id in one direction there has been proposals but it's also like some additional complexity it's not here if it's needed the other option is also like put the zero length connection id support in an extension or whatever but yeah that's the feedback we're"
  },
  {
    "startTime": "00:32:00",
    "text": "looking at so i think um the the the zero connection id case is mostly a client use case maybe i think some browsers use zero connection ids um so supporting on only one side might be okay yeah um but then i guess someone who is actually using that use case might comment more that's what we're looking for i think lucas is next hey yeah i i don't need to queue on the chair i could have just interjected but i thought it'd be polite um we've had a few comments in the chat about my squaring uh for anyone that wants to be not in the room during the initial um uh chess fights this is a requirement of your attendance to to wear a mask at all times during these meeting sessions so please uh take this seriously um this is important for our health we all want to be able to go home at the end of the week so thank you very much thanks lucas thanks uh ian's right google um i i was more of a single packet number space fan as an individual um but i have to say that uh requiring both seems worse than just requiring multiple packet number spaces in terms of zero length connection ids the use cases i think probably could be preserved so like for example there are cases when chrome will open two different ephemeral ports at the same time and so chrome would have like two different sockets and i would think it would be perfectly easy to distinguish which path uh a pack arrived on based on the socket arrived on things like that so uh that's my general comment but i need to go back through and kind of look at all the discussion about like how you ended up where you didn't make sure that i kind of understand everything that that happened so so i don't usually stand up just to say plus one but there weren't that many people who were super into just a single"
  },
  {
    "startTime": "00:34:01",
    "text": "packet number space and i was originally one of them and i would strongly agree with ian that having both seems strictly worse than having one or the other and having multiple packet number spaces based on the things that we've been discovering as we go to implement is seeming more and more attractive me again um so to be clear i'm not implementing this but to me the attraction of the single packet number space is to like not write a lot of code and get like cheap multi-path uh cheap in terms of coding effort but if we're gonna require multiple packet if we're going to acquire a multiple packet number of spaces then like that's that's that's are you you have to pay the entry cost like the point of single it seems much weaker to me yeah and i think this is actually kind of a wrong assumption because it looks much easier from the first place but then actually getting all the packet scheduling delay calculation right and so on it might be actually more code at the end and it's it's code that is kind of not well specified in the end so you end up with like having different implementation doing different things and then because the other end is doing something weird your performance goes down and you don't have any control yeah certainly optimizing it would be a lot more code but i'm thinking just like if people wanted really minimal support that would be attractive but so it's i mean it's not only optimizing right it's really just like the the delay calculation example showed this this just doesn't work correctly it's like giving you some crap and like yeah if you just want to make sure you can send one packet from here to there it's fine but if you actually want to do the right thing it's you cannot ignore it okay well if if like if the if the coding simplicity is like illusory then like to me that that destroys like the the case for a single single packet number space entirely yeah thanks so i mean the point is um multiple packet number spaces don't support zero connection id in both directions there is a way there are proposals to so if we need this use case then we need something right"
  },
  {
    "startTime": "00:36:01",
    "text": "um if there is a way to maybe support your connection id in one direction but it's also giving you more complexity and more ambiguity so like it's also additional code so it's really the question what do we do with your connection id is it a use case we want to support or not ownership your apple like many other early informators we are leaning towards multiple packet spaces and one if we are having trouble finding uh current evidence that the zero lens connection that single spaces are needed today uh we may consider uh emulating uh zero lens cds by injecting a known constant and essentially making it a uh nonzero lens and hence injecting that problem space into the other hi luke from twitch i'm noticing in a few working groups that there's this tendency to put a context or like a packet space id in um to distinguish between sessions um mask has it web transport we're talking about a session id and here we're part of the path in there i'm wondering if overloading the connection id is the right approach um is this something we want to do for like making load balancing more difficult or do we want to make this part of the quick api like just literally put packet space in the header or something this is more of a question to the room but i feel like there's an api here that a lot of different working groups might use i'm open to have this discussion but like one of the decisions we made earlier on is that we want to keep the changes to the um base pack minimal right and that would be a big change i just think when we start"
  },
  {
    "startTime": "00:38:01",
    "text": "making copies of every frame like here's an act frame but we put the packet space in it here's a flow control frame but we put a packet space in it not saying the spec does that um it it just screams out a little bit to me but all right off my softbox point taking question to on this discussion i mean i started experimenting with indonesia very early i mean it was my original design a couple of years ago and i did implementation of single number space and multiple space to measure the complexity the complexity of multiple number space is that you have to add a bunch of statements in your code to check whether this is the multi-pass or the single pass case and there are more checks like that for multiple number space than for single number space but on the other hand the complexity of single number of space is that you have to add a lot of structure in your code to handle that and for example you have in my original implementation the single space solution was five percent less efficient about five percent as efficient than the multiple space solution and i took that as a challenge like can i make them to be equally efficient and the answer is yes i could and i did that in in my prototypes but to do that i had to add a lot of code in the act scheduling pass i had to add a lot of structure in the store of packets to make sure that packets carried not just the packet"
  },
  {
    "startTime": "00:40:01",
    "text": "number but were also linked to the path in which they were sent and that we can retrieve how many packets were sent before them after them etc etc overall that complexity as miria said is much more than the single number of space so that's the reason why i did support and in fact that proposed this unified proposal if i mean the reason for the additional complexity is the need to support the zero length cid case and we need feedback from implementers there whether that zero length cid case is actually needed with multipass or not that's what we need yes i think you know that's the main question and uh if people are having a use case for zero links id and multipass um it would be good to learn about that maybe put it that way you answered google i was just gonna add that i mean a byte is fine i mean at least for chrome and other browsers and such but um also i'm still i think i'm not fully understanding the problem it seems like if i can distinguish at the receiver which path i'm receiving something on then i can decide to use a zero link uh connection id and if i can't then i need to use a non-zero length connection id and this seems true in the existing server deployments we have today right i don't i guess i'm not understanding like why multipath makes this actually different like like if i can tell from the affirmative report like this is pathe or about b then like i don't need a connection id but if i'm a server i'm receiving on 443 and i have 10 000 connections then like clearly i need a connection id to like multiplex them so it does work but um it it just"
  },
  {
    "startTime": "00:42:00",
    "text": "makes your scheduling and your acknowledgement code more complex to get it right right you have to decide which packet numbers to use on with which part of the packet number space to use on which path and that impacts a lot like how your acting will look like it impacts your ex size and you also don't really know what the other end is doing like how often do they send x or not and when you get the packet back there's like ambiguity about ecn markings usually you don't know where the ecn magnets belong to which path so you have to if you get an ecn marking you have to reduce your congestion window on both paths because that's the most conservative thing to do or you have to be somehow smart about acting but that's the other end that you don't control and the same is true for um egg delay calculation usually you will only be able to calculate the delay on the longest path so that's what you have to cope with so you cannot fill the shortest path delay in whatever and you have to have logic for handling all of that to make it work nicely and i think correctly i mean like as i said if you if you need like a few packets it doesn't probably matter but if you actually want to have a performant connection it it's not that easy anymore yeah i i would completely agree i was i was merely asking i think i didn't fully understand why we can't have zero length packet uh zero length connection ids with multiple packet number spaces um i think that's the question i still don't fully understand but i can take it off because like that because you get the same packet number on both paths and you don't know how to handle it like you have to distinguish like what the fact where the packet number belongs but if i have two different sockets then like i could just use the socket and be like suckaday socket i don't know if i can i think the issue is not rebinding um if you're not reminding on the path there's a lot of ambiguity no i mean it's also like this doesn't it the crypto to encrypt your packet number right doesn't allow you to have the same packet number twice or like these kind of things"
  },
  {
    "startTime": "00:44:01",
    "text": "it just doesn't work christian yes it's exactly what you just said i mean is that if you have the same packets i mean if you have packets arriving with the same number from two pass then you have to inject a pass id in the crypto to understand which uh key to use i mean and and if you don't do that then basically your crypto is broken so i can maybe you should take it offline and if you are interested i could have a discussion with them on exactly the complexity that we encounter there but i mean or you might trust me but i know you don't donna thank you um so i i it sounds to me like there's a fair amount of agreement on doing multiple packet number spaces it also sounds to me like there's a possibility of considering what else we might be able to now"
  },
  {
    "startTime": "00:46:01",
    "text": "this is an issue that's been like you know doing the rounds for quite a while uh and we've got some experience here i'm happy i feel comfortable and confident in saying that we can move forward but that's my opinion i yeah so the audio wasn't super great um but i think you said we should decide now if you want to support zero links id yeah um i don't know can you hear me now is it better a little bit yes i don't like my video i it's also in the audio from here where stand is not great for me so like yeah i mean um as i said is probably janna also said i believe and the question is really not about like doing one or the other doing single or doing multiple pack number spaces i think might what we decided already is that multiple factor number spaces is the way to go and single might be optional or not so the question really is do we want support for zero lengths connection id that's the question to ask i'm happy to make a decision now i'm also happy to like wait for a little bit more implementation experience and because we have like a few not super hard open issues but we need the discussion so it's like we can decide now we can decide next time or sometime in between but yeah at some point we have to make a decision cheers thanks miriam um just just from me with my hat on but my opinion um this has been some good chat and some good engagement thank you folks um we've"
  },
  {
    "startTime": "00:48:01",
    "text": "taken the comments down about the potential consensus call look out for that um we'll discuss the follow-up on the list if there's anything to be done um one question i've got you talked a couple of times mary about implementation experience and you know alexandra mentioned that you know the the quiche project is looking to do some work i wonder if if maybe at the next hackathon in in november we should try and form another quick table and actually try and bring some folks to do some implementation and interrupt and create a target and maybe come up with some test cases we don't need that answer right now but if people are interested in that let's try and take that discussion offline losses in the queue yeah lars eckhart so i'm not super closely following this discussion in in quick um but back when we did multiple tcp the research not the standard we actually tried to do the single space um which we quickly found out you can't for tcp right because if middle boxes along a stream c gaps in a sequence number space they just break in all kinds of different ways but that was the original intent because we thought that sort of felt natural and then we had to do the two space the the one space per path and we sort of made that work with quick right we actually have the option of uh doing the single space um but maybe we have now enough experience with multi-graph tcp in the multiple spaces to see benefits that we didn't see way back when but um i kind of sort of without as i said not having followed this in detail i kind of sort of wondered that the one space back then that we thought would be the way to go uh wouldn't be the way to go here but since i'm not implementing it i'm not going to deploy it uh take that as you will thank you so i think you're talking about different economic spaces here than where am i um yeah because in tcp you have the the on path sequence number"
  },
  {
    "startTime": "00:50:00",
    "text": "and then you have a separate thing right yeah and this is not the kind of packet number space we're talking about here it's really like the onpass packet number space and it's really like do you use the same numbers on both passes or do you use different numbers the different set of numbers on on passes but it's the same element right it's not like in tcp where you actually have this additional number the reason we have the addition number is because we otherwise couldn't make it yes so but what you wanted originally is to take the sequence number the sequence number field and put multiple packet number spaces in that field and that didn't work and so you had to have a new field anyway we were going to use a single sequence number space across all paths with a single field originally i think we're digressing and that didn't work because of all the packets are out of order in these kind of things it mean because middle boxes see gaps and they don't like that yeah yeah so we don't have that problem but having those gaps is also not optimal for the interning processing so it's possible but not optimal jana again hi hopefully you can hear me uh reasonably i'll shut up if you can't hear me yeah it's a little bit better for me now okay um uh largest point class yes exactly i think that uh the difference here primarily and so i i'll also speak about not just mpdcp but uh even almost 20 years ago i did this for scdp i've used sctp multipath implementation and uh the reasons that we did that and that those are exactly the reasons that don't apply to quick specifically both sctp and pcp had assumptions about linear sequencing both the protocols the senders the receivers and metal boxes in the tcp case all have assumptions about sequencing of packet numbers we don't have that in"
  },
  {
    "startTime": "00:52:01",
    "text": "quick and so that's actually a freeing thing in implementing this with single packet number space in sctp one of the things i quickly found out was that eventually near the end uh uh near the end meaning after after a while of working and implementing this my uh um conclusion was that separate sequence number spaces would actually have made things far simpler because a lot of the things that a transport does its recovery condition all things that are happening on a path and there is a general assumption about sequencing on the path and a lot of these mechanisms that we we build and we deploy have those assumptions so if you move to single single but if you move to multiple packet number spaces those mechanisms just work with a single packet number space you have to go revisit every one of those implementation bits and you find it we typically tend to build a certain amount of reordering tolerance within a path but sort of really blows us out of the water and you don't want to use the same ideas of reordering tolerance to multipath because in multiple we have a lot more information we actually know that there are two or multiple different paths within which we can expect a certain amount of sequencing so all of that to say that multiple sequence numbers actually make except what trying to that was max in the sctp case it was backward compatibility it was the fact that ag compression wasn't very good when you split the sequence number space and so on in quickly explicitly and deliberately allowed for those things to exist and by design and by philosophy we've"
  },
  {
    "startTime": "00:54:01",
    "text": "allowed for multiple sequence number spaces to exist and so i strongly urge us to go towards that and if only things are falling apart badly should we consider single sequence number space yeah thanks that's i'm used to input and actually last um we already have two secret number spaces in quick one is the packet number and one is a frame id right and and that makes it nice to actually have this the split and using not to pick a number space on the packet number uh jonathan just disappeared yeah we're kind of at time i locked the queue just just so we can make progress and not spend too many times on this question um if if anyone really wants to say something do so now um ian um no not ian i think ian yeah um i think this has been very helpful and thank you all for um your inputs yep definitely thank you that's it all right do you want to drive the slides or can i drive it from my laptop yep what do i have to do just keep hitting the button maybe maybe oh uh the quick time is stopped right yep no no that's right"
  },
  {
    "startTime": "00:56:05",
    "text": "thank you uh i'm ian sweat i'll talk about act frequency and give an update um great so uh there were some updates recently mostly uh editorial as well as uh some additional normative text in areas where there really was intended to be normative text but we never actually bothered to write anything down such as with ignore c um so this is an overview of the two frames that we have the act frequency frame it allows you to tell the peer how often uh they should send acknowledgements to you how long to delay them so on and so forth and then an immediate frame that allows you to elicit an immediate acknowledgement um so i'm going to try to go through these in order of what i think will be easier to resolve issues first and hope of getting some resolutions um and leave the thorniest one to the last and if we don't get to it then i'm happy to take it up on the list so the first one's an easy question should this be a one byte frame type um we probably want to stick it with uh pto packets in a lot of cases um so i guess it's slightly more likely to fit if it's one byte we might want to send it reasonably often this is a issue filed by martin seaman um what do people think i see a thumbs up and a thumbs down would people like to to give their thoughts or i'm honestly not sure how to resolve this uh you know um but i just left the meeting so i can't press a button um i think it depends on how often you expect to send this right and"
  },
  {
    "startTime": "00:58:00",
    "text": "that's not super clear to me but like if you send it off and you'll be on mine who clicked the button faster come on martin you got this um like we only have like 60 of the short bite packets and extensions on average might need one maybe sometimes two we're going at a rate of one maybe two extensions per year so we're not gonna run out anytime soon i would say for working group extensions where it looks like the majority of folks are going to implement them yeah get a one by code point done thanks yeah so martin thompson i i've implemented this and the number of times i send one of these things is vanishingly small and i have stats to show that the number of times we send ptos in practice is also vanishingly small so um i don't think an extra bite is a problem here and i'd rather see it left as it is i i realized david's argument is a is a fine one um and we do have space but i mean we also don't have to use it so um leaving it as it is is fine with me by the way the stats are like we don't get ptos on like 80 plus percent of our connections it's it's crazy yeah when the internet's working well you should not be getting very many of them and and it turns out it works well most of the time"
  },
  {
    "startTime": "01:00:00",
    "text": "okay um did the chairs have any suggestions on how to oh john martin oh yeah [Music] i'm trying i'm trying trust me um so we can hear you this i i don't i wonder if this is again one of those uh trying to optimize it well before we really have a problem here situation i i think that we should i mean we can we can go with one bite we can go with two bytes it doesn't matter much i would suggest that we leave it as it is until we have more experience uh from from various folks and we can always come back revise the draft come up with a new version and stuff of that sort at this point getting this out and getting this into people's hands is more uh useful in my opinion so the uh uh leaving it at leaving it as it is is what i would suggest i agree with uh um martin's uh uh comment that this is not something we expect to send very frequently immediately if you're if you're immediate if you're asking immediate acting to happen all the time it seems that it seems like there might be a problem somewhere else but um yeah uh martin duke google i would suggest that we are approaching bike shed territory on this and um probably nobody has is gonna lie down on the road one way or the other so having gotten pros and cons i suggest the editors just like make a decision um and i'm sure everyone can live with it and if that's not true then someone will correct me right um it's been pointed out that i didn't properly state my affiliation last time so sorry davidskenazi quick enthusiast"
  },
  {
    "startTime": "01:02:02",
    "text": "um i stood up to say something similar to what martin said what a beautiful bike shed you have i really wonder what color it should be um so yeah i'm not going to lay on the tracks here i don't care that much have the editor's pick move on that sounds great all right how about uh we have martin talk and then i'll um i might actually just shelve this for a few months and just get some more utilization and different use cases of immediate active as well but yeah so sorry for opening up this bike shed so the reason i opened this issue is because the draft says that there are some some cases where you want to send this frame at least once per rtt um i'm not fully sure if i understand why you would want to do this probably we can add some more text about that if if you actually only only send this on the pto then i don't care but if you send it once per rtt this might be worth doing yeah that could probably use more decks i think the idea was if you wanted more fine-grained control over when you received acknowledgments um it's particularly if you were using particularly long active layers in terms of packets in time but um yeah yep thanks let's move on um so there's a lot of discussion uh in various pr's about whether ignore ce is useful um or one could say harmful but maria was very kind in her issue title um it's also been restricted to pheromone um so it's it now says it should not be set if the sender sets ect1 in its outgoing packets such as for l4s i think at this point i don't think anyone is using this um and so i can see an argument that given it's not seeing active use we should just jettison it because we don't need it right now and"
  },
  {
    "startTime": "01:04:02",
    "text": "um and i'm not sure i as an editor want to spend a bunch of time like arguing for ecn features that i'm not going to immediately use but um before we remove it or something you know we didn't have a consensus call to add it so i want to like bring it here and just ask like what do we think so martin thompson i'm not going to be using this in the foreseeable future but i can understand why other people might say that they they would like to to do something like this the question not that i have for those people is if you're doing something like l4s or planning to do that would a transport parameter suffice in this case hmm i might have a similar point so like i think actually i personally think the answer is no but that's not my main concern my main concern is that i think actually this shouldn't be part of this specification because it really depends on on the congestion crawl and the and the ecm mechanism to specify what's the best thing to do and and so like you shouldn't have you don't shouldn't have you shouldn't have a need to single that because it depends on like the algorithm you're implementing to to do the right thing and not the other end telling you to do it all right um did does anyone have any other comments on this glory we can't hear you agree okay can you hear me now good yes so we don't currently have an ecn method"
  },
  {
    "startTime": "01:06:00",
    "text": "in the transport area which allows you to ignore ce except by just dropping in the packets so this seems highly tied to the ce method you use so i agree with mira the answer is we should not keep it sir you said we should not need it is that what you said no we should not keep this function okay thank you sorry i'm getting echo from i don't know maybe it's just the speaker's part um yeah it's also like if you want to do it right then it would be more complicated than that so because you should kind of never ignore the first ce but you might not react to like follow-up ces immediately or whatever so it's like this is also too simplistic to make it useful um it sounds like i'm inclined to remove it given where we're at we can always add it back later if if people decide that there's a compelling use case and actually want to implement it but i do not want to ship a draft where we have a feature with no one who's implemented it oh janna i agree i was coming up to say that we can use the transport parameter idea for martin if we really want to do something here later but uh i agree when we put this in just to be clear we weren't fully certain how much how much traction this would get but we can offer it in expected thinking that there was there was an ask for it sure we have ignore uh uh reordering so we wonder if we should have this as well and so yeah if there's no strong push for it we shouldn't add features that don't have people asking for it cool thanks okay uh the next one is kind of a can of"
  },
  {
    "startTime": "01:08:02",
    "text": "design perspective but just uh it's it's a challenging problem which is exactly how much text do we want to put in there about examples or suggestions of how to use this extension um there's one open issue but there have also been other side comments uh particularly in the context of like reno or cubic about what is the right number like is it 8x per rtt or like what what magic number do i need to stick in there to get like decent performance out of like reno and cubic and i honestly don't have a really good answer off the top of my head um but i guess the real question is like what should we be attempting to do here like how much more text or less text do we want um about providing guidelines on how this actually should be used potentially with congestion controllers that are widely deployed so martin zeman given that rfc 9002 specifies a congestion control algorithm that should be safe and performant for for the use on the public internet would be really nice if this extension also suggested something that will be safe and performant on the on the internet not saying that you can't do something better but it would be really helpful to have something in that text for people who don't have a internet wide deployment to run their own measurements as a follow-up question um because i don't have this off the top of my head does anyone have um a set of constants that they've used with regional or cubic with uh that frequency frame that they find work well um or is this something where i need to go run some experiments and try to like get some data uh because we currently run bbr by default it at google and currently don't have any cubic or reno"
  },
  {
    "startTime": "01:10:01",
    "text": "experiments but we still have the code so we could go back and run those if if that was kind of the way forward on this so i was going to jump in here airhead off has someone that has been thinking about experimenting with this with an internet-wide deployment the i don't think that we should put anything or rather have the onus that this document prescribes something because i think if we have that requirement this is going to potentially be blocked on publication for essentially an indefinite period of time because the answer is we really don't know what to do here i don't think anyone has experience using this successfully yet with a variety of congestion controllers uh certainly not the one that we specify in the base draft which uh you know is not really being widely used um and so my thought my personal opinion is that uh this document can serve as a mechanism document and if people have strategies that they want to use uh that they they find a success with those can be follow-up documents that are specified hey this is how you can use act frequency for this this is how you can use act frequency to achieve this because even with a single congestion controller you're not necessarily going to be have the same strategy so for example if you want to use cubic with that frequency in a data center versus cubic with hack frequency on like the internet with a very low latency uh network on the internet like or high latency network on the internet i think the strategies are going to vary i mean we don't know at this point what those strategies are going to be at least i am not aware of any thanks matt uh great all right i guess you knew i was going to say that"
  },
  {
    "startTime": "01:12:01",
    "text": "10 is not a bad number to kick out on for a starting point it's probably a bad number in the end it's probably a bad number at a data center it's probably a reasonable number for an internet path um can we just see what we can come up with here and see we can make a recommendation before we publish of a starting value from which we can work for that we consider safe because surely two is very conservative and surely an rtt's worth or two rtt smith is highly um controversial so we must be able to say something well i suggest we pump we we work on it thanks cory yeah i mean there there are some boundaries i think about like trying to get an hack every rgt typically and such just for guardrails but that's about all all that there is right now okay why don't i just keep this open and we can we can work on it over time it doesn't seem like something we need to fix today thanks [Music] okay um so this is one that we talked about uh quite a while ago um a number of other people have looked at the pr again um how much time do i have left by the way sorry how much time do i have left you have about five minutes in about five minutes okay thank you all right so we can at least go over this again and uh swap it back in and if we don't get to the end that's that's completely fine um so this is the idea of uh okay so one act is sent immediately upon a missing packet um however the next ack say you have one missing packet and then 10 packets in a row arrive and you set 10 for your uh act eliciting threshold then you'll get one immediate ack and then you know you'll have to wait for the other 10 packets to arrive and then you'll send another immediate act and"
  },
  {
    "startTime": "01:14:00",
    "text": "then you'll finally detect that that like missing packet was lost uh because if it's threshold based and it's a threshold based loss so you start out with the last threshold of three the first act wasn't enough to immediately declare the packet is lost and so you have to wait for a timer and so timers can be relatively slow especially in like low latency like data center sort of applications so this can actually substantially delay loss detection in certain situations versus the base draft once you start bumping up the active eliciting threshold so anyway the potential solution to this is to communicate the reordering threshold to the receiver instead of just saying ignore order uh and then the receiver sends an immediate ack whenever there's a missing packet in this range that basically would allow the sender to immediately declare loss so it's basically an optimization as well as kind of a safety mechanism to say i'm going to declare loss at this many packets and so whenever you see you know a missing packet that is in that range and so that would allow me to immediately declare loss like send me an act immediately so i can like do that as quickly as possible is basically what this is saying is a mechanism um it slightly reduces the number of acts when packets are also received out of order because if you get a twiddle you know one software the other instead of sending an immediate act and then sending another immediate app when the gap is filled you like don't send the first ack and then the gap gets filled and like everything's fine so it turns out for uh reordering on the internet twiddles i think are something like 60 of all reordering it's some huge number at least last time i looked um so there might be some small performance improvement in low scale reuterium networks um but anyway that's an overview of the issue"
  },
  {
    "startTime": "01:16:02",
    "text": "um especially now that we have dropped we're going to drop ignore ce we have the bits to you know make this like a one byte number uh which seems like a sufficient amount of granularity or we can just make it of our end it doesn't really matter so if if people have comments please jump in the queue now otherwise you know we we can uh let's move on but thank you ian no drama [Music] jonah [Music] to me this seems more useful than just ignore order but i also didn't review the whole discussion on the github issue thanks that's my take as well thank you very much uh for your time and i think i will give my career hi i'm martin duke from google i have no slides but i'm here to talk about quick lb um jonna did you want to have a comment for the previous talk [Music] no okay very good um so uh the authors for quick ob are pleased to report that all github issues are currently closed um if you're interested in that i invite you to look at the change log uh or the uh submit closed prs there the only one that probably worth at least bringing up is related to the crypto"
  },
  {
    "startTime": "01:18:01",
    "text": "review that we received on the for pass algorithm uh so this is the short cid encrypted version um uh there was a simple fix that they suggested that we made there was another fix there was another suggestion to uh kind of the best practice would be to make it a 12-pass algorithm we declined to do that for reasons that are probably obvious and um christian submitted a pr thank you christian that um explained the reasoning there in the security considerations i invite you to look at that email file an issue if you have a concern and if you have any questions about that now i'm happy to answer them but i really wanted to talk about is just the future path of this document um as i said like we've kind of reached the end of our um editorial process as far as we can do it uh so one option is just to go to working with last call um which will of course generate more issues but then but basically move the expedite moving the document forward uh the second option is to wait for some more code um the current implementation status is that they're to my knowledge two load balancer side implementations um one is mine based off of nginx and there's one from ant financial um there are a couple drafts out of date but because they're both load balancers and no servers have implemented at this time we cannot interrupt because we need both sides of that interaction um i am currently uh my current focus and my day job is to implement quick lb for google quiche on the server side um so we will have a server implementation soon and also probably certainly within the next 12 months to deploy this at google so option two so option one is working group last call immediately option two is to wait for that implementation deployment process to continue um and then working with last call and um option three i guess would be to wait for uh more people who are not martin duke to implement this thing but uh who"
  },
  {
    "startTime": "01:20:00",
    "text": "knows how long that will be so um that is all i have to present and i would love to hear comments from the community of what they would like to see before we work in loop last call jonathan when my audio comes up yeah uh janna wanted to uh read wanted me to relay because he has audio trouble um about the difference between ignore order and reordering threshold um if you want to retain ignore order please speak up now otherwise reordering threshold is the lightest direction will go okay so that was a comment for ian i guess do you have a response or okay for that was about the reordering threshold right okay yeah i know the audio is very bad over there he just said like if somebody wants to keep ignore order please speak up now otherwise we will switch ah okay um wearing someone else's document but i don't see anyone commenting nobody in the queue so all right so returning to the question does anyone have an opinion about the maturity of the quick lb document and his readiness working group last call yeah cool event again um i don't have a concern to go to work in google glass call right now but if there's like no good reason uh that we need to publish it immediately why don't we wait for your implementation okay i'm seeing thumbs up uh sprinkling a thumbs up uh with that sentiment um which is fine with me so you will"
  },
  {
    "startTime": "01:22:00",
    "text": "probably not hear from me for a while about this document um and uh like i will do another report once things are a little more mature at google um in terms of doing this work uh and retry offload just while i'm up here is in a similar stasis state but there's no active invitation going on so that is sort of in the deep freeze until somebody is is shown to care um are there any other questions for me or comments ian ian sweat google i had a question for the group actually i was curious if anyone else besides you at google is working on a quick lb implementation eric in yours getting up eric kinnear apple um i don't think we have plans to immediately deploy anything but we've certainly been looking and trying to provide feedback as we go and need something with a very similar set of attributes so i would not give up hope and i i would just like while i'm up here make an appeal to like server implementers that i mean one of the intents here like the original target use case for this was the cloud that um cloud l4 balancers um would implement this and then you know people could deploy quick servers and they could all interoperate um i think certainly our long-term plan is to do that in google cloud um so you know i think those of you trying to position your servers like this might be an interesting market opportunity um anyway if there are no other questions i'm going to return the balance my time to the group thank you thank you"
  },
  {
    "startTime": "01:24:00",
    "text": "cool next up we have christian talk about quick time stamps as we enter the um phase where we stop talking about adopted work items um and talk about related or potential new work do you want me to present the slides or can you do it you can you can take remote control christine if you would okay i i i did not expect that i'm sorry take me some time we can do it if you prefer yes please okay christian the sites are uploaded you just have to request click the request button and then you can control them you don't have to share your stream so what is the request button the request slides just click it and select your slides looks like a piece of paper the other people think it's the second it's a second i think it's the second from the left just click it second from the slayer it's not visible on my screen there's the raised hands and then the next one to it oh that one yes there you go and now you can you know one learns every day yeah here we are sure good okay so timestamps so what time do you talk the question is shall we work on it and [Music] i i have been working on the timestamp"
  },
  {
    "startTime": "01:26:00",
    "text": "project for some time and it has not been progressing very much because i mean some people did implement it but apart from pico quick and i believe the light speed implementation i have not heard feedback the first design of the time stamp was to solve the asymmetric case in which there is high bandwidth available on the same path but limited bandwidth on the or congestion on the return pass and the the failure mode there is that you can often observe congestion on the global pathway it's only caused by the return pass and the response shall be very different so with a time stamp what happens is that the acknowledgement are sent in a packet that is time stamped and the the congestion controller can use that timestamp to understand which way the condition happened and trigger a different reaction and that suppress things like a longer t measurement or wrong decision condition control or even stereotype transmissions so that was one one use case for the timestamp the next use case for the timestamp is clearly for multipass in the case of multi-pass we have configuration in which we have one high bandwidth path with a relatively long rtt and one a low delay pass with a slightly smaller bandwidth what we see is that if we send acknowledgement through the short pass that's not the"
  },
  {
    "startTime": "01:28:01",
    "text": "path on which the packet was sent we improve responsiveness we get better connection control we get better loss recovery but we need to have a smart way to measure the actual rtt of the pass that combines half delays one way and the other and a comment i received on that implementation is that it it requires a phd to understand you do rtt measurement well i don't think it does but then i do have another phd so who knows the um the timestamp implement timestamp option fixes that very cleanly because i mean if you have a timestamp in your packets you can measure each half delay each way and then you can the implementation becomes completely straightforward and it's much easier now there are other use cases for timestamps i mean they are very useful if you do delay-based congestion control things that led back for example but also high start i mean if you if you use timestamps during the high start phase in cubic you will very quickly notice the increase of one way delay that high start will realize on and so i think that there is a strong use case for having time stamps in quake now that dwarf i have is at version 10. a very simple draft i mean uh it just defined a timestamp for m with a type and the timestamp as a number of microseconds [Applause] and an enable timestamp transport option which is basically what you find for every extension the couple of a frame and a transport parameter to negotiate"
  },
  {
    "startTime": "01:30:00",
    "text": "its usage i would think that given that it is very simple and they are well understood use case we could take it on in the working group but it has been a long time and so since that is a long time my question there is what next i mean shall i go ride against another windmill or maybe just enjoy the sunsets okay we'll not leave you on that leave you there so next step really is does working group want to adopt the draft does working good want to discuss it maybe it's not the right way maybe we need something more complex than the secretize time there was a proposal to completely change the arc format so that we can have add a delayed enact delay on each packet we could do something a bit more complex at ntp which has basically three parameters middle center timestamp the last received timestamp and the delay since the last receive which makes measurements even more precise but at that point uh i am really wondering whether we should pursue this or not so questions university and i'm a newcomer of the itf"
  },
  {
    "startTime": "01:32:01",
    "text": "and i'm really interested in the quick the quick working group and the art group is is also working on adding time and stamp to the quick so i think it is really important to add some timeliness signature like time step to the quick mechanism but i think there are there are really something that we need to have a discussion and some implementation should be more discussed in detail let's say firstly is it mandatory for the sender and the receiver to do the clocks uh synchronization if they have to let's say generates their own time step or is the timestamp is only generated by the server itself if the time step is only generated by the server itself so the time step cannot be uh be used by the client but to tell something about the information of the timeliness but only to measure less the the value delay from the center side so i think maybe some more specification may be included in the draft and is it also necessary for the for all the package to contain a time step frame or maybe some kind of package that says acknowledge the acknowledgments the package visa acknowledgment that needs the time step so i think maybe some details may be specified okay then my opinion thank you thank you for the suggestion i think that uh at a high level i agree with you that we should have a discussion inside the working group about what is needed and get feedback"
  },
  {
    "startTime": "01:34:01",
    "text": "and i think that's one of the reasons like to see some adoption so we can get these multiple people working together on the issue of clock synchronization we had a debate on the structure of the firm we had feedback actually several years ago that there is that the frame should be as simple as possible and the document should only specify the firm that the mechanism because the mechanisms will be the combination of say that frame and the immersion firm etc but i mean at a high level yes we should absolutely have a discussion ayan uh yes ian sweat google um i i was a co-author on one of those documents that you mentioned about multiple timestamps or timestamps per packet and um that is not being actively pursued by us right now except we we do have it implemented for one internal project which has a very non-standard congestion controller that i think actually get published at some point um i could dig it up but uh it's so it's kind of in this weird state where we basically just published it just so it was kind of out there but it's not something we're actually pursuing i will say just from an implementation perspective it's a lot more convenient to like be able to curry the time stamp in when you receive an acknowledgement now there's a number of ways of like making that happen in the code like you could store the timestamp and always have it received right before the act or something like that but like just from a code structure perspective having the timestamp available even if it's only one times down at the same time that you're processing the act is is super convenient from a code structure perspective but um i guess i don't think you should give up on this work i just am not i don't have really good advice on what to do next unfortunately but those are my experiences yes i mean i i i do i do agree with the fact that"
  },
  {
    "startTime": "01:36:00",
    "text": "it's that's what we have in the pico click implementation that the time stamp is always sent together with the axe but yes martin martin do uh google um so i i believe i understand the case for multipath but i'm not sure i understand the justification for single path benefit i mean if if you detect that it is like reverse path act congestion would you like send more aggressively and just make the act congestion worse or what is the response if you obtain this information in the single path case well one way would be to tune the frequency for example okay but also uh the the other way would be to uh understand that you i mean you you can avoid use i have seen two usage avoid spur use retransmission and in high start avoid exiting i start too early because of events on the act pass okay i mean i'll think through that i'm sure you've reasoned about it correctly but thanks you know i think china's maybe having some audio issues again i'll dive in with a simple question there's some talk about um time synchronization stuff that sounds super complicated like i i think starting with something simple if if we were to do this work at all but starting something simple wouldn't preclude more complicated stuff coming in the future"
  },
  {
    "startTime": "01:38:00",
    "text": "we have space for extensions and frames like if there's some usage of this thing um go that way you're sure some specifics could change but um i'm seeing a few use cases and this is without a chair hat on with the chair hat on i'd really like to to understand in if people would really object to this kind of work like we're this isn't adopted but we'd be seeking a clearer idea of if if we should be seeking adoption or not for this work because it's been kicking around for a long time now i think we we need to make a decision what to do thanks uh jay collin um i i think that there are some good use cases for this uh you can apply this to do like chirping and find and apply some of the path dispersion bandwidth detection techniques um with this and i i you know do we need it i could use it i think but uh you know so i'm broadly supportive but uh i would i guess like to see some development in the use cases maybe um to kind of get a better handle on on other ways people anticipate using it but yeah i think to me this looks useful thanks john linux i mentioned this on the zulip but i'll repeat it here the uh i think this would be useful for a lot of the real-time media cases all the uh or most of the rmcat style uh feedback mechanisms um most notably uh google cc which is the congestion controller and the webrtc use time stamps for one-way delay to measure to get the low delay uh real-time media"
  },
  {
    "startTime": "01:40:02",
    "text": "congestion so i think if we want to for a lot of the use cases where we're going to do either rtp over quick or media over quick i think this would probably be a useful thing yoshinichida from amazon web service i'm not very you know following up quick discussion but uh so i'm wondering about measuring one weight rate for high stats because no high start is center side logic so you need to receive back anyhow so unless there is a condition on the arc path i don't know if the measuring one-way pass is that useful maybe we might want to see more use cases spencer dawkins uh christian am i understanding your your ask is uh about adoption of this draft in the working group is that is that right yes that's really the case i mean because i mean i i don't want to be continuing to push a draft and there is episodic feedback every two years yeah if it is adopted i mean then i'm going to continue working on it otherwise whatever could i ask the chairs to help at least me understand what the bar that christians should be looking at for this is for for adoption in the working group i mean is it is it close enough for you all to call and say who's interested speaking this is a chair with my hat on um i think having this discussion at this session has been good some of it is is knowing if the author wants to seek adoption at all if they feel ready um"
  },
  {
    "startTime": "01:42:03",
    "text": "i i'm seeing generally positive comments like that this could be useful um especially because of the format and that they could apply to different things but then a few people saying they don't quite understand the use cases like i think what probably needs to happen is a little bit more follow-up discussion on the mailing list um and that um probably i need to confer with matt as well but i i'd kind of be leaning on issuing an adoption call just to see to flesh out people who would strongly object to adopting that work right now um and and the reasons why and therefore you know if if if those reasons are things that christine wouldn't want to address then he can he can go away and maybe in the future somebody else can come and pick up this style of work that's what i'd be looking for does that make sense spencer yeah yes it does uh so uh the other thing i'm thinking you know i think jonathan linux and i were headed the same place roughly which is uh as we are you know like uh avt core has just adopted uh an rtp over quick uh dra draft uh and we had a uh media over quick buff this week that was not as much of a train wreck as it could have been um you know if if if especially if that second uh chunk of work uh gets chartered between now and november uh i think i think it would be you know i would i would i would like to see uh an adoption call go out sometime between you know like i say roughly uh when it seems right to the chairs but uh i know which i know which way i would"
  },
  {
    "startTime": "01:44:01",
    "text": "i would um on an adoption call but do the right thing of course thanks um i also see there's some some discussion in you know in the in the jabber that i've been trying to keep up with um i you know we will take on both these comments and uh matt and i will have a discussion i think all right do you have the ones from china the big one from janna yet janna had problems joining or like not joining the cube with the audio i guess um yeah i can read john's comment uh is mentioning that they should be work on the variation of the one-way delay which is correct i mean all the measurements of delays over the internet are uncertain in the sense that you don't know which the reference is exactly so you'd never measure absolute delay because there is all your equations all your systems of the question end up on the terminate and you have to make an arbitrary choice of whether the delays one way or the other way what you do measure then are the variation of the delay and and that is correct and and we could go explaining that i it's kind of obvious too but the kind of stuff that i would like to see after the adoption"
  },
  {
    "startTime": "01:46:02",
    "text": "because uh clearly uh that could be done by a pr in the draft and a discussion and i don't disagree with china that's what actually used but but yes i mean i think i'd be very happy to watch with jenna if someone if someone is actually working on this and wants to improve and want to become a co-author or something i'll be happy to have one too i mean it's not okay i want to own that same i think my audio might be working again okay oh wow okay awesome so yeah i agree with you i look clarify this because i think this is a point of contact confusion oftentimes for folks um just calling it one-way delay delta is it makes it clearer however i think my higher order point here is that i think the draft is useful because it allows people to actually have a mechanism to experiment with yeah but in in in without actually having a lot of people asking for how for for usage of this thing it'll be difficult to design a mechanism that's going to um be worth standardizing that's my opinion i think that we need people with use cases before we can actually adopt and standardize something okay thanks johnna um we're at time for this item um i could say we'll we'll go away and do some discussion um with various people um this has been very useful so thank you all for the comments thank you christian thank you thank you next up is uh jake holland hello"
  },
  {
    "startTime": "01:48:00",
    "text": "are you at slides you're uh i don't have any means to run the slides doing oh on my phone is that the way i'm supposed to do it could you uh bring up if that's possible oh great yeah hi i'm jake holland multicast enthusiast um i'm here to talk about uh our multicast quick proposal uh so i'll just be mainly going over the actual proposal uh if you want to see why uh i went over that there was a barb off at the remote 111 and there's links to that and the slides there we also went over a little bit at that went over a little bit of it in uh sec dispatch at 112. this proposal is largely a follow follow-up to that sick dispatch 112 discussion where some of the feedback we got is that we needed a specific protocol proposal for the security characteristics we were looking to get with our multicast security consideration stock that tried to go over the security considerations for using multicast to deliver web traffic specifically but also for generalized non-web traffic delivered over multicast so that's the well i won't go in depth but uh i'll just try to give a kind of a really basic overview next slide please thank you so the basic idea is we're just supporting source specific ip multicast this is anchored to a unicast connection so there's always a unicast connection"
  },
  {
    "startTime": "01:50:00",
    "text": "between a server and a client and then the server can tell the client by the way some of the data that i'm trying to send you the way you should get it is you should join this multicast session you'll see some 1rtt quick packets there here are their keys to decode obviously since it's multicast the packets are identical for every receiver these are shared symmetric keys that's one of the things we get into in the in the security consideration stack that's essentially the security consideration section for this document um and uh because they are widely distributed shared keys we also have separate um integrity packets that are anchored on the unicast connection to that mean that even though the keys are shared the packets cannot be spoofed they must be proven to have been sent by the actual server that you're talking to the server controls the entire sort of multicast listening process on behalf of the client they tell the client what to join the client's not required to join these these sessions but may join in order to sort of get a better delivery scalability and the client provides limits so the all the all the flow control and congestion control is different in a way sort of dictated by the requirements of multicast and we go over some of that in the document but some of it is sort of why we're proposing this as experimental at this time uh but what the client provides is some limits that the server's responsible for staying within as it tells the client what to do right and the client sends ax for all the packets that it receives and this is very similar in a lot of ways to the multi-path work um the the packets are just interpreted as a part of the unicast connection that you've got uh it's anchored on a client on a"
  },
  {
    "startTime": "01:52:00",
    "text": "channel id rather than a connection id but that's essentially just a layer of indirection to the connection id and the reason for that is basically that again it's shared identical packets that are sent to many receivers and so the server has to choose that destination connection id essentially connection id but that's a channel id in the place that the connection id goes in the multicast packets and the the unicast frames communicate what those channels what those channel ids are all right next slide please so the the multicast channels are only for server to client data only for server initiated streams essentially it can be used for datagrams as well not so much for web traffic because h3 datagrams have a have a client chosen id inside them but for non-web datagrams it still should be possible for server initiated streams it also we think is possible um and the packets are just interpreted in the context of that connection the the of the unicast connection that you already have um yeah uh any any questions about any of this i'd be happy to go over but uh let's just let's just move on for time here um again for discussion uh yeah next slide uh for discussion um uh so we're working on an implementation uh this is in conjunction with uh several members of the w3 multicast community w3c multicast community group we've been basing it so far on the on the google quiche implementation working toward a demo that could run in a browser but you know we just want to prove to ourselves that it will work essentially but we think that it will"
  },
  {
    "startTime": "01:54:00",
    "text": "um in terms of implementation status or the the maturity the of the spec and what we think we solved uh we don't know of any reasons that this does not match the security considerations document but we do think for web traffic we've recently noticed that we probably need to include something that more strictly enforces the origin policy or allows the the browser to enforce origin policy on the uh multicast packets that are received we do think this is possible we have a couple ways to do it that's not yet part of the draft but just by because the packets are integrity protected and can't be spoofed we think that for example we could include an origin frame in the multicast packets and this would uh give the browser the information it needs to avoid including anything for a wrong origin uh in the data that's processing uh so we we don't think that this should be considered mixed content but opinions may differ on this point and we'd be interested in hearing reasons why we might be wrong here next slide uh we have a number of protocol extensions and the reasons are basically all just driven by um you know we're asking for basically ten new frames at this point um we outline them in the draft uh we have you know this is part of the implementation that's uh that's working there uh but there's essentially just management of the multicast channels in the unicast stream where the server tells the client about them management of the client state and then the acts that are associated with it plus the integrity guarantees that require a separate uh integrity path and we can go into the details of all"
  },
  {
    "startTime": "01:56:00",
    "text": "this uh i'd be happy to talk offline as well we probably don't have uh time for as detailed a discussion as what we saw in some of the more uh uh standardized pieces earlier in the session but next slide the basic question i've got is is this interesting to anyone we think that we can solve a big scale problem here um we certainly would like to deploy this we're interested in deploying it uh non-browser at first because we've had sort of a skepticism i think from the browser community so far but from the isp community we have uh you know some consensus that uh the scaling problems need to be solved there's too many there's uh too many events that are sort of exceeding the capacity of the networks to deliver when there's popular content this is primarily driven by [Music] by popular sports and by large downloads most large downloads would not be web would not be web traffic but the popular sports often is particularly when you consider that a lot of the smart tvs basically are using web apis underneath uh something like 60 plus percent growing last i heard um but you know in addition various forms of uh you know browser apps and and a lot of the clients actually using browsers inside as well so um yeah i guess my first question is like does anybody think this is worthwhile a and if so like what do you recommend we should do before asking for adoption or you know where should this go uh and we'll be discussing this more in mbo indian next session by the way so"
  },
  {
    "startTime": "01:58:00",
    "text": "come there if you're interested we can get into it more alex hi alex ranowski google um i was looking over the slides before you're talking i mostly had the observation that i feel like what you're proposing is not something that is in quick but something that could be built on top of either tcp or quick with the type of control channel stuff that you're doing so my question is really is quick the right place to do this or is this a separate protocol for here is how i want to join a multicast thing and a separate framing layer for here is the multicast available data well uh we're looking for some frames in the in the iana registries for quick um you know does it have to be quick uh no i mean you can have different multicast protocols that's possible we could do something that has the same security properties that's not quick or we could have our own proprietary extension if we're going to be shipping fat clients um but uh we would like to aim toward an eventual uh something that can eventually be included in browsers so that's kind of where i'm coming from with the quick effort thanks hey larry zachary so i i like to select this um already when lucas came up with it a few years ago and i'm glad to see that it's sort of being progressing um because we all had this gap in transport right between the unicast and the multicast and we could never sort of figure out how can actually leverage the power that multicast potentially offers more easily for applications and for um you know users and then this goes in that direction which is sort of kind of exciting um i don't want to speak about the adoption call but sort of in terms of what i would see do you think about this sort of how general a mechanism this could be i mean there's some obvious use cases right that are motivating this but um this is sort of the first time that we have an approach that mixes unicast or multicast pretty easily and"
  },
  {
    "startTime": "02:00:02",
    "text": "efficiently it would be great if it would be usable for lots of stuff thank you yeah that's a good question um i guess i would point to uh the the warp and rush proposals we think we can just do as is because these use server initiated data for pushing from the server side so uh that that's the demo i'd like to run uh you know inside a custom browser is just use web transport with the existing thing and it doesn't have to change the application at all i think that you know server push based hls or dash could do the same thing although server push i gather is being removed from the browsers unfortunately so uh i'm not sure that'll be any better in the near future but we could also like keep keep a fork that keeps the server push cache around and try it there for the same kind of thing but yeah we think it should be transparent for the apps yeah thanks mike bishop um i'm a little surprised that you're putting quick frames on the multicast stream i would have would have thought the multicast stream would just be raw data delivery and then the frames on the unicast quick session tell you what to do with it basically uh so one of the one of the actually nice design choices and in quick the consequences of this are that it works pretty well to just put the the quick frames on the uh on the multicast stream and then use the same encryption strategies and use the same use the same deframing this is actually a very similar problem to like the multi-pathwork right this is just you know you can have a stream that's partially carried on one path partially charted on another path they can be combined as long as they have the same data at all the same offsets right right so that's essentially what we're doing here and so you have a shared path on the multi-path connection"
  },
  {
    "startTime": "02:02:00",
    "text": "well it's an alternate path for the multi you know the the the multicast uh the multicast channel is just a an alternate network path that carries the same data so once you've decoded it then you merge it as part of the same connection for your packet processing and you can have almost all the same behavior it's just a you know yeah there are some differences in like flow control and congestion control there's some differences in the uh the connection id versus the channel id um but most of the packet processing is actually the same and and works out pretty well chair interrupt here we're we're over time now um i'd like to thank everyone uh jake sorry to interrupt you but i think we got some feedback there um which is kind of good i'd encourage hallway discussion for anyone that didn't make the cut we had one more talk from emil um which we didn't make time for i apologize emil um this was in relation to a side meeting that was held yesterday i believe some of the participants of the quick working group were there um i i wasn't able to attend unfortunately my understanding is this relates to quick observability tooling visualization analysis my kind of chair comment with a hat on is that's work that um is completely related to quick and i'd love to see happen um in the quick working group where it makes sense so um i'd encourage that kind of thing to be proposed that the next quick agenda if it makes sense and please speak to the chairs about that um in in future and we can have that discussion a bit earlier maybe um that would be great i don't know if matt has any closing comments nope um okay on that note i'd like to thank our jobscribe and notetakers very much i'd like to thank zahed for being on the ground and helping with uh the delegation of responsibilities and everyone have a great rest of your week and a good lunch"
  },
  {
    "startTime": "02:04:00",
    "text": "thank you bye and the youtube folks are like in theory but they're like this doesn't work"
  }
]
