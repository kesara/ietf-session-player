[
  {
    "startTime": "00:00:09",
    "text": "[Music] [Music] [Music] [Applause] [Music] so hello hello is a guru here okay so our you know pre-planned no take it actually is on a some sort of an escalation call right now so he\u0027s like another volunteer you Ignace you microphone is better perfect so we had pre-planned and note-taker and unfortunately the person actually is on some sort of an escalation call so we are looking for another volunteer right now to actually take notes and another volunteer for you know being part of the jabber so anybody willing to you know pick up that particular task Wow everybody\u0027s looking at a screen looking everywhere you can we found a jabber scribe okay anybody willing to take some take notes it would be very appreciated I\u0027ll give you a free coke Ignis okay thank you okay so let\u0027s get started because we do have a pretty intense agenda today so first some administrivia and so that is the quick agenda here so you probably have seen this thing but I know by now already a few times the note well keep in mind you know even though you\u0027ve seen it a few times you know whatever you say you know it\u0027s gonna be record it\u0027s gonna be public record and be I know aware of that so you know keep the note well your attention we have also given around the blue sheets so fill those things in then at least we know and we have an idea how many people actually were here which again you know is one of the data metrics which we intend to capture for "
  },
  {
    "startTime": "00:03:10",
    "text": "this particular session of DC routing so the next two and a half hours we do have a pretty intense agenda we have a lot of topics to talk about some of the elements will you know we have the expectation that will be lots of Q\u0026A and discussions you know going around and all that actually no it\u0027s pretty nice so we\u0027re going to be covering first like a few presentations and discussions around the requirements in the problem space that\u0027s gonna be followed up by some people I\u0027ve been looking into some of the changing requirements from the routing perspective and have been looking into like new protocol work on how to address that so how to make like you know new tailored solutions for this changing problem a requirements space now there\u0027s gonna be followed up with another approach where we actually or where people actually have been looking into augmenting existing technologies so not really tailor-made but you know use what we have augmented for this changing you know data center routing you know environment and then at the end we\u0027re gonna do like a quick wrap-up I know of where we are so now because me and Victor you know we have been wearing these batches like you know ask us ask us about the buff yeah so now before we actually start with you know these birds of a feather about data center routing I want to make clear that everybody in the room understands you know what is the intent of this particular session here so the main intent here is to actually know provide our you know wonderful area directors you know with input which they can actually use you know for potential future actions know regarding the developing problem space for data center routing and in addition to that we also would like to get like better understanding what are the you know modern data center needs with respect to routing management resiliency programming operations you know traffic flows and so onwards also it is not the direct intention of this particular buff it\u0027s an on working group forming buff that there will be you know as an outcome at general PC routing working group or that may be a result of this but that does not necessarily have to be the case so what is the key question we\u0027re trying to address here and the way we look into it is that you know for us as a result here you know we\u0027re trying to understand you know if it makes sense for the IETF to actually increase their focus around data center routing and the operational specifics so what we expect is you know from you is to actually receive some some guidance and input to understand some of the leading indicators you know of change for the routings in the modern data center environments also in addition you know "
  },
  {
    "startTime": "00:06:11",
    "text": "it is not the expectation that from any of the requirements that we gain consensus on the requirements as they are right now so what we\u0027re trying to understand here is if these requirements you know go towards the you know the environment where you know that there are changes from the traditional one environment so that the needs from a data center in a routing perspective no are different from the traditional one routing environment and also another assumption we are making is that you know the potential to augment existing protocols you know that should not exclude potential work upon new protocol work so this actually means you know there we may actually do some things you know in parallel at the same time we may actually you know want to work upon augmenting existing protocols but at the same time you know why not actually know create some purpose-built data center and Oprah that isn\u0027t their environment and or routing technologies so now you know to conclude you know the intersection here so when going through the through the buff here I want you to think about this you know following aspects so the first thing is if you do Q\u0026A please be concise we don\u0027t have that much time and we have a lot of content to cover here so the questions I want you to think about and for which we\u0027re going to be asking your feedback is you know our data center requirements cleared off to justify you know IT additional IETF focus or what else actually is needed and again it is not our intent to create like full of you know consensus on everything another question you need to think about is you know would it be you know can a single solution actually achieve all of the requirements so that is something to think about in addition do you have interest to actually work upon the solution space if there are like you know different sets of requirements you know for you available and as a last one it\u0027s like you know how it should I TF organized you know the working upon the solution space environments so that means you know should we just leave it as this right now nothing really dedicated should we create like you know like a data center routing working group which is like recovering general elements or should we actually it all really carve up a set of requirements and then follow them up with with the solution space you know protocol work as such so I think that is what I wanted to say here so I think Jeff I have a clicker thing for you good "
  },
  {
    "startTime": "00:09:16",
    "text": "morning everybody it\u0027s great to be here put a lot of work and effort to make this happen Thanks so our 80s he makes it happen so we\u0027ve started about a year ago thinking about what is it how data centers are different than the rest our network what\u0027s been done before what needs to be done in next few years we form the team and not because we are longtime friends we are we know each other we trust each other we respect each other however the criteria was we are people who know how to design protocols we are people who know how to operate networks and how data center looks like so if you look at outer least most of us well-known here so hopefully we can represent wider community in ATF and outside of ATF so why do you feel the requirement draft is needed we really want to avoid the beauty context we\u0027ve seen this kind of debates my other safes are better than your last piece and if I run the internet akhirin anything else many times the idea here is to create single set of requirements and definitions that every contender or any potential solution could be compared against so really to avoid beautiful and it\u0027s not a Finnish definition we are just starting bril need your help no Network is alike we really invite you to provide your comments your unique use case is something that what we try to define doesn\u0027t meet your requirements please be local there\u0027s lists you know where to find us so first your slides are the definitions of the fabric this is what we see today this is what we see happening maybe within year two we know new types of silicon are coming that will provide some new functionalities some next year some two years from now so we\u0027ve tried to provide high-level set of definitions that identifies what this is fabric today so to be very clear fabric provides basic connectivity so we received an equation is it EVP and is it list is it something more fancy it is not it\u0027s really basic connectivity if you wish to run a VPN on top it\u0027s an overlay service which still requires reachable next hop and this next hop will be delivered by the fabric from the main separation within the fabric it\u0027s not provided by the protocol itself it\u0027s provided by an overlay if needed right so we are not looking into separation within the fabric so some characteristics we\u0027ve seen and again "
  },
  {
    "startTime": "00:12:18",
    "text": "many people who have built rather large data center we\u0027ve gone through probably 50 60 durations of initial draft contributed to this so hopefully this is something you can identify yourself in the fabrics are mostly regular they\u0027re repeating itself and they\u0027re usually recursive very high perceptual bandwidth and path diversity half thousand fathers fan-out is not unheard of so we see today most data centers are ECT based meaning we\u0027ve got many many different paths to the same destination and can load balanced per flow however it just an anomaly of certain graph characteristics so non EC in P but are possible and should be support in the future and again here we are looking into new generation of silicon which will make this able to react perhaps on q-tips perhaps on some other characteristics of the network that are not available today limited physical Demeter is important what we see today is no more than 80 kilometers between data center within the metro and fabric could be within this location however we see more and more requests to extend fabric beyond single physical location so it\u0027s not unthinkable of heaven fabric extended our Metrolink so it\u0027s not classical DCI in a way it\u0027s really stretched fabric and such construction should be supported another very important property is very tight propagation time which is much shorter than desired conversion delay we will see number later which gives ability to build protocol in a way that the conversions using protocol itself is shorter due to very short propagation time going through additional clothes fabrics where every entrance is icky distant so you know exactly the time it takes to get from one point to another and back we are seeing more and more data centers where fathers are a symmetrical so bus to one destination could be longer than another one return pass again might be different than forward pass this should be accumulated in when designing new protocol or extending existing one and link-state obviously number of links is significantly larger than number of knots this should be well understood when we think scale when we think converters then to convert what "
  },
  {
    "startTime": "00:15:18",
    "text": "we\u0027ll set two requirements we believe fabric must support non equidistant endpoint so it\u0027s not classical today\u0027s data center where everything is exactly the same number of hops from each other since spine and lift today is the most prevalent architecture it must be supported however we don\u0027t exclude support for anything else however it\u0027s an evolution we are not trying revolutionize data centers however as we go and design new topologies they should be supported given the physical distances between metro data centers particular cases fabric might only provide interconnection between two different fabric and such cases should be supported kpi\u0027s so maybe the product management language but to make it clear the KPI to identify here a single dimensional single failure and they will be changed will look from application perspective what an application can survive typically we looked at what does it take to propagate a failure what does it take for control plane to understand there\u0027s a failure process the failure don\u0027t load it into forwarding plane and react so we\u0027ve provided three different set of KPIs based on the size problem to read the numbers you can read themselves we would really like you to look at them compare them to your use case and see whether they meet your requirements you need something that\u0027s faster do you think it should be relaxed it\u0027s really cool for your input it\u0027s not the numbers we impose on you and very important the total conversion stem it\u0027s always a combination of number of routes number of paths and time it takes to detect so we are still looking into combination of different factors and we\u0027ll work out probably more coherent numbers so if we look at today\u0027s data center we must support load balancing fusion ecmp which is the case today we should be able to support weight it is simpie where besides metric to the past we can also provide weight to particular class and we need to support an actual load balancing as well based on particular metrics and those metrics might not exist today as we know them might be something new with an new protocol so we may support any new Lord metric we are not familiar today with that would bring more granularity so as we know fan-out is huge size of flows and number of loss "
  },
  {
    "startTime": "00:18:18",
    "text": "is huge so load balancing good load balancing is extremely important so now we are moving to routing protocols requirements rather than fabric so it\u0027s a protocol that\u0027s going to run on the fabric distribute reach ability make sure fabric is working so obviously you want numbers and KPIs mentioned previously to be supported we seated a fan out of quarter thousand BC requirements coming to 512 cards it sounds large but it should be supported we believe there are requirements to do so classical dirty fabric distribute IP reach ability however we believe that a new protocol should be able to distribute any other type of reach ability being MPLS labels being any third party data meaning additional placeholder for metadata so think about segment routing data center some people think it\u0027s really important so being able to distribute labels or binding from prefixes to labels within same protocol as we do today in classical segment routing is important telemetry is important as well so if we need to distribute some data that\u0027s not necessarily reachability data but any metadata related with quality with third party metrics the protocol should provide us ability and easier of encoding to distribute such information and we\u0027ve been looking into encoding for quite some time we decided to postpone this discussion it\u0027s a lot poisonous but there\u0027s definitely interest in new encoding so what we believe must be supported support for invent an out-of-band management mandatory zero - provisioning mandatory as well as neighbor discovery that would facilitate GTP when new we should be in placed Interac we really believe that the amount of information that needs to be configured should be minimum so potentially switch should be figure out where it is what it does but their cabling is done right and figure out who are the neighbors we believe mccannon such as B D and B D here specifically mentioned because this is the facto standard today there might be something else in the future however if you look at what supported on today\u0027s silicon how we do fast failure detection today BFD our ability to track state of BFD that which result in changes in state in protocol is mandatory we also believe that a protocol should be able "
  },
  {
    "startTime": "00:21:20",
    "text": "to bootstrap B of this session so when there is no configuration on both sides we should be able to leverage the routing protocol to provide enough information to bootstrap give the assertion that could conceptually be used to track loveliness of the link operational requirements so getting more and more interesting because amount of data expected amount of information expected is growing really with the day so we believe new protocol must support real-time notifications or near real-time definitely in milliseconds it should be able to communicate each state state relative to its neighbors to a potentially out-of-band system that could change the graph recompute the graph do some through this information we should be a as a snapshot at any given time so when we want to figure out what happened 20 minutes ago we should be able to look at snapshot that was taken 20 minutes ago compare it to operational state and understand what has changed what has happened and while maybe we live particular routes to the world very important requirement is to be able to Commission the Commission knew not without degradation of operational Network so we are not talking about gracefully start or non-stop routing will know the switches their complicated they bring huge amount of bugs and potentially we would like not to see them we would like to see a an evolution of gracefully start if you wish so when we finished the Commission you not it doesn\u0027t affect operation of existing it so there are few items that we looked at they require additional study and we would like your impotence items so non-classical and codings GRP Seadrift similar cells self-describing encodings when rather than trying to encode something any TLV we could provide brought a file that describes the data it gives innovate gives ability to build think within our own data center it may be not requiring their ability to do it in minutes not in months ability to function as an overlay the focus of fabric protocol is underlay however running same protocol to provide overlay services is definitely possible it will bring new requirements with regards to scale with regards to liability however it\u0027s possible and we "
  },
  {
    "startTime": "00:24:22",
    "text": "were looking to it floorlet signaling so floorlet it is a sub flow of a flow it requires particular knowledge about interleaving it requires particular knowledge about how to reassemble flow so there is no reordering at the receiving site will be looking into it we believe it\u0027s an interesting technology multicast the most people in data center other afraid of multicast I hate it so there\u0027s some requirements for multicast there is beer that\u0027s coming next year in implementations so I will definitely look into how to provide facility to replicate traffic how to address broadcast and multicast traffic in the fabric out aggregation and conditional delegation is very interesting topic so most data centers today run low and silicon so in terms of scale you should always watch where you are in terms of number of routes number of labels when we aggregate we can increase amount of state in the network since is aggregated however as a result of aggregation and they\u0027re very good description in BGP and large DC RFC about potential black calling and other artifice of abrogation so being able to derogate dynamically based on particular condition is an important thing in protocols and we\u0027ll be looking into it stay to present so ability to build graph that could be analyzed upon real-time or non real-time is important so to facilitate easily understandable networking graph at scale is important so I\u0027ll be looking into how the networking state could and should be represented northbound and the last point here is pretty much the consequence if you want to do traffic engineering in data center you need something like PC someone has to take the topology apply the constraints and compute the bus so integration with PC like entity or what we call SDM Thursday\u0027s is important and we\u0027ll be looking into it questions comments everybody agrees so we have about 10 minutes for the Q\u0026A session hi I\u0027m Artie angora rooster I\u0027m a little concerned about the requirements doc kind of missing basic problem statement essentially like if you have looked at the MSD C which you do refer to in the document what would be good to understand is this is not a new problem right like data center people have been building requirements are very varied and what seems to have happened here is a collection of all sets of varied "
  },
  {
    "startTime": "00:27:24",
    "text": "requirements becoming a god list secondly from a fundamental prakit of problem statement we do not have a requirements document to tell service providers how to design their backbone networks data center network design is how companies compete and differentiate themselves and people run everything from for way to 128 way to maybe 4 5 12 in the future why are we coming up with a set of requirements to standardize what does I need to standardize the requirements is my question I\u0027m fine with the proposals if you are saying that oh there is in one case you can use BGP some people run links to it and some people may want to run RIF vgp SPF perfectly fine but why are we trying to standardize the set of requirements which are so varied as different customers plus it is their business differentiator and all these requirements together really don\u0027t make much sense as a whole so that\u0027s kind of my comment so we are not trying to impose an architecture on everybody saying this is how you are going to build your next data center there\u0027s you kind of odd because we\u0027re defining what the fabric is you are saying there\u0027s a leaf spying Plus there is certain requirements which are kind of your said may based on what we\u0027ve said for example Dec always a me but you see I could be extremely important to some customers who operate data centers but are there\u0027s a structured leaves point super spine may be fine so point being it\u0027s just varied right like and one set of requirements doesn\u0027t apply to everybody so as I said before it\u0027s not finalized version of requirements that you will go and implement it\u0027s a request for comments we value your input there\u0027s a list in the DC routing please send your comments and we\u0027ll try to address them the best we can yeah just second yeah please do understand we are not imposing an architecture we are gathering requirement that people who are going to build either new protocol or extend existing one and the problem space is well-known in both iGPS and bgp right they could look into thing hey my design doesn\u0027t address this not the DC design the routing protocol design because this is about routing protocol it\u0027s not about how you design datacenters yeah happens clarifies so so and I would like to compliment also that you know one of the reasons we are doing these requirements you know overview is to come to an understanding you know that you know is it sufficient to just augment you know existing technology or should we look upon like you know more tailored solutions you know going forward so a question to ask yourself is is it\u0027s basically around that so this way you know is what we have right now is a sufficient to fulfill these sets of requirements I don\u0027t think it is the general purpose to standardize requirements for DC\u0027s because that is a "
  },
  {
    "startTime": "00:30:24",
    "text": "very hard thing to do and will incredibly controversial like it is on the informational track to becoming standard so I don\u0027t understand so I that it is kind of being proposed as an informational kind of draft eventually to become an RFC for on that track or it\u0027s not right so it\u0027s fine if it\u0027s like a guideline but it\u0027s not it is actually on the informational RFC tracks so the meanings of what information it is for information right it\u0027s not that you have to go and implement in particular and coding and format this is something we advise because we believe this is if not the best way it\u0027s a viable way of implementation so sorry just add a comment for clarity as well remember what Gunther stated in beginning was we\u0027re trying to understand the leading indicators of what\u0027s driving various things in in the environments and so for this bla this understand are these leading us to like what is this leading us to understand the environments to dictate for us in terms of new potentially new work etc we\u0027re not here specifically trying to say these are the finalize that are part of this this is trying to understand where is this leading us are there augmentations is this is it\u0027s significantly different are the drivers signaling different that we should look at additional focus here so we\u0027re not specifically what you know trying to narrow down on this but it\u0027s it\u0027s good data right and we want to discuss what does it look like so if you look at number of proposals there\u0027s definitely it\u0027s not a vacuum proposals are coming they\u0027re trying to address limitations of protocols today right so it\u0027s not something we\u0027ve invented and importing to the rest of the world we are seeing the need for better more scalable faster converging protocol that could be either enhanced existing protocol or a new protocol and we are trying to figure out what is it okay so by the way we\u0027re closing the mind also what\u0027s that yeah go on but I lost one on you any more in ten minutes with Jeff anyway ah a lot of these requirements is they seem to be all encompassing it seems like you should at least classify them as being routing control plane requirements or data plane requirements or platform requirements because they when I saw the list some of them are closer to routing and some of them are closer to platform but yeah but there I mean it\u0027s it\u0027s a good start I think I fully agree with you so the intention to publish in such a way it was to request your comments after more comments and date have been gathered in our lives we all start structures in a way you can read them say this is what I want from management plane this what I work on her plate this is for data plane should in this conversation "
  },
  {
    "startTime": "00:33:24",
    "text": "my following Modi said the names combination of the platform Ronnie knows maybe maybe you guys need to just split different document different concept because this one I love man management I love my management it\u0027s really not as much a frauding problem then maybe the other things you have in this segment so I think the doctor ego I started to put too much chain in one document which makes it kind of hard to digest and use so on one site I kind of agree with you ultimate management is not a routing problem however we are not trying to make its routing problem but we are saying that protocol should support out of management and what it is it\u0027s like creative each so mad man is it bad management or whatever it is it\u0027s a routing problem is going to do it so it\u0027s not really matter of the support it needs it should be there by default Greg we actually be close to the guy behind you we closed the line so Dan McGann which with regard to the encoding you mentioned particular you know frameworks for encoding but it would be good to say what requirements you know we would like to see from the encoding instead of naming specific frameworks so as you know by the time this is done there might be something new you know that might satisfy that and we don\u0027t know what are the requirements that we need from dear colleague the intention of encoding so specifically to say that there are different encoding that what we do today for routing protocols and we will be looking into them it\u0027s in no way meant to say just you are thermally candidates it has etc at the end Greg mirskiy is et so you are emphasized and called out the role BFD in triggering conversions that\u0027s very good I appreciate it what about degradation of links paths so you think that in direct and analysis of telemetry like buffer queue utilization is sufficient or there might be some requirements towards the performance measurement so we would like to keep functionality of a protocol limited in a way it doesn\u0027t increase size of link state database so given ability of telemetry systems today we believe that amount of information that could be provided with regards to drop packets reason why it was dropped key occupancy is enough to declare that there\u0027s degradation and this is where where did he seem he could come into play for "
  },
  {
    "startTime": "00:36:25",
    "text": "example another question is that if I understood correctly are you mentioned two routing protocols requirement is distribution of telemetry so is it - routing or - management because - routing I wouldn\u0027t imagine that it will be flooded information you think that telemetry needs to be divided in in the fabric or you need to deliver it to certain host that will be do analysis you know and digging and such so it it\u0027s a little bit different functionality and I think that then it has to be watered differently it\u0027s a very good question thank you for asking so the basic telemetry is system to management interruption I don\u0027t think we need to distribute counters across the fabric however there\u0027s some amount of metadata that might benefit how the load balance how we choose the path so finding right balance about what should be distributed versus what should be sent up to the management if yet to be figured out okay I think I\u0027ll take it to the list but I think that it will be very interesting to really discuss how the flow goes whether it goes are always north and then go south or there is some east-west so distribution of information within the fabric is obviously it\u0027s worth distribution of information to the management layer is north an amount of information sent north is order of magnitude higher than Agra thank you Randy Randy Bush I could you pull to slide seven please um the we provide trance Goble transit we also have massive banks we also build data centers we see those as very different things and I love this slide because this makes clear the delineation between a data center and an enterprise right and I was initially horrified at the enormous laundry list of requirements which as I posted on list kind of implies there\u0027s something behind them I don\u0027t know if there is or isn\u0027t but I guess in another sense this being the ITF if you hadn\u0027t laid out the requirements you we complain that you hadn\u0027t so what the heck um but it\u0027s an enormous list and I think some focus is going to be needed absolutely so my first commenter on anything is please don\u0027t try to boil the ocean however if zero zero draft wouldn\u0027t "
  },
  {
    "startTime": "00:39:27",
    "text": "trigger any comments from you any unhappy faces we wouldn\u0027t have done great job so it\u0027s really to you to read to figure out hey I disagree I don\u0027t like I know better and let us know we are here to listen to you we are not imposing any of what has been presented we would like you to assess what\u0027s been presented and let us know what you think okay thank you so much so we have 15 minutes for this okay hi I\u0027m Melanie Elkins and if you can hear me we\u0027re we\u0027re just doing this to kind of give a different perspective if I can kind of I supposed to introduce ourselves we have we I said I don\u0027t want to say we represent anybody it\u0027s it\u0027s um but you can see the people who have written the draft are have a large corporate I don\u0027t know if legacy is the word but and so we use the word brick-and-mortar these are the the people who\u0027ve been around for a long time and so we\u0027ll talk about that and it if I can give a little bit of history I suppose it\u0027s like I I mean I might call us the 99% and we\u0027re probably the 99% and I don\u0027t know what that number is but we\u0027ll probably the 99% who are not here at the IETF and but in terms of I guess data centers or presents there\u0027s a lot of them I mean for example like Mike like Michael he has he\u0027s with Blue Cross in Michigan which is a large healthcare enterprise and I was we were sitting around they meet all the the healthcare industry insurance companies a lot of them they meet like once every quarter and I was sitting around the table with him one time and somebody looked around and said that\u0027s 10% of the US economy sitting right here and I myself have a very small software company and I have probably 2000 enterprises and data centers that I talked to you know my my email list on just that I\u0027m nobody you know I\u0027m just a little software company so it\u0027s just to kind of give you a little perspective and we have I guess different requirements and we\u0027re not maybe at them you know some of the stuff that you guys "
  },
  {
    "startTime": "00:42:27",
    "text": "talk about with you know fabric and everything else it\u0027s an aspirational goal for us but a lot of us are definitely not there and so just to kind of give perspective on where we are and I was just going to say one of the reasons that were here is to find out if our requirements and demands on our networks are different than are being taken into consideration today and to ask the question if they should be so help us with that kind of round out the conversation so okay so so who are we by and large very geographically dispersed that means that like he\u0027s for example he\u0027s Blue Cross in Michigan they\u0027ve got regional offices you know all over the state of Michigan yeah I can say I was just telling my new friend Linda that means you have to wake up now we have the the typical set up of data center campus and then unique to Michigan is a statewide network and that\u0027s important to us but we have Blue Cross plans in every other state and we connect to them via private network that\u0027s called lose net so that report that may represent a unique set of requirements and that\u0027s what we want to take into consider but away can everybody hear them or is the audio okay can you guys can you guys hear okay but anyway so art we use the internet for business but that isn\u0027t our business and for maybe I don\u0027t know if I if I if the analogy of the 1% works but for that for the the people for whom maybe I don\u0027t know I may be speaking you know out of turn but maybe for the people for whom fabric and all that stuff is a huge consideration the Internet is their business and for us that\u0027s definitely and we use a thing called private networks which maybe a an unfamiliar term here but that\u0027s heavy and then a lot of business-to-business ins VPNs over the internet yeah and we have multiple kinds of networks and we\u0027ll talk about that and we will focus on what happens inside the data center because I know that\u0027s of concern but as I said we get questions like why don\u0027t you just go to the cloud and so I\u0027m gonna kind of tell you why we why we are the way we are quite and why it\u0027s really hard to change okay and and part of it is because we got in early we\u0027ve been computerized for the last 40 or 50 years not just like the last five you know I mean it\u0027s it\u0027s very it\u0027s it\u0027s there\u0027s a lot of inertia I mean a lot of stuff to move very large IT staff we have a lot of baggage yeah and very large IT staffs thousands and thousands of people and what\u0027s important in a lot of decisions is as in any business is you know time to market return on investment business reasons and the "
  },
  {
    "startTime": "00:45:29",
    "text": "business reasons are not how fast can you get your content out under the internet that it\u0027s how many insurance policies can you sell so it\u0027s a very different kind of motivation um so what kind of networks do we have with usually people have large campus lands private lands you know going out that for example to the state of Michigan I used to work for a very large oil company and we would have you know our own private microwave networks in the swamps of Louisiana because there was no service and so we had to build our own microwave towers I mean so we we do stuff where we need to a lot of extranet a lot of business to business connections or he\u0027ll connect to the government to Medicare you know so on Social Security and we do have internet facing apps you know and but that\u0027s everything is firewall but let me show you this is but inside the data center that\u0027s because I know that\u0027s what we\u0027re interested in what is the inside of our data center look like it looks horrible it\u0027s it\u0027s tons and tons of layers of servers middle boxes and so on and all I\u0027ll kind of I\u0027ll face this out lots and lots of different kinds of applications let me see if I can give you we use the term multi-tier application which i think is pretty common and in many cases it\u0027s multi Multi multi tier which explains the messy diagram that Nalini just Multi multi multi tier and and a lot of the the pathways are go through those layers and layers and layers and so we have tons and tons and tons of middle boxes load balancers Nance firewalls app servers all this stuff and the the routing is going through all that and if it\u0027s equal cost and that kind of stuff it\u0027s all this pathing is done through this through this these kind of things and the biggest things which you cannot forget is that in in so so so so many of these kind of data centers there\u0027s we have a back-end mainframe application I it\u0027s it\u0027s I know it seems unfamiliar to a lot of people but you cannot take these large mainframes and shift them over to the cloud I\u0027ve worked with people for years and years and years and I\u0027ve only known to somewhat medium-sized companies who are able to do that and a lot of the data resides on these on these large mainframes they\u0027ll have a web application but the back end is the mainframe and so so that\u0027s that\u0027s kind of where we\u0027re sitting so let me go back the in terms of the proud routing protocols we used a lot of people use OSPF these guys use a lot of P is the most prevalent in most of the networks when with some BGP usually on the edge rip kind of declining now I hope and we "
  },
  {
    "startTime": "00:48:29",
    "text": "do and we do a fair amount of static routing I\u0027m sorry and it\u0027s all different I\u0027m just I\u0027m just collecting this from different not everybody does all of this I\u0027m just collecting it from different people I know but but this is this is what we do and when I\u0027m getting tell you to is like I asked around people that that I talk to all the time I mean I talk to these companies by five six seven of them ever every single day and I said are you guys having problems with routing do you think latency is a problem and everybody says I know I mean routing just works and they have yeah just works it\u0027s not a problem they we\u0027ve got a lot of problems don\u0027t get me wrong we\u0027ve got a lot of problems that\u0027s why we\u0027re here in the first place that\u0027s this isn\u0027t one of them and if you\u0027re talking about like reducing link-state advertisements or rap summarization any of that stuff I mean that\u0027s your job I mean that\u0027s we know how to do that that\u0027s what we do it\u0027s just like it\u0027s not a problem you and I really don\u0027t have much to add to it I think our situation especially contrasted to the previous speaker we\u0027re pretty simple I think but we\u0027re here to provide our requirements if they\u0027re meaningful yeah and we totally if if other people find that they need other solutions or they need a problem I mean we\u0027re that\u0027s totally totally fine it\u0027s just and you know we don\u0027t speak for every single enterprise every single data center in the world that\u0027s we\u0027re just you know I to a number of them and and they all kind of said yeah and finally I think I would say is to the extent that we\u0027re not providing the type of input or feedback that y\u0027all would want let us know what\u0027s missing and we\u0027ll go back to our folks and find out that might be and what you need then let us know but it we\u0027re open I\u0027m hoping to take questions if anybody Randi do I know do you have something you wanna say or do you guys want to do I take the mic or I can finish it\u0027s up to you yeah so we have about another probably five to ten minutes on this yeah so we can ask them now if you want it\u0027s fine yeah cuz we\u0027re almost you know it\u0027s pretty simple yeah I\u0027m totally open Randi go ahead Randy Randy Bush i I J two comments one is probably half the people in this room understand what there is to understand so far about Enterprise routing yeah I GRP by the way it went away once we got multiple vendors um sorry you gotta snuggle up to this one so I\u0027m gonna do it again okay um most of the people in this room understand or at least flail at enterprise routing you\u0027re a GOP runaway once we got multiple vendors a decade or two ago um but I think you\u0027ve done an "
  },
  {
    "startTime": "00:51:29",
    "text": "excellent job of delineating your selves from data center rally and I\u0027ll point you again to slide 7 of the previous presentation write it this is a different problem it doesn\u0027t belong here [Music] thank you for your work I really appreciate it the time you are spending with us here it happens completely different world most of us believe in bubble we assumed thinks our modern build replaced every three years in your case it\u0027s very different was I opened her for me so when looking at what you\u0027ve been doing what I really could see is the network has been built in a way that either represents your entire organization or vendor future it\u0027s managed to sell you over the last 20 years so the intention of work here together is really to give you tools to fight this to tell next time salesperson our next IT manager comes this is what we figure out this is what could work for us it\u0027s a simple its operational and this is what we want rather than trying to fight unknown so it\u0027s really to make things known for you this was the intention of publishing BGP enlarged EC so thanks to Illya for pushing it through and I believe it published it in less than a year it\u0027s really to make you aware to make information public how things could be done leveraging previous experiences so could work for you and you have enough Emma to fight sales pitch you know Jeff I really I really really appreciate that because and and and I\u0027ll let Mike talking but where I think a lot of us would like to go to somehow is this is kind of how I think we see our future is we\u0027d like to go to Sdn containers and that kind of stuff hybrid cloud we have a problem though is how fast we could possibly go we\u0027d love to have some VC PS some paths you know forward best practices on how to do that telemetry we\u0027re talking to some people about the kinds of visibility and stuff we need the problems because we have some problems maybe that out that that others might not have which is that we need to encrypt inside the data center you know we have regulatory requirements for separation and visibility and so on and we have tons and tons of applications some which were written you know as I say in the Stone Age of of computing which are very difficult to convert and we have relationship and expanding on one or the thing that you said were very "
  },
  {
    "startTime": "00:54:29",
    "text": "heavy into the need for management and monitoring and telemetry and whatever else that might hold for us but but we would very much like to have guidance and best practices and work with people if there\u0027s somebody who\u0027s ahead of us great we\u0027re all but we want this to be as much of a two-way street as possible we\u0027re going to learn a whole lot more from you than you will from us but to the extent that our requirements are valuable great I\u0027ll echo what Nalini said about we\u0027re somewhat anchored in the past that kind of hits at what randy said as well that the IG RP has gone away it hasn\u0027t gone away in our environment I\u0027m unfortunately we have about a minute left for the whole slope and q and A\u0027s and what is left one minute 20 P jeepers so I do not think that this is that much disconnected on a metal plane because you\u0027ll be facing two problems that kind of the hyper scalars and the modern corporate enterprises that had the luxury to start in the last five or ten years yes solved in a different way right the one problem that you facing that\u0027s killing the IT staff right continuing on this trajectory will probably not be feasible so we have us to ask yourself how will I reduce IT staff which leads you to your automation question and in a sense and more regular consumable that\u0027s why the building fabrics right this stuff is regular it is easier to manage the second one is what is the trajectory in terms of consumption of bandwidth that you see right because ultimately those are components you consume CPUs as components you consume memory as components right you consume storage as components you are not running any more punch cards and no tapes anymore right same thing is happening to bandwidth well then what is a more abstract resumable right it\u0027s much more perishable but it is a consumable which if you want to consume it in high qualities at low price-point has to become an component right and the whole data center routing the building the fabrics or in in in meta terms is nothing else but trying to make networking which resisted for a very long time bandwidth consumption basically component all right and that\u0027s why I think this buff is driving that\u0027s where the world is going right and that the bandwidth has to be packaged into something you buy it fries which it is not today right so if you own this trajectory it will push you into this direction because this is how it is being packaged as a component right the way the world is going so all this stuff that you have today will go more to more and more regular pattern to reduce IT staff and basically allow for cheaper consumption at larger scales so this is very valuable these were this is all the world that we have to move forward because as you say a table of people "
  },
  {
    "startTime": "00:57:31",
    "text": "represents 10% of the GDP right and not for the last 10 years and it will not go away okay let me need to conclude actually otherwise you\u0027re gonna be eating in your own time Tony you make a lot of very good points and and and I we we\u0027re not as different as maybe I\u0027m making it out and I\u0027ll agree with what randy said I like that slide seven presentation as well so we do have a lot more in common than maybe I\u0027m letting on so we do look forward to work MIDI if you can continue to tolerate us okay we need to we need to break it guys we\u0027re gonna you know yeah I was saying was it we\u0027re just we\u0027re really slow though okay thank you thank you everybody may ask a question so guys so much this is number so so actually we have close to lines I was really gonna I know so sorry there\u0027ll be more opportunity in the next sections so thank you very much so what we want to do is we\u0027re gonna try to be very definitive gonna have a few questions now after this first section hold it so sorry you can\u0027t hear me we\u0027re gonna ask a quick questions that for each section to try to do a bit of a wrap-up to get a census of the room I\u0027m gonna we\u0027re gonna you show hands here again we\u0027re not voting for any leader here we\u0027re just you know trying to get a sense of what how people in the room so gonna ask three questions and it\u0027s either you\u0027re gonna feel yes or kind of yes or no position on that so the first one is and we added this to our deck recently so do you feel that the requirements so raise your hand if you do feel that there\u0027s requirements that are not yet captured from what we discussed today that should be within the focus of DC routing war potential DC routing work so if you feel that yes there\u0027s there are requirements that we have not yet discussed but they should be considered within DC routing potential routing work would you mind raising your hand so see me about 20-30 okay okay so a few hands now if you feel that this pretty much captures at all and there\u0027s really not a lot additional requirements that would really go into potential DC writing work you can now raise your hand little less okay few hands okay so I guess a few of us still okay that\u0027s fair enough yeah sorry second question do we do you agree that the DC frat like there\u0027s enough augmentation in some of the clients that we\u0027ve seen here that make it different from traditional routing or work we\u0027ve done in the past so there\u0027s an so the question here is do we feel that from what we\u0027ve seen in these requirements that there\u0027s enough augmentation to what it the demands of the DC the new DC routing environments that we should that we think there\u0027s a lot of augmentation well used to do so is there enough differences here that we\u0027ve been exposed to that we feel that it\u0027s quite different than traditional "
  },
  {
    "startTime": "01:00:31",
    "text": "work we\u0027ve done or traditional networks we built or data centers we built okay so show of hands if you do feel there\u0027s a significant difference if your hands and if you feel that no there\u0027s not a significant difference from what we\u0027ve done historically yeah so nothing changes so if you feel nothing\u0027s really changed and we\u0027re just just more of the same please show of hands sorry you can have a question I have a comment on this you are asking from the requirement presentation from those two people\u0027s presentations any differences I will say that it\u0027s not because of they presented is actual data center today is different then there\u0027s like service provider network and data center then it has specific characteristics and I haven\u0027t seen that being presented because the topology is very simple and very kind of far different than the traditional ones I think there\u0027s significant differences yeah I understand so again unfortunately yeah you\u0027re correct you you are high percent correct and we agreed we didn\u0027t see all the potential requirements that could possibly go into this we could only cover sone so much we took some initial data and that\u0027s why I tried ask question up front are there additional requirements that we\u0027re not yet just right here that would go into it I would not call that requirement as the lady earlier saying this is not really requirement I would say it\u0027s more like characteristics of the data is in the networking like for exemption it\u0027s actually maybe less maybe you need to remove some of the routing things we need it before it\u0027s kind of stream down removing features not adding features that\u0027s my point yeah it\u0027s you know you probably you know I think you\u0027re right I mean only some perspectives we need to remove certain functionality but at the same time you probably need to add functionality to in all compensates for like the new interchange to kind of in a routing environment so by the way this is not like a Q\u0026A cam we\u0027re just trying to you know fill the sins of the room because otherwise we\u0027re gonna be running totally out of time okay so no comment just hand raising basically an last positional question facing what we what we\u0027ve seen so far do the DC writing requirements driver do it do you think that this would drive us to new protocol work just from what we\u0027ve seen so far and it\u0027s a show of hands so if yes please show of hands look 20 sorry repeated DUIs you repeat the question is that what you\u0027re asking okay so the question was do we feel from what we\u0027ve seen from the DC wrote in current or characteristics that we\u0027ve seen exposed to us that this drives us to do potential protocol work so I\u0027m repeating the question so you\u0027re asking it\u0027s the same question I just asked a second ago I apologize because "
  },
  {
    "startTime": "01:03:35",
    "text": "it\u0027s just can I just make a comment let me just finish the question please if you don\u0027t mind okay and so if the answer is no you don\u0027t feel this drives us to new protocol work facing what we\u0027ve seen please show of hands booked an or okay okay so just to wrap up and then if you want to ask a quick question so it seems like from that last question more people believe that yes this seems to indicate that potential new protocol work might be needed to help address some of these characteristics I\u0027ll use the word characteristics maybe that\u0027s a better better term to use at this point that versus hard set of requirements do you have a very short question quick comment youngster and Bloomberg LP from the comment that I heard was that there is definitely you know there\u0027s definitely a need for possibly more protocol work but does it absolutely and there\u0027s definitely need to be developing more things for fabric-like implementations that are distinct from let\u0027s say how traditionally data centers got done but does it really have to be I mean I could see how this could be called you know fabric but does it absolutely need to be called DC routing that\u0027s there\u0027s got to be there has to be in it there has to be a middle ground to this but the the different things that they see many of the people here are discussing are not necessarily specific to pure you know data center so just to comment that you\u0027re correct the DC routing was the name of the boss and we were charged trying to get input there this is a non working group foreign Boff and there\u0027s no specific indication we\u0027re gonna have a DC routing working group per se this is just trying to get input from the group in community so just to share personal experience besides my rollicking life there make it short okay that\u0027s another type of life relevant to your model is computing some others and I think what we are doing here is going to be very relevant to extending crowding to the edge with all the new IT endpoints with everything that\u0027s happening in the radio and sorry 5g so named I it will evolve okay so now we\u0027re gonna move into another section of the Boff where we\u0027re going to specifically dive in to a couple of proposals that were put out there that have been circulating and we\u0027re going to capture some that potential work here we have a clicker thing here so in your gender at 35 minutes now you have 25 that\u0027s good enough Thanks morning folks I\u0027m gonna talk about bgp based SPF extension this is the work that I did with my fellow co-authors from Cisco Nokia and LinkedIn so the main problem we were trying to "
  },
  {
    "startTime": "01:06:35",
    "text": "target was again massively scalable data centers they\u0027ve implemented some sort of layer 3 routing this is this is this has been done today there has been some kind of centralized route control using a controller based solution no surprises there and from operational simplicity most of these folks have deployed BGP as their routing protocol um the route controllers we talked about have a very similar functionality like a route reflector in sense that they may reflect routes and it acts as a central database for policy enforcement or any kind of management that you will need to do typically the route reflectors that are used inside BGP that are not in forwarding paths or are assumes a presence of an IDP like protocol to resolve the next stops the underneath next stops for BGP in MS DC\u0027s or typically closed networks this problem is resolved by having hop-by-hop sessions set up and therefore the next stops are just recursing over a directly connected interfaces so the solution that is proposed here helps a deployment of a controller type loss model where you could still use BGP arm and get a wrong get more convergence from BGP by avoiding head-of-line blocking that you would have to otherwise incur using a distance vector protocol here are the standard advantages of running BGP oh uh SPF sorry over any distance vector protocol nodes have a complete view of the topology therefore it\u0027s an ideal sort of an algorithm that you want to run as an underlay protocol only Network failures need to be advertised when you have a link failure or a node failure and that results in two typically a faster convergence and better scaling lastly SPF also gives you a much more optimal shortest path routing as well then you look at the advantages of BGP here are the following advantages it\u0027s robust and a far more scalable protocol has a wide acceptance has a pretty good reliable transport over TCP it guarantees in order delivery has incremental updates there\u0027s no flooding in there and lastly it allows you to work over multiple different peering models so with that in mind if you were to combine both link state as well as BGP together here are the changes you would probably need to make to the protocol you want to define a new Safi so you carry this over a "
  },
  {
    "startTime": "01:09:35",
    "text": "completely different address family we have done that we have defined a Safi that mimics pretty much a link state Safi that has already been defined inside bgp and the reason to do that is that the packet formats and link state Safi very closely mimic the IGP packet formats you have a new capability that lets you exchange the new Safi and establish the connection only if it sees on the other end supports multiple peering models and essentially runs the Dijkstra instead of business vector so in your own attacks basically the next hop and the path attributes that are carried to resolve the next stops are kept intact so that you don\u0027t break the base RFC 42 71 you however replaced the decision process of bgp in particular the phase 1 \u0026 2 with SPF phase 3 of the decision process can be short-circuited because you\u0027re talking about node IDs and prefixes here and finally you need to ensure that when you are announcing this updates only the most recent version of NLRA update is accepted this is something that BGP already does you could augment it with sequence numbers on an update message and actually take care of this problem from an SPF standpoint that is defined in this proposal it\u0027s a fairly simplified SPF that up runs with a point-to-point connection scales very well could be augmented to support LFS segment routing seeds and other IGP features also supports a dual stack of 4 v4 and v6 the the part of the work that is not covered here is if it runs for any underlay surface or address families how do you stitch it with overlay bgp address families we think this is a matter of local implementation policies that can be taken care of by individual implementations and therefore it doesn\u0027t need to be standardized lastly are the changes that I described for SPF can p can support multiple different peering models beat a route reflector or a controller based model the route reflector could be inline or a controller could be inline or it could run somewhere in the cloud and it should still work and the SPF computation here is fairly distributed so the controller doesn\u0027t have to run any SPF algorithms or a route reflector doesn\u0027t have to run it and therefore you would technically avoid any head flying blocking that you would have within distance vector where you run the best path compute the best path and then announce it out this should further help "
  },
  {
    "startTime": "01:12:35",
    "text": "reduce any delays and increase the convergence if any so that\u0027s all I have questions my way in fact I have this custom about this idea with some OTT guys some OTT guys who has the plan B GPL I GP for a long time so their interest to explore this solution although they believe it should be taken as a long-term solution but as they think it has big potential good thanks [Music] uncut the way VMware a question is in the slides you mentioned that will be replacing the phase 1 and phase 2 processes you know but BGP provides you know policies like kind of central to those decision processes and you should be able to you know continue having the capability to apply the policies you know at ribbon rip out so what I have thoughts there I mean why do you say that it\u0027s replacing so the policies that are applied within bgp whether they are at an inbound or an outbound they are typically done either before you run a decision process or after you run decision process those policies would still be supported by this protocol but the decision that the best path process that you would run post an application of a policy for inbound processing simply gets replaced with an SPF so you don\u0027t need to run any in rib evaluation into bgp best part which is the ten step best part process that is what it meant so it makes sense yeah sure welfare phase phase one and phase two processes do mention that you applied there even propolis is I mean in the phase one you do apply the policy before do you say that okay you know this route is now selected to apply to you know selected for the local rift and same thing for Phase two as well you do apply that out policy is outbound in this I believe are considered as part of the part of the decision process and in the phase processes so I mean it\u0027s maybe for clarity it\u0027s good to say that okay you you know you are applying the policies that is not affected so that you know that remains clear and then you can do the SPF calculations and all this time "
  },
  {
    "startTime": "01:15:36",
    "text": "and that\u0027s all good absolutely yeah question here are Alan Carr con cast so we are solving only one part of the problem here right it\u0027s just the SP fps I mean looking or going through the requirement from two or three different slides this is very specific one problem that we are addressing here all right that is correct we are after making a protocol specifically in this case BGP far more optimal in sense that it would help towards increase convergence if that is what you are alluding to yes that is what we are after right what I\u0027m trying to put in perspective here I mean there are many other things that are you know we captured as I won\u0027t call it requirements but as part of the wish list that we need to solve these you know ten five different things and what we are addressing here is very specific one which I I mean I agree like so we need some kind of SPF sort of thing right we need to bring some link state kind of thing in the in the datacenter but what what we are solving here is just one part I mean out of the ten things this is this represents very small portion of it yes and my answer still stands that we are looking at optimizing the protocol in a way that it helps reduce effects of a network by achieving a better convergence and better manageability by introducing a presence of a controller like model yes there are other problems that are listed out in the requirements and we can address them maybe this to address them you would require a different set of solutions while this is specifically focused towards solving some of the topics issues of the network so we are in agreement make sense this wanted to echo one of the other things that someone brought up that we are and we want to solve the problem but we are looking for more like simplification and removing features then adding large I mean I agree we need enhancements but it\u0027s not like overloading some existing BGP and trying to solve it simplification is also very important least that\u0027s required and that\u0027s my opinion ac Linda MA just a couple comments on some of the previous with regards to the phase one and Phase two BGP normal processing you really have to modify that for this Safi if you want to get the same convergence behavior of an IDP you can\u0027t keep it exactly as it is that\u0027s this one comment and the other comment is I think this all of the requirements that are actual routing requirements and not data plane or management or platform requirements "
  },
  {
    "startTime": "01:18:36",
    "text": "that we\u0027re in I think it covers more than just one of them I mean the whole thing about auto-discovery is a separate topic that you may or may not want to just use the routing protocol to do I think that\u0027s a separate you know you could you could separate it you can do it with BGP you could do it another protocol or whatever fully aj um we have a long history of things that we designed for the local area network goes out to the wide area network and gets in trouble one of my hats is a research i think i could model this in a data center i think i haven\u0027t had enough coffee to understand if somebody tries this on a large transport backbone and i don\u0027t think i want to have that much coffee so i think this has the limited for the moment i can put up with this on the limited scope of a data center um and i think it scales well for that and i\u0027d relies on the homogeneity and so on and so forth so i see that just red flags if people start to either add requirements to make it work on the lan or try to apply it to the way and I think we\u0027re stepping over the line there remember so if you actually are trying to do noose a fee-only to signal Lincoln note failure it appears that you can actually do it today without that noose a fee you mark a loopback address as the node with the community pick your own and the link address with another community and you can run your SPF in the background without extending protocol if you really wanted to do something like that today yeah absolutely I mean there are several different ways the reason to use new Sofia is to make sure that you have a clean solution and when you\u0027re running across multi vendor implementations you sort of don\u0027t want to make changes that are not seen across on the wire so a new Safi pretty much gives you a clean slate to do this and more specifically it lets you break the protocol into a different set of implementations or different processes if you want and scale it out as well if you need it right right just one more comment on that a Salerno coopera is that you know right today we\u0027re already using BGP LS for other purposes and we\u0027re reusing a lot of those in codings for the link and node attributes so you know if we want to change the behavior like we are to get the faster convergence and use and and do a different reception to determine which version of an NRI we use we really "
  },
  {
    "startTime": "01:21:38",
    "text": "need a different Safiye to attach this new behavior we can\u0027t just poke it you know you reuse and put communities I don\u0027t think I think what so good to have in this draft is you know take an example topology like you know typical leaves point apology in the data center because we are trying to do this for the data center and and and kind of walk through that you know today if you have BGP or OSPF this is how the routing works and you know this is how it\u0027s going to work with with this new proposal and these are the benefits you know so I mean that that will give us like a better understanding of you know what we are trying to do that\u0027s an excellent suggestion we could have we could augment the draft with a section in there describing thank you how the behavior would look like speaking as a commenter that you are forcing so as an implementer of you know one that\u0027s focused a lot on using BGP for a number of reasons one is I see a lot of potential personally in so much that you know a lot of us have gone to try to do holistic BGP deployments for a number of operational reasons second item to what Randy had mentioned potentially something like this were to to occur we could put applicability statement on it to define what is it specifically want want to try to address and so such that we don\u0027t allow some of this to kind of be used for other or people are aware of what it was tailored to do I got host adjusted some generic tool yeah sure Justin cirrhosis future request to the chairs rather than trying to compare apples to oranges it would be great to have all the apple and oranges and other fruit to be compared to single set of requirements in particular topology so I believe that was a comment for you yeah yeah so I took my chair hat off when I was speaking as a person who influenced data centers [Music] no further questions okay thank you thank you my point of order or question in reviewing these to the chairs may ask this at this time I wonder some of the things fix might "
  },
  {
    "startTime": "01:24:38",
    "text": "address things for the brick and mortar problems and might not a breast for the other are in presenting these solutions for example the last one addresses a problem we\u0027ve had a long time with ibgp and it\u0027s been the topic of study for a long time and might indeed help the people in the brick and mortar where whether it\u0027s doing everything that is in the the final thing that jeff indicated what is the scope of the discussion you\u0027re hearing is is this question germane to it because it\u0027s hard to determine what I\u0027m listening to and what I\u0027m trying to provide input yes so I think we mentioned that you know during the beginning of the the session here the scope of this buff is first to get you know to come to an agreement of an entire tournament understanding that routing in the data center you know maybe a different kind of beast than routing in the one environment so majority of the routing protocols right now they have been developed for the one environment like 15 years ago or something like that so you know in that envisioning yeah so we first you know tackle that by looking into some of the requirements and then come to an agreement that yes you know potentially something actually has changed and that has been followed up with you know some people have realized that they developed like new concepts new ideas and that\u0027s what we\u0027re presenting right now with the BGP SPF other people actually have tried to solve this changing you know require and all these changing requirements by augmenting existing technology and that is what is going to be presented often the new new new protocol so okay so the problem is there\u0027s a third option some of the things we\u0027ve looked at it for a time like the bgp SPF LSPs but they didn\u0027t really fit what we had but again we need more requirements out of the brick and mortar and the rest in order to say gee it may not fit the delay were characteristics but it might really help data centers in the brick and mortar to really do a much better job so I would ask you if you would maybe make a third category because I think it would be helpful to some of these people if I may comment so we are not trying to choose a kink and it doesn\u0027t have to be single thing for everybody if your IT staff is happy with OSPF maybe just reducing flooding or optimizing this would be good enough so mileage varies per company per type of IT staff so it\u0027s important again to make people aware there are different solutions and it\u0027s a it is good to provide guidance for implementers for designers to what it should be what people are looking for "
  },
  {
    "startTime": "01:27:42",
    "text": "[Music] J would be good if we didn\u0027t try to boil the ocean I know it\u0027s a tradition to do so in the ITF but having a narrow focus on ice I mean I love the fact that slide seven was great you know and um excuse the drift but David C Karros a very famous Mexican painter of a previous century who did ceilings and churches among other things he has one of the man who was so open-minded that his brains fell out I don\u0027t think we need to do that here thank you about 30 minutes you guys hear me yeah okay that\u0027s constraining good morning yep whatever time zone we\u0027re in so I\u0027ll be showing something radically different I got interested in this specific IP fabric problem ten years ago well it was might be fair because making Mac was like the hottest thing and work with people you know along those bgp angles looked at eyes eyes modifications being around all these things for quite a long time and these requirements were emerging I was talking to a lot of people and what I observed is that the most interesting angle and this IP fabric development is that we have a chance to build a component we have a chance to build a ram chip yes we are ITF and we networking are used to OPEX right we were driving insane sizes of IT departments in corporations and that can be done right we can continue on this IP fabric along those lines right and hyper scalars are showing that objects is free well reality for the majority of people is not that right a very interesting angle is to get the IT departments down and make bandwidth networking a consumable like you go to fries and buy a RAM chip I know what does a Cass configuration on RAM chips so basically what I my angle is that I was looking as the most important requirement for a good IP fabric data center fabric solution would be something that would have a zero op x right so you pay you cap X and you pay you cap X as you go to get more bandwidth and you don\u0027t have to scale your op x train people configure stuff build controllers do all these things so what it boils down to is that this thing is not working ok it is so the requirements draft is a good wash list right like lots of good governments have been made like 512 is not really the "
  },
  {
    "startTime": "01:30:42",
    "text": "reality 512 ecmp reality for most people but if we do the job well it will how much memory do you have in PC the memory has been done well as a component look look how many core what what what capacity you\u0027re running how many CPUs are running how many solid state how much storage are you by and compared to what you were buying 10 years ago but it isn\u0027t because the solid-state disk are so bloody complicated to run like punch cards one of the main reasons is that that stuff became a component and the volleys were driven down and prices came down volumes are prices down called the McKinsey curve so how could we take something like an IP 5 break there and make a bandwidth and easily consumable component for corporations whatever right whoever needs bandwidth can consume it very easily so the Walsh list is pretty good but there\u0027s more stuff that needs to be met for it and that\u0027s kind of the additional things that Rift us on top and I\u0027ll be not be talking you know if you expect protocol formats and understand how the protocol is put together in a Gradius detail there\u0027s a draft there\u0027s more authors on it and so on I\u0027m giving here more of a pitch why it would make sense to do something radically new and why the stuff just incrementally progressing with what we have will not by bring us into a new landscape all right so the most prevailing thing is that we need zero zero config CTP you just punch those things in just like you punch in RAM chips buy more switches buy more links and you\u0027re done so there\u0027s two flavors to that you can assume that people miss cable that leads you into different area where you need some minimum configuration or you can assume that people cable correctly which leads you to a complete zero provisioning solution and the drafts that I put out doesn\u0027t have it yet we have it solved so the next revision will talk about that so we can really make an IP fabric zero config you just buy more switches and you cable them up the next one is an interesting observation that if you look at the data center fabrics the main volumes are tours and servers and those cannot hold large fits all right the FIPS have to be small so on the tours you really want d4 routes because that allows you to run a really cheap silicon the next one is that we will be going to high degree of ecmp because otherwise you have to build very deep fabrics and the delay starts to go up so the way I see the economics and the delay requirements will force us into a - out so I think a solution where we have to address fairly - outs is necessary there seems to be that the prevalent thinking is still that we will need some kind of a traffic engineering on the fabric so get this flows around in different ways depending on load and so on so rift address is that I do not "
  },
  {
    "startTime": "01:33:43",
    "text": "think long term that will prevail we do not optimize seventeen different types of RAM and fourteen times of storage okay we just have a lot of it and we make sure that we can saturate all of the stuff that we buy and I think that\u0027s where the IP fabrics will be going we\u0027re talking IP fabric we know talking when right the problem changed when you are restricted by geography expensive bandwidth a lot of top to Malaysian problems here we are talking locality where I can put this thing in a highly structured way together predictable way together and just by very cheaply and quickly a lot of it and provision it if it is zero touch provision we nevertheless have to see the whole fabric fabrics will not become well we have in RAM chips ECC I think the fabric will still need more so I need some kind of observation what\u0027s happening inside the fabric where do I have lost is what\u0027s fails so we have to address the requirement did you see the whole fabric at a certain point the Rif does that as well the carrying the opaque configuration I skip over that I think that is very valuable they\u0027re taking out of production quickly I think is a real problem switches will need to be taken in and put taken out and put in into the fabric relatively quickly and during this time you cannot afford black holing okay so I think that being able to take something within only second out of production and put a new one in is an important requirement the automatic desegregation falls out from the other requirement where you really want you towards to be cheap if you Tour\u0027s have to hold large fits the silicon will be expensive and there will be lot of sloshing information which will lead to convergence problems all kind of anomalies but once you are gate and links fail or node fails you have to da greg 8 so that has been well explaining the RFC 73 98 don\u0027t remember the number and what is very beneficial if if you keep a minimal blast radius so what would I mean by that if you grow this fabrics and you end up with the solution we\u0027re adding a virtual machine or a link failing shakes the whole fabric you\u0027ll be inherently limited in scale and stability of this thing and he has to do it multiple facts like how much information does note have to hold how much information do you have to exchange to converge the whole thing so everybody has an information necessary and the fluctuations on the fabrics are actually quite phenomenal just on large fabrics just rolling dates of servers are generating quite a significant amount of routing churn and I think it will only get worse because "
  },
  {
    "startTime": "01:36:43",
    "text": "it is a consumable okay so people will just do whatever they do without thinking what stress it puts on the underneath and align infrastructure fabric should become an infrastructure so those are kind of the additional requirements what I see when you drive the whole thing towards the 0 of x and y I think it justifies a completely new protocol or rather this set of requirements to get you such a low of X fabric is not possible to meet with the extension of the traditional protocols which I helped to invent and laugh and make a good living for a long time alright so what I show you is quickly general concept because the concept is radically different from the routing we did so far I show you an example how an automatic desegregation would work which on traditional routing so far no one addressed and I think will be very difficult to address and I talked about small data about horizontal links to show you how different the whole thing is working and then I give you a couple of more things that fall out elegantly out of it yes but it\u0027s so new so either we stick to the shore and sail along and a lot of people may be comfortable that way or we leave to shore and that\u0027s the only way to discover in new ocean that has been done before we were in ATM in the frame relays and things have been done certain ways and there was the only way to do things we\u0027re in a different place because certain people questioned circuit switching which was considered completely crazy initially by the way and of course I must be crazy I am an unreasonable man if I\u0027ll be a reasonable man I would try to adopt to the realities as they are I try to change reality having said that what\u0027s the concept that falls out and it all falls out from pretty deep thinking a lot of math and so on but that is irrelevant right the product is the consumable is this so when you look at the fabric what strikes you is why is it such so different is the irregular pattern is enticing but the fundamental change compared to what we are running today as routing as generic routing is that we have a sense of direction so it is a ordered lattice whatever it means so assuming this compass direction we know it is not about itself and with that of course we know this east and west which is interchangeable but it\u0027s different thing we can kind of assume some kind of numbering of the levels so let\u0027s just assume the Leafs are level zero and then we go level one level two recursive so having this sense of direction at the head and the tail the first concept that we applies that we have this topological sort that\u0027s really what it means is the sense of direction I\u0027ll we we put the concept of link state "
  },
  {
    "startTime": "01:39:43",
    "text": "flooding going up only so the Leafs propagate up and everybody propagates up so the guys at the top have basically the whole topological information and can compute trees and that is actually necessary because once your traffic hits the super spine you must know which way to go you have to choose whether it\u0027s left or right you can just the buck stops there right everybody passes the buck up but the buck stops up with someone and that worst case is the super spy on the other hand when we going down we can just use distance vector routing and in most extreme case just propagate D for outs just in initiating for outs so the tours at the very bottom end up simply with D for routing they end up with a d4 out which has a high degree of ecmp and if we take this concept farther we can take it all the way to the server which becomes quite interesting and if you think that this we can pull it down to the server the requirements the lower you are in the fabric that the easier it gets you basically just do D for outing ecmp D for out so that\u0027s that concept so it\u0027s put together now the last requirement is not not the last concept that you have to add is that not that obvious the clothes fabrics which is what is mostly used do not have horizontal links it has to do a lot with blocking probabilities and your fabric starts to become behave funky if you do that you can do that but you run certain risks so you want to note at the same levels to see each other and since they just propagate this vector down and you only float up flood up north the red nodes would not see each other so to make them being aware of each other and the links the state of the links we have to bounce off the lower layer and it\u0027s completely recursive again so every layer bounces the upper layers note information up so the red nodes are aware of each other the Leafs are not aware of each other and there is no need now why would that be so one example so once we aggregated everything up and links fail we could black hole very easily and that\u0027s one of the examples so that\u0027s showing a hyperplane slightly flip problem not very relevant so what we have in here are the hyperplane of two red nodes at the top and green aggregation switches and the blue Leafs and if you look to the left that you see that all the defaults get distributed all the way down to the Leafs from the right side from another port we have a prefix a p1 that is being advertised flooded flooded up right all the flooding goes up so the "
  },
  {
    "startTime": "01:42:45",
    "text": "red nodes can compute down into p1 unless the orange link fails a worse was never there so imagine what the routing would look like the Leafs from the left have just defaulting so they just pump the traffic up so if they pump it to the left green node everything is fine so I blanked out the other hyperplane through the other green switch so let\u0027s not even consider that and the green switch has defaults again - the red spines so it would just try randomly load-balanced to traffic which all works fine if the fabric behaves correctly or is you know in perfect state but if the orange link fails and you try to go to p1 through the top red node you will black hole so you need some kind of desegregation to prevent black hole \u0027ok now if you run this kind of protocol and we remember the red notes each other because there\u0027s a reflection of the green layer the lower red note can after the SPF computation understand very easily that the p1 cannot be reached by the upper node because the only possible next hop is the green switch on the right and it has no adjacency to it and the reaction is very simple you just disagree gate the p1 in the lower red node so you don\u0027t only advertise the d4 out but also p1 that does not propagate to the all the way to the bottom to the Leafs observe it just propagates the green layer so the very left green switch will have a forwarding table d4 out over those two red but the p1 only through the lower and the longest prefix match will take care of itself it takes a while to sink in because it\u0027s so novel but it works like a charm so what you get is a maximum aggregation to make sure that your all you blast reduce our minimum that the Leafs which is just full defaults and on failures the algorithm again in kind of synchronous fashion observed as a fully distributed a synchronous algorithm will take care of healing the fabric for you just like a solid-state disk just relocates a factor which failed which is exactly what you expect you don\u0027t want to reassign sectors on the disk when it failed is it used to be the case as far as the remember on magnetic which wasn\u0027t a lot of fun as another oh we can talk quickly about what the desegregation does on the horizontal links so observe what happens in a topology like this a b12 and z word the link between 1 and C would fail ok so the B would get rigged desegregated on the second switch and the horizontal "
  },
  {
    "startTime": "01:45:46",
    "text": "links behave just like the southbound links so a would see B and go through 2 and if anything goes wrong y1 will also see B and traverse the horizontal link to go to through 2 but the horizontal links are only used in failure cases ok because the default will not be propagated over horizontal links normally we have also addition that for the leaf nodes and that\u0027s what people do is if you run a horizontal link it is used as a leaf to leave shortcut so nothing will traverse it but you can shortcut to servers if you have very fat floats um so that was just a small smattering especially the disaggregation is very novel and only made possible by those concepts what falls out on top when you run this kind of very novel approach to routing is that we can solve a lot of stuff which has been worked on for a long time and cannot be addressed with traditional routing and one of them is for example automatic flood reduction which is a significant problem if you go very wide in terms of fan outs on iGPS so once you start to flood up a couple hundred adjacencies and you try to implement it you understanding the implications of that so you can use the fact that you have such a dense connectivity to load balanced flooding but normally that has been done through OPEC so people configured special flooding meshes and to trees and they made sure that don\u0027t overlap and so on this protocol can run automatic flooding reduction low end and flood balancing just like EVP and active active so I\u0027m not drilling into detail because then will be here for the whole day like I said the leaf to leave bi-directional shortcuts are allowed we do have a flooded DV overlay which allows policies and can be used for something like traffic engineering okay so the requirement is solved like I said what it will be used we\u0027ll see the packet formats have been pushed towards being completely model-based okay so that was not feasible performance wise until fairly recently it does not at this point in time posed a significant roadblock it can be done at very high performance the advantage of it is that we come off of very uh north or nor pocket encodings which have been optimized now over a long tradition in networking - for performance we were always bandwidth restricted right or also CPU restrict in terms of processing those restrictions are going away especially in data centers we have enough RAM we have impressive CPUs we have been do it to burn ok what we are lacking with the traditional protocols "
  },
  {
    "startTime": "01:48:47",
    "text": "is that the turnarounds to introduce any kind of feature is very very long we have a lot of non orthogonal encodings we cannot generate code very well everybody is writing the note parser with their own set of bugs and we go through those rafts which frustrates a lot of this data center guys who see the fabric like a CPU that\u0027s just something that should there should be enough of it and it shouldn\u0027t get into my way I don\u0027t want to deal with routing bugs that crash my box so that\u0027s one of the way to innovate the protocol much faster the much lower risk because you\u0027re on a model so the code is largely generated and it\u0027s much harder to build non-orthogonal models you can but it you know it sticks my out much more the channel deliver is actually arrestees agnostic so the initial discovery goes of a multicast it\u0027s really still the simplest way because you don\u0027t have to configure anything right but once you get to flooding distributing traffic the channel really don\u0027t matter all that much we have all these religious battles you just go hope to help and you take whatever whatever you want you can go over UDP quicks promising we can run it over TCP each of them has its pluses and minuses one thing the drift addresses is that it gives you very wide what you would consider LSA spaces right it is very feasible today to maintain a lot of information elements in IGP which earlier was a restriction and their BGP had a leg up one prefix change you send one a one update rift gives you the possibility to generate an LSA per prefix it\u0027s an implementation thing and we the spaces are wide enough the identifier spaces that you can very easily come up with implementation like that and basically have the efficiency of BGP or actually significantly better because if you think that you maintain 500 TCP sessions and you flood this change over this 500 sessions out unless you play some reduction game you have to generate 500 updates for 500 peers and even if you\u0027re really smart and you generate the update once you are cranking TCP 500 times if you go over UDP this is much much cheaper right you don\u0027t have 500 tcp state machines trying to get the work done you just push this thing over UDP out there purging has been completely taken out that\u0027s a small detail and we have a key value store support which means you can push during convergence time certain things which are important for you over the fabric and one of them is for example service configuration when you run an overlay you want a very quick service configuration you don\u0027t want to convert your each ability which is really nothing it\u0027s like your PC booted now what unless I ran an application well cool I got the login so if we put a key value store support to allow for example service configuration into the rich "
  },
  {
    "startTime": "01:51:49",
    "text": "ability protocol the moment you reach oblique converged your basic service set converged so it\u0027s like a mac booting you mac doesn\u0027t boot into well it boots into login you need you need to get in but once you log in all you windows pop up all your plication are up there because the empty desktop doesn\u0027t do any work for you you have a working set so we can try we give people a possibility to very easily to put the working set onto the reachability protocol so when they bring up the fabric the services are up as one possible application no good so summary we have a limited time here so when you look at drift and you give it the thought necessary if your interest into technology you find that you get advantages of both of distance-vector link state because the vexing thing is the data center fabric if you want to really drive it zero or pax is neither so you get the fastest possible convergence because everything is being flooded you get automatic detection of topology which is really precondition for ztp unless you run a controller and you believe the controller is a ztp solution which i don\u0027t you get a minimal route on tours which allows you and actually the solution gets simpler and simpler the more you go towards the server the server is a very simple implementation so you can pull it all the way down to the server so that allows you to run cheaper and cheaper silicon at the bottom where you have more and more of the stuff right that supports very easily very high degree of ecmp because it\u0027s where the ecmp kicks in it\u0027s a link step what distance-vector is not very good at ec in pink it can do it but it\u0027s no a little bit of a crutch we can get very fast decommissioning of nodes out of the fabric because of overload bits on IGP it\u0027s the same mechanism and we have a maximum propagation speed because we don\u0027t wait until we decide something like in a distance vector protocol right is the link stage we just propagate the information you blast it the maximum speed out and because we have this flexible number of prefixes we can get to be GP efficiency in terms of a prefix / update element if you want to run it that way but we do not face the disadvantages of both in inter fabric so the flooding is normally what limits the IGP at certain point in time performance wise this can reduce flooding automatically there\u0027s a load balance flooding sorry and we have automatic enable detection which of course can be added to BGP I mean any protocol can be molded to the point where it looks like another protocol right it\u0027s not software software which is right enough of it it looks like just like something else and it brings some advantages again driven all toward zero up X which the other protocols will have a very high a hard time to address unless we get to the point where they start to deal with the concept of north and south this is really like the deciding piece do you build the protocol it understands the direction of the fabric or don\u0027t you so "
  },
  {
    "startTime": "01:54:51",
    "text": "it does to automatically segregation on failures the key value store is another act forget horizontal links the minima blast radius if you think through the protocol it skills extremely well because adding more leaves does not push any load on any other leaks Leafs or other ports it only adds at the very top information okay and when something fails the distribution radius of the change is very very limited and what all the folds out of the protocol and that\u0027s very non-intuitive the protocol cannot loop there are no loops here the packet goes up until it turns down and then it goes down which allows you something very interesting it allows you to run a completely different class of path algorithms rather than just a shortest path which we all laugh and try to stick to so riff can actually saturate all the feasible paths through the fabric minimum hop paths and it\u0027s up to you to decide how you use the weights you can just blast through all the feasible paths or you can just take the shortest one or short the second shortest and so on modern beats over PowerPoint yes lots of real work has been done so this is not just shut out there if you interested further to participate look at the stuff have a chat with me we take it from there and thanks for your attention and then he\u0027s already standing in a very threatening pose so hello so you about like minutes for Q\u0026A but before we go there anywhere the blue sheets who has the blue sheets nobody has ablutions okay yeah go ahead please Dan Bogdanovich so I have I don\u0027t understand the automatic flooding reduction and I\u0027m trying to yeah because I took out this one slide simply not enough time seemingly boring we talked it yeah so and and and that is one part for me to be able to follow the other part is this is sort of the the basic but I have a it I have a it\u0027s a very stupid thing but your name is already taken by a company in Boston and you should really think about renaming it because they they have trademark on that my last name so there was only the grow work which expired long time ago so point me to who took the stuff as a trademark okay I I did a search I didn\u0027t see that oh it\u0027s risk I oh so okay another troop so Tony I have one question here I think you have very good control plane worked out perfectly well but the problem is with the failure the failure is "
  },
  {
    "startTime": "01:57:52",
    "text": "addressed by the aggregation right you inject the p1 down from the spine to leaves and you fix the problem but doing that it\u0027s actually convergence problem because it takes hundreds of milliseconds before you advertise the route in any protocol today going to re and from re so I think it\u0027s way too slow to actually be you know production quality solution so I mean riff can run without aggregation so that\u0027s one of you think but you pay a tremendous penalty on the tour since the ones it it\u0027s one simple answer the second answer go see me to look how fast it can go and that\u0027s why we need BFD we can get it around FRR times without FRR I\u0027m looking for something like an LFA addition to that if you can do it but technically you cannot write because you don\u0027t have information ahead of time so that goes down to the how fast do we have to converge right if people are really looking like in white area to go I want on my data center fabric to have something which is 30 milli 10 milli we may get pushed into LFA FR our kind of things no discussion I don\u0027t think there\u0027s a way around it but then the whole solution will become no more expensive and more complex if people can leave with something like 250 milli single failure convergence very comfortable with that ok and I found that this requirement is very squishy depending how people built their service architecture on top ok some people are very comfortable with like one second blips some people are very uncomfortable with 100 millisecond blips we both know that right so that can you FRR be strapped onto that stuff LFA yeah very easily but that\u0027s of course significant addition in terms of complexity I agree with your answer but it was just an observation yeah we wildly agree right if you if you aggregate you basically do a controlled line so if you have to unravel the lie it will take some time and if you go through a control plane there\u0027s certain limitations which today with good implementation it can take you in hundreds millisecond range but if you look 30 40 50 well that\u0027s why we did all the bf the LFA and all ever all the chip vendors are doing all the crazy stuff so I mean I can\u0027t change the speed of light and we can only push electrons about 5 gigahertz I mean next thing see me when entanglement is possible right ok ok so we can actually take the the question or to people still on the mic but then we gonna close the line to clarify the requirement drop specifically head requirement with support for BFD and none for LFA in regular topologies you don\u0027t wanna let say your links are per definition loop 3 what you want is fastly harsh and hard work so support for LFA from my perspective yes yeah so that\u0027s a shitty "
  },
  {
    "startTime": "02:00:52",
    "text": "box you don\u0027t want the real game is really that give you hardware kicks in and does the bf correctly in the first rehashing a lot of these problems go away but then of course we talk very close to the chip level right ok second three Google and are you assuming that the control plane runs on the switches what if the controller is completely off running on some other server I know I assuming the stuff runs on the switches does it so for the controller side it has two flavors one is the distance vector overlay where you can actually go and push prefixes and policies onto these things and they will be preferred onto the shortest path but when it comes simply to the shortest path convergence I assume that the stuff is distributed for simple reason that it will be always faster than anything you can push from a control all right unless there is some absolutely impossible magic involved ker-rah Cisco Systems it may be worthwhile because we are talking about different solutions over here to compare them in after implementing them to see which one converge better yes so I can give you a simple number like that converges three times faster than a host-based IGP both well implemented so that you get about 3x convergence speed-up over a reasonable like three layer fabric you know so this is a number I have and I don\u0027t long enough with BGP to know that well I\u0027m not here in a beauty contest and I do think don\u0027t think the convergence speed is really like the highest requirement what I found for the people I\u0027m interested in who are willing to pay for solutions this Europe X is really the most interesting requirements people who wear up X is free are very insensitive to a lot of requirements they have very drastically different ideas I found right I mean conversions one of the requirement I\u0027m saying just compare the solutions on all the DC requires that you have listed over here I couldn\u0027t parse that yeah okay it\u0027s Alan Carr Comcast just sorry that the lines were closed we need to to move forward I mentioned the two lost people were you know on the mic sorry so we can address that question if you have additional questions we can address it in the list then the DC routing list please okay so thank you Tony alright so one simple question we saw is this this was a second section of the Boff so one simple question for it for the group so with a show of hands this is the from a positive size so we\u0027re gonna ask one question do you would you agree that the proposed solutions is a logical or by the way would you would do "
  },
  {
    "startTime": "02:03:52",
    "text": "you agree that proposed solutions address unique challenges in the DC routing fabric space so from what you saw at least last you proposals do you would you agree that they address unique challenges in DC routing space show of hands it\u0027s about 40-50 okay perfect thank you okay so the next set of speakers so this this is the review of work that\u0027s going on in existing working groups [Music] dimension Cisco Systems and this is the work I co-authored with less and Sanjay so since we are talking about the routing in the data center and this proposal are utilized the other routing features has been developed in the last 25 years plus it supports full routing entries for quite large sizes of the PC network and it can also serve as the underlay routing for the DC and this proposal is auto key or discovery to support the CTP requirement and that this minimizes a link stages IDP flooding problem due to the DC spine leaf topology and this proposal also handles lake and the notes down events to avoid black hole so there is a one new trv is proposed for this proposal and that the P bit is to indicate which tier level the node is at and the tier in the current proposal is from 0 to 15 with 15 as a amplifying the level and that the orbit indicates this is a leaf on the adjacency the op it is for the spine to indicate you can use this adjacency as the default gateway and there are two optional sub t "
  },
  {
    "startTime": "02:06:52",
    "text": "redefine the four but like hole avoidance so in a very basic example here we have a few tears and to the bottom layer it\u0027s the leaf and the top one is the is the spine and the believe know the were set to the tier level to zoo set to the orbit and the flood its own LS people\u0027s the spine and the spine we are eternal in the hollow set to the orbit before the leaf views its self as the default gateway and that you are not flooded by LSP back to the live modes so the spine nodes has the full eye size topology database but the leaf nodes just point defaults towards the spine so it supports the more cutie level this is the oddity or discovery and by this we mean the tier 0 is defined as long as the tears or to find the all the other tiers are dynamical runs by by exchange the packets and when its own position so you can see that tiers view is believed to the tier 1 and the tier 1 can in turn be believed of but here to our notes so in this particular case are on the more light at the air 6 we have only flood its own LSP l6 toads the s3 s4 which is a human notes and the bear for example the s4 in the tier 1 will flood the air for l5 air 6 plus its own as for ports there\u0027s even in a situation so the last slides and talk about how to handle the linked our events to avoid the black hole so in this particular case talking about to the convergence actually this requirement is only a optimization because if if you think about this s 3 to s 6 the link is down and even if the l4 is sending a packet to s 3 to reach the l6 the s3 by PFD or some other mechanism you may need to know that it cannot further to ethics it can fold up link to the c1 or c2 so in the long run you don\u0027t want to do this because it increases the pack the latency but in a "
  },
  {
    "startTime": "02:09:52",
    "text": "short term what we can have to do as you follow the ports the upper layer and that the upper layer had a 4th apology of the bottom layer so that convergence can be actually served in this particular case but we in the long term we don\u0027t want to do this so this introduced tonight to the routing between pin the kiyose and that neck learning is the only necessary doing the tears you\u0027re in the tier 1 so that\u0027s all my presentation for this proposal thank you thank you so we have time for one question if there would be one if not that is okay also [Music] [Music] unless white no Russ White LinkedIn this is ice I support for open fabric this is the if you haven\u0027t noticed the section three of this agenda is actually the humble section we\u0027re not omitting any protocols and doing weird things were just making simple things another thing I\u0027ll point out is that this draft is compatible with or complementary to the other two drafts in this section which are naming and Cisco and those guys we\u0027re actually working together to make sure that our drives use compatible signaling and things like this so we\u0027re just two different kinds of things on the same thing so the idea here is that with open fabric is that we really want to just simplify things we see a lot of complexity in the data center fabrics LinkedIn runs a pretty big data center fabric a bunch of pretty big data center fabrics we just want something really simple to do to give us IP reach ability and label distribution and we don\u0027t want all the policy stuffed into it that bgp gives us we don\u0027t want anything else we do want to link state protocol because we want to have full topology of the information so separate reach ability from possibility from policy we want to minimize or eliminate configuration the way this is set up that there\u0027s only one router on the fabric that needs to be configured and everything else is computed and can come off of servers or configuration systems or whatever you want to link state when optimize convergence and optimize scale so what we\u0027re going to do two separate complexity from complexity is topology and policy is we use a distributed control plane like I said to give us reach ability and label distribution and then we want to have a controller based overlay using something like piece F or I to RS or an undefined thing that actually pushes policy to the top of rack switches and uses segment routing to do all te and other things like this so that\u0027s kind of the thing that we\u0027re doing here the goal in distributed protocol is to build the simplest possible distributed links a protocol we don\u0027t want any policy we don\u0027t want any "
  },
  {
    "startTime": "02:12:52",
    "text": "configuration we want any extra stuff I don\u0027t know about your data centers but in my day Center one of the problems I have is I have sre and net ops guys and Eddings guys who want to go use BGP policies and stuff to play around and make traffic engineering work and we really shouldn\u0027t be doing that because it just creates all sorts of weird stuff all right so fabric location is really simple as long as you have one top-of-rack switch or t zero configured as a t zero when you boot the fabric you can compute using the hop count to and from that T zero as defined everything else you can compute your tier level no matter what your topology is from that one T zero once the fabric boots and a bunch of other people have computed themselves as T zero the requirement for the single one to be configured goes away the requirement for a single configured T zero box is only to boot the fabric when you have nothing up to begin with so you can read this in the draft it\u0027s actually pretty simple trivial there\u0027s some IPR around it but it\u0027s been released to the ITF and stuff like that so forwarding optimization there are two stages and forwarding opposition one is forward optimization are flooding I\u0027m sorry flooding optimization one is forward so we look at the flooding coming off of a t0 and going towards the spine there is a particular way that you use neighbors neighbors which by the way we have experience in from mobile ad hoc networking these are actually extinctions in OSPF that does something very similar to this already in the field deployed and working so we have experience with this and we know this works in this direction Nico\u0027s who works for LinkedIn suggested a reverse optimization which is just to not flood back down the SPT towards the originator since you already know what the topology looks like in your LS dB so this actually tells the for the back flooding the way this turns out is depending on how you have these parameter you can set parameters for how many neighbors neighbors you want to pick a reef letters or things like that every is on the fabric and by the way this is grounded in is to is if you read the draft only receives one or two copies of every LSP on the fabric so once you know your location we assume you\u0027re gonna connect to a controller you\u0027re gonna get things like you\u0027re gonna get things like DCP pools if you need them you\u0027re gonna get maybe you\u0027ll get your label pools if they\u0027re not calculated locally things like this all of this stuff is going to be pulled off of a controller probably using a pub subsystem or G RPC or something like that to make it very fast and efficient so that gives you the ability to pull receive status as possible we are currently working on an initial implementation this in the is IIST part we don\u0027t have an implementation underway or undertow for a controller yet we\u0027re still working on that but we actually have an initial implementation undertow in free range routing so there will be a free range routing implementation of this hopefully well no no mechs three to four months five months something like that so further updates to current drafts in the queue I have a couple more things I need to do I need to change something about the hello processing to make it a little bit simpler we\u0027ve received a lot "
  },
  {
    "startTime": "02:15:53",
    "text": "of help from the community I\u0027d like to say thanks to everybody who sent the comments on that there\u0027s a huge contributors list it\u0027s been really fantastic getting a lot of people from the community so that\u0027s kind of it for eius eius support for open fabric any questions at the mics I see nobody is there read the draft and make comments and tell us whether it fits your use case or not and what could be changed to make it better I know we\u0027re not gonna solve every use case with this draft and that\u0027s okay it\u0027s a pretty simple concept and we\u0027re just trying to do a few things hi this is a mom from Wahby Russ I send you some comments regarding the monkey topology the current draft what is proposed what you are removing from ISS you described related to multi topology lot of things not remove a lot of stuff for monster - I just went there okay good yeah that\u0027s cool yeah one of the parts of the draft is we have a lot of stuff that we\u0027re pulling out of I so Tim 589 to make the protocol simpler we\u0027re trying to get down to very small lines of code [Music] I\u0027ll say so this work is probably you know this is probably gonna be a tackled by the IITs working group you know going forward so hello everyone so you have five minutes and it\u0027s gonna be a hard stop unfortunately let\u0027s talk about how to reduce the OSPF ice ice flooding in last kill data center the reason that well PF eyesize in not my suit boeing last skill data center networks the due to is flooding problem but on the other hand the beauty of the IGP is it can support a full view of the global topology that\u0027s why many other guys pursue to extend the protocol such as bgp SPF but all I had mentioned be SPF may be a good candidate for a long-term solution this solution it tended to be a very short-term solution the course is required very minor changes to the existing MPs protocols since its of time I will not discuss many details about the solution I just saw the general idea of this solution it\u0027s a mix of the central ID link state distribution and distributed them SPF "
  },
  {
    "startTime": "02:18:54",
    "text": "calculation for instance OSPF I see rotors within Clause just need to exchange hello packages to fight each other there are no need for them to further exchange RSA is wires piece they only need to exchange arrises and piece with the controller where the management layer in fact the controller is exactly as PF dr OSS yes so you and you can sing from this finger it\u0027s heavily depend depend on the existing Perko capability the only requirement is you need a dedicate network manaman network to in connect them controller and all the routers within cross networks if you are familiar with google fire pass there are many familiar the similarities between these two solutions the differences include first this is amperes this is a high tier protocol complaint solution second it then have heavy dependency on the reliability of the meta meta layer the counterparts in the high passes CBN because this solution has a very simple rollback Magnus in the world when a router in the cloth lost connection to the controller it can roll back to the traditional IGP mode that\u0027s all any comments suggestions all right thank you sir yeah okay so we\u0027re gonna ask one question at the end at this point and then maybe comments from the ad if he wants to share so the one single question this point then is noting that contributing to any new solution is not mutually exclusive so show of hands who here would be interested in participating and contributing to potential new work and in the routing area yeah so we\u0027ll 40-50 okay by the way I will publish the questions I asked with kind of the reception we got onto the list after just for for note okay so Alvaro that "
  },
  {
    "startTime": "02:21:57",
    "text": "you will have any comments you wanted to share with with the Hoff if not we can open up to general additional questions or comments from the from the group I am a little bit than I\u0027m the running Edie so yes well I was hoping you guys would ask next if you have another question you\u0027re gonna ask her not they do whine about generalized vs. no not that one but so the the purpose of office you guys said has been really beginning was to try and see what the landscape is I get some requirements see what people are doing and figure out if there is a need for a new work and I think that many people raised their hand and said yeah we may be new work to do to be done the ITF now many of you I think also raise your hand saying yes we would be willing to work on something new now there were two proposals here of New York what I think I want to ask extra is to get a sense here in the room of people who be willing to work on specific proposals that were put forward in other words if we\u0027re going to use this buff to in the future at some point maybe gets whom you work in the ITF it would be nice to know if you want to work on that solution or not that make sense yes hum something anyone raise your hand whatever okay good so so the question we ask is this two questions since we have two proposals right assuming that we go forward with work in those proposals let\u0027s take the BGP SPF proposal because it was presented first how many of you would be willing to contribute to that work meaning before you raise your hand you know where\u0027s one hand meaning would you be willing to not only read the drafts make contributions the reviews you\u0027re participating some kind of effort the type of effort I don\u0027t know it may be a new working group it may be something that we roll into RT WG or something else but how many of you would be willing to contribute to that effort of bgp SPF you can now hands there\u0027s a light back there that I can\u0027t see anything okay and then you guys taking some take notes okay and the other question is of course for the other proposal for rift how many of you would be willing to contribute to that effort again read the drafts discuss on the mailing list proposed enhancements the reviews you know it said are all the work that would be needed to be done there okay now you can go racer here for that okay and I noticed that some of you raised your hand twice which is fine "
  },
  {
    "startTime": "02:24:59",
    "text": "one of the reasons that you know wants to do this poll is because we obviously have limited resources in the routing area right so you can\u0027t go to 27 working groups in a week so it would be nice that if you do want to contribute to both efforts if we go forward with both that you do participate in both and you don\u0027t end up with three people who are working you up and two and the other right so that\u0027s it I think this is what we want to get out of out of this room right the the sense that there is whether there is a problem or not and if we\u0027re going to go forward with anything this is a decision that we gonna make with the chairs and the other route any of these hopefully shown as to what do we do next did we charter something did we do more discussion did we get you know some more presentations on before we move somewhere else so that\u0027s all I have thank you so much okay so we actually have about like two or three minutes left before you know the end of the time here actually so if anybody actually you know has like any other comments or things they you know want to radiate from their hearts or whatever now is your opportunity otherwise we gonna close the session nobody okay thank you you "
  }
]