[
  {
    "startTime": "00:00:12",
    "text": "okay i think we're top of the hour good morning good afternoon good evening wherever you are welcome everyone um this is the computing in the network research group meeting with jeffrey he uh eve schooler and and me uh jose mourpitzi uh we are the chairs of this uh i think exciting uh research field that uh i think is is um more and more important um in the development of the internet uh you move the slides eve so can you hear me by the way i think everybody is uh aware of this uh i think it for this group yes it's important but also uh you know that we're recording this because we are um still virtual and i guess even when we'll go not virtual we'll do it i think what's also very important is all the uh disclosures of intellectual property which is on on the next slide or something and uh the code of conduct uh which is also very important and i think a few of us have had experiences that show us how important it is uh eve maybe you want to go to the agenda very fast sure can you hear me my audio and can you hear me am i not on um on this slide yeah i think we can yes so this is the oh you can hear me okay harassment copyrights and everything uh next one sure but maybe they can not hear i think it's very important okay we are meeting at the same time as uh"
  },
  {
    "startTime": "00:02:01",
    "text": "the ietf but the irtf is not there to do standardizations it's there to conduct research and you will see from the agenda um um you will see from the agenda that the um there are important uh um research papers that are going to be presented today and i know there was an audio issue and i think i fixed it so thank you for the people who uh mentioned it um uh next slide eve so uh we have a very uh packed agenda uh so you know i'm already two or three minutes into this so we're going to go very fast i think we're very very happy to have scott chanker to present the extensible internet which was a ccr paper and essentially raises a lot of interesting questions about the future of the internet and also is is connected to some of the uh questions that we've had in this group so we're very lucky with this then we have the very interesting in network aggregation paper then also the information-centric data flow for distributed computing from dirk and um we we had well it's actually ideas but actually there's only one alessandro because it's a holiday uh is not going to present i'm going to present this it's about an operating system for distributed applications and in coin we are very big on thinking that the internet is moving to be more like a computer board and if we have a computer"
  },
  {
    "startTime": "00:04:00",
    "text": "board you need an operating system uh next slide then we have the drafts update uh obviously a research group is not too about doing only drafts but we do have very interesting work this is happening in the drafts with this research group and we would like to give them a chance to talk about the ideas so use cases again uh to show that there you know there's a way to use these things transport protocols because the this has been an issue once you start putting uh computing in the system and then uh the security and privacy and again a big issue we have a new uh draft that was submitted by china mobile and then we'll have uh we would love to have 10 minutes left to talk about future groups and future meetings and we would like this a little bit to evolve into but i think uh the main goal is to have the papers presented so we're going to do this the meat deco uh please uh you found us you're here this is good yeah you found you know if you're there you found the meat deco which is good um the live minutes uh it's integrated to meet techo codim uh the mailing list uh well if you're here you probably know about our mailing list uh if you know if you don't well please do that and you know we have a ton of documents um turk your document expired that needs an update because it's an rg document we have two rg documents we have again a new new drafts i think this needs an update because we have the new draft from china mobile also and and we have a ton of other um drafts that you know are a different level of uh maturity and essentially this is something that we will raise on the list the idea that we need to do something about it"
  },
  {
    "startTime": "00:06:00",
    "text": "the milestones um we've done a lot of it and and we need uh i'll go very fast because i know one minute over um we'll go very fast to uh the last thing which is we need a milestone review and we'll do it and now we have the presentations and the first one is scotch anchor from berkeley and uh the extensible internet so uh scott thank you very much for doing this because it's so early in california oh well thank you very much let me share my screen i don't see oh here no it's coming i just clicked on share screen oh okay good do you see it again yes here let me give you permission is it coming up okay this could people see this yes okay well thank you very much for inviting me i'm typically up at this time so this is not a problem uh this is uh based on uh i'll be talking about the extensible internet and this is based on a ccr editorial with 18 authors there's a much smaller group that is trying to actually bring this into reality and i gave a talk on this at another research group and so i apologize for people who saw that talk adrian and others but this will be a large overlap but the goal of this talk is really to give a very very brief overview of extensible internet just enough to initiate a discussion both about the merits of the proposal"
  },
  {
    "startTime": "00:08:01",
    "text": "but also how it relates to the the charter of this research group um and obviously more details would be available in follow-up conversations so the core subject of the talk is really about architectural change so when i say the word architecture i'm really referring to the arrangement of the data plate functionality uh that is the layers and the basic functions they've been assigned it's not about specific protocols ipv4 to ipv6 it's not an architectural change in in my lexicon nor is it about the control plane um now there's been decades of architectural research trying to make changes to the the basic architecture but i i think it's pretty clear that you know over 20 works of 20 years of clean slate architectural research there's no discernible architectural impact and the public internet at least in the eyes of certainly the researchers i talked to seems doomed to architectural stagnation that we've tried it just we don't see any movement but that's not what the hyperscalers think um the cloud and content providers are building their own large private ip based networks they've got many points of presence and what's relevant to this working group these points of presence apply extensive in-network services flow termination caching load balancing and so forth and these services have had a very significant impact on customer latency and reliability so they put a lot of money into it because they actually see very tangible benefits in their user community now this is great for applications users get much better service but it's bad for the internet because the internet is becoming increasingly balkanized these private networks are defining their own in-network services the public internet which can't do this kind of interposition is lagging behind"
  },
  {
    "startTime": "00:10:03",
    "text": "so that we're in a situation where the the benefits of in-network processing have been shown but the public internet can't do it because it's not part of the architecture so rather than these ad hoc and proprietary designs what we need is to figure out a way to build this coherent this in-network processing into a coherent architecture and you might say at this point but didn't you start off by saying the internet is impossible to change so you know what's the deal and so we don't think it's impossible to change even though i've failed personally many times in the past but this talk is about why we we think there's a chance and so the rest of the talk much like a seder is going to be organized around four questions why is the internet so hard to change how can we overcome this barrier what would this new internet look like and given the failures of the past why do we think now is different and so for the first question you know you know far better than i do about the architecture but let me just review one of the critical aspects which is that layer 3 has two roles on the one hand it was designed to allow layer two networks to interconnect and therefore by definition it must be in every router that makes it almost impossible to make change that is it's in too many places it's got to be implemented with very good price per performance uh so it's typically baked into hardware so changing that is impossible now and in the future i don't see that changing on the other hand it provides the service model to host that is this best effort packet delivery is exactly what anything you want to do on a host that's the service that you see and you have to build on so it has to support all application requirements and these requirements are becoming more"
  },
  {
    "startTime": "00:12:01",
    "text": "stringent which is a reason to change the architecture and that's exactly why the these hyperscalers have built out their own networks to extend the the architecture for their own purposes so they can meet these requirements but it's this dual role that prevents change because you have demands coming from above you have constraints coming from below and they are meeting in the middle in a single layer so the network must meet additional application requirements but the only layer that can address those requirements is also the only layer in the architecture that can't be changed so that's the the nub of the problem that they coincide in the single layer both the demands and the constraints so the second question is well how can we overcome this barrier and there this is where the extensible internet proposal comes in it's really very simple you use the current ip protocol unchanged you don't do anything but you do introduce a new layer above it we call it service layer or l 3.5 and the service layer offers new in-network services to host so this is the relevance to this working group is this network services this is flow termination and caching um and and beyond which i will discuss later and the point is this is really the architecturally coherent version of what's going on in these private networks this is really trying to articulate how to think about it in a coherent way so why does this solve the problem because it decouples layer three's two roles currently it's the interface to both l2 which gives it the constraints and l4 which gives us the demands and in the ei l3 is still the interface to l2 it does a perfectly good job of that but l 3.5 is the interface"
  },
  {
    "startTime": "00:14:01",
    "text": "to the hosts and there's no reason for l 3.5 to be in every router it can be much more sparsely deployed so it's more easily deployed and changed and at the same time there's no reason for it to be narrow so it can meet a wide range of needs so we've separated the constraints from the demands and so now l 3.5 can meet those demands while l3 is meeting the constraints or obeying the constraints so what would this design look like you have a service layer sl it's implemented in what we call service nodes these are racks of servers deployed at the network edge and here by edge i just mean somewhere close to clients it's not at a particular place in the network every host is associated with at least one and typically several service nodes for reliability and the typical communication pattern is packets go from the source to the service node associated with the source to the service node associated with the destination to the destination all the service layer communication is tunneled over ip and the source specifies which service to invoke using the tunneling protocol when the packet gets to the service node and it's this ability for clients to signal to the service node what service they want that allows this to go beyond what the hyperscalers are doing which needs to be backwards compatible to actually we're able to offer services like multicast where the host needs to know that you've changed the semantics of what they've asked so this is the basic outline of the design this diagram is just telling you that no matter whether you're at home or office or a cellular network or in a car or in the cloud there's a service node nearby that will mediate your communication now the key point here is that all these services that these in-network services that i talk about are in software"
  },
  {
    "startTime": "00:16:02",
    "text": "so standards are not detailed written specifications they're open source code and there are three necessary software components on the service node one is the service modules that is for every service whether it's multicast whether it's ddos protection there is a service module that's implemented that's running on a service node it runs in a standardized execution environment if this execution environment has a very simple set of primitives pack it in out ephemeral storage stable storage maybe one or two others but it's a write once run anywhere environment if you write your service module to run inside this environment it can run on any service node and then of course in the service node you need some kind of runtime or orchestration to scale up scale down recover from failures this doesn't need to be standardized you know there'll be open source versions available whether it's kubernetes or openstack or whatever comes next and when i talk about in-network services this is limited computation this is not where you run your machine learning jobs this is basic packet forwarding payload processing simple functions like caching so it's more complicated than just simple ip forwarding but it's not general computation it's fairly limited and these service nodes don't just have to be generic processors they can have secure enclaves they can have hardware accelerators but everything you do has to run on a commodity processor but if there happens to be an accelerator that can be used to to get better performance now choosing the services this is where you have some kind of governance process whether it's itf or something else you know it's way too early but there's some body that decides what are the set of public services and and their implementations and all of these public services are run on all service nodes that is if you're offering internet service so you're supplying a service node you"
  },
  {
    "startTime": "00:18:01",
    "text": "have to download all of the service modules that are in the service model that have been approved and so this is really the biggest change to ei meaning that ei brings this deployment model that is because they're approved software modules they can be rolled out they can be deployed there's no per vendor per domain decision process of is cisco going to support this is juniper going to support this that is 18t or deutsche telekom going to deploy it it just gets rolled out as part of the standards process now so after all this talk about you know the process what what are the services that might get offered you know the basics like flow termination and caching and load balancing which you already know bring a tremendous amount of value their support for other delivery models multicast pub sub redirection qos and qos here not in the inserv diffserv conduqs but in the sd-wan kind of qos that you can pick paths or different delivery vehicles that meet your needs support for security and privacy it's very easy given this architecture to build a very good ddos prevention but also if you have secure enclaves you can have scalable attestation so that a client when it's accessing google or some other hyperscaler can be assured that it's dealing with a legitimate node that is running the current google software you can have odns like privacy tour like privacy you can also incorporate other frameworks that are becoming quite popular they can be running on these service nodes istio and then oppa for the policies various telemetry kinds of frameworks and then support for radical new architecture something like icn whether it's the donor style or the ndn style this now just becomes a service you just roll it out you're not replacing all the routers it's just a piece of software there'll be host support uh obviously of course for all the services and"
  },
  {
    "startTime": "00:20:01",
    "text": "this just gets rolled out like another standard um so it really lowers the bar to this deployment and that's why we call it the extensible internet because once you get this uh framework set up it makes it very easy to extend the service model in rather radical ways this is the last question i have a question for you would you like to take questions from the audience now or at the end i've only got a few more slides let me take it let me finish just because sounds good i given the failures why do we think this might succeed and there are three reasons one backwards compatibility ip just continues to be used if you don't want to change anything you just use ip you know maybe 20 years from now that will go away but when we roll ei out nothing changes about ip and the kind of resources you need you just need the service nodes which could use edge computing or existing pops they have these facilities out there i mean we've talked to some carriers and they say oh you know we could run this tomorrow that we've got all these facilities available obviously there's more involved but this isn't talking about you know major capital outlays the second reason is uh fear is a great motivator um you know the internet architecture has resisted change and mark handles this great essay on you know why the internet just barely works and his reasoning is you know sort of you know basically why should it change unless it has to but what's implicit in his essay is that there's no alternative that the internet is just there there's now an alternative you know if you talk to the clouds they they basically think that 10 years from now the internet will just be the last mile and they'll be providing all the networking that's clearly their vision so uh there's now an alternative and so the internet has a simple choice it either changes or it shrivels to being a last mile provider and we think the extensible internet is"
  },
  {
    "startTime": "00:22:00",
    "text": "one possible change that will preserve its role and the third reason is that ei is based on a simple conjecture that in network support for applications it's important for current and emerging apps can be done at service nodes rather than each router can be done in software not in hardware and the point is the private networks have proven every aspect of this conjecture they're up in the running they're running at scale they're running with real traffic at an unimaginable scale so ei is really the architecturally co-version of what of the approach these private networks have already proven to work so that's why we think it might succeed so my last real slide is you know where are we we built a prototype really murphy mccauley who has built the prototype while teaching four classes a week so progress has been a little slower but you know we want to finish development which should be in a couple of months deploy it on fabric and other test beds engage the community by providing this test but we say if you have a new service we can deploy it if you want to write applications on top of these services like pub sub or whatever you can do that too uh continuing our discussions with industry and we'd love to have your participation you here meaning as individuals as a research group or the broader community with that thank you and i'll take your questions and eve why don't you kick it off sure um actually i wanted to just give adrian uh the opportunity to uh he's first in queue and then there were some interesting questions in the chat that um i encourage people who wrote them to get in line to queue up as well adrian thank you eve um and thank you scott for for this um as you said the second time i've sort of heard this and and finally it's sinking in a little bit um my question which um draws on some of the the stuff in the chat is about um the layering here and in particular"
  },
  {
    "startTime": "00:24:00",
    "text": "the relationship with with transport protocols so in in your very simple example of uh source snsn dest um would the would you see the transport protocol running um end to end so source to destination uh or um would you imagine that those transport connections are terminated and restarted at the sn's so there will definitely be an end-to-end reliability layer uh and because for reliability we want the failure of a service node to be no more serious than the failure of a router today so they'll definitely be end to end whether the pipe that goes we view that ip is providing a pipe uh whether the the pipe that goes between these two implement some kind of reliability is uh an open question but but it's not mandated by the architecture the essential um transport is end to end where congestion control is done there's a much more complicated question right because i think i think this sort of factors into things like uh um transport layer encryption as well uh and whether the sms are able to access the data to do anything unless those transport sessions um are terminated and restarted and this is kind of making me wonder i i don't dispute at all the uh all your points about needing to introduce some additional layering to to get the development and i'm just wondering whether this is really go should go in at layer 4.5 rather than at 3.5 but i'll let others talk well i actually can i i mean this is a very active area of investigation for us and our current thinking is that the end to end first of all all the pipes will be"
  },
  {
    "startTime": "00:26:00",
    "text": "encrypted at the low level uh but but that's trivial but and service node to service node pipes but at the end-to-end transport layer there will be encryption but i we want to have an option for the endpoints to say here are the parts that we are willing to let the intermediate node see versus here are the parts that they can't and so they can decide like they want to take care of uh you know take advantage of caching then they might expose certain aspects but if they want to keep certain material private they don't and so we want to give that leave that flexibility to the application itself does that make any sense adrian i yeah thank you and i i kind of take that as um this needs to be thought about more and and nailed down the details right but what i would say is i don't think there's a single answer that is mandated but there has to be some flexibility okay um why don't we uh dirk trossen you're next in the queue yes uh thanks thanks scott for for the presentation um similar to adrian some things that i heard again just slowly sang in and there was one issue that i stumbled across which is the sn must implement public services we need to agree somewhere um possibly in the itf by governance isn't that a barrier to entry though i'm i'm not really quite understanding the reason for doing so because also the culture asks the right question who has the incentive to really deploy this but if i'm if i have to ramp up my sn deployment by really essentially running any public service over it isn't it preventing me for just quickly roll out my owner's end where all i want to do is run my service on it because it's a low barrier to entry"
  },
  {
    "startTime": "00:28:02",
    "text": "that our point is that once you support the execution environment uh implementing these other services is simple it it does require extra resources but if you don't make it uniform then the internet you know the beauty of the growth of the internet is you knew wherever you plugged in you had a set of services you could depend on if we now go to the space of ip options which you know is no fun that's not going to work and so that really is critical to this that you can build an application that relies on a public service and that will be available wherever the user is so that that's critical part of the proposal and given that it's not about changing equipment it's just about the scaling of your uh service nodes we think there's a chance that that's gonna take off so so so maybe i misunderstood your your slide then so you're saying because of the um standardized execution environment you can run any of the public services on any of these ends that have been deployed but i don't so if somebody comes and wants to run a public service on my execution environment or my sn it will just simply run because it's a standardized execution environment doesn't mean i have to provide it as a catalog of possible services i i i could run on mine somebody can just load the app onto mys and then it just runs no ah no it is the let's say it's the itf the iatf decides what the standard services are every service node that claims it's supporting the internet has to run all the services you know it's like once it's standardized within two months it must be up and running that's that's just the rule so it's not like somebody asks you to do it if you're either a cloud who's supporting this or a carrier who's supporting this that's the rule that it just automatically updates"
  },
  {
    "startTime": "00:30:01",
    "text": "okay thanks dave iran scott just talk for a second about what you think the service composition model is for this that's the service model composition bottle uh what do we have for your attention no no no what i mean is we we specifically do not allow users to arbitrarily compose services that there will be sort of sets of approved compositions that work because we we think it is an impossible way to provide a general rule for what can compose with what um so yeah that that is one of the things that if you wanna you can't just say well i want services 28 and 37 because you know what if it's some security measure that's point to point and the other one is a multicast so that we we actually sort of proactively say that kind of linking is done at the definition of the services level not at the user linking them together so there might be like a particular stack that says okay you're going to get an ip-like service and on top of it you're going to get this kind of transport service and on top of that there's going to be this kind of ddos prevention and you know whatever as a a set of what we might think of individual features but it'll be a single service so that you don't have to manually compose things and and the market will show us what you know how that rolls out but we really sort of uh discretion is the better part of valor decided that taking on general composability was going to be a losing battle so then one quick follow on which"
  },
  {
    "startTime": "00:32:01",
    "text": "is okay so from the point of view of the user they see um you know a composed service as what what they talk to the service node about what about the internal communication among uh services is this some sort of private interconnect is it part of the architecture is it something like service chaining or vnfs that we see today that are so messy um any thoughts yeah so in our current implementation what we would do is we would say well if this is the composition then we would have that ball of code i mean take those several different pieces of code merge them together and then have them run as a single service module so that we've actually made sure that these code that the code bases work together and and so we're not doing enough chaining it is actually we decide to put it we won't be using vms probably but if we were we would put it in a single vm so that it's a single coherent piece rather than dealing with the nightmare of service changing in these clusters great thanks king would you like to go you're next always and it looks it looks like an interesting topic to me and it also reminded me of two things the first one is service function channing sfc and the next one is sd1 so uh i do see that these two uh jose service has the same or similar uh model of this i'll say ei network so what's the difference or any relation between the sfc and s8 and sd1 and ei yeah so i i think for uh both of them it is the extensibility"
  },
  {
    "startTime": "00:34:00",
    "text": "um so for service chaining you tell it to go through various boxes but it's not the network is not making you a promise of i will deliver multicast packets for you it is go through this box this box can do something but but um service chaining doesn't give you any global service definition that that's your job you have to sort of you know if you say you go through a firewall and then go to this and go through that you're the one figuring out eventually what what that's giving you so i think that's the difference so something like ddos prevention um that you know we're architecting it to provide it's a service-wide thing it's not just a single box it's not just a scrubbing box so that's what i think that the key difference is i don't know whether that helps so so yeah i may need to read more of the details yep and there's a pointer to the paper um i think it's it's got uh could confirm i think it's an is it an april uh ccr paper yeah yeah and uh but we have a pointer in the agenda um and slides and um i see there's one other person who's joined the queue tng oh yes uh hey scott it's a good presentation here ever one thing to ask about your ei here you know it's running on the ip and the ip normally has some protocols to communicate among all the entities so for your ei as a 3.5 so how does one ei node in your term to communicate with others or will you configure or were you through some random no not run some centralized controller those type of things in that case well i don't know whether what the difference from the like uh icn model something like that thank you ah so yes we will have some kind of"
  },
  {
    "startTime": "00:36:01",
    "text": "discovery um process for service nodes to discover each other but if you want to have an sdn like control for a domain to have the service nodes know about each other that would be fine so the control plane we i was completely silent on the control plane for this we're open to a wide variety of how you're going to manage your domain obviously a sdn like approach you know for knowing where all the service nodes are and telling them how to reach each other would be very easy to do and probably the right approach thank you it's everybody in the queue but i wonder i know there's a very um uh thriving uh conversation going on back and forth on the chat is there anybody from there who would like to um introduce i think i think we we're getting so late eve uh we're we need to send this to to the to the chat and uh offline to the list um because we're already sounds fine scott thank you so much for coming in and um initiating the discussion about this no doubt there will be further conversation so thank you very much okay thank you all and and please feel free to contact me if uh you know whatever is going on in the chat let me know if there's anything i can help with thanks very much and you have access to the chat so you can peruse what's been stated there and the and the chat actually gets posted after the fact okay when say um over to you you're our next presenter here we we viewed our working data center so we have the flexibility to customize everything and it's different from the internet"
  },
  {
    "startTime": "00:38:02",
    "text": "i'm happy to introduce our work in network application for multi-tenant learning so this is a joint work from colleagues from chiang mai university and the university of wisconsin medicine [Music] machine learning algorithms is used in various scenarios such as uh natural light reprocessing uh computer reading so with the increasing size of the data set and the model so the algorithm is implemented as a as a distributed system so the ps architecture is a typical architecture that can support this kind of distributed system in the ps architecture the data set is partitioned and put on multiple workers so in in the iteration initiation of the algorithm each worker would compute the gradients all workers gradients are aggregated and used to update the model random algorithm get into the next study iteration in the ps architecture all workers gradients are sent to us to a parameter server get aggregated and the result is sent back to each worker so this kind of or many-to-one transmission could possibly cause put on that in the transmission so they are studying showing that the performance degradation of such kind of uh such kind of bottleneck so recently the chain or network competition provide an opportunity to solve this problem the programmable switches offer in intrinsic packet processing so the switch pipeline has registers which can store the network states and the user specified programs can be loaded to customize the packet processing"
  },
  {
    "startTime": "00:40:00",
    "text": "so there is opportunity to move the gradient aggregation into the network which can reduce the communication and consequently the overall chain time there exists there is an existing work that applies in network of aggregation to uh to distributed chain html this work target a single chart a single rack setting so there is a wrap connect multiple workers so the switch is offload the ps is offloaded to the switch so the workers gradients are aggregated in the switch to support micro jobs the switch resource is statically partitioned and assigned to each job so we think this uh network aggregation can be further improved uh because in in much in in motor training the algorithm takes uh takes efforts to compute and communicate each iteration so for static allocation when the job is in um is in competition time this switch memory would be a email would be a list and this design assumes the topology is a star topology with a switch in the middle connecting multiple workers so it should be extended to multi multi-rack setting um for example in bird training there are tens or hundreds of nodes which cannot be put into a single rack and in addition uh static partition add complexity to integrate the switch memory allocation with the control plane so we propose our solution with three key goals uh the first goal is we should uh maximumly uh you use the network condition for"
  },
  {
    "startTime": "00:42:01",
    "text": "performance gain on targeting the production network we should the solution should support multiple simultaneous job change efficiently and it should also support a multi-rack quality we designed our solution atp short for aggregation transmission protocol with the multi-tenant and multi-rack as the requirements and we additionally saw extra problems with which can further improve the performance so in adp the switch memory is organized as a real-world details then each job has a job id with multiple workers and each worker has a sequence each worker's gradient is organized as a sequence of packets so to support multiple tenants we do not uh statically partition the switch memory instead we organize it as a whole array as a as a shared resource pool then therefore for aggregation uh each packet is randomly hashed to a unit aggregator so the assignment use a hash function to hash the job id and sequence number so for four packets from one job with the same sequence number but from different workers they would be hashed to the same position and get a repeated there so the the vision result is sent to the ps then the ps uh return the result to the switch it would do two things first it would de-allocate the switch memory then the packets are is multicast back to all the workers"
  },
  {
    "startTime": "00:44:02",
    "text": "so we also overcome to or overcome two challenges first because the switch the aggregator assignment is decentralized it's possible to to have hash clearance uh for example um job tools gradient uh packet is a is a assigned uh it's hashtag keeper which is occupied by another job job three in this case uh all the packets would pass through the switch and arrive at the on the ps the ps do the aggregation and send the result back to the switch the aggregator is not the allocated part of the result is sent back to the workers so the switch service agreement service is the best ever and the ps provides a fallback to handle the corner cases there is another case of inconsistency or membership inconsistency it causes incomplete aggregation for example the first packet is sent to a aggregator which is reserved by drop three then this packet it passes through passes through to the ps but at this time job3 completes its application and deallocated the switch memory the grid guitar the remaining package is sent to the aggregator and the aggregated there so both the switch and the ps uh have a partial result they are waiting for each other this is a deadlock and uh they are what's worse is that there is no return packet of results to dialogue this switch memory causing a memory leak problem so we design a re-transmission antenna and host with the duplication in the switch so for for packet a uh it has that uh it"
  },
  {
    "startTime": "00:46:03",
    "text": "is a stock under the switch and the ps but it's uh it's a following package uh may be maybe successfully aggregated and returned so if the sender observed reduplicated the ack the result packet it would re-transmit the missing the packet that is missing the result it will all workers would retransmit a so um the the regulator is a has a bitmap to track which worker has already participated the aggregation so a2 to a n would not be aggregated twice it would be duplicated and the a1 is added to the partial results so we get a complete result which is sent to the ps and the reply back so with this design we have a uh correct protocol that can guarantee the um the correctness uh to support microreact aggregation uh we need a aggregation hierarchy in the topology ideally this aggregation hierarchy can be uh can be of many levels um we consider the data center topology in the data center topology the core network has multiple passes and it uses non-deterministic locking so we it's not easy for us to pre-compute the hierarchy in the topology so we only implement the objection in the course torque switches because there are we points that the uh gradient packets must pass must pass must traverse currently we have a two level aggregation design so uh all the packets would aggregate at the walkers tour first then they are sent to the ps4 and the second level aggregation is happening at the ps4 the"
  },
  {
    "startTime": "00:48:02",
    "text": "final result is send it to the ps um so in this uh in this multiple racks part we also overcome another challenge because uh um the higher level switch this is the hierarch aggregation hierarchy or the previous uh in the paris page so uh in each of the guitar we have a big map which uh where each b that he knows uh i always children um but so there is one bit of four in the switch two which indicated the uh switch to zero but the one meter here cannot denote the all the possible aggregations these always work because there are two workers there are four possible agreements these um so we overcome this challenge uh that by falling back to the ps we use the one in the higher level switch to denote the successful aggregation of the whole sub tree and in all other cases we regard it as a feeder and send it to the ps for back to the ps processing um so uh the previous design also overcome other challenge other challenges on our reliability when the the re-transmission with the duplication can also handle the packet loss correctly and uh one pack loss happens in the host of the re-transmitter and the bitmap in the switch can guarantee there is exactly wax application and we also redesigned the congestion control uh the essential problem is what should be the congestion signal because some many packets are consumed in the switch so they do not have round sweep time so there is no rtt we use the ecl as the congestion signal use eimd for congestion control and on the in the host the switch can only compute on integrals uh"
  },
  {
    "startTime": "00:50:02",
    "text": "it does not support floating point arithmetic so we do compensation uh we scale with scale the float point block points two integers bioscanning factor and uh this because we skills uh the floating numbers it's possible to have overflow at the switch uh to handle this problem we just use the fallback mechanism we reuse the fallback magnesium when switch detect detects uh our flow overflow the then host will fall back to the ps aggregation with the floating part of arithmetic so we implemented the uh user space networking stack on hosts and we implemented a network of rehab services in a switch so in the evaluation we have nice servers so uh little automatic workers and one and the ps we compare atp with other piece like pictures such as ps architecture with different networking stack and the ring or reduced architecture with different steps we use things rupert and time accuracy as the method we run workloads of models in image imagenet contest for a single job we showed our performance of a single drop so we run different models and measure the chinese rupees we compare different architectures in each group we have three observations first atp in all model trim etpr performs other approaches sometimes it's the performance skin is very significant atp can benefit the network intensive uh workloads more than the competition intensive workloads and comparing atp"
  },
  {
    "startTime": "00:52:00",
    "text": "with the ring or reduce with hardware acceleration atp is slightly better than ring or reduce but more importantly it only uses half of the batteries that we already use then we show the performance of multiple jobs uh we compare atp with the static memory allocation in a static approach we evenly partition the solution are you close to conclusion yes so thank you thank you yeah okay so we tune the switch memory for three jobs and uh get that throughput the peak throughput then we reduce the memory to cause contentions we can see dynamic allocation degrees more greasefully than statical approach okay there are more evaluations so in atp we co-designed the host and the switch logic the switch to service security service is best effort it has dynamic resource allocation the host networking stack has fallback mechanisms for correctness guarantee and also reliability and congestion control to ml jobs such a design can provide performance skill and correctness and we achieve our goal of multi-tenant but to draw support um so there is takeaway so usually when we do a network competition usually we can get a very significant performance scan switch computes much faster than a server but correctness guarantee is very difficult because the network competition introduces new schematic into the network for example practice can be consumed instead of lost then hosts need to distinguish these cases and usually we do switch and host the co-design so we can get both credits and perform skin so for different applications we"
  },
  {
    "startTime": "00:54:00",
    "text": "can we can make different designs that's all about my thought i'm happy for questions thank you very much for this um i think we um are you know we're it was great the first discussions were great so we're losing a little bit on time uh maybe we can send all the questions to your paper uh either on the chat or on the mailing list thank you very much for this uh and dirk you're next yeah uh i need to stop okay let me see how i can get to my slides so i'm trying to use to show the slides that are uploaded to the data tracker [Music] do you have to put them up or how does it work again all right it says no slides available you know what in the case let me do screen sharing here you go thank you you see my slice okay great okay yeah thanks for inviting me it's uh great being with you again um if only ritual but um let me um tell you about some some recent work um on a system that we call information centric data flow that is a product of our picollo research project that"
  },
  {
    "startTime": "00:56:02",
    "text": "investigates there are new ways for integrating computing and networking so in some sense this could be seen as a customer to a system like i'm the one that that's got um talked about um so let's see and this is john worked with my students laura alvardani and tm reindeers so in coin so far um in my view we have mainly been discussing like two strengths of of work so like one is coming from like data plane probability and then seeing how this could be put to you know a useful purpose for um you know having distributed applications and improving their performance and so on maybe also evolving some protocols to support certain use cases better um the other strand you could say is coming but from the distributed computing um where we say okay what can we learn from this will be computing and how does it affect our view on networking and maybe then re-imagine the relationship and in the end have more programmable systems also like the ones that um scott talked about and so i think the the like the thesis of this group in in general is maybe is there some confluence at the end of these two strains so this work here is more on the on the lower strand here and so this is a paper that we presented at the acm icn conference this year and there's also an associated demo um and so i hope to get you interested in this and then you can i invite you to check out the paper and the demo video um after this so in distributed computing um we know"
  },
  {
    "startTime": "00:58:01",
    "text": "that there are many different types of interactions so like simple message passing remote method indication data sets implementation key radio stores and so on and so i think some itfs ago we presented a system that we called compute first networking that was essentially a um yeah too incomplete really general distributed computing system um based on an icn um today i want to talk about a like competitively simple abstraction so data flow which is really a structured um direct direct and basically graph based uh data processing approach where you have a system um say nodes in the in the network and um like data objects that are sourced um at some some endpoint or in some node trigger computation and other nodes which then you know leads to new designs that trigger computation somewhere else and so this deck here can be implemented in different ways so you could say because this model is so simple um and so if your application semantics allow it you could also say you want to parallelize the execution by opening up a second subgraph here and um then just run everything faster if you have the resources for example so what you have is um something like a um like the the data flow specification um that could be um you know laid out in the network in in different ways uh different levels of parallelism and so on so that so the semantics in this case here in this like word count example would be that um you split the input here in this text-to-lines box"
  },
  {
    "startTime": "01:00:01",
    "text": "but you can also have other use cases where you you know reuse the same input for different types of computation of course so some some concepts so data flow can this fundamental paradigm can be used to implement batch as well as um stream processing so in steam processing you you like conceptually you look at each data object independently in an unbounded stream of data in batch processing you group data and typically the systems allow you to um you know implement groupings dynamically so based on some predicate or some time window specification and so on so um windowing is a common concept here that allows this grouping and slicing and this can also lead to situations where you have something like a predicate that allows you to um put like one data object into like multiple windows for like consumption by by different functions for example but you can also split this up in in different ways time is an important concept so you can imagine that the whole system is has to be kind of elastic so you have often asynchronous data production you can't really predict the processing transport delays and so on and typically what you try to achieve is that you match the production rate with the input rate and so the task of a data flow system then is to adjust the processing graph to the application requirements and the data production rate and you can you can see there's all like some kind of variable uh performance and"
  },
  {
    "startTime": "01:02:00",
    "text": "systems can be compared how well they maybe keep up with the offered load perhaps there are a couple of really widely used implementations so apache beam is basically the unified programming model that many data from invitations use um so sometimes they're called runners so you may have heard about apache flink um spark of course um google cloud dataflow is a product and so on and the picture here on the right hand side um depicts the architecture of a system i think it's probably inspired by flink um where you um on the bottom here you you see the the notes so these this uh what is called task manager here so this could be something like a a compute node in your network which is offering several slots for computation and um this like the job manager on the top right here is kind of orchestrating this whole system so it's kind of having an overview about the available slots and is responsible for allocating um tasks for for certain jobs and then also managing the connectivity between those those jobs and so in flink in particular of course performance is is also an important topic so flink just had their annual flink forward conference recently and there was quite a bit of work on you know trying to reduce buffer upload and improving in elasticity and so on so fundamentally this is not so easy because for example flink is really using a connection based approach to connect these uh task managers and so"
  },
  {
    "startTime": "01:04:01",
    "text": "it's not going into the tasks but really the the nodes if you like and so these are like tunnels and so the flink task manager is basically tunneling all the um or configuring the tunneling of all the task communication inside um these connections and of course this needs um some some um yeah control um so like credit based schemes for example uh to reduce um buffer load or our queue sizes and so on um so fundamentally um this is not a trivial task so you've been looking at this we find that well these overlays do not match the inherent logic of like processing immutable data objects very well so as i um presented the data is really locked into connections and these are like virtual channels between hosts and you always need this orchestrator checking the resources and making the task relationships and so on and so you treat the network as a black box and then you tunnel the the task communication inside virtual channels and this makes it um yeah difficult to have a really agile you know matching of like your compute performance with the network performance and having also really responsive systems in the end you don't you have you don't have this full visibility of both the computing and the networking resources now in the system i wanted to um talk about today and so we call this ice flow information senior data flow um we assume that we have a network of nodes and in in icn we we name everything so the assumption here that is that we have a network of named nodes and there would be some routing infrastructure that allows us to"
  },
  {
    "startTime": "01:06:00",
    "text": "discover them and forward forward interest packets and data packets in the system and um so on top of this of these nodes we would instantiate functions also with a certain naming convention they would also be announced in this routing system and so we'll be able to construct compute graphs and what we do in this system here is that we are kind of not establishing connections to functions or to to nodes we are actually just asking for input data and so when there is new input data we this triggers computation at the like downstream function and so on in this system we are able to for example you know split up the computation as i showed before but also um have something like like like a multicast um system where you can have reuse the same data item in like icn idiomatic way quite efficiently so in ice flows we just talk about names for the infrastructure and for for the actors in the system and the computation of the actors they return name data object with the usual icn properties so they are immutable they can be cached and they can be authenticated encrypted and so on and um so the interesting challenge is here is that we have asynchronous data production so we have to know when data is available so like push semantics which is typically not idiomatic in icn and um then you have to think about flow control so how do you couple consumers and producers um and then in the icn system typically you you publish data that means you make data available um so one challenge here is that you also have to know when the data has been consumed which normally you you don't"
  },
  {
    "startTime": "01:08:00",
    "text": "really know um because requests can be answered by caches and and so on so there needs to be a system like basically implementing a bit of tighter coupling um as well so the system yeah has a certain naming convention um where we named the application the data flow actors and then the produce data objects um um that the functions produce and with that we are able to to set up the system and um so this this um tasks of um you know making data available and learning about new data we are using an icn technology called dataset synchronization for that where so logically the producers produce data under a known prefix and consumers can subscribe to that prefix and then they would learn um when there is new data under that prefix and then they can decide okay i'm interested in text to lines object one and then i can fetch that so um there are implementations of this concept like psync in in icn uh which in the end which you may have heard about um and so that means um so in the on the like very low layer this that means consumers have to um kind of send update interests uh to learn about new names uh perfectly and then um from an application perspective it's quite a convenient interface so it's a bit like reactive programming so you just get notified when something um you know shows up that you are interested in um we implemented a few additions to make the system yeah a bit more scalable like to have like less updates so we have a grouping concept where we group data objects into windows and we're actually just publishing"
  },
  {
    "startTime": "01:10:00",
    "text": "these windows so uh and so we're using icn manifests uh um for that and so there's like a like a a two level inter indirection um scheme scheme here um that makes that makes the whole system more efficient and a bit more scalable um i'm almost done and so in addition to the data flow communication we also need to share some you know runtime information and configuration information so what is the static flow graph what is the actual dynamic flow graph um what are available compute slots in the system and so on but also implementing this loose coupling between consumers and producers so we have um conceived something that like what we call consumer reports where basically we publish what windows have been processed by each consumer and so this is um also a um like a organized data structure in in icn way and we also use this data set synchronization scheme to share this information okay just very quickly so this approach allows us to deal with congestion control and um yeah like a proper receive window configuration in a different way so we can really adapt like the interest rate for example to our actual processing speed so avoiding that we asked for too much data we cannot process in in real time and observing the performance of my downstream consumers um could be a trigger for an upstream producer to initiate scaling out and so by creating a new subgraph so if i'm constantly realizing"
  },
  {
    "startTime": "01:12:00",
    "text": "that my downstream cannot keep up so this is the the trigger that we can scale the system so there's a demo on this please check it out and yeah wrapping up so these data flow systems are really important they're driving many many data science applications today and you can see also more coming up so in the itf this week there was a buff on a project called ppm that is also using similar similar concepts the current systems are quite limited because of their say overlay approach so centralized orchestration limited data sharing and we found this icn approach quite promising so this uh data sensing synchronization approach um works reasonably well um but um to be honest we are currently looking um a lot into performance optimizations for that so there's a lot to do in reducing the overhead and so on um this system would basically need additional infrastructure so like a name-based routing infrastructure for example and there are solutions for that that in principally principal work but maybe in terms of research this system could also be supported perhaps better by a routing system that gives you more information so for example [Music] resource education information directly and not only reachability information and so for coin we think this could be an example for like new protocol work so i'm not saying that this is the the best way of doing this so this is has to be something that you know ends up in a protocol spec but it's it's an interesting example so how you can break up overlays and leverage systems like icn to do that"
  },
  {
    "startTime": "01:14:01",
    "text": "and so today i talked about data flow but you can imagine that other interaction classes could be promising as well so other systems may be like kafka like published broker systems and so on with that thanks for your attention are there any questions thank you so much dirk um that was super interesting and um i just want to point people to there's the you know icn paper and demo and um i also wanted to kind of go back to one um presentation and remind people that his uh presentation and paper are available from nsdi21 um so uh i think in the interest of time we will um go forward in the program we had segmented the program into you know papers that have been published elsewhere and that's been really fruitful thank you all uh for your presentations uh the next section is gonna be brief uh it's sort of new ideas section um and then we're gonna get to the drafts um and draft updates and then then a new draft so with that i think i'll hand over the floor to marie jose who's going to introduce the moda ideas on behalf of her colleagues um yeah um i hope my screen oh my god what is this and i'm gonna try the timer clock yeah but i i have the the great the great the great screen problem of um miteko yes if i can fix this"
  },
  {
    "startTime": "01:16:01",
    "text": "why why am i seeing all this it's supposed to be just the powerpoint uh did you there you go if you could put that into the display uh we did for a moment there oh okay so let me go back into powerpoint you can also select a specific screen versus the your entire there you go well i that's what i can't find oh um oh my god let me stop this it's crazy um i i cannot find the thing to choose only one part of the screen i see uh okay i'm gonna give it one last shot and then i can and then i can download in the interim yeah can you okay you seem to be better at this than me uh i i'm sorry i'm sorry i i know there's a way of doing only the part of the slide but i can't find the button maybe if i could maybe if i close my oh my no i can't close all my my screens yeah you should try it i'm sorry people i can start talking anyway so that we don't spend too much time um the presentation is about moda it's it's a currently a a proposal that's been put together by a large group of people um and it's a european wide project and the idea started from essentially a lot of us actually a lot of people on the moda team or uh will be when we once we see the slides uh are very familiar to uh this group because a lot of them are involved and one of the ideas oh my slides are coming thank you thank you so much eve um"
  },
  {
    "startTime": "01:18:00",
    "text": "and the the id and we can go directly to the first slide if when it's you can go to the next slide if please okay just bring it in um presentation yeah in mode yeah so the idea started from this uh a lot of us who work in iot um have a big problem is and i think it was interesting that a little bit of that the scot presentation touched it a little bit everything is very verticalized everything is fragmented and for essentially in iot what people do is they put a sensor somewhere they put a gateway in they connect to the cloud and they claim that the problem is solved the problem is not solved because the minute that you want to start having uh applications and services and artificial intelligence that cover uh more than one thing you you're in you're in your agriculture well you may want to take decisions that are based on the market but then how does that work when you need to det you need for that to know or how much disease do you have in your farm or maybe what's the building temperature because that will increase or decrease depending on you know the time of year or the time of day and it will increa increase or decrease your production which yeah will go back to your your modeling of decision and now you would the minute you do that well you have people involved you have a ton of different um systems that don't talk to one another next it's like let's see if i can so oops so essentially in this fragmented environment the application development has very very important pain points so all these fragmented systems that require overlays multiple gateways different cloud applications the different cloud providers that don't talk to one another and there's issues always of that in security and privacy because there's data privacy there's digital sovereignty"
  },
  {
    "startTime": "01:20:01",
    "text": "there's multiple customers who are involved and it creates a big problem at the same time in this group coin we started to think that the internet paradigm is much more than is much more like a computer board than a telephone network and if we have a computer board then we need an operating system and the extra cloud view that we have in moda where there's computing inside different nodes that actually collaborate with one another at different levels in the network is one um realization of this vision and eve you're the one who said that the data is the fuel of the 21st century and in such an environment also for iot and for all these distributed systems data valorization is key i think scott mentioned that you know you you use open source approaches and this is exactly what we want to do because in fact it is not the algorithms or the software that's worth and the words a lot it's actually the data itself and so we want to make sure that we can actually maximize data valorization by acting it on it inside the network next slide so what is moda it's the operating system for a new distributed internet i hope scott are you still on maybe it's the operating system for the extensible internet it provides an infrastructure that allows applications to be um easily developed and it has a lot of things that were discussed in in coin and almost uh we have almost uh one one draft per each each topics here discovery services communications and publish subscribe uh we haven't talked a lot about semantic integration but our"
  },
  {
    "startTime": "01:22:00",
    "text": "uh our friends and things to things have done it a lot um the implementation of common use functionalities that we need uh including you know forwarding and and forwarding to different cpus and forwarding not across just uh layer three but for functional french implementation of forwarding based on name and so there's a link there to icn and ndn and also obviously in moda what we want to have going back to the pain points we would like to have apis and tools for writing class across though to running code across those uh multiple terrorists oh my god here's a typo heterogeneous nodes uh inside the network next slide um so the format didn't work very well here uh the main mode of functionality i don't want to go uh i'll go fast because i don't want to go into everything this but obviously we want to do some orchestration there was discussion about orchestration before but we feel that we can actually look into this and um again on device computing over a trojan system we want to have if we can as much as possible reusability and that's actually a problem on those verticalized applications is if you change vendor or if you change supplier your system doesn't work anymore because they use different protocols different semantics different everything you would like to have modularity as a design choice i usually when i teach my class on distributed systems i talk about the lego blue brick approach so that you know we want to have a lot of lego bricks and we connect them when we need them and we don't use them when we can't or when we uh we won't and actually also in iot systems it's important because a lot of times you have limited resources um you want to be able to manage the the network processing units"
  },
  {
    "startTime": "01:24:02",
    "text": "themselves so the in-network computation uh that allows uh the packets and the information i would think i would speak more of information in this case uh to be properly managed and properly uh sent to the right interfaces and the right end devices and end system and obviously we want to support data and intelligence services um more and more data driven until it is is actually the basic of a lot of networking uh last year with andy schuster we did a study for the nsf on the future of broadband and actually the conclusion was that everybody who does broadband wants data so data driven made required to have new new features inside moda and inside the network itself and of course there's there's ai that is not only in the applications but more and more in the network in terms of network management and even uh management of loads inside data centers and elsewhere next slide i think it's almost the last one um yeah i think the previous slide is an overview of of what it looks like but the picture is not very good so let's we can skip it um and um the link to the coin rg well actually um again it's not just that there's a lot of people involved in mode that are also members of this community is actually there's a lot of common research topics i mentioned discovery uh we would like to discover storage function from you know functional functions and and computation uh so functional discovery storage discovery computation discovery we would like more and more i think both and that's actually since i know i have two hats here so i can say as i as now as maybe as a current chair that distributed distractions uh and protocols are also very important to both groups they centralize security and trust uh obviously important and"
  },
  {
    "startTime": "01:26:01",
    "text": "there is going to be a draft update later in this talk about security but it's also an important thing when you start distributed systems federated learning we haven't really addressed that a lot in coin but it is something that we could look at because federated learning needs some form of in-network maybe not pure computation but at least uh some form of um trending and and connections and orchestration and obviously there's all the use cases and actually the people right after me will talk about use cases so i won't go there so this was this was a very short presentation this is moda we know that this is right now being proposed we've been evaluated we don't know if it's going to go through but i think the team um and i'd like to really acknowledge uh the architecture team that did this and a few of them are on the call right now this was an incredible effort this was an incredible fun thing to do and we're thinking that maybe we want to continue the work inside this community and outside thank you thank you so much marie jose um there is a question on the chat asking if there are any pointers that you or others uh can share um that would help people read more about or if there's a website for example so um in the interest of trying to take that to the list or the chat yeah i will yeah i will um um i will see what we can share uh i i see this is from from daniel king um i will see where we can share i'll ask um the the the proposal management which i was not one of so thank you daniel for this question and i will ask thank you okay it looks like um the use cases uh presentation is next we're into the um"
  },
  {
    "startTime": "01:28:01",
    "text": "some draft updates and i'm going to ask if folks could please uh shorten their presentations to uh eight minutes each just to kind of reclaim a little bit of try time so let's see oh and actually you know what i i see that uh um i did take the advice of those on the chat window to uh load things into um mid echo which i hadn't done before uh so i admit user error on my part um it says okay so do you know how to request the slides looks like you've already done that ask to share the slides perfect thank you so that looks good perfect yes thank you for helping me out yeah so hi everyone i'm ike and i'm here on behalf of all the co-authors now for the use cases draft just giving you a bit of uh update what we've done since the last iteration um so the first thing you can already see on the on this slide so we've a couple of new co-authors so xavier david and miguel have joined us um apart from that let me first give uh or remind you again of the purpose of this draft so the uh in our coin charter we have this second item where we want to do research on use case driven requirements analysis and where we also want to identify potential benefits to these networks from in-network compute functionality and this is kind of what we're trying to do with this draft now so until now it was rather loose collection of use cases so stemming from the industrial ones uh right in the beginning um and yeah now we've quite a few of them uh and what we're now currently trying to do is actually go more towards providing input to the second charter item that we have there"
  },
  {
    "startTime": "01:30:00",
    "text": "changes in a nutshell since the last iteration so first of all we have regrouped the use cases so from the historically grown structure to the one that we have right now so we thought of four ways how we could think about the use cases the first one and our really kind of new user experiences that can be enabled using coin uh so the uh arvr um stuff that marie jose has uh presented quite a few times is for example in that category uh the second group is supporting new coin systems so where we think that the new interaction between communication and compute can actually enable new kinds of systems or so entirely new systems the third category um is on improving existing coin capabilities so we already have uh networks which use some form of coin and uh so for example uh cdns have kind of already some coin functionality in there and those use cases are now really on how those networks can even be improved further and then finally we have entirely new coin capabilities that can be enabled as a second aspect we've then also tried to sharpen and tighten the taxonomy so it was not that um focused before and we've tried to do that now i'll come to that in a moment again and finally we've also already started to prepare the actual analysis that we want to do later mainly focusing on the research questions and requirements so with that let's have a look at the current draft structure so what you can see is that we have actually done the regrouping already we also have this one new use case provided by xavier that he has already presented last time or in our last meeting and then also we have already the analysis in the structure right now um so we're all only"
  },
  {
    "startTime": "01:32:00",
    "text": "waiting to get started with that basically and then finally the aspect that is at the top of this slide so the terminology part that we've tried to really start aligning the terminology across the different drafts and the other research group documents or the draft by dirk which has been expired already but yeah we've tried to align it in some way and that is the following now so everything in green that you see on this slide is terminology defined in this draft by dirk and and the other authors and we've tried to use that terminology throughout our draft as well or actually at least have started to do that um and we've then also added quite a few additional uh terminology um that we thought might be necessary for our for our draft and overall our idea would be to sort of get an an overall technology all across coin where then the question obviously would be where would actually uh place that terminology um so whether we have that in every draft or whether we have one draft that collects all of that terminology then quickly regarding the use case taxonomy um so not that much that has changed here so we have basically only split up the opportunities and the research questions so that they are now more clearly identifiable we've also tried to link the description of the different use cases really to the new grouping that we have introduced into the draft and finally we've also tried to um focus the requirements uh a bit more really only on the coin capabilities so that we really describe or try to describe what the coin capabilities themselves need to do rather than what the overall use case has to"
  },
  {
    "startTime": "01:34:00",
    "text": "satisfy and with that i'm already at the end um so next steps that we are planning to do is first uh finish aligning the use cases according to the the new taxonomy so really pinpointing the requirements and aligning the descriptions a bit better um then we're also trying to uh think about the terminology that we're currently using whether that's everything that we need or whether there are additional terms that we might require and then also maybe as a question to the the complete research group where we would like to collect that uh terminology as the uh draft by uh um and and york i think has been expired uh some time and yeah that's just a matter of things where we collect that and then finally we would also like to start uh with the analysis so we first try to condense the opportunities research questions and requirements and then afterwards try to identify aspects similar across all of the use cases so that we can then uh perhaps give a collected input into the second uh charter item of this research group and that's it thank you and i think comments we can take them to the list if there's not that much time um thank you i i cannot really um i'm a co-author so it's hard for me to be both co-author and um and and share on this but i think uh there will be a discussion uh amongst the chairs to to bring this as an orgy document but we'll i'll let the other chairs talk about it since i'm an author thank you i'll just confirm that that does seem like an appropriate next step that we can discuss on the mailing list okay thank you thanks for moving this forward this was great much appreciated dirk truston you are next"
  },
  {
    "startTime": "01:36:00",
    "text": "okay um we see you yeah and there you are good you can see this lines right yes very good still a bit new with the ui and colin has given a good tutorial thank you transfer protocol issues uh even network computing systems that's another draft um like clouds and myself have been working on um it had expired initially in uh in the last meeting because of the focus we had mainly on research presentations and and and and it's not quite as outstanding outdated as the um date on the first slide gives it was obviously presented at the 11th of november 2021 or not 2020. the the premise um i highlighted the main piece i don't intend to read this so this is about um what if you have functions of the communication system i'm sorry if the part of the intent function may be provided as part of a communication system that's the network premise really of coin this is a quote from the end-to-end paper from the original indian paper and and we're looking at the challenges to the traditional intent transport protocols um you know trying to outline opportunities research questions uh for the design of transport protocols that may arise uh from the availability of in-network computing capabilities that's the the premise really as a recap so the intention is to provide insights into various transport technology areas uh research question changes already mentioned ongoing efforts and concepts that are currently under studies so we're not limiting ourselves only to existing rfcs but we're also looking at ongoing work research whatever we can find um outline possible future work in coin and elsewhere that's starting with coin but doesn't"
  },
  {
    "startTime": "01:38:00",
    "text": "necessarily mean that all of the work needs to be done in coin obviously and provide that in a gap analysis so the goal is to contribute to the objectives of coin and i mentioned i i quoted here the coin chart or scope number four namely the research and potential new transport protocol required or enabled by in-network commute resources so that's a dedicated item that's chartered really and we believe that and hope that the draft contributes to that the general structure we made a similar changes a bit you know aligned uh in some sort of locks up to to the um changes we also made in the in the use case traffic presenters are trying to sort this a bit more into various areas of discussion so in this case for the transport draft we divided the or collected the uh separate um chapters from before into a single technology areas uh section now that's a section three so that has been entirely restructured and everything has been fl uh was was was integrated under that section um and then started with the gap analysis this is just to present the technology areas and the gap analysis more visibly in the structure so there's edit here for future extensions um and in particular with a possible formation of future needed work so this is much more gearing the the i'm the document towards the charter item that i mentioned before right um so in section three um so everything that really was before in the various i think section initially three to seven are now all found here um and uh and that also helps us then to link back from the gap analysis in section five into the appropriate technology section later on so it makes the editing easier um there were smaller updates here only apart from the larger structural change in section 3.1 by linking this to the ongoing in the area of work on internet addressing but there's a discussion going on at the moment and"
  },
  {
    "startTime": "01:40:03",
    "text": "the ic energy and then the working group um so these were content changes as well to the structural changes um question obviously to the group we would like to ask here and happy to get any ideas either in the chat or on the mailing list is what research question related concept ongoing efforts our intention is really to you know also approach the new community with that question specifically and because we're sure that we're missing concepts simply because we're not aware of them or we you know haven't worked through all the various ones so if you have anything any research question in a related concept you want to share with us please do so gap analysis i said we edit this with the same subsection structure so you will see that the subsection structure in section three is mirrored um in order to later really have a clear relation between technology areas uh the the the the seven subsection we have to the actual gap analysis in in these various areas at the moment we only added an introductory paragraph outline the intentions we have no content in the individual sections yet um uh question obviously also to the list is or to the community is to be one such uh such uh section or not for coin so we believe that it may help us to um working towards the charter if we don't want this we can remove it but that was the suggestion we made and that's a question we would like to get some feeling from the community future plans um so we mentioned already the last time and we we haven't really made too many changes to do that but we want to because we also made changes to the use cases and i think now that we structure the use cases as you could see in ike's presentation more clearly around the notion of capabilities we want to also make clearer linkage to the use cases and particular with that taxonomy that's being used now in the use case draft in a in the next version um of the"
  },
  {
    "startTime": "01:42:00",
    "text": "transport draft we want to add more existing work um we've already uh there was a communication between the chairs but unfortunately we're a bit too late to bring this to the chairs um work that has been presented at hotnet's 2021 and bharaji the one of the co-authors of that piper is actually i hope he's still online but at least he was online before um so we tried to bring more existing work here even to the point that we may also you know have this work that we find present to the wider community as i said we were a bit late for the agenda for this time so it may come in the january meeting um also similar to the uh use case draft to turn the research questions maybe at a later stage into some sort of requirements language that would allow us to uh you know formulate um research work around these requirements um gab analysis to fill this i mean if the question i asked before is answered positively so we would like to have a gab in ours because it's useful to have that we really would need help here would invite uh contributors to that uh and and and with the view on the charter um the question here that was the bit of the confusion i had wanted to do because i thought the use case craft was an rg document already um i think it's the transport draft that isn't an rg document yet which is why it's written on here to maybe adopt that as a potential key output towards the scope of the charter as an as an rg draft as well that at least that was the memory that i had about this the issue and again to repeat we would really welcome contributors gab analysis related work new work that exists even if you just send us links if you can't be bothered to get it as a course or into properly written up we're happy to get any material that you'll find that you think is relevant should be listed in the in the uh draft please do contact us to do that if you want to become a bit more active in"
  },
  {
    "startTime": "01:44:00",
    "text": "helping us absolutely welcome um certainly wouldn't that would want that dirk i think you're spot on in terms of what the future and next steps are um you have a question from peng liu um so pain if you'd like to jump in um just one question i see the draft mentioned that live for with doncast so i want to ask that if the quick could be a potential way to help the flow affinity or the service equivalence since it has fixed the connection id regardless of the change of address or part yeah yes yeah so so the the linkage to diecast is still in the draft even though i think the draft actually have expired and there are aspects that are mentioned in the dyncast work that i think um would need to be pulled more properly into the draft it's correct so it's a little bit um a loose issue at the moment of the draft it needs to be it needs to be clarified yeah spencer um okay yeah i was just asking i'm seeing uh answer it also in the chat but uh question about uh if there was a github repo that we could contribute to especially in the gap analysis i think uh the gap analysis would be really helpful if uh you all could provide some uh insight there yes absolutely we we uh uh ike thanks for putting the um the get up into into the chat i i'd seen that i couldn't type quickly enough um yes i i would really welcome um and and that's certainly a way to contribute to that we do the same for the use case draft as a separate one for you yeah we've uh that's that's really improved uh us to get get uh input from contributors"
  },
  {
    "startTime": "01:46:02",
    "text": "uh in other venues so thank you thanks yeah appreciate it thanks so much dirk um really great progress um i in the interest of time we're going to um move forward in the program um let's see we have um the enhancing security and privacy draft and if again if you could condense your talk i know we've given you ten minutes originally but if you could cut it down to seven or eight that would be wonderful thank you very much okay i will try and i also try to share my screen okay okay do you see the right screen we do okay great so uh yeah thank you um you didn't hear from me in a while but we still have the draft on enhancing security and privacy with in-network computing ongoing and actually a lot has been happening in the past month and year in this field of research so before i go into the update first a short recap about the scope and intention of this draft so the main focus is to implement security and privacy mechanisms within the network and the idea behind is behind this is to reduce the latency and also to improve the scalability and provide a quicker reaction to security incidents especially in comparison to middle boxes which are typically used to provide security functionality today and potential use cases for this are to retrofit security for resource restricted or legacy devices which cannot implement security themselves and this might be also beneficial for industrial networks with high performance requirements or to provide scalable and transparent anonymization"
  },
  {
    "startTime": "01:48:01",
    "text": "so the goal of this draft is to provide an insight into the potential of coin for security and privacy and to show research questions and telephone inspections [Laughter] okay yes [Laughter] so first related um [Laughter] sadly aina can you hear us oh aina can you hear us we're having terrible difficulty hearing you you're completely broken up um we don't oh that sounded fine uh i wonder if it's your headphone usually i don't have well when you continue we i don't"
  },
  {
    "startTime": "01:50:03",
    "text": "uh so i'm just wondering i don't know if anybody heard anything unfortunately because your your audio problems started almost when you immediately when you started presenting turned off your video to reclaim some bandwidth but um could you okay so and let's see if we have a little better luck i mean really it's been broken up since the beginning okay let me hear you now but now you're showing that uh we i think we got about one or two yeah if you could go back um yeah well okay so you didn't hear anything that's that i think we got we were on your first slide we looking thank you very very much this slide we lost entirely and and again we're sort of pressed for time so um okay okay let's make it crazy start again yeah sure maybe okay to make it quick the focus is on implementing security and privacy mechanisms within the network um yeah for better performance and faster reaction um and use cases are for example retrofitting security forces [Laughter] um"
  },
  {
    "startTime": "01:52:10",
    "text": "um okay i know i know i think we're going to we're going to need to defer your presentation every time you we we speak it's fine then as you um present the audio is really okay i understand that's amazing sadly um i'm sorry yeah um really too bad yes i know hannes is saying he really wants to hear this as do with the rest of us and now as we converse it's fine but as you can presentation mode it's completely broken up um so i think we're gonna yes it's very strange i wonder if it has something to do i can't tell whether you have a headphone or not but yeah there may be a way that you could just share the slides for her in case it's the slideshare i'm gonna have to do somebody already have it down let's see how do i display it if i because it really seems like we can hear a perfectly well as long as he's not doing slight sharing okay let me let me download and um let's see where do i go to get the okay so i've uploaded everything to manage the slides and then if i want to show this button on the left hand side near the um near where you mute and unmute on the left it's right next to the screen share button"
  },
  {
    "startTime": "01:54:02",
    "text": "i'm just not hang on oh got it sorry all right new user here uh cher okay are you all seeing it now [Music] okay all right let's start this again so you uh you again missed everything yes yeah and we only have five minutes so i think if you could just give us the high level we're going to have to punt on other uh presentations sadly go for it we'll give you the last five minutes and we'll take the and the very last minute yeah the next tell me which slide to start with oh yes okay so um yeah very quickly there is um work which tries to implement um encryption also secure cryptographic functions for encryption and hashing in the data plane so this lays the foundation for security and privacy applications such as implementing security standards for example ipsec or macseg in the data plane and it might also allow for onion routing or message authentication in the future the next slide yeah there is also an effort to implement authentication in the data plane by offering basic mechanisms like port knocking or one-time passwords and this might allow for continuous authentication without latency overhead or additional middle boxes then next slide"
  },
  {
    "startTime": "01:56:00",
    "text": "um other work is about allowing scalable transparent oh obviously yes no problem yeah so our work is available transparent and lightweight anonymization in the data plane for example by rewriting source addresses and hiding path information by using randomization or encrypting ipv4 addresses to obfuscate traffic and the idea behind this is to address the performance and usability issues of existing anonymity tools like tor okay the next slide um yeah we can also use um computing in the network to improve intrusion detection um one one potential here is to allow for inline detection and a quick reaction to anomalies but we can also reduce the load on existing intrusion detection systems and this is exactly what lewis and i'll do so here they apply rule-based pre-filtering in the data plane and by that they achieve a traffic reduction of up to 75 percent at the actual ids okay next slide and last we looked into network monitoring which can be for example used for network forensics and here soundcheck i for example propose flow monitoring on p4 based hardware switches so they pre-process packets in the data plane and then very efficiently create floor records in the control plane and this offers a high performance and cost efficient alternative for accurate flow monitoring in comparison to existing solutions okay the next slide um yeah so to conclude we see increasing interest of research"
  },
  {
    "startTime": "01:58:01",
    "text": "community and that underlines our intuition about the possibilities that coin provides for security and privacy and also its relevance and this is confirmed by recent publications which match the ideas of our draft and they're published at high rank values for example usenix security and existing work also provides first proofs of concept which show that implementing security in the network is actually feasible so we derived from these observations that enhancing security and privacy within network computing actually turned into a hot research topic and there are many exciting ideas left to investigate in the future therefore this draft underlines the potential of coin to benefit diverse and quite significant applications which also go behind beyond typical networking and um yeah to drive this draft forward now we are strongly looking for your feedback and also contributions so if you're curious about this draft or have any comments just drop me an email and otherwise i thank you for your attention and i'm yeah looking forward to any inquiries okay that's it thank you so much uh really sorry about yeah that was true the network itself got in the way of your presentation but um thank you for that terrific update yeah some of the research that's out there any questions the audience in our last minute i think where was the question in the chat before but i can um get to this offline i think okay okay that sounds good um great then i will stop the slideshare thank"
  },
  {
    "startTime": "02:00:01",
    "text": "you so much for persevering everybody uh in our last minute i think um they're really two topics uh one is just that if you have don't know already that hot nets is going on in parallel this week and there's some very interesting work um and the second point of interest is that of course we've been threatening to have um an interim in order to um review our scope and i think we really we would like to do that um halfway between this ietf and the next um so stay tuned for that we welcome your inputs on the um mailing list and thank you to so many of you for sticking it out despite the network errors for arriving at whatever hour it is in your time zone many of those time zones it's an inconvenient hour so thank you for being here and to all the presenters uh for the terrific presentations i know there's been a really healthy dialogue in the chat window so i look forward to um sharing some of that uh we we all look forward to sharing some of that in the minutes um so thank you once again since we're at the top of the hour for for being here any other comments maurice jose or jeffrey okay i think thank you very very much everyone and uh yes see you on the mailing list and um and see you in um hopefully in person soon and thank you to those who helped hand hold us through some of the user interface issues that was greatly appreciated in real time thank you have a great day evening afternoon bye-bye"
  },
  {
    "startTime": "02:02:37",
    "text": "you"
  }
]
