[
  {
    "startTime": "00:00:13",
    "text": "okay good morning everyone welcome to AI ppm and welcome to IETF 106 so since this is the first meeting we hear it was the note well please do take the time to look through this and have know the what you\u0027re getting into by participating here and I\u0027d have up if you haven\u0027t look at this before the blue sheets are going around so please do sign your name on all of those as well so we have a pretty full agenda today so today we will not be necessarily having time to go through any lightning talks at the end we did have a couple people with lighting content and so you can look at that just on the list of slides that we have so we will be starting with just some status on the documents that we have in the group and then we\u0027re gonna cover some stuff from Greg about stamp the stamp en document and the option TLV after that we\u0027re going to get an update on our registry documents from Al as well as the route the ARA document we\u0027re also going to get some time to talk about multi-point Altmark and the updates there and then we\u0027re gonna have a fairly long chunk to talk about all the work on the various I am documents so these are all the working group documents recovering after that we do have some time allocated to go over other work that is not yet adopted but has been receiving good discussion on the list that we are interested in as a group do we have any agenda bashing any thoughts here all right so looking at the status since we met last time we have adopted two new drafts so we have the IOM Flags document was officially adopted and then we also did adopt the ipv6 options document and this is based on the feedback from 6-man the stamp document is in the RFC editor queue so that\u0027s great couldn\u0027t get that shipped and the metric registry and initial registry documents are currently and I is she waiting on ad write up and we did get some feedback there from Ayane about the registry request and I assume that I will be going into more detail on the status there and the other document we have that\u0027s out there is teaming up being still remaining in miss Roth and hopefully that gets cleared up as all these other documents push through so I\u0027ll be great I think that\u0027s all we have here so with that Greg do you want to come up it\u0027s working it\u0027s working so far let me know if it doesn\u0027t all right okay "
  },
  {
    "startTime": "00:03:19",
    "text": "so the update on the yang model is that trying again okay yeah okay so we discussed and we had a very good comments from Hendrick neidell from Assyrian about their resolution for their timestamp and looking at applications that are in out reliable latency silo of 5g we agreed that it will be beneficial if their unit would be a nanosecond not a microsecond so that what we have changed we changed units for their delay and delay variation from microsecond to not a second okay any questions objections to that these are 32-bit nanoseconds it\u0027s a gauge - yes so every two-bit minute seconds means the max val is four seconds yes that\u0027s that\u0027s what we thought for delay we made it 64 god I think that we need to go back yes for the delay it\u0027s a 64 so which gives us plenty of delay but for delay variation yes your variation okay okay okay yes we actually we discussed that and we decided well we might get by with the 32 for delay variation but again because this is a working group document we made this change and we proposed it so if there are any questions concerns please bring it on the list next okay no just a little bit back so and for the percentile we made the change because it was a different units so but now it\u0027s the same nanoseconds because that that would make sense for the percentile another change that we made is their way of expressing the request for the percentile to be reported it used to have three decimals "
  },
  {
    "startTime": "00:06:19",
    "text": "now actually now incorrect myself it used to have two decimals which allows you to do 99.99 there was a suggestion okay why not to make it bizarre and make it five decimals because we do have space so now if you can express five nines after their decimal point so if there is any questions concern please bring it up and we\u0027ll talk about it but that\u0027s for now what is potential to express as a percentile request next please so there UDP port we had a discussion in the list about what could be used as a destination UDP port and the agreement is that one it\u0027s 862 which is a sine 2t womp test protocol and then it\u0027s a user range and a dynamic range the user range does bring their potential for the using stemmed packets as a denial of service attack but well in another hand we were told is that operators would like to use this tool as a simulate certain application so basically to be to to have a higher level of guarantee or chance to follow the same path if you share the destination port address with your application and usually that probably can be done and used we need you\u0027re doing a service activation testing so when there is no active traffic so you\u0027re not piggybacking on your active application again if there are any questions concerned please bring it up next slide please okay so we still all a lot of updates and improvements to the text with the reference and everything to address all the comments received as early young doctor review we\u0027ll be working on it now more actively once the base specification is out of our hands and being published and comments are welcome always any questions okay so let\u0027s discuss it then on the list next please extension draft next no it\u0027s fine yes "
  },
  {
    "startTime": "00:09:21",
    "text": "so we do have two new coffers who welcome Eddie and their Nesta and they contributed to new extensions that are excess report TV and follow-up telemetry TLV please next slide so what is excess report Tilly it comes from performance measurement function that is worked by 3gpp and their intention is or at least one of the proposal is to use stamp protocol as a performance measurement function in a TSSs but it does require or with it was considered that it will be convenient if the same protocol in addition to measuring performance will be able to do some specific 3gpp pass information in in way of announcement and this is the purpose of excess report TV which provides the information about excess identifier and the return code which simply signifies basically working available and not available that\u0027s for now and excess identifier actually it\u0027s a very simple clear for characterizes the XSL whether it\u0027s one of the technologies that define by 3gpp or then just identifies as non 3gpp access and because this is more like even its announcement but it requires explicit acknowledgment so once there note sends this packet stamp packet with the access report TV its arms the timer and then if timer expires then it may repeat the report and there should be a mechanism to control the timer I think that their default value is set probably like three seconds and default number of transmissions consecutive transmission is four but again that requires control so that can be change okay let\u0027s go next follow-up telemetry TV I think that back in or one T one documents with this especially in t1 this document we discussed that for the reflector and the same for the sender to get accurate timestamp is a challenge because the timestamp better "
  },
  {
    "startTime": "00:12:22",
    "text": "to be recorded in the packet as close as possible to the physical transmission of the packet so to exclude queuing which introduces delay variation that\u0027s effectively that\u0027s the purpose of this but that\u0027s basically to collect the follow-up telemetry from the reflector and I can explain why the sender is not critical because especially in a virtual machine environment the sender may get their physical timestamp from their hardware once the packet being transmitted so it can associate this value even the value in a packet was different so the sender does know the real transmission timestamp the problem is more on the reflector side because the reflector puts the timestamp then time stamp is passed for the transmission and when their Hardware passes this timestamp to their reflector application in the packet long gone so that\u0027s the purpose of follow-up telemetry TLV it may be included in a consecutive stamp packet so that real transmission timestamp for the preceding packet will be returned to the sender in this way basically you can associate and because it has a sequence number of that preceding packet so you can associate the real time stamp of a sending packet on the sender with the real time stamp on the reflector and get more accurate measurements hi Sarah banks correlate I have a question are you are you proposing then if I set the time stamp in hardware send it to a reflector the reflector can\u0027t consume for whatever reason that time stamp you\u0027re allowing the reflector to overwrite the time stamp with its own time stamp no no no okay no again in a packet in the stem packet there we are collecting three time stamps so their sender transmission time reflect the receive time and reflector transmission time this is the follow-up information so basically in addition it\u0027s a TLV that follows stamp packet and it collects specifically on the information that the reflector may have from the hardware if we believe that it may it has access to their more accurate time stamp for the proceed Paquette it doesn\u0027t override any other information at times them that we normally collect in the stand packet it\u0027s an additional information okay "
  },
  {
    "startTime": "00:15:23",
    "text": "let\u0027s okay let\u0027s imagine this their sender sends the packet it received by their reflector and then the reflector prepares that reflected packet and sends it back so in the packet that it sends back it has three time stamps okay the next packet from the sender will come with the follow-up telemetry T of V so the reflector will put all three time stamps relevant to that second packet in a stamp packet and then there are times them it acquired for the preceding packet it will put in the follow-up telemetry TV so there is no override there is nothing no information is overwritten its additional information is the confusion about what that extra time stamp is representing because it\u0027s yeah the actual transmission time right well again the reflector doesn\u0027t know it\u0027s actual not but if there is a situation that hardware provides or system provides with a more accurate time stamped on the physical transmission of the packet and this information are at least in some systems available to their application that requested the transmission it\u0027s not clear to me how an application would make the decision that the reflectors time stamp was more accurate than the sender\u0027s no it\u0027s not the sender\u0027s all there don\u0027t come to the mic how do you know this the sender didn\u0027t put our Hardware time stamping in the first place no sender doesn\u0027t involve here this information is returned by the reflector Albertan this this seemed to make sense to me but maybe maybe deserves more description in the draft great okay thank you sorry Rick Taylor um I understand what you\u0027re doing here I I have a couple of quick questions and it might be my lack of knowledge in stem no how does the application that\u0027s actually consuming this telemetry know that follow-up tlvs are going to be used for this particular path and perhaps not for that path where is the negotiation with the user application ah okay that\u0027s under some random fourth TLV oh I forgot about that sequence no okay again the assumption is that this information is returned for really immediately preceding packets understood yes okay so true there is no assurance that our packets even consecutive packets follow the same path understood again but if if I\u0027m writing an application this is using "
  },
  {
    "startTime": "00:18:24",
    "text": "stamp how do i as the application in my control loop know that I have to wait for the follow-up telemetry TLV you know don\u0027t get the accurate timestamp or just say hey I\u0027ve had my response I\u0027ve got my three times I can I go act okay okay good question because instantiation of the test session is based on some management and one of this would be yang model so you would know whether you want to use follow-up telemetry and whether your reflector will support follow-up telomerase so pre configuration here has depend on both ends yes okay understood things you could have in a I have a little bit more a process related question and it\u0027s more on the previous till the Edit because since this document was adopted as a working document you not just added to T of ease to it right so like how much more do you want to add and how do you like why does it have to be in this document you could also follow doc documents that will be discussed separately by the working group and especially the previous govt so there\u0027s no work in 3gpp that is adopting stamp already for any kind of measurements and I\u0027m not sure how likely does is so maybe this TIV is not even needed at all there\u0027s no decision made so maybe that\u0027s too like premature to actually edit to the draft okay point taken thank you sorry Rick Taylor again I\u0027m going back to the previous TLV and I suppose I need to go away and read the draft properly actually there there is mercy reflected so it\u0027s basically it\u0027s very special case for their 3gpp PMF requirements what they want to do and decision was that yes if we use stamp as a performance measurement function then why not rather than introducing a new mechanism use existing mechanism to do this state reporting okay so I\u0027m involved with doing a lot of performance measurement on non 3gpp radio rings so I\u0027m really quite interested in sorry I\u0027ve got a cold I\u0027m really quite interested in this sort of extension point for non 3gpp radios so a I need to get involved and be I\u0027m not sure this is cooked yet I think there\u0027s some more that could be done here to make it a bit more powerful so I need to understand better what you\u0027re trying to achieve with this kov there will be excellent so let\u0027s talk together let\u0027s bring you the to the list and let\u0027s make it okay so "
  },
  {
    "startTime": "00:21:24",
    "text": "yeah comments are welcome I think that working group was go it\u0027s a premature yeah yeah that makes sense alright thank you for sharing this I think based on this discussion a couple follow-ups a you know since we are adding tea leaves here which I think you know can certainly be appropriate but since this is a working group document we do want to get a good idea that the working group user this in consensus oh I think we can maybe have like an just kind of getting consensus on these TVs make sure if there\u0027s any concerns about them that can be raised and we can see if these are appropriate to take for the access report TV I think we\u0027ve heard feedback that having something that\u0027s generic would be good and to the point of the interaction with 3gpp it may be easier if we you know allow it to be used for other cases such that if 3gpp doesn\u0027t decide to use it it\u0027s still fine and I think we do want clarification I read through the texts again for the follow-up flow injury and at least my impression is training to help you measure the delay between software and hardware transmission reflector in a way it was like kind of explaining why you\u0027d notice I think would help get to the point because I think it\u0027s useful but we need to have clarification sorry very good question again underlying my lack of knowledge on stamp is there any kind of inbound session in band extension negotiation within stamp at all it\u0027s all band pre-configuration so from a process perspective you could move ahead with the raw stamp but the basic core stamp documents and then produce later drafts which say this is extension have an eye on a registry or whatever is required for I don\u0027t need a registry there\u0027s no code place so you can split this up from a process perspective okay thank you okay hi everybody I\u0027m al Morton and we\u0027ve been working on big teams basically have been working on a performance metrics registry and the initial contents of that registry since 2012 can you believe it let\u0027s finish it up okay and and really we\u0027re damn close now and you can see that right here the IETF last call is complete and willing about that after me Ria\u0027s review Mary\u0027s review was very good we made a lot of changes and drafts 11 and 20 the contents and the registry design itself are the result Thank You Maria so now we have the Jen art and the security reviews and and they really were mostly about helping Ayane do its job and that\u0027s good but we also got an i "
  },
  {
    "startTime": "00:24:26",
    "text": "Ana review during last call and they basically said what the last line is you know they their reviews complete but they\u0027ll work with the author\u0027s to establish and populate the registries and that meeting happens this week I\u0027ve seen Michele cotton she\u0027s here if anybody\u0027s interested in joining that meeting with Michelle and me and the rest of the Guyana staff to sort out the the last details Thank You Benoit will will do that Ronnie even who commented for Jen art may be willing to do that as well so the big knockout for understanding this registry and what it is has been a mock-up that we created and we\u0027re trying to get Ayanna to post that in a like a temporary place so we\u0027re going to work that out this week too and here\u0027s why the registry itself is is fairly complex it\u0027s got all these categories of summary metric definition method of measurement output administrative info comments blah blah blah I Anna is only going to deal with the summary column and have a link to a text file that displays the rest and here\u0027s how that looks in practice so we start out here we have the registration procedures now incidentally that\u0027s going to change when I first met with Michelle about this years ago she said please make this expert review please make this expert review please make this expert review just like like saying Beetlejuice three times and and so I got and so I heard her and we made it expert review but it looks like it would have it\u0027s better to make it specification required because that includes expert review so now you know we\u0027re gonna make that change and who are the experts its performance metrics experts and and so forth so this information should really go in the in the registry design draft as well not just the mock-up so it\u0027s a it\u0027s a performance metrics group we\u0027re gonna have a performance metrics registry and and really there\u0027s two registries included below there but there\u0027s I think we\u0027ve actually designed more now so the summary includes an ID a name for the metric the URIs which we\u0027ve reduced now just to the URL to the text file that\u0027s HTML eyes that brings you all the good stuff and about building a metric and method of measurement that\u0027s compliant and we have a nice description there that nobody\u0027s going to be able to read today at all my apologies the reference the change control is an open issue Michelle told me when we added that it should be IETF there was a discussion about it that maybe it might become iesg I don\u0027t know if that discussion has concluded but we will do the right thing we\u0027ll put the right value in when we\u0027re done okay so let\u0027s just take it as that we know that the that the names like RT delay active IP "
  },
  {
    "startTime": "00:27:29",
    "text": "UDP periodic blah blah blah that\u0027s that that\u0027s going to that\u0027s going to evolve over time as we get more metrics so we want the metric name elements to be a registry that people choose from as well extensible registry just like the metrics and we will be able to add to those four different things that we measure okay does anybody in the working group want to raise issues on these two drafts now or forever hold your peace thank you Mir yeah I mean I just noted that it says expert review or maybe specification credit in future and IETF standards action is that on purpose that you only want proposed standards and not information or documents in here we know we don\u0027t want we do want specifications I think we have a fairly high bar for this it\u0027s got to be metrics that are in widespread use and we don\u0027t want to be sort of blasted with with metrics that are yes I think you don\u0027t have industry consent let\u0027s put it that way you want a specification and expert review if you if you assign based on a specification or if you have an hour see you don\u0027t need the expert review is that weird oh no no you still need expert review because somebody\u0027s gonna build somebody\u0027s gonna have to build the metric entry and the experts are going to review that as well as the specific with the support of the specification because at least on your slides it says ITA expert review or standard section right right so that\u0027s gonna be standards well I guess it\u0027s gonna be reference required that\u0027s a special occasion yeah that\u0027s we\u0027re gonna change it to yeah I haven\u0027t I didn\u0027t update this I and then sorry sorry sorry sorry good good catch I have to fix that yes please Rick Taylor very good question are you still reserving some experimental space these things are hard to do and will need a little bit of random space to build experiments to work out whether what we\u0027re doing makes any sense at all before bringing it to the IGF here\u0027s what you can do we already found out when going through working group last call that there was like an experimental non experimental but a private version of this registry being built in Brazil and that was kind of cool and they helped us clarify a few things so I would say you know build your metric test with it be able to tell us what how widespread it is and and and whether the meth whether the met specification of it works with sufficient energetic clarifying answer can you go to you go to your example a registry thing yeah show me a show me the thing with the IDs I\u0027m going the wrong way yeah yeah yeah right so that ID number that\u0027s not a code point that\u0027s gonna "
  },
  {
    "startTime": "00:30:31",
    "text": "show up on the wire that is that is you int in you int infinite length T right right so like there is the practical limitation that like you probably want your your names to be less than four kilobytes so the set of names is like the set of possible four kilobyte UTF strings which i think is large enough that I mean there\u0027s no that yet there\u0027s experimental space because there\u0027s no there\u0027s no scarce identifier space that were allocating either okay if someone were to then build a protocol that used this registry and say I\u0027m going to use the ID we\u0027d have to talk to them but um like the registry here has like a whole bunch of pointers to this working group so as long as this working group is here and it\u0027s been here since 83 so I think it\u0027s gonna be around for a while um then you know they would come to us we would say don\u0027t make this a UN date T but I don\u0027t think that needs to be in this doc though because the doc is not like though the level of metrics that we\u0027re dealing with in this registry and the transports we\u0027re gonna use for those are going to be not such that we really have constrained space like it would not make sense to have this this this ID be a constrained integer space okay thank you better answer much better good thank you cool um no requests for the floor so I\u0027ll send to the list when I schedule something with Michele cotton and her staff from I Anna folks are willing to join us please do and let\u0027s move this along this is too big drafts that\u0027s been around for a long time it\u0027s it\u0027s helping get stuff off our milestones thank you thank you so yeah we\u0027ll get you\u0027ll send out an update of when you\u0027re gonna meet with Ayanna and then hopefully after that we can get update of yeah the outcome of that to the list and then when do we expect probably to have another Rev to incorporate that it could be this week I\u0027ll try to edit while Michelle and I and Benoit are talking do it live yeah yeah yeah yeah yeah okay so next we have Al yes okay so a couple years ago we started on a draft on route metrics and Carlos Pena Tarot gave it a cool name the advanced unidirectional route assessment aura that just identifying that acronym so we\u0027re at version 5 last time we had Frank bottners review which helped us change the terminology instead of the old RFC 2330 host we\u0027re using node now throughout the document and but we\u0027ve retained that where it makes sense that\u0027s a quick one there so that\u0027s cool we\u0027ve differentiated some of the "
  },
  {
    "startTime": "00:33:32",
    "text": "limitations that we were discussing in section 3.6 about trace basically they\u0027re about tracer a child methods and getting back the same my IP address from the from the host that\u0027s not going to be true for hybrid so we fixed up a lot of items and 3.6 to say that there\u0027s differences when you use the hybrid methods so then the formatting of the results we decided not to update RFC 53 cents punt the future development of that to a yang model and the working group agreed footers comments and questions were also about that resulted in adding another requirement for for the future model the original sender\u0027s time stamp so we got a list of requirements now in Section three six so then we said in IETF 105 back in Montreal we said we\u0027d like to see a working group last call at IETF 106 started here at least and now with the result resolution of those comments and the new drafts we think we\u0027re ready for that and that would help us you know move what we believe is a very mature draft on its way and so working group last call please see a show of hands of how many people have read Beck recent version of this document v6 hey Carlos put up your hand all right yeah sounds good so I mean please do take a look at this and I think if we do a we can get a last call going for that sure I mean we will send out an email you wanna send out an email hmm yeah give it three weeks and yeah please do read through this and race any issues you see but yeah it would be great to get this moving along cool thank you thanks everybody I think I saved you some minutes oh geez FAA coming up and like this was the so we we wedged this onto the agenda at the last minute and a great expense because we just completely forgot about you I\u0027m sorry we gave you five minutes we\u0027re actually running a little bit ahead so you can you can use a little bit more than five okay good morning this is an update of our multi-point alternate marking draft just a summary about the idea we want to introduce this intelligent rhyme approach so we started you may recall from our FAC 8321 that allowed unicast point-to-point flows performance monitoring and we moved to this multi-point alternate marking approach "
  },
  {
    "startTime": "00:36:32",
    "text": "that allowed monitor of general multi-point anycast flows without any constraints so what is the motivation the main motivation it is resource consuming to monitor all the flows and all the paths continuously so we should figure out a way to make these in a flexible way the solution is to put intelligence into the system so to give the intelligence to the controller to be able to manage the performance measurement and for example it can start without examining in depth the network and then go deep if the problem have to be analyzed in some position so there are two ways to act if we go to the implementations the first way is of course to specify the traffic filters to find more detail it flows to be monitor it or the second way of course is to activate you new measurement points and how we activate two new measurement points by using the clustering and network clusters partition so networkers clustered partitions allowed the division of our network graph into clusters the third and smallest sub networks that we can identify and this class that can be also combined in different way to activate of course new measurement points the change that we did from the zero to version was in particular we added the reference I remembered the comment from Jose Ignacio and we added the reference from to a new paper that is about the definition and mathematical formalization of the algorithm for cluster partition so that it can be applied for every kind of graph and you can download this paper but also I became aware that it was published so you can also find on I Tripoli Explorer by yourself anyway is also available on this website just look again at the algorithm this is the iterative approach we can if we have a network graph we can the first step is to group the links with the same starting node so for example in this network graph in figure we have we can identify five group with the same starting node and then in the second step we can join the group with the at least one ending one node in common so in this case for example the group number two and group number three will be joined to to be the cluster two so in general for these Natur graph we can identify for cluster this is just to give you a view in this case it\u0027s very simple but you can imagine that if you have a network graph more complex we can make this at a different level and we can also combine the different clusters at different level to activate more measurement "
  },
  {
    "startTime": "00:39:33",
    "text": "points so just a recap on what kind of measurement we can do so this is a complete performance measurement framework so we can measure the packet trust on multi-point on multi-point path basis or we can do that for single flows or for dual network so these give you the flexibility to measure packet loss at a different level such as I said and in addition of course the delay measurements that in same way can be done can be done it at different level so we can measure the delay on multi-point but basis that this means that the delay has the meaning and is a representative of the multi-point path or we can do this in a traditional way on single packet basis in this way we should ask the help of the another FSC that is the 5475 about the hashing selection because the double marking methodology of the fs8 321 does not work for multi-point pot so we should use the ashing approach to couple the samples and but in this case it will be simplified because we can use the clusters to couple the our samples in terms of space and we used of course the marking method to couple the samples in terms of time so in this case the decouple the coupling of the our samples is very simple okay the next step so the document is stable and we want to the outers suggests to begin the path to become erect see if you have additional inputs comments of course our work so I\u0027d like to see a show of hands on this one who\u0027s who\u0027s read and reviewed this document one two three four or five okay good so by beginning the path to become an RFC is that a is that a sidelong request for a working group last call or yeah yeah I think we\u0027ve seen well let me actually have a look so many data tracker tabs open there\u0027s November 4th right yeah do we want to start it go ahead and start over last call on this one we can do that - we can pipeline it yeah okay so we will well we\u0027ve had bad like like we\u0027ve done a couple of times where it\u0027s like yeah we have a working group last call let\u0027s start another working group less call after that first working your close call "
  },
  {
    "startTime": "00:42:33",
    "text": "and we forget about it because like the this working group last call just started for um to end on December 9th so then we do want on December 9 that ends at Christmas are we gonna do that we\u0027re gonna remember they do that as you\u0027re saying our track record here is not great okay cool so okay Tommy is putting a reminder in his phone so we\u0027re gonna start a working group last call on this one once the working group last call on on Ora is done which will be December 9th and then we\u0027ll click the buttons and make that happen and then that will probably end yeah like a couple of days before Christmas so and then yeah we\u0027ll definitely have it done cuz I must present yeah exactly there you go okay cool alright thank you very much so next up we got Frank oh you\u0027re okay I was looking for you earlier you\u0027re hiding back there we have loads of time right 25 all right so um quick update on clicker so quick update on the three wicking group adopted IOM graph so not the whole kind of enchilada so the oldest and I think most well discussed document it\u0027s the data draft as we typically refer to it so the data draft has seen two updates since the last meeting and I\u0027m going through them sequentially so the first one is addressing or the zero six two zero seven update basically adjusts well a detailed review from are you and another detailed review from Tom with the additional feedback folded in from the hackathon that we\u0027ve done at 1:05 and so there\u0027s two types of main updates that we received from there so the first and that\u0027s I think the the technical real update was a rearrangement of the flags feel that we have for the various data fields in in IOM and so the starting point of that rearrangement was that out of the hacker column hackathon came the suggestion saying well you have 24 bits for the the various data fields what if we ever run out of that so we need to go on at least have one field or one bit dedicated to future extensibility so let\u0027s reserve one of that if you reserve one of those well make it the last bit in the field oh the last bit was taken so 23 was already taken so we wanted 23 means like from there you kick off another range of additional changes that roll into that so we made room for that "
  },
  {
    "startTime": "00:45:35",
    "text": "and then the additional thing was if you look at how I um data fields a structure right now we have two types of data fields we started off the the initial journey with always fixed format data fields were they with a defined syntax so all the fields like node the interface identifier timestamps and the likes they all have fixed format which is good for a hardware implementation but later on I think people said well I want this and this and that I want flexibility so we also have to Yuri style formats which is what we call the opaque snacks rabbit so opaque snapshot field a little bit of a funny name for a Tod formatted content there and the argument out of the hackathon was like don\u0027t really give me a field structure if I need to go and implement that that I have something fixed format then I have something flexible format followed by something fixed format again so not a great idea so it\u0027s much better to have something fixed format and then well have the flexibility later on so that from a parsing perspective things become more simple that meant okay you want to go and have the opaque snapshot field after all the fixed fields that means opaque snacks takes reference to opaque opaque state snapshot especially at 3:00 a.m. in the morning right it is now bit 22 so that everything is fixed prior and then you have opaque state snapshot I\u0027m getting better at it and then you have well anything future that we don\u0027t really know about so bit 22 was taken and then well we kind of move the other stuff around to go and not end up with any holes in the current allocation so that\u0027s the bids rearrangement that we\u0027ve done as part of oh seven the other thing that really changed is and that\u0027s what hyoh drove is when we started writing the document it and if you looked at the the recent version of the document it would became very apparent that the document had a shitload of different authors and given that we did this kind of really get hop style so everybody implemented something and then we ended up with things like some times it was referred to as IOM type some time it was called the IOM header some times it was called IOM option but basically we all refer to the very same thing and now the very same thing is called the IOM option type again not a beautiful world would but a kind of it\u0027s a perfect compromise everybody is equally unhappy about the compromise why not the IOM option type header "
  },
  {
    "startTime": "00:48:35",
    "text": "header option type don\u0027t create a pull request we\u0027re happy to hold it in but make sure that you\u0027re catching all of them so I tried really hard to go and catch all of them and yeah so that cleanup happened along with so we had certain things that were really given a dedicated section like nomenclature for the namespace section but other of things didn\u0027t really have a dedicated section in section 4 or a subsection that again got cleaned up so there I hope that I kind of made the overall thing more digestible cleaner with the same level of kind of nomenclature throughout the document so that we are really ready towards well ultimately working group last call and then I pushed that oh seven out on the mailing list and well what typically happens this there\u0027s something else that you forgot about and that was issue 135 where one little thing that we forgot about to specify was is what happens on D cap if well what should know do on D cap and well we believe that it should go and get rid of all option types no matter what for a particular namespace that it\u0027s kind of responsible for and that was also not said in the draft so again a little bit of additional cleanup that is in there and so sent to the mailing list no comments received but well that\u0027s usually there is one more comment you can not do that unless you well we\u0027re an idea and we brought up the question earlier on so I think at 8:30 teller was friendly enough to go and share you\u0027ll a breakout meeting or a site meeting like it\u0027s called here and bring up the question again is there any comments on the data graph and guess what it was one and then Greg had one and he posts the question and I want to go and reflect it back to the room it goes back to the earlier thing about what data fields we have today we have data fields that are fixed format and then we have this kind of catch-all tle style for everybody else now if we look at the fixed format data fields there is a little bit of flexibility into the fixed built into the fixed data fields because we have short and long format of for different types of fields node ID interface identifier timestamp and namespace specific data and well these fields can be kind of combined even right so you can have short + white make it super white so that was us to give "
  },
  {
    "startTime": "00:51:35",
    "text": "one give us fixed format fields with a little bit of additional flexibility so that people who are really constrained on space and I know that several people are because they say well I\u0027m ready to go and record 60 bytes I have a diameter of say five hops in my network and how much information can I get really fit in so people believe that there is use but the question that Greg and I think for a really good reason rice is do we need this kind of interim flexibility should we really have something that is really small and fixed and then if you want to go a little wider you end up in the TLV land or do we want what we have right now which is small and fixed a little larger and fixed and well if you want to go crazy you go crazy with TVs and make your heart where bleed so I think the question two the room is do we want to go with what we have and if we want to go with what we have I think we can go and consider really being ready for finally after 15 iterations of the document 7ss personal draft and and another aid as as working group document that we were able to progress the working last call or kind of how do you want to go and deal about the question that was brought up in in the earlier in the early uh breakout sorry site meeting it doesn\u0027t matter what you call it you can call it an option type hitter option type header so uh me from the chair it looks like we\u0027ve sort of lake accidentally adopted a model of pipelining working group last calls which I actually kind of like so then we are so then we\u0027re basically saying okay well we\u0027re gonna we\u0027re gonna be doing a working group last call continually at this point probably till the next meeting can we sign you up for the third slot which would be a working group was called beginning on the 6th of January and ending on the 20th of January so this gives you time between now and Christmas to make the changes do we wouldn\u0027t start the workers comp we don\u0027t even need a change so maybe you can go and at least get a sense in the room for whether we need more Jesus otherwise I\u0027m sold yeah does does anyone here think that that we will not be ready to start a working your boss call in January on this one like I you know as somebody who\u0027s looked at the document and watch the progress you know we\u0027re now sort of seeing you know the the scope of of what we\u0027re fixing narrowing and narrowing and narrowing and it\u0027s like we\u0027ve done the editorial changes and it\u0027s like it\u0027s it\u0027s looking kind of don\u0027t we think that is kind of contentious was moved out right right yes and so yeah we should have a stable base and I think it\u0027s really beneficial for everything that we do to have a stable base so that we can "
  },
  {
    "startTime": "00:54:37",
    "text": "go and get the stable base progress to working group last call and then we can go and deal with the edges right so what one question I do have about the stability so it definitely seems to be getting worse we did from the last hackathon and ITF 105 get good input that\u0027s it can you give it update on the status of essentially those implementations that did hackathon work have they updated to the new stuff do we have kind of like we have confirmation that the changes are I seem to be good and well so if you just be good to do that also potentially before do we have people in the room that we\u0027ve done the hackathon before who have been at the last hackathon it\u0027s just in here I don\u0027t seem oh no did you include the oh seven changes in the I um of the notation for the kernel not yet it\u0027s not yet all right would that be possible to play with before January yeah okay great and see whether it\u0027s breaking anything right so that would be helpful okay so you\u0027d be appreciated to like set an update to the curtain on that so yeah we will we will then start a working group last call on this first thing next year so 60 January you obviously if you have issues you don\u0027t have to wait for the working group last call before then it would be neat if the working group lost call is just everybody saying thumbs up so between now and then um greet the draft figure out if there is anything that you know needs to be fixed I would I would say let\u0027s presume that there if there is going to be a new Rev of this that we just bang that out on like the pro KS or vi is a holiday in Germany so bang that out before Christmas it\u0027s not a holiday in Germany it\u0027s only a holiday in Bavaria oh great I was the difference between these services which are commonly forgotten about someone so like Germany take the politics out of the IETF soon yeah so what various norman germany for you right the closest bit um so yeah so let\u0027s say if there is a rev that comes out of discussion let\u0027s like sort of try to get that done in december so that on january we can just hit the button and everybody says yeah thumbs up and then it\u0027s done on the 20th and then we can try our luck with the isg before we do this right so could i get a sense in the room maybe a voice or two or maybe we can even do a hum whether people feel like we want to "
  },
  {
    "startTime": "00:57:38",
    "text": "stay with the current format of what we have from a fixed fields rate of that yeah so this is this wide and short do we want to go and keep it or they want to go and not have white format I think that was the the thing that kind of bubbled up in the the earlier in the earlier site meeting I think any any any common any way would be greatly appreciated because well if we have say some sense in the room that people say well let\u0027s go ahead with what we have at least some indication right or they go and confirm that on the list and I\u0027m gonna go bring it up but let\u0027s any opinions are there people in the room with silicon other than you that would be great because this is that yeah so um hey Rach do you have an opinion yeah let us let us get right because this is like that could be you like it just I think there\u0027s people on on online yeah want to go and rape they can\u0027t see it they also can\u0027t hear you now they can see and hear yeah I\u0027m for it because 16-bits may not be enough for many cases so you want to stay with what we have yeah although far the cases where you don\u0027t need too much telemetry they may be enough so okay we say with what we have is like what\u0027s here in this yours yes what\u0027s in zero eight is good enough yeah okay any other opinions just also to clarify so the options that were discussed is essentially sticking with what we have or going to just the short fixed correct you so some people no one ever talked about Lysa just doing long fix know that the suggestion is to go and well we had that as well but kind of that was like we got rid of that in the site meeting already okay so some people said like let\u0027s use the lowest common denominator which means the lowest common denominator would be 48 bits for 4 node ID and then 64 for interface IDs and whatever so that means like packets quite large and and I think that was like for all kinds of reasons not considered so I think the option is to go and drop anything white format which I believe is fairly constraining as someone without a dog in this fight this seems like a perfectly reasonable compromise for it so this is let\u0027s definitely confirm this on the list but I think most of the people who have an opinion your were are in the room or were in the room at the side meeting yeah so I mean it seems I would be surprised "
  },
  {
    "startTime": "01:00:38",
    "text": "if this comes back up so it was there for a reason right it was put in for a reason yeah it is there for roughly three years for a reason yeah but yeah well some people always have a different opinion and I think it\u0027s worthwhile to confirm that we\u0027re still off the same okay now that we\u0027re three years further sounds like an opinion to me great and I would say yes this would be surprising to me please don\u0027t surprise your chair um so if yeah this is we\u0027re you know the working your boss call is where it is because we want to focus the working groups attention on one document in time to get these things out but please do you know speak up as soon as possible because the idea is is that like what either what we have right now or a tiny revision will be what we do working your boss call on based on you know discussion between now and the end of December okay cool all right you watch so next one flags so that\u0027s actually a draft that tell wrote wrote for all of us because it was text that was originally in the data draft was seemingly contentious and well following the strategy that we just discussed like create ourselves a stable base let\u0027s move everything that is like mm-hmm not entirely sure in to dedicate a document that\u0027s one of the the examples and well the the direct export thing is later on another example where we kind of moved things away from from the the core so since we discussed last time we working a rube adopt at the Flagstaff what they which is reasonable because it was in an adaptive working group adopted talk iam and document already and it even went through additional revisions now that we had three flags in there one was called active one was called loopback and the third one was called immediate export given that immediate export had another raft of discussions and well it has its own dedicated document now we removed even that one out of there so that means by now it\u0027s only two flags in there loopback and the reactive and we had a bit of discussions last time like well what does that mean these flags because they drive a set of behavior and maybe you don\u0027t want that set of behavior for all the packets because well it could go and course harm if it\u0027s really for all the packets so we clarified and specified that well absolutely you want to go and put constraints in so that you only do this for packets that you really want it in the site meeting and that makes let\u0027s make sure given that my brain is not entirely working I\u0027m able to read I\u0027m not really able to remember so that site "
  },
  {
    "startTime": "01:03:38",
    "text": "meeting today revealed another set of things we definitely want to go and add to the draft on loopback and there\u0027s that documentation is already in issue 138 and Barack created 138 for that so the first thing is with loopback you were recording on the way and you\u0027re also recording on the way back nobody knows why we\u0027re recording on the way back and we couldn\u0027t really go and find a good use case for that so we\u0027re gonna go and I think the the suggestion from the site meeting is don\u0027t record on the way back the second thing is loopback only makes sense if the sender is also the encapsulating note cuz otherwise is a little funny because loopback is a probing mechanism it\u0027s kind of a fast trace route mechanism that\u0027s what it\u0027s built for and so Sandra needs to be on cap node we need to go and explicitly state that and in addition well loop back only makes sense because you have a node that is supposed to go and send traffic back to the source if there is indeed a source address in the packet so again we want to go and specify that it\u0027s only making sense for encapsulations where the source is indeed part of the encapsulation or the packet for C and in addition to that well we want to go and have a statement in encapsulation graphs on how loopback would actually be used for a particular encapsulation I think that again makes perfect sense and in addition to that we want to have a statement that well even add notes so a in an IMD capsule ating note receives a loopback packet that it hasn\u0027t originated it should go and drop it again I think from a security perspective makes perfect sense it should not happen if it happens go get rid of it so that was really the discussion on loopback refining what we already have in the draft which is why I believe I think these site meetings are super useful you get far more active discussion rather in the bigger room so things are more entertaining even if it\u0027s really early mornings so that is something that we want to go and do refine on the loot bag option in in flax the other thing that we have as I set the second flag is active active didn\u0027t really have that much of a discussion but there is one thing that again great mirskiy brought up as well a bit of a clarification and his question and i think his question is a worthwhile he said well there is active OEM protocols there\u0027s many of them you can add iom information to an active or william protocol check right "
  },
  {
    "startTime": "01:06:41",
    "text": "so you could run BFD and you can add IOM to BFD why not what is the purpose and why do we need the active flag in addition to adding the IOM information and well we the active flag was put in to basically help implementers and help hot where to go and tell them like with a single bit you need to go and look at that need to look at that deeper and so that that particular behavior wasn\u0027t entirely clear from the text that we have so we\u0027re gonna go and add a paragraph that says well the active thing is really to help implementers and here is why and so the likes of Parag are gonna go and help specify further why this is needed in order to clarify the use of this you could call it a helper bit for implementers so that\u0027s nothing the discussion that we had on the active bit in the flax draft and that\u0027s pretty much if did I forget anything tell I think you took detailed notes in the meeting today okay cool so that\u0027s it on the flax draft if we\u0027re mature with that could we go and consider like want to hear anything else about this whole thing the original idea was to go and keep it in lockstep with the data raft because these two were good friends question is should we go and do this pretty much at the same time line I would love to because these guys go hand in hand well but only if we are really kind of stable Miriah you could have been so I\u0027m still a little bit worried about the loopback flack because you designed a really nice protocol for an amplification attack here right and that\u0027s usually what we tried toroid sorry what are we trying to do you designed a really nice protocol that makes it possible to have an amplification attack very easy on your network I think actually that the flags draft is the is the reflection attack the direct export is the amplification attack rate because it\u0027s direct so loopback is send a message back right you said well right yeah okay right this is this is the amplification attack right and and like the only countermeasure you have in the draft is you should like each device should limit the number of packets they can send but it\u0027s like on a per device basis but then if you have a past you have a number of hops and it\u0027s still not great I think this is just like not acceptable I think you need to do better than that you need some kind of you know excess control you have to make sure that not everybody is just can select your network just sending one packet I think you need to do more work here if you want to get it through the whole process so do you have "
  },
  {
    "startTime": "01:09:42",
    "text": "an idea of so you I mean the idea so you have a problem but no solution that\u0027s always saying no okay I have many solutions but they might not fill your requirements I mean like one is used traceroute because that is not an implication to take it slow that\u0027s we\u0027re trying to solve that trace trace route is slow for a reason but it has the same number of peg if going in and out that\u0027s so so the the fact that a single input packet can only result in a single output packet of a generally smaller size is a we have come in the transport area to understand that this is a core requirement for networking at that layer if you want to not kill yourself I would suggest that you set up IOM Flags loopback on a routing loop in a hackathon and then this was this is this is like instructive right like so if you can create a routing loop with a loopback flag you have now melted your network like and I think that\u0027s not like set it up and and watch it burn just it\u0027s educational it\u0027s fun right like the lights they come on in and they stay on forever and then probably a device reboots yeah I mean the other I mean whatever you do here it\u0027s more heavyweight right so the other option would be to have some more access control so that you actually can identify the device that has send a loopback request and only reply if you know that this is a device that is actually supposed to do that so you have to add more information to your bacon so from a strategy standpoint the question is do we want to hold up flags in IP p.m. while we try to make it safe or do we want to send the two documents up to the iesg and IOM data goes through and Flags gets killed right like so like in the state that it\u0027s in right now it\u0027s not gonna pass sector yeah we have other packet replicators right so if yes but we shouldn\u0027t make any new ones so so that is actually another thing that we could do is you could take if you believe the document is ready then I would I would say we should request an early sector review and we will see what sector says we know what the yet we know what thing if no no wait wait no we know what one of the answers will be we in this room have have identified the amplification and reflection attack there may be other attacks wait so it might be um well yeah but if you give it to sector they\u0027re gonna find the first hole and then they\u0027re gonna say no they\u0027re not actually gonna dig on it so we should do something about the amplification reflection attack before we send it this idea good point let\u0027s do that right so I think this is why we brought it up and so let\u0027s if you have an idea on how we could go and deal with that by constraining access I\u0027m still not sure "
  },
  {
    "startTime": "01:12:43",
    "text": "how to do that I have an idea on how to solve the loopback thing the routing no problem we have solved that but yeah so I think the routing loop problem that they talk about is indeed severe but it\u0027s not an infinite because you do have TTL and when we restrict the loopback to collect data backwards then it kind of saw the problem so I think it\u0027s important to solve it but it\u0027s not infinite now what but I think it\u0027s important to solve it I think that\u0027s the thing let\u0027s first solve it before we consider any further sending a single packet will not melt the entire internet forever does not mean that the problem is you know that is not that is that is not an acceptable um retort to just like yes this exists you don\u0027t want to go on a pylon yes okay good yeah perfect we have enough we have a tier we have enough foot guns so foot guns are okay or we have enough foot guns let\u0027s not make more very like and I\u0027m leaning toward the second one so yeah I think the next step here is between now and Vancouver um we go off and think about this four flags and we basically have a session in Vancouver that is let us let us come to conclusion on the solution for this problem let\u0027s first see whether we can go and converge over email and so that we we have this significant yes I think also we need to off this other feedback in and then we can see whether we can go and come up with something that at least solves the the packet replication and when we talk about the process for this because we\u0027d essentially agreed for the other one that we could kick off working group last call I think you know if you wanted this to be aligned you would have to delay both of them so do the author prefer to do the data last call separately from flat I think I have a personal preference everybody else can go and speak up but I do think we decouple the thing for a reason yes we did and we did a couple of the thing in order to have a stable base and everything else can then progress at the speed that it can go and progress that so well let\u0027s not could put all about the ex back into the one basket given that we have started to own distribute them all right so we have Mickey and Q I\u0027m gonna go ahead and be men hello can you know we can we can hear you so I guess a couple things so one is I wonder why every node needs a send back so we do have node IDs in the in the trace information it\u0027s not an IP address but it\u0027s something and then the other is "
  },
  {
    "startTime": "01:15:43",
    "text": "there has been some interest in having IOA info go back to the source for closed loop control or other purposes back to Nix and that overlaps little with what we\u0027re talking about with loopback and I think it\u0027s worth discussing that ah afterwards yeah makes sense buck I agree that we should further discuss your second comment but the first comment has to do with the whole intention of it so if you want to see where their pocket can actually traverse the whole network then you would have to collect it and then drop the packet you will not see it so it\u0027s the same thing that as you have for trace hole if it makes sense there some implementations may be able to turn it around if they think they\u0027re dropping but yeah that\u0027s a lot of implementations and you don\u0027t rely on the implementation of effective devices in the middle okay we can discuss that further cool all right thanks okay one more so that\u0027s the v6 options draft which is basically asking for allocation off to v6 options again it was a draft that used to be an individual draft from the IOM team which is why it\u0027s ima and it all originally from a names perspective has been working group adopted there\u0027s been nothing really new in this draft other than the working group adoption thing and well a little bit of informational reference to the deployment draft and what-have-you and we basically asked for this draft to be adopted to go and follow what\u0027s the RFC number 70 120 something to go and ask for early adoption of ipv6 option types so that the VPP implementation and the kernel implementation can start to use Ayana allocated option types instead of their own ones and well they\u0027re already diverging today these implementations helps other people as well to go and use the same numbers even if it\u0027s early so I think that was one of the main reasons why we kind of pushed for a working group adoption I already send an email to the chairs to kick-off that process yeah you have to do that according to RFC 71 something right I refer to that in that email and yeah so would be good to go and get additional reviews end of the day I think it\u0027s it\u0027s asking for two coal points the draft the all the "
  },
  {
    "startTime": "01:18:43",
    "text": "considerations are in the deployment draft that goes hand-in-hand with a v6 rav4 now so that the draft itself is very simple sharmeen city I have a question on these jobs when I read this chapter I found it it seems there\u0027s a alignment issue between this chapter and ion data because in section 3 of this draft says unless a particular interface is explicitly in your enabled for IOM route to a master job packets which contain extension header carry arm data fields but I am data it says if not all nodes raising a domain are I am capable I am tracing information the oning be collected on Rose knows which I am capable so it seems they are not alignment I think that didn\u0027t entirely catch it so so if you go in summarize what the discrepancy is that you believe is there in this chapter it says if this note is not I am capable it should job any packets that you received with Rio and a third option data did you drop the packet or does it drop the option packet yeah it shouldn\u0027t drop the packet if that\u0027s really what it says then that\u0027s that\u0027s odd and wrong right so that could you go and drop an email to the mailing list with that particular thing we need to go definitely fix that so thanks for catching that that\u0027s just like yeah well it\u0027s it\u0027s wrong yeah thanks for thank you very much thank you we can even get a ref out real quick that fixes the replication problem by the way [Laughter] okay so more more reviews thank you thank you very much so yeah we will we will sit down and figure out the go read 7120 um we\u0027re gonna have a look at the process I think the process is that we need to do a call for consensus about this on the working group list and then we need to go we need to determine consensus right like we can determining consensus by Fiat but that\u0027s not really "
  },
  {
    "startTime": "01:21:43",
    "text": "how we do things here and then we need to go ask Miriah and then she\u0027ll say no I\u0027m area but they\u0027re also a bunch of experimental well you said you can\u0027t just use right we can do so the overall thing why we started doing this was that the two implementations that were knowing in the open in open source already used different code points and it would be nice if the two had some suggestion of what a use they\u0027ve chosen something for so you should go to the people and show it at them and say like this is not how because this is a really short craft right it would be just like to bring it so the process very quickly and then have like this final allocation and sometimes we hopefully yeah we\u0027re hoping for what we need to go and fix this one that\u0027s actually a good question is do which do we think will take longer the cheers figuring out how 7120 works or doing an accelerating up the chairs are very experienced with that process I would assume that but I\u0027ve never done a 71 point okay like it\u0027s Tetris it starts with a 7 that\u0027s like that\u0027s newfangled I don\u0027t know out to you right so we can go and try on so a the the reason that I would say we should do two reasons why I would say we should try the the early allocation thing instead number one it\u0027s a new shiny thing that I\u0027ve never done the ATF and it would be fun to try the actual reason is that it\u0027s a little bit weird to last call this document before we last called data check agree so let\u0027s do the early allocation yang do you have this desert esteemed area director all right so that\u0027s that means we can go and dovetail with the thing that we haven\u0027t really discussed which was immediate export which ended up now in the design team output on we have we have we have like one minute and I actually would like to take this time from the chair to start the consensus determination process is there anyone in this room who believes in an early allocation for these code points so that we can stop squatting on weird bits of the option number space is a bad idea if there\u0027s dissent in this room about doing that I\u0027d like to hear it because we\u0027re gonna start it we\u0027re gonna do a quick call on the list a quick call is probably three weeks because we you know when people actually read it to understand it um to early allocate code points is there anyone here who thinks that\u0027s a bad idea please stand up now "
  },
  {
    "startTime": "01:24:45",
    "text": "okay that that\u0027s that signal um so yeah we\u0027ll go ahead and get that started cool yeah thank you thank you so much alright so that can our section of working group adopted documents now we\u0027re going to get into the related documents such as the one we\u0027ve been discussing the direct export file Thanks my name is Tom Mizrahi and this draft is about direct exporting is a new draft but it\u0027s actually not a new idea it\u0027s the product of a design team which consists of the people listed here a little bit of history so this concept was actually discussed in the working group a couple of times and it\u0027s based on two different proposals one is called postcard base telemetry and specifically we\u0027re talking about PB TI which is one of the flavors and there was also a second proposal which is called the immediate exporting flag which was first introduced in the data draft and then when we split it into a data draft and a Flags draft the immediate export flag was part of the flag draft so an IDF 105 we had quite a quite a bit of discussion about these two proposals and the working group chairs instructed us the authors to work on this together and to come up with one a single solution and this is what we did so first of all I want to thank all the people who took part here and worked on this combined solution it wasn\u0027t trivial but I think we came up with something that makes sense so last month in October we introduced the first version of this draft which combines the two concepts so what is direct exporting so if we look at the figure here from the left side we see a data packet that goes into the i/o and domain and when it reaches the encapsulating node encapsulating node can incorporate a direct exporting adex option into the data packets then when the packet is forwarded along the path the transit nodes just Ford the data packet they don\u0027t modify it finally when the packet reaches the decap slating nodes the decks option can be removed so each of these nodes along the path can also export information to a collector but the transit nodes don\u0027t need to change the decks option so that\u0027s the the main difference here compared to what we saw in in a conventional iõm so the question that comes up here is why why is this needed so the the main motivation to use direct exporting is first of all since "
  },
  {
    "startTime": "01:27:47",
    "text": "transit nodes don\u0027t need to modify the packets it makes processing by transit no it\u0027s easier simpler so that\u0027s one thing and another major advantage is that since we\u0027re adding a constant overhead it\u0027s a relatively small overhead so compared to the conventional io m which adds per hop overhead it\u0027s a significant significantly easier or less less overhead for each data packet so there\u0027s potential improvement in terms of the data plane bandwidth so these are the main advantages of using the Dex option so this is the Dex option format we can see the different fields here well some of the fields are similar to some of the fields in in the IOM incremental and pre-allocated trace options so for example we have the namespace ID the same as we have in the trace options the i/o and trace type has a good exactly the same semantics as in the IOM trace options and we also have two optional fields here flow ID and sequence ID let\u0027s see an example and in this example we see a Dex option which is encapsulated in an ipv6 extension header which is based on the draft we saw a couple of minutes ago the IOM ipv6 options raft so as we can see here we can see the last four which are exactly the same as we saw in the previous slide we see the first word here which is option type and length which are the beginning of the ipv6 extension header one thing to notice here is on the right side at the top we see the IOM type this is the indication that this is a decks option so we\u0027re allocating a specific IOM type to indicate that this is a decks option the other thing to notice here is the option data length field indicates the length of this option so like we said it\u0027s a constant sized option but this length indicates whether the option includes the Floridian sequence number or not okay so that\u0027s how we know what the length of the option is quick yes sorry to interrupt you but I think that it\u0027s convenient that we have this slide so I understand that you are implicitly defined by the length what\u0027s option included or not but I think that there are possible situation when or at least is it possible that only flow ID or sequence number included because then it "
  },
  {
    "startTime": "01:30:49",
    "text": "will my create a ambiguous situation to understand what this field is the way it\u0027s currently defined it\u0027s either you have both of them or you have none of them because we wanted to limit the number of possible combinations and to make it simpler so we only have two flavors one is without these two fields and the other is with these two phases okay thank you okay so this is the format over an ipv6 extension header so basically this draft is based on the input we received from the working group it\u0027s a combined solution and we believe it addresses what we heard from the working group and we believe currently the draft is pretty stable so we would like to consider working group adoption and we\u0027ll be happy to hear any comments gregory CCT again I have a question and probably it will be interesting to discuss as part after my presentation but this proposal is for any network or only for unicast networks do you see there\u0027s a difference between yes I see the difference and I\u0027ll bringing this for discussion in my presentation of implication on Io a.m. and other telemetry collection methods in multicast networks yes okay so that\u0027s that\u0027s the point to think about in my opinion I know this this work is only considered for the unicast and for the mother cause the we need more extensions thank okay any other comments or questions I think one more thing what we might want to go is Frank runners one more thing is we want to go and adopt the nomenclature that we have in the data graph now so it\u0027s X option tied right so I think getting those two in lockstep I would I would say that like so that would be a comment for the 0 0 Rev like so you know so the feedback here you know if we if we choose to adopt we\u0027d say you don\u0027t take to take this feedback and put it into the 0 0 arrive at the dock the dot the I thought I D fib M 0 0 F the dock box I do agree with the comment on the multicast and I think we have to have this discussion in this dress because sometimes you get multicast behavior and intentionally so for example flooding in the switch in the middle so I don\u0027t think we can prevent this discussion and we have as part of this first I have a question so as an individual I\u0027ll go back to the "
  },
  {
    "startTime": "01:33:51",
    "text": "back of your architecture diagram this one so this this looks like a different kind of reflection and amplification attack then be loop back um so I would say that when we bring this into working group change control we should probably ensure that the you know whatever we come up with to protect flags can also protect this right like so have we don\u0027t want two different sort of like amplification protection strategies for the two different for the two different strategies in in IOM for the to the extent that that\u0027s possible because these you know these look like these look like differently sized cannons pointed in different directions but they\u0027re both cannons and if there\u0027s an approach that we can come up with that will protect us in all cases that\u0027s that\u0027s worth working on right so you\u0027re saying the analysis should be done for both even though the solution is not necessarily the same yeah and if it comes out that like that like these are different enough that you we need two completely different ways but like so I\u0027m thinking of something that basically involves like a little bit of cryptographic protection that if you don\u0027t actually pass that if like is this is ripping through the network in the transit node doesn\u0027t see the token where the token can be something pretty lightweight then it basically just says no I\u0027m not gonna I\u0027m not gonna point this cannon over there and I think that\u0027s like a token based approach could work for both of these but I think we need to do a little bit as a working group meaning to do a little bit of working around looking around and seeing other places that usually in the transport layer so like send cookies or an example of this sort of thing right like so looking at the at the approaches that I\u0027m gonna use like in higher layers to do this and see if we can make them lightweight enough to work in these environments because I think that so one thing that I know is the same between - between direct export and loopback is you need to apply the security at a low enough layer that it needs to be pretty lightweight right like so the constraints are the same in both environments and that\u0027s going to constrain the space of solution such that I suspect that even the solution is going to be unified suspect I haven\u0027t done the work yet okay Gregory I think that this comment brings another point for discussion is whether loopback and direct exporting are or whether direct exporting is a more general case of loopback and whether we really need to loop back or we can get by with the direct exporting and providing special encoding for loopback scenario because as I understand encapsulation so there is no identity of their destination for the I am data to be exported in the data package so probably you envision that is "
  },
  {
    "startTime": "01:36:51",
    "text": "a part of their management data plane to set it so something that we can discuss how to extend our decks encoding so it can be acted as a loopback function yeah I think in my opinion there\u0027s a distinction here between the data plane and the management plane I think the loopback is intended for to be used by the data plane but whereas right that case just I\u0027m just you know just speculating why not to have a flag in the Dex saying oh by the way this is a loopback okay and then you don\u0027t need it and then you don\u0027t need it I look back flag at all okay but we\u0027ll talk about it later so I\u0027m gonna make an observation as the chair um we\u0027re already kind of having this discussion within the working group so this is the factor or your backing announcer we should go ahead and and do a call for adoption Popo kick that off okay let\u0027s get you bail do you envisage that at some point you may have a need for a filter indicating what kind of data needs to be exploited or do you think that implicitly you always know what needs to be exploited yeah so the Assumption here is the encapsulating note doesn\u0027t insert a Dex option into all the packets it does that based on some kind of filter which can be either a statistical say I have five types of data in might know that I\u0027m a report and sometimes this may be important and the other time this may be important and maybe you want to say eh you guys are on this path report this particular information and I see that you don\u0027t seem to have any filter of any sort be able to indicate that sort of thing what you mean is that the encapsulating node you\u0027re looking for something for the encapsulating no to indicate what kind of traffic versus no can we go to the next slide yes so the flags is encoding of what type of information to be exported it\u0027s a part of a standard IO am yes so we got one more minute for this drivers on real quick clarification so that the trace this is Frank the trace type tells you what is collected right so the encapsulation note will put into the packet what is going to be collected so the assumption is that you only collect what you want to go and see exported it\u0027s not the way that you say okay that you have the encapsulator it\u0027s going to go decide what is going to be collected and then the exporter would just export everything that is collected right as opposed to so you have the encapsulating note decide on what should ultimately be exported as opposed to correct as opposed to the exporting note "
  },
  {
    "startTime": "01:39:52",
    "text": "will decide on add well I think otherwise like if the exporting note would don\u0027t decide what to export right then you can do that today you don\u0027t need anything IOM for that you don\u0027t need anything signaling for that so the whole point of Io a.m. is that the encapsulating note can signal to the intermediate node this is what you should go and grab and send off otherwise I can go and implement like something like otherwise this is a good discussion I think going into this on the list to discuss offline would be great so I think this is a very interesting topic and thank you again for doing all this collaboration we really appreciate it so um tall one quick question I just give me a thumbs up or thumbs down on this um after adoption there\u0027s no need for the continuing collaboration infrastructure for this document as a design team we can go ahead and close the design team meetings or I think in my opinion it\u0027s useful to have a list a mailing list a mailing list is still up yeah yeah and also probably bi-weekly meetings you want to keep running okay it\u0027s like so keeping them running isn\u0027t no op for me I\u0027ll just let them stay we may want to change the scope of the list and meanings but I think it\u0027s important for us to keep it going okay good let\u0027s let\u0027s take the scope change offline thanks thanks get to the end hi I\u0027m al Morton and Len Chabot owned and ridiger gibe who have been active members of the group here have been working on a draft with me called metrics and methods for IP capacity this is a this is a draft where we\u0027re defining a metric for something that we can really hang our hats on this maximum it\u0027s kind of like a physical capacity we should be able to measure this and it\u0027s a it\u0027s been kind of a holy grail of this group to measure capacity of links and so forth it\u0027s it\u0027s becoming extremely important in the part of the world where we\u0027re offering a gigabit access to every user who can spend I don\u0027t know $300 a month so it\u0027s a it\u0027s an important thing now it\u0027s at the IP layer that\u0027s where we\u0027ve defined it with header plus payload and we have a word definition and this equation definition in the draft also a method of measurement so we\u0027ve got these time intervals they are typically one second where we\u0027re generally using 10 second measurements "
  },
  {
    "startTime": "01:42:53",
    "text": "and we try to find the maximum interval that\u0027s going to be the maximum capacity and we also have a search algorithm in the method of measurement and and so the typical search kind of looks like this where it converges maybe overshoots a little bit at first but it converges on a one of these intervals where we can identify the maximum capacity and we\u0027ve also got this aside measurements of loss delay delay variation and so forth these are the kinds of things you can\u0027t get with TCP but TCP is being used everywhere and TCP falls short when you\u0027re trying to measure gigabit access even half a gigabit access there\u0027s always going to be some error there and and that\u0027s the problem with the current methods of measurement now UDP is coming quick is coming this is a simplified way to measure IP layer capacity so we\u0027ve had an enormous amount of discussion on the list and I\u0027m not going to go through all of this with you in fact I sitting in the back there I realized that this is meme readable from the back so let me just pick up some points bimodal features are still being offered in Internet access where you get a turbo boost in the first couple of seconds and then it drops back down we\u0027ve got that covered now in the in the methods of measurement also I peel our capacities different from goodput or throughput so let\u0027s not mix those together it\u0027s a it\u0027s a different metric okay and by the way in the in the slides which I hope you all take a look at the action points are in the at and then the plus plus plus is what we did in the draft to deal with it so we\u0027ve got about 15 of those just in the September list points so feedback from Yocum Fubini who\u0027s here welcome back Yocum he said formulate this as a statistic based on the singleton that\u0027s the way we\u0027ve always done metrics here before so we\u0027ve got that done now and we\u0027ve also got a qualification measurement that we talked about in the in the considerations methods of measurement and we\u0027re proposing that at a fixed 99% of whatever we come up with in the search algorithm so that\u0027s something we can talk about and of course we\u0027ve got the with the zero zero draft we spelled the capacity right that\u0027s at the bottom line so then the October list discussion between mostly three of us and also a meeting where we had five or six of us together in Darmstadt basically we were we were looking at averaging times the ones that we chose seem to work very well of one second and ten seconds for the whole test Matt was proposing Matt Mathis was proposing something a little lower we did some tests with 50 milliseconds and didn\u0027t seem to improve things actually so one of the things we did this time we haven\u0027t done this in previous drafts was to go with default values for the input parameters to the metric and and that\u0027s what the the "
  },
  {
    "startTime": "01:45:53",
    "text": "defaults are the one second for the measurement short manager and intervals the time for the singletons and and so forth so that\u0027s that\u0027s new and let\u0027s say the consideration for testing parallel flows we\u0027ve added that and the sending rate measurements we\u0027ve we\u0027ve also added defaults and some additional information there so all of that went into the draft zero one which is now the third version so next steps it\u0027s good to recognize that there is still an active method some measurement community in the working group among the all the protocol developers that are here and I\u0027m sure you guys are all interested in a little of both so let\u0027s continue this that means I\u0027ve got five minutes left and that gives us a little time for discussion so that\u0027s cool so the authors believe it\u0027s worthy of a working group adoption the key points there are harmonization there\u0027s parallel efforts going on which we tried to coordinate with the liaisons but the best way to do this we\u0027ve found is to get the spec going someplace and that way we get the feedback of review of the metric and the methods from the community we get their unique perspectives and we get to the point where they can do the things that they want to do is a unique thing we can do in AI ppm after we\u0027ve got the metric to point to and and our unique perspectives as to as to helped us out with a protocol enhancements in whatever protocols we choose like t WAMP like stamp all of those so I think that\u0027s that\u0027s the kind of second step beyond adopting this and and reaching consensus then we can work the protocol aspects so the point is to reach consensus fast and then start that protocol support I I would suspect that we could because we\u0027ve got so much experience with measurements year-and-a-half of measurements and this is being worked in other standards bodies as well but we could probably get this done by May and then start the protocol stuff so but that that assumes that we go for a working group adoption sometime in early 2020 if we if we do that so I\u0027d like to open up the microphone for comments and let\u0027s for I guess let me ask the question who here in the room has read the draft and there\u0027s two people Ignacio has read it on the on the jabber and there are obviously there are three authors and so forth out there Dave that was a time for you to raise your hand - okay comments Mike Ackerman al thank you this is really good work I\u0027m excited to see this in action how do we move it along and how can I assist if in any way I\u0027ll review it on the list that would be the best way okay I see a lot of potential good uses and corporate networks and "
  },
  {
    "startTime": "01:48:53",
    "text": "others cool thank you knowing Elkins yeah I yeah I support this document too because that\u0027s certainly a question that I get asked a lot is what is the capacity of this link that somebody you know is px has just sold to me and and we\u0027re gonna be getting higher and higher bandwidth all the time and at more and more expense and the other thing I really like is the harmonization part because I certainly I\u0027m sure we can all point to several different groups where there is a heated battle between different standard organizations and I really commend you for doing harmonization so I support this flesh-and-blood Winnick liaison that was almost a tee up for my comment so Dave sanic rope I\u0027m using the IETF liaison manager the broadband forum hat this work is of interest to that community they\u0027ve actually taken note of the draft and there\u0027s some ongoing work that would utilize the draft and look at its application within that community so that I\u0027m not speaking on behalf of the forum but I do know that there\u0027s interest from that community in heaviness draft progress thank you yeah I think we\u0027re just gonna go ahead and start a call for adoption now like will run the we\u0027re running the working request calls like in a pipeline but call for adoptions this seems separate enough that yeah I think everybody\u0027s already made up their minds so we don\u0027t actually have to pipeline it so we\u0027ll start one oh I\u0027ll just basically copy and paste the last message essentialist and change the draft names um and yeah and then we\u0027ll that\u0027ll end on the night to December as well cool thank you thank you everybody please read it and enjoy it this should be fun thank you all right Greg yes close us out okay okay um ah yes of course okay so as I mentioned in the comment earlier we want to bring up for the discussion and consideration is the special specific aspects of a telemetric collection on on path in multicast networks if we can go next slide doesn\u0027t oh okay okay no okay so monitoring multicast traffic is important as important as monitoring unicast so that\u0027s important it helps to reconstruct visualize the multicast tree "
  },
  {
    "startTime": "01:51:55",
    "text": "and performance monitoring and troubleshooting so on paths telemetry collection technique is a promising as a complement to the active om measurements method so what\u0027s the problem what we see the problem the currently on path telemetry techniques may have an issues that when we are doing when they\u0027re harp collection is used that means that the same data collected upstream will be replicated on all branches and carried on all branches which will lead to unnecessary data being collected because if you can imagine from the upstream so you\u0027re doing the replication in any nodes that branch nodes downstream so all the information collected upstream will be replicated on each and every branch okay if you agree with this mental picture then what objective to provide the mechanisms that allow optimization of telemetry on path collection in the multicast network to avoid unnecessary replication of data okay next slide please so for the perhaps solution so there are postcard or Dex proposal the branch node replicates the packet and branch node needs to add the branch identifier so that their collector of the telemetry can do Association and reconstruct their multicast tree so it doesn\u0027t really change their method the only thing it introduces additional informational object that needs to maybe consider it to be included in an encapsulation and again yes I haven\u0027t synchronised that with the latest Dex proposal just a question can you please explain what you try to optimize you try to optimize the capacity on the disk that will hold this territory or the okay or the network okay there okay there are several methods of collecting telemetry information what we\u0027re trying to optimize okay what\u0027s this points to is that if data is collected in the packet itself then aspect it get replicated down the branches it leads to unnecessary information being collected in the network or carried in the network you mean that you could have dropped the "
  },
  {
    "startTime": "01:54:55",
    "text": "initial I can hear you do you mean that you can you might have been able to drop the initial that\u0027s collected you know instead of replicated I\u0027m not saying what to do I\u0027m saying that there is a problem if you agree with this problem then we are not sure if area understandable and so okay the problem is if you collect our telemetry information in the packet itself and as you replicate the packet so the information and being collected upstream to the branch will be copied in all replicated packets okay would you agree with that this that\u0027s a problem so again I agree that you replicate I\u0027m not sure it\u0027s a problem but that\u0027s fine okay thanks okay so this is for a very special method of doing a telemetry collection using either postcard basically it\u0027s a per half solution it can be said this is a deck solution or post card telemetry so which each node exports the data and the observationally of the draft was that the current information then being proposed is not sufficient to do their Association so their proposal is just we\u0027re not changing their method of collecting information from each half but we are just adding additional information that helps to correlate collected data from the transit nodes as belong to the same flow through the multicast distribution tree yes one quick question here have you considered so we don\u0027t really specify the semantics of the node ID today in iom data in the data draft right so what you can go and do is you can then coat the node ID plus your branch ID in the node ID field right so yeah I don\u0027t believe that you need an extra field for that okay again we can discuss whether there is already sufficient or not this proposal was I believe that from the same offers that are part of the design team for the decks so they looked at their earlier proposal with their postcard and they suggested that whether there is already informational element that can be included into their exported data how you from vitória so currently we haven\u0027t if I are how you will actually implement this labor branch but "
  },
  {
    "startTime": "01:57:56",
    "text": "you can use any existing mechanism in the text or whatever you can use but but here we just point out what\u0027s actually information you will need to identify where the branch helped you reconstruct the entire multicast tree mm-hmm okay okay our next one please okay so another method is that collecting information is a person per segment solution based on a hybrid 2 step so the hybrid 2 step is not collecting data in the packet itself but rather construct their similarly encapsulated special packet that collects data following their packet that is a trigger which is a similar with the postcard and Dex the only difference is that postcard index operate on per hop and this collects from end to end all the information so you can see that without any modification hybrid 2 step will lead to the replication of unnecessary replication of collected data because there HTC packet that collects data uses the same encapsulation so it will be replicated as a data packet and thus will lead to the unnecessary the multiplicity of information so instead to do optimization is that basically it\u0027s very simple saying that the first packet is being transported with all information and if packet needs to be replicated so if this in transit node happens to be a brain that note generates a new HTC packet as a result you can see here so that only information on this branch will be collected then you will have all only one end to end on one branch and everything else is per branch from the branch note to the leaf and that\u0027s basically order that so in post card may have enhancement for optimization yes oh sorry okay so it has a position we asked for your consideration comments on the list and discussion and down the road somewhere adoption thank you thank you for bringing that alright so if you haven\u0027t signed the blue sheets they\u0027re over here and other than that we have a lot of new stuff out for adoption and for last call so please respond to those on the list and thank you so much for your time we\u0027ll see you didn\u0027t Vancouver "
  },
  {
    "startTime": "02:01:04",
    "text": "[Music] you "
  }
]