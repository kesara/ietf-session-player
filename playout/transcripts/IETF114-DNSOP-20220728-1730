[
  {
    "startTime": "00:01:24",
    "text": "here the one that warren is dancing with"
  },
  {
    "startTime": "00:02:05",
    "text": "all right let's get that thing going qr code well here we are it's nice to see everybody here in in the room in real life welcome to philadelphia and for those remote we're happy to see you too wherever you may be um your your chairs are benno tim and suzanne the lovely and talented tim is in the corner here our a.d is also here warren the jabber room is alive for this time we're i guess moving to zulip after this meeting we have paul hoffman taking minutes thank you paul thanks please the same old same old familiar notewow please read it um understand it think about it carefully because we are held to enforcing that next also a reminder about the code of conduct which i think is common sense to everybody here but these these principles were expected to abide by also thank you for your cooperation it makes everything easier when folks"
  },
  {
    "startTime": "00:04:00",
    "text": "are willing to follow this next and some iutf meeting tips particularly for hybrid some reminders for the in-person participants everybody should be signed in to meet echo that's our blue sheets it's the only one we get now meet echo also manages the the mic queue allows us to manage the mic queue keep audio video off if not using the on-site version just to keep things simpler and please we've we've been asked to remind everybody wear a mask unless you're actively speaking at the microphone remote folks we all know how to do this by now but make sure you're already on video or off unless you're us or presenting and for those listening in whatever space you might have a headset makes everything better next please so we've got we've been through the uh introductory commentary we've got a message from our sponsor rad who wants to speak briefly on an initiative he's setting up we've got the usual updates of work in progress hackathon results and working group business discussion of current drafts and possible new work yep so the the agenda this is what's in the slides is also online so and and all of the all of the drafts and all the slides are in the meeting materials repo so i think this is warren's turn"
  },
  {
    "startTime": "00:06:10",
    "text": "i turned it on wow i fixed it so actually two things um first off i've been doing this ad thing for many years now and i am still enjoying it but my term is up in march i probably will run again but i would like there to be other volunteers etc other people willing to serve in the role so if anybody is interested or considers being interested or even just wants to know more about it please come along and talk to me and i'm happy to explain what the role is actually like what the time investment is you know the good bits the bad bits etc so please hunt me down i'm sort of generally around i've got a big hat so you can find me so the actual topic dns directorate this is something which myself and eric think are organizing so apparently this dns thing that we've built is really popular and now everybody wants to do it dns related topics show up in a bunch of working groups um 25 of them are actual sort of documents working group documents 13 and dns up 3 and dns sd 3 and 80d you all can read that the thing is there are currently 59 existing drafts that mention dns somewhere in the um name or abstract so obviously there is a lot of dns work happening um and also there's a lot of places where people are doing dns work sort of in other drafts but they kind of mention dns and then try and build on it and that doesn't always end well uh next slide so i mean here's a whole list of"
  },
  {
    "startTime": "00:08:00",
    "text": "non-working group documents as you can see some of them are aimed at dns up but a surprisingly large number are aimed elsewhere next slide one of the problems is many of these documents don't really get any dns up or dns focused review until they show up at isg evil so recently on the telechat there was a document which tried to make extensive use of the dns to do mapping from sort of some type of physical things to who owned the physical thing and it's not clear that the dns is the best way to do that but you know there hadn't been any review until then by by dns people and some of that is there's no easy way for people to get dns review from dns people so what we're planning on doing is creating a dns directorate um there has been one before there was a dns directorate many years ago this is sort of dns directorate v2 and it's going to be like most other directorates in the ietf like opster and sector and similar it's going to be a way where actually next slide where documents which are being progressed through the process can have a set of dns people look at it so this is you know not actually what the actual purpose is but this is the upstair disclaimer with a little bit of text changed basically some things so that drafts that are progressing can be reviewed by somebody they get reviewed by a member of the dns directorate the comments are written for the ads document editors and chairs are supposed to treat them like any other comment but a way to get some focused review on it we will be asking for volunteers who are willing to serve it shouldn't be a particularly onerous job not that that many documents and hopefully it will be fun"
  },
  {
    "startTime": "00:10:00",
    "text": "so as they say we are going to be looking for at least two secretaries and a bunch of volunteers please send myself or eric think email you know find us contact us etc if you might be willing to serve we might also do something where for some of the drafts we try and get both an experienced dns soft person and somebody sort of newer to to the ietf to both review so people can get some experience learn some more etc are we taking one or two questions or if there are one or two really short questions happy to take them but i think it's relatively self-explanatory you know folks who are willing to review stuff great and we need to we've got some updates and tim is the guy that wrangles the uh list of what's and where so uh thanks i don't have warren's knees so i'm going to actually probably just stand up here and do this so unless i can bar your knees warren so and the reason this sort of came up with the document up the dns director is because i usually do that search before the every iatf looking for some like whoa that's a crazy document sort of thing we better sort of think about that so but um i'm we're kind of good on this idea hopefully sort of keep tabs on stuff so let's do some document updates so since our last meeting insect 3 well it's actually north 48 and victor and wes are busy sort of wrangling the editors and i think they've come to some um consensus on that victor yep i saw that this morning so we've got 92.76 service b's and they've been in the editor queue mostly because of the ensi document but there has been some changes"
  },
  {
    "startTime": "00:12:02",
    "text": "and martin hoffman um suggested some abnf changes recently that are in the repo and i think we should probably they want to sort of bring that to the list just to make sure nobody has any issues about that but that that thing has been sitting for a while and people are still commenting on it in the repo just trying to understand the basic things which is good and i i figured let's do that now before it hits the editors and then it all sort of goes sideways so thanks authors on service be and as you've noticed there's lots of service b documents in the ietf now so it's it's not just becoming an archetype it's become like an archetype that everybody wants to use to solve everybody else's problems so that's for good better or for worse the next slide starts so i threw this in at the this morning the 5933 biz i'm gonna have to do this um dimitri submitted a new version this morning and i think we need to review it there was a comment that came up about switching it to informational which actually they've done so i think um mr hoffman who had sort of made that i think that's that that has been sort of addressed very well and i believe they've addressed i'm going to go through it all of mr st john's comments as well so we need to review that but it came in sort of right off the bat this morning um in working group last call we have a void fragmentation we put a three week call on that because of the meeting in august and stuff and so we'd like people to sort of review it and give us some feedback we think it's in good shape and we hope everybody else does so next slide you're going to be busy for a while so some upgrading work through class calls and you'll see this from paul we believe"
  },
  {
    "startTime": "00:14:00",
    "text": "the dnsec bcp is ready to go and we'll probably fire that off this week as well probably with a three week working group last call the glue is unoptional again this one's done and so the only question i have with the working group is do we start it now do we start in september um the chairs we always kind of sort of feel august is a you know where the civilized people take holidays and so which that's usually europe um and things get a little slower and we respect that and i you know our question is you know do we fire everything up in september or so do we let this run through through august you know through now as well so if anybody's got opinions on that please speak up um like to hear it so next slide but wait there's more so catalog zones catalog zones is done but the authors are all actually on vacation right now so we're gonna start that working group last call early september so get ready to be busy and also validator requirements um daniel's gonna speak but we kind of feel it's been through the process enough it's ready for that too that's another one we'll be in september so you guys are gonna be busy reviewing documents and we're gonna sort of be chasing people down uh next slide what do we have here oh so ns3 validation we've been sort of being patient with the authors on this because they've been busy and there are a few outstanding items and schumann's assured us he's getting some a distance assistance to finish these items so we can sort of move on with it so that's a great thing um and we've hoped on that so and then 84.99 biz which is another one that's kind of been stuck a little bit what we want to do is have it interim to finalize that belly wick definition and the interim is the right way to do that it really can sort of focus the conversation"
  },
  {
    "startTime": "00:16:00",
    "text": "and so we have two choices once before the i can the other one's after and we'll probably send out a doodle bowl to figure that out and maybe after it's better you know we'll let people decide on that um next slide what do we have oh air reporting uh the authors are on holiday as well right now they got back to us but um there is some stuff happening and we're waiting they're gonna get back to us next week when they're back from vacation to tell us give us more feedback on that um we don't know about zone version other than the latest updates so we need to sort of talk to the authors about that they've created an edness registry which i want to sort of read more about and sort of understand what they're doing there so the dnsec automation they need to add another author from the data c folks and um but johannes implemented the entire protocol so that's kind of a bonus here he's like we always like to see that um i know peter's submitted a new version of dnsec bootstrapping and it's it's been sort of moving along so we we think that's in good shape so we're pretty happy next slide what else do we have going on oh yes we adopted two documents yesterday uh the domain verification techniques and the caching resolution failures they've got enough sort of comments but the service b dain document which did very well in that warren poll that we did um didn't get a lot of comments so i'm hoping some people can walk up here and take the microphone and sort of speak you know yes you know we should adopt this sort of thing so hopefully there's somebody in the room because we we feel that there is interest in it but we're surprised by the lack of sort of email comments about it so yes yes no no okay or just send email about please"
  },
  {
    "startTime": "00:18:00",
    "text": "um because we're kind of confused by that one because we got a lot of positive feedback but then we didn't get a lot of positive a lot of email about in the call for adoption so someone can help us sort of understand that that would be great um next slide okay yep our stuff's in the data tracker we're also in github in the usual places we do i keep we keep data tracker very up to date in terms of everything including you know repos for where documents are and stuff like that so um we try to be pretty much on top of that which makes our ad very happy so yay and i think that's it on the document updates anybody's got any things to say i know there is things to say about the poll and stuff but i think we work through all that but we're willing to sort of take any and all complaints so and i'll put my cane down so i won't beat you when you start complaining so comments no we're cool so and i think the next thing then is hackathon up the hackathon updates yep so i think this is peter peter thank you so much oh neil you can also stand up here thank you hey everyone uh i'm niels from the esec and the hackathon update from our site is we implemented uh oh i can i can actually look here uh we implemented dns bootstrapping in uh two fashions the first one is a period of cron job implemented using dns python i believe thank you john o'brien and it sort of scraps the cds and cdns key records from a trusted source and generates the signaling zone"
  },
  {
    "startTime": "00:20:02",
    "text": "and then pushes it pushes it out to an appropriate place so it can be then read from the actual signaling domain and the other part is work from peter and jerry and myself we implemented the signaling zone in powerdiness using their lua records so in that implementation everything is generated synthesized on the fly when you ask the query it also connects to a trusted source obtains the records signs it and delivers it to you and both both implementations are actually deployed and i think the links are on the on the slides thank you yeah so there are immediate questions you can raise your hand otherwise i would like to invite thank you niels nielsen invite george yogas hello club hello i'm jorgos uh from internet labs for this hackathon we wanted to start implementing the nsa reporting uh tom carpe was in spirit with us because he did have some preparatory work we tried that happy path seems to work testing for that path seems to work but we still have more work to do like some new introduced features in version 0.2 we need to make sure that the resolver doesn't uh kind of attack the authoritative name server [Music] yeah more implementations come any questions"
  },
  {
    "startTime": "00:22:01",
    "text": "no okay thank you yogurt thank you uh niels yeah so the dinner chairs i think it's very important work the itf hackathon resource is presenting also here because it makes it helps to make the draws to make progress and give implementation feedback so thanks again to the to the hackathon enthusiasts and the software developers next we can get started so can i invite the first speaker paul thank you hi i am a new deck being shared oh no i'm sorry um so i'm paul hoffman i am the author of this document they've given me 15 minutes to talk i believe i'll take five unless there's a lot of questions this has been on the list just pretty much since the last meeting for those who haven't followed it's a description of dns tech that's really all it is it says go look at these rfcs these are required you know if you're implementing dns sec one of the things that we really wanted to do was to have in a single place where you want to refer to dns sec as an rfc a place to go next slide that's fine um i put the zero zero up had some good discussion had some suggestion on rfcs that i had missed oops um put up the zero one basically no discussion since then um and there are no open issues at this point that we know of so it's not like we're in a rush to get to working group last call but if there really are no issues there's no reason not to um sort of move it through again it's not defining anything new it is simply"
  },
  {
    "startTime": "00:24:01",
    "text": "saying if you want to refer to dns sec this will be a single rfc to do it and by the way it's a bcp we actually believe that it is the best current practice to use dns sec there was a little bit of an issue of oh well i don't want to use dns sect there's a lot you can always not do bcps in fact lots of people don't do bcps but the dns community believes that dns sec is the best way to secure dns messages at rest authenticate them and such like that next slide oh i thought sorry i thought i had like a longer thing than that uh just on the issues i'm thinking my other presentation which will also go fast so actually we're at the last bullet is it ready for working group less calls the chairs have just said that they're probably going to do that soon okay so um does anyone have any questions or want to say anything now again it hasn't been in working group last call so if you have a i wish this was changed and such that's perfectly appropriate to do in working group last call does anyone have any issues less than 15 minutes i'm going to use these stairs yeah the the um the bc the dns bcp seems like a pretty straightforward thing to do the document is short if you haven't read it or you haven't read it lately there shouldn't be it it seems like something that should be we should be able to move forward relatively quickly so um please speak up and on in the working group last call particularly if you have issues but also if you want us to move it ahead and to give us positive support for it thanks"
  },
  {
    "startTime": "00:26:08",
    "text": "hi everyone so as the previous presentation i have been given a 15-minute slot it's going to be much faster so this this draft has been around for a couple of months it's recommendation for it's a recommendation for dnsx resolvers operators next slide so the goal is to describe operational recommendations to implement sufficient trust in um that makes dns validation accurate and also to respond to many of the questions of people that are willing to deploy the nsx which includes isp but also other software vendors the recommendations are described and include provisioning mechanisms monitoring management and the good thing is that most of the recommendations are actually being implemented by some rfcs so we just have to refer and exp explain why it is necessary next slide so we have three kind of recommendation those you need to do before you start the resolver those who have to do um when at run time i mean regularly in an automated way and um those are needed on demand we spend a significant effort to say don't try to mess up with the dns mechanics so um that was one of the key message and the the topic we discussed uh are of"
  },
  {
    "startTime": "00:28:01",
    "text": "course regarding time how you manage the trust anchors the negative trust anchors um key keys that are not trust anchors the cryptographic duplications reporting of invalid validation next slide so we have received a significant amount of reviews most of them were needs or clarification i up to what i know we address them all [Music] i mean um this is all documented into the github but we also review and rewrote the document to clarify this as a wall so we expect the document is pretty well shaped to be sent to the next step so if you have any comment that would be really appreciate that you provide those during next month thank you thank you daniel so as danielle stated the working group chairs also think the document is well we want to make progress and it's ready for working group last call but we definitely want to have people review this version thank you of course also for the previous reviews um no people in the queue no okay thank you danielle and we won't we want to make progress and push the document forward thanks today yeah we're good we're doing good um [Music] next up yeah thanks"
  },
  {
    "startTime": "00:30:04",
    "text": "there we go yeah sorry should i start yeah oh sorry yeah please start yeah hey all um i'm siobhan and um schumann paul and i have been working on this new draft um called well not new anymore called domain verification techniques um it was adopted i think yesterday um so eager to hear any feedback next slide um so just a quick intro what is domain verification what are we talking about so many providers on the internet need folks to prove that they control a particular domain before you grant them some sort of privilege uh on that domain so as an example let's encrypt has a dns based challenge for a user to prove that they control a particular domain it's called the dns-01 and if you can prove that if you can add a particular record in a particular place then you get the assert for that next slide yeah so what the draft looks like right now is that it's a survey of the different techniques that different um providers use and then there's a recommendation section um but the draft is informational so it's purely um like a kind of like a survey of existing techniques so the two main techniques are that we found were text and cname next slide so the text based method basically says that please add this dns text record with a random value unguessable value at the domain being verified and that proves because this it's an unguessable value that proves that you own the domain and um the the service provider is able to uh check that um and it's supposed to expire in a few days or the guidance around this is pretty vague and not very well documented often and"
  },
  {
    "startTime": "00:32:01",
    "text": "that's an issue like people have had outages because of this so there's there's wide variation um and part of the goal of the draft is to is to say that this is you know a good idea and this is something that you should be doing next slide so just a few quick examples in the in the draft a lot of these named examples where the feedback that we got on the list was that it's not a good idea to have them in the main docs so we're thinking about just moving them to the appendix for now i just completely removed them but i think it probably makes sense for them to be in the appendix so this is this is an example where um on a particular website like bbc.com you might have a key value pair for and the key says okay what is the company that is trying to do the domain verification and the value is the um there's a unguessable value next slide but you can also have something that acme does and github does you have like an underscore prefix um underscore acmechallenge.example.com if you're trying to verify example.com and then the random value and you can imagine that um like this is this is i guess the spoiler here is that this is a better technique in our opinion but next slide and you can also just have absolutely nothing and then you just have a random value as a text record on the on the apex next slide there's also the cname based option which is often touted as this as a fallback option so different reasoning for why you might need this but you know if the if the if you already have a cname then you can't really have another text record um so so that's why you might want this um and this typically points to a service provider property so then you can then the the provider can verify that okay actually this this exists next slide"
  },
  {
    "startTime": "00:34:00",
    "text": "so this is another example where you have a random value dot the domain you're trying to verify and then that is a c name to um something dot um the person the the company or the provider that is will check for the value um next slide yeah so um i guess as i mentioned um it seems like it's best practice to target the the these records that you're adding to target them to a service so that and the second one is that this is also good of their time bound and the guidance around that is pretty clear next slide so yeah so this is similar to what acme and github do it allows a service provider to only get the records that they need because in some cases for example when i was doing bbc.com from my home computer it fell back to tcp and because of the sheer number of text records and my isp at the time was just like dropped the connection because they didn't support that so this is the problem is bloating and it's best if you can just query for the the actual the actual service um that you're trying to get to next slide uh and yeah and time-bound checking like um i have definitely been in a situation where um someone removed um a very like one of these tokens and then the service provider is like oh i guess you'd no longer own this and shut off the access and that's also not great so there should be clear guidance around when can the record be removed and arguably maybe you don't even need to um for those records to exist in perpetuity um but yeah in any case there should be some clear guidance um around that next slide uh yeah and we got a bunch of feedback on the list i think i've addressed most of that but"
  },
  {
    "startTime": "00:36:00",
    "text": "i removed mention of specific companies removed use of normative language it's a purely informational draft and also put a summary of the recommendations in the intro and it was adopted by the working group um next slide uh but yeah so there's one thing i need to do is and there's a github issue for this um i need to move the examples to appendix and um also i guess admin move to the working group github um that i think that's it yeah um happy to take any feedback and um or if folks think i think it should go in a certain direction uh i think we've kind of like solidified on the the structure but uh happy to take any comments john o'brien university of pennsylvania thanks for doing this work very very helpful and i'm also particularly glad to uh see commentary about uh the time limited uh having things that are time limited um the other uh experience i've had which you may wish to add to the document you can tell me if you'd like me to put this on the mailing list or a github issue is i've come across some service providers who require either that the domain that's being validated is strictly a second level domain for exampleunder.com.edu or in some cases uh that it's at a zone cut and that causes some problems especially in organizations such as mine right i think we have an existing issue but yeah if not then that sounds really helpful yeah okay joe yeah john levine i mean i i like this i like this draft but honestly i think if it's worth doing it all it's worth doing as a bcp because i think we can all agree that there are some there are some ways to do this that are much much worse than"
  },
  {
    "startTime": "00:38:00",
    "text": "others i mean like not using it put putting the stuff with the name instead but it's instead of a subdomain not putting some sort of tag in the in the in the text strings so that you're not spoofed out by by wild cards and my guess is that is that there is not much disagreement about what's good and what's bad and that seems to me it's a natural thing for bcp you know i would like to have a bcp i can point out that says please don't do it that way here's here's concrete advice on how to do it and not screw other people up yeah um yeah that's an it's something we have thought about um you can imagine that you could be more drastic and say that there should be a new r type for something like this but uh we didn't want to do that um but anyway i think we had the rr type argue fight a decade ago and i hope we cannot have it again yeah i think um so the intention for this draft was to be a survey but if folks think that it's it's better if it's a bcp then very happy to hear comments on that actually thank you we have a remote participant brett please go ahead um yeah i've got exactly the same comments as john really basically um i think this is great work it's good to see a survey being done uh but i'd like to see you know when the survey is done that i i really support the next step of moving this into some kind of bcp to be able to point people at um the right way to do it and uh in particular to get rid of the bullets and get rid of the records that sit there forever um i i think um i i think there are too many different ways to do this currently and we should be trying to encourage people to cut them down thanks thank you and uh sorry anthony thanks anthony liquid um yeah good"
  },
  {
    "startTime": "00:40:00",
    "text": "document um not to sound like a broken record definitely bcp um just a comment on the tcp thing i think definitely draw more attention to that because as you pointed out with the whole retry there's some quite critical records that sit in the apex like your your spf records and causing issues with email is not something that we want either so it would be really good if we just draw attention to that as well ben schwartz uh i i like this draft i i think it should probably have one sentence about d name how about what sorry i think this draft should probably have one sentence about d name i don't really care what it says but it should say something thank you no other comments thank you for your feedback thank you shiva yeah if we have some time i guess i'd be curious if folks think that we should just make this obesity or whatever it shares yeah yeah i think that's a good discussion and i see uh tim also annoying yeah yeah i guess does anyone object to so maybe yeah good question to the room are there any objections for bcp no okay thank you yeah we will talk with each other with the chairs and [Music] adjust the status tim you want to say something thank you so next up is yogos thessalonikis all right"
  },
  {
    "startTime": "00:42:01",
    "text": "uh no sorry not this one this one hello again i'm jorgos and together with film and rogue we have an idea that we call dryer and a sec next slide sorry so how it all started we got extended dns errors so the clients can understand what's happening with the resolution fails that was nice uh we're getting in the future sorry previously yeah we're getting in the future dnsr reporting that is very nice so zone operators know what the resulting errors for their clients are and next slide and then there was a random lunch discussion that's all fine but if i want to adopt a dnsec that doesn't help me the report is way too late and then we thought dmarc what dmarc does is before you actually push the buttons and you apply policy you can just get reports and see what's happening and then when you feel comfortable you say okay let's go for it next slide so what we're trying to do with dry rent in a sec is go from this is fine to next slide this is fine actually without fires and smoke next slide so how it works you sign the zone and you published in dns so everything is public but instead of an actual ds record you just put a driver and ds record in the parent then that gives a signal to the resolver let's say do validation but this zone is dry unsigned so if something fails generate a report back to the zone operator and fold back to a known dry"
  },
  {
    "startTime": "00:44:02",
    "text": "run ds record and by the way validation succeeds yeah you can go ahead and give that 80 bit back and you may have opportunistic security already but with some caveats that i will address later on next slide so a couple of use cases first and foremost you we can have dnsec adoption you can test your zone in the wild and you can expect some reports back if everything seems to work fine then you can go ahead and push the button so there's a turnkey action to deploy everything is signed and ready there in your zone and then you just need to replace the dryron ds record with just a new ds record no need to do any more changes so what seemed to work previously with dryer and dns should also work with the ds chains next slide you can experiment with dns if your zone is already signed you can do weird stuff with dnsec and then provide the drier nds record and the resolvers could tell you what's happening on their end next slide and yeah you can test kira lovers this can work by using your real ds as a driver on the s and then sign with a nikkei introduce it as an another dryer on the s you do your gear lover stuff if the resolver finds an error it will report to you hopefully and then if everything works then you can do the same thing but instead of the driver and ds things you do the actual ds records next slide you can also break it uh you can use a dns option so that the clients can signal the resolver and say you know what i know that you understand dryer and dnsec if there's an error you will try to fall"
  },
  {
    "startTime": "00:46:01",
    "text": "you know i opted in just so please just give me the error back i want to be part of the test so this can enable a couple of scenarios you can get easier and faster debugging from the client side instead of waiting for the for the reports from the upstreams and you can also use that in your application so yeah if i want to sign my zone i want to see how the application will behave if we get a dns error so this will turn like next slide this is fine by the way all these memes were already there for me to find so people are weird next slide please uh so that details we got some feedback on the previous itf uh mainly because what we propose as a dry run ds record is what most people including us at first perceived as a ds hug and there were a couple of suggestions instead of doing a ds hype we can use the flags in the dns key so they are now ignored so you can use that space there and although that seems nice when you actually need to go from testing to actually signed you would need to change the dns error set and we believe that this is no good so the if you want to go from testing to actual deployment it should be as simple as possible you don't need to change the data another feedback was there are a lot of ds hugs so maybe we can use a ds hack to rule them all and we thought about it but we now see that the dryer on the s is more than a ds hype if it's going to be adopted we see it as a integral part of dns because it will give you the ability to actually test dns more feedback about these hacks so why don't we normalize all the different"
  },
  {
    "startTime": "00:48:01",
    "text": "ds hacks with a range of arrow types on the parent side that they convey information about the delegation itself yes we agree for that but this is another draft but by the way this could work by having the dry run type as a new arrow type on the parent and then have the same ds data as the actual ds but that is for another talk next slide please so what we currently have is two different timelines the single timeline which is the uh the simpler one so we have a specific dry round digest type and then we push the one byte of the actual digest type as the first byte of the digest data so this could work for is always not understand the dry run digest type but it will results in variable length digest and we got some feedback that this may upset some people especially the registries so an open question to the room is uh how bad is this can we live with a variable length digest can we change our tooling or this is a no-go next time please and then there's a multiple time line where we introduce a dry run ds type for each ds for its dsi for its real dsi uh yeah this will uh half the space we currently only have four assigned and yeah there's an open question to the room can we afford this or this is too aggressive next slide by the way all signaling is backwards compatible if the resolvers don't understand that the the digest type then they just ignore it so that's fine next slide so provisioning because you need to signal your parent that you want to do"
  },
  {
    "startTime": "00:50:00",
    "text": "dry around in a sec if your parent accepts ads great just give them the drive on the s if they only accept dinner ski because they want to generate the ds themselves you need to either supply them with the ds because they need to see the dryer and tent there or maybe you do it through a ui and you need to check a box cds works the same cdns key you need to have the compiling cds for that so that you see that the child wants to do dry run next slide so the security caveat that i was talking about is that if a zone goes from one signed to signed and then there is the intermediate step of dry run between you can get opportunistic security but if dns occur if the nsx fails validation fails then the resolver will fall back to insecure so and this is a feature not a bug uh so that means yeah if you get the adb that's fine but if you don't you can take uh you can't get any results from that so for example because they will talk about dane that won't work next slide please we started implementing the nsl reporting which this this draft relies on we are in the early stages but when we're done this could be a next step for us for unbound next slide that was it so i would like to see what people have to say about it and maybe provide some answers thank you yogas with all right thank you thank you um i really like the idea i think it's a it's a great idea you know it's worked well in mail it's worked well you know your comparison to dmarc was spot on with with one caveat though which is"
  },
  {
    "startTime": "00:52:01",
    "text": "that anything you introduce in this had better not disrupt any validation behavior you know in regular dns sex so to me that immediately rules out any sort of ds hacks and immediately rules out you know uh algorithms and things like that as a mechanism for doing that because we already have cases where ds algorithms provide funky things when one is unknown so if it's a completely separate record either something somebody will understand a validator will understand it or they won't and if they do understand it then everything's fine if they don't or even if they do the validation chain has to be totally the same for the actual answer yeah i agree thank you uh remote participants steve please go ahead thank you i'm not sure i understood completely but when i saw uh going insecure um i start twitching i don't like going insecure at all uh sorry can you repeat that yes i i as i say i'm not sure i've completely understood uh the totality of what you were presenting but at the point where you said uh if if something fails you go insecure that raises a red flag for me i don't like the idea of going insecure you don't like the idea of going insecure right that's what you said yeah that's what i said yes but but uh so for that use case you're starting from an insecure zone right and then you try to sign it and if everything validates it's going to be secure or it will fall back to insecure but dryer and dinasek is not meant to be a secure final state so getting the insecure answers back is at least from my point of view fine because you're still testing"
  },
  {
    "startTime": "00:54:02",
    "text": "um you're saying it only arises when you're starting from an insecure state from a from an unsigned state uh it it doesn't arise when you're uh in the middle of say uh changing this the signature now uh sorry i didn't get that uh does this is is this used when you are rolling the key so uh so if you want to test kira lover so that means that your zone is signed and then you want to test uh if rolling a key would work so this would start in the dry run let's call it the dry run in a sec part of the zone and if anything fails there the resolver should fall back to the actual ds zone so okay the previous plate yeah okay okay so as long as it doesn't introduce a period of going insecure um from a secure state then i'm okay yeah yeah i want to clarify that that if there is an error in the dry run state you go back to the previous it's going to be insecure for an insecure zone and it's going to be secure for a secure zone thank you thanks uh victor de koffny google uh my concern is that uh okay the mic is low sorry about that warren's fault um so [Laughter] um my concern is that you seem to assume that all the resolvers out there in fact behave as specified"
  },
  {
    "startTime": "00:56:01",
    "text": "and will do the right thing when presented with an unknown code point uh just two days ago i tried that with algorithm zero and was deeply disappointed from at least one major resolver i think all such assertions unfortunately need extensive field testing to determine whether that's true or false and i'm also concerned about uh considerable implementation complexity in an already fairly complex stack so i don't know how much adoption such a thing is likely to see from from major providers where you really want good coverage for this thing to be effective right if they just treat the new code point as unsigned no real testing happens until you actually go signed so you'd need the cloud flares and the googles and the binds everybody to really adopt this before it's particularly useful so it's a pretty tall hill to climb i don't know how we get there we have we did some preliminary measurements from the previous itf although it was ripe butler's measurements so we used a unknown uh digest types the results looked pretty good but still it was just right battle so yeah you can't be 100 certain okay paul i'm sorry um paul hoffman can you go back about three slides to the single track uh three slides i guess that was are you able to yes uh to to one that he had single track and dual track and i was looking i wanted to comment on this thing on double timeline yeah yeah the one before this"
  },
  {
    "startTime": "00:58:02",
    "text": "i'm sorry this one sorry this one yep um i think even though some um folks might not like the variable length because it's hard for them blah blah just as victor said things that are are not implemented well are going to have a hard time regardless i think this is absolutely a feature for us testing um new ds types where in fact the digest length is going to be surprising such as for post-quantum crypto so i think if we which fortunately is way in the future um although whey can mean a lot of things different people if we have this with a variable length digest type defined before we start fuzzing with the post-quantum algorithms and some of the things that go along with that we will absolutely get much better data about you know how things are going to break so i would strongly recommend going to this with a variable length digest type thank you ben schwartz uh thinking about this from so i'd like this functionality i think this would if if somehow we we lived in in the future that victor outlined uh where all of this had eventually rolled out uh this would be a very useful thing for use cases i've run into i uh one thing that i would like if i were using this is to be able to answer the question what is the error rate of what what is the error rate that you've created with by by adding this and i don't think this proposal actually gets me there right i get error reports but i don't have a denominator i don't know uh i don't know how what fraction of my uh recipients actually implement the specification so i i don't know the error rate that i'm generating"
  },
  {
    "startTime": "01:00:01",
    "text": "so i think maybe a more flexible system that could do things like request a certain number of positive confirmations to say yes it actually is working that would be helpful so i can comment on that because this draft relies on the necessary reporting and the new version has support for a no error flag so as an upstream you can signal to your resolvers to say okay you see me but please send me back nowhere so that i know you're there because if i don't get any error reports how should i know that everything is okay right so you get that thumbs up if nothing is wrong from your resolver and then maybe you can also use that information for uh for what you described okay and would that actually work for this because knowing that the resolver supports the dns error reporting is not sufficient right i need to know uh the denominator has to be the the fraction of responses that went to resolvers that implement that and also this yeah what you would see is that okay uh x amount of resolvers contacted back with no error so i know they're there and then i start getting errors okay it's not the actual number that you want but it is something more than just getting the errors back okay thank you yeah hi um lars lima from net note this is a good thing um you mentioned that this is is viewed as a temporary measure for for a transition between states and i i totally agree to that in order to avoid having lingering stuff in in the system um that may or may not interfere further down the line would it be an idea to either put some kind of timer into the system that kicks out this this functionality after a certain time or to have a recommendation to implementers to put timers in the software that that interacts with this"
  },
  {
    "startTime": "01:02:01",
    "text": "just to avoid you know as the same thing with with the records for for domain authentication we don't want lingering rotting stuff in our dns just an idea yeah but with timers you mean how long the dry run state is going to be yes right yes but but this is easy because it's about the ds record so it's a manual intervention you can either turn it on or off yeah i mean if you leave it there you're just going to be on dryer and insect forever i mean we don't want that yeah but if you want to transition you need to manually do it do it there's no automatic transition for either state or you just uh you leave the ds record that uh expire on the keys but yeah that's garbage but that's what you're saying what i would like to see is to have it removed after a certain time unless someone manually kicked it and say yes now we are in running state uh yeah i don't know how to do that automatically but i understand what you're saying yeah at the risk of making my good friend steve angry with me um i actually think this would help if you were going from secure to a new algorithm role to insecure i mean i wrote a draft a while ago about that that sort of got 50 support for people liking it 50 that absolutely hated it um [Music] this might help in that case right if i could point them to it saying if you know algorithm roles are tough try doing this i will say that i think there's a lot of corner cases that you need to work very carefully through like a dds pointing to the same key versus a dds pointing to a new key and how many rrcigs must be in the in the file when you're doing all this testing and the zone suddenly grows and you're returning a lot of extra large packets and things so do make sure you work through all of the weird corner cases and then start talking to the registries very early about having a new ds record and inserted dds record"
  },
  {
    "startTime": "01:04:03",
    "text": "hi it's peter from the esec peter thomason so a few things regarding the cleanup of stale dry run records i would think the only entity that may feel that it's harmful for those to stick around will be the corresponding registry or parent authority and i think it's not a problem for the registrant or somebody else so i would make that local policy for the registry right i mean they can write in their documents that we will remove your drive on record after four weeks or whatever they feel like that and then dot d is gonna do that perhaps and some other registry you're going to do some different thing i think that's okay um maybe perhaps a minimum should be specified or you can use the ttl of the dryer and this record for that well perhaps that's a bit too short because um so i mean you can put a huge value there but then yeah okay yeah for example yeah anyway i think it's mostly a local policy issue the second thing regarding the digest field length being variable i think it is better to sacrifice a bit of the digest type field and i also don't think that with post quantum crypto it would be necessary to expect that the digest field length will vary because even when the key length varies the sha hash is still just a shar hash um and then last thing i wanted to say um maybe to point out as an implementation note in the draft so far if my understanding is correct although i don't know exactly the provisions in the dynastic specification is that when you do validation of a chain and you encounter ds records essentially you need to find one that matches and then you start processing and it's not necessary to keep scanning"
  },
  {
    "startTime": "01:06:00",
    "text": "and matching other things and it's up to the result of whatever digest type they would consider suitable for matching now if a resolver does have support for ignoring validation failures when there is the dry run bit set and even they would send out dns error reporting stuff in that case that they may not be hitting that code path if they earlier are encountering a ds record that does validate and then they stop processing so um i can imagine that a naive implementation of support for this could run into this problem and perhaps the draft should point out that you need to continue scanning the other ds records to actually run this kind of experiment so we haven't implemented anything yet but the way of thinking was when you encounter the dsr set pick one of it so one dry run one uh real one the ones that you would have picked normally and then try the dry run and then if that fails see if you have a non-dry run and try again okay fair enough i guess but this is without an implementation just an initial idea yeah implement just need to be aware that it's not permissible to stop after the firm after the first match if you want to support this that's all sam weiler we have a proposed standard rfc on how to do dnstec experiments sure we have a proposed standard rfc for how to do dnsk experiments that says to use the algorithm number not the ds record i'm not convinced that we need to pop up the level this level and do it here if the working group is we should probably be updating 49.55 however i'm really concerned about what i just heard which was about changing the validation rules and maybe only looking at one of them preferentially"
  },
  {
    "startTime": "01:08:00",
    "text": "that you start to go down real slippery slope there and i'm i'm again i like 49.55 i think it's probably sufficient okay uh neil swizziel the isec sorry for being late to the game i just wanted to float the idea of sending the s records with edns client to the um to the resolver instead of propagating all this information to the parent zone i believe i'm not a resolver implementer but i believe that could keep down the complexity so this is a reply to victor's comment about complexity so if the client just sends ds records for testing along with the query the resolver could validate against that rather than looking in the parent zone but you mean the client itself provides a result with a ds record yes so you you deploy you sign your zone and then you want to test it so what you do is you query let's say google public dns but you also include the ds records that you want to propagate to the parent zone and then the result would validate against that and give you the result so you don't need to talk to the parent registry that would work for the for the opt-in case right so the client is opting in for testing yes yeah okay but this will not work for on the while let's see what's happening on random people right but how would you get the results from testing in the wild anyways uh through the parent with the driver nds no i mean what you described works perfectly but only for the case that uh the clients are opted in right yes you can test random people on the network right yeah okay yeah a couple of comments uh i'm not sure that client's idea signaling is viable"
  },
  {
    "startTime": "01:10:00",
    "text": "because cached data has already been validated it's rather unclear how this would work with uh results already in the cache in terms of where to put the dry run bit and the variable and digest type and so on i do agree that uh stealing a bit from the hash algorithm number is saner than moving the digest up digest type into the digest value hash algorithms are introduced exceedingly rarely symmetric hashes are very stable there's no evidence that shatu is likely to be compromised anytime in the next hundred years we're doing very well with that one we may add shot three at some point though this little demand uh so um steal some space uh from the from the hash algorithm if this is to go forward uh that seems to be the uh the way to do it uh if i can comment on the casting because it's a good point we thought about it and this will need to keep both dnsec states in the cast because clients may need to get the error may need to get the non-error response yeah mark andrews um that's a variable length we're going to need them anyway for private and private ds dns key algorithm types so don't be scared of that [Music] in terms of a replacement for c dns key um i think you should be looking at doing another key another type which signals the similar semantics to cdn is key that signals that you're wanting a dry"
  },
  {
    "startTime": "01:12:00",
    "text": "a dry run ds be produced and in terms of ex exp experiments um uh it wasn't doesn't matter earlier um it really doesn't matter for experiments for experiments as long as you don't have both values that are known you've got a new value in either those two fields in the ds record that's perfect you make about you have a safe experiment so okay that makes sense kind of yeah thank you thank you for all the feedback and thank you yogas thanks um yeah next up is paul hoffman hi i'm also still a new deck being shared um so now we're in the space where things have not been adopted by the working group um so this is a request for uh people to consider adopting a draft even though this is abyss draft of something that we finished not that long ago 8109 is just a few years ago it's on a topic that a lot of people find really really boring but it's on a topic that happens all the freaking time so it's sort of important that we tell people how to get this right which is dns priming next slide so 8109 is out we believe it's well implemented because it doesn't really"
  },
  {
    "startTime": "01:14:00",
    "text": "say much to do other than the very basic thing that has been talked about in the dns really since 1034 1035 which is when you have a completely empty cache and you get your first query the way you prime is that you do this and then this and then this so 8109 didn't change that it helped resolver operators understand better sort of some of the the i wouldn't even call them edge cases but interesting considerations of that and yet still we see some resolvers that don't necessarily always do it the way that would be best for its users and since it is so important to get priming right getting it wrong affects a lot of people pretty much silently which which is really bad so um as this working group was was working on what became 8109 some issues came up and people said we just want to get 8109 publish that's you know that's a controversial issue or whatever and we punted on them which is fine but now with the fullness of time we can actually take them up again and also since we brought it up some new issues have arisen next slide so this is a a large list but and by the way this is my second to last slide i'm not going to go through all of these but i wanted to point out if you read the draft you will see this list in the draft these are things that we've already looked at so the reason why i put up sort of it's not as bad of a wall of text as some other people's but is that we've already seen a bunch 48109 a bunch of issues that so it's more than one or two issues is what i'm trying to say we think that it is worth opening this up again even if some even if we punt on some of the issues there are plenty of other things in here um that are more or less important to uh"
  },
  {
    "startTime": "01:16:00",
    "text": "more fewer people um the uh some of it's just terminology uh the root server operators have a very specific way that they want to be referred to i think it would be good for us to do that um we uh this working group loves the tc bit at the same time that it hates the tc bit we we want to deal with that more and such like that um but it's it's we think that it is worthwhile even though it's not the most interesting subject to actually get this nailed down and so this would be abyss it would actually obsolete 8109 i mean again if you find any of these interesting please look in the current draft see where it is next slide um so this is what's not yet in the draft pre-fetching has become a much more popular topic in the last few years and certainly for the root zone many people would say pre-fetching would be a good thing so we should be talking about it there's nothing we can say you should you shouldn't but we shouldn't pretend it doesn't happen um and then there's also the question of post-priming strategy so after you've primed after you have this set of um ns records thank you after you have the set of ns records everyone knows that different resolvers do different things to pick which is their favorite authoritative server for any zone not much less the root zone but some resolver software actually treats the root zone as special we can say don't do that as often as we want but they do and so there should actually be a discussion of once you've got the zone sitting there how do you pick which authoritative server to use do you pick the fastest do you pick the one underneath the thing so these are all things that we know that people have talked about we don't need to say you"
  },
  {
    "startTime": "01:18:02",
    "text": "should do this or that but we should certainly admit that in fact there are these post-priming considerations particularly for people who are watching things on the wire it is completely believable that a resolver comes up does a priming query goes to q root reboots does a priming query and then goes to our root instead you know like like within the course of 15 seconds for whatever reason normally we would think oh it'll always go to the fastest we know that that's not the case q root and r root might be approximately as fast there may have been a round switch things like that this should be discussed in 8109 is our belief so i think that's the last slide oh and then i guess the the obligatory last slide is does it work and who want to adopt it we don't need to talk about that now there's no rush but i also think that there's no reason to wait for it it's not like we're expecting any new news going on so that was that was it thank you any questions comments from the room okay thank you thank you thank you very much paul that closes the for consideration section of the agenda and now we're up to the time permitting so a remote presentation by then then wing i think you want to run your own slides uh if you could share them that would be great thanks um so i'm dan this is uh basically now an extension to um ede um which was nice to see some progress on earlier next slide"
  },
  {
    "startTime": "01:20:01",
    "text": "so the purpose for this is when there is dns filtering to provide more information about what was filtered and by whom so that that can be tracked down if that filter was sorry sorry then you moved the microphone maybe we can't hear you or difficult to hear you uh i had some papers next to it my apologies that any better thank you sorry about that very good next slide so the changes we made between zero one which is the last time that i presented on this to the current version is uh further constraining the json and when it's displayed to the user and the reason is there is concern that the browsers would be unwilling to display information that was controlled by the dns resolver that the browser is talking with uh and instead just throw that into a log that it or a more sophisticated user can look at but not have it be something that's popped up straight in front of the the user's face um and also to help constrain those messages uh instead of having the freeform text that we had previously uh the second bullet talks about uh sub-error numeric codes that we introduced in this version so for example what's defined right now one is for it was filtered because of malware two for phishing three for spams and numbers like that and then also to ensure that we don't have a cash poisoning problem uh is to require the the add resolver info to signal that there is support for this and that the information returned in there is actually from that first top resolver and not being sent down from something else and causing the ede to get propagated and the next slide please it shows an example uh what the json is that's being sent and that's currently"
  },
  {
    "startTime": "01:22:03",
    "text": "text field uh so instead of sending you know human parsible text it's got kind of human parcelable json and that's my update next slide please and i'm wondering if there's any further interest in this uh there seems to be um and would certainly like any comments from the room or from the 108 other people thank you there's one question from the room john i'm john o'brien university of pennsylvania um i'm curious if you've looked at uh whether this could interoperate usefully with response policy zones um and uh if not uh that seems like it would be a valuable addition okay thanks um better please mike is yours hello once again can you hear me now yes yes excellent uh i like the changes and how i hope that the browser vendors will like them as well do you have any feedback from either google people or firefox people how they think about the latest changes because that that was the manage last time right right that was the main issue um tyro has received some positive feedback i forgot from whom i'm sorry um but uh it it seems um more amenable to browsers especially you know getting rid of the freeform text and going to error code numbers so that then the browser can localize the message and also especially not to display that the freeform text that we had previously"
  },
  {
    "startTime": "01:24:02",
    "text": "that seemed to resolve uh most of the issues that we'd been getting feedback on for those but unfortunately turo is asleep right now so i don't know who those uh specific folks were unfortunately sorry i'll get him to follow up with you peter thank you brett also repo remote participants i just want to add my support to adoption of this and um and also um agree with the previous guy who uh who mentioned supporting of uh of rpg this would be uh really useful operationally for us for our protective dns resolver that we we operate in the uk cool thank you ben schwartz uh i i do think that this this revision is an improvement um i i still wonder i wonder a few things i wonder if dns op is uh is really well placed to to deal with this i think that this is really getting to a much much deeper and thornier question uh of how uh how malware type filtering ought to work in uh in a user device setting uh you know the major browsers today already have functionality of their own here in the context of things like safe browsing and those systems aren't simple query response systems in the way that the dns is for example instead they tend to use complicated quasi information retrieval algorithms to avoid revealing private user information uh you know"
  },
  {
    "startTime": "01:26:00",
    "text": "we can yes we can we can sort of stuff this kind of functionality into the dns but uh should we okay thank you i well tim wants to respond to that sorry i made i made this comment in the chat but the chairs are interested in this but we'd like to hear from folks who are actually going to be implementing this because that will actually help us drive you know the sort of where we if the working group sort of takes us on as well so if folks are interested in implementing please speak up and give us some heads up and that'll help us a lot thanks yeah in indeed so uh outside the genius of working group we did hear from other people that were interested in the draft and also probably interested in implementing but we would like to hear that also in the working group and maybe well um we will plan a call with with dan and the chairs how to go forward and get more well uh support or explicit support of the implementation potential implementations okay yeah awesome thank you thank you oh sorry we still have two oh i hope dan is still online there's still still two questions here uh jonathan uh yeah i i was just giving another that's one i support adoption of this um i can't personally commit to akamai implementing this but i could see uh uh i could see it being very feasible for us to implement for our uh services that we offer that provide filtered responses um and i think you know and ben's question about is dns upright is a fair one um i do feel like every time there's a real world problem that operators are coming and saying this is an issue the question comes up of is"
  },
  {
    "startTime": "01:28:00",
    "text": "this a right working group and my question is if not this one which one right because i think you know this is a problem for operators and i think we are empowered to address this and this is the right place for this thank you thank you uh dr carney uh my comment is mostly actually back to ben i think uh i don't think this is a new mechanism to you know implement uh virus scanning that isn't of any sort this is really to deal with reporting existing rpz feeds and and their classification of a domain it's already blocked we're just adding transparency as to why so the user can be told why the dns name isn't resolving so i don't think it's in conflict with any more sophisticated or active filtering that the browser is doing with the content at hand it's really reporting dns resolution that's going to happen one way or the other and telling the user why okay thank you um thank you chris hi chris box uh bt uh as a customer of powerdns uh i would like to see this developed and we would deploy it in the network okay thank you thank you john it just occurs to me that since the web or the internet hasn't been completely overtaken by http uh that this would be useful for applications other than web browsers that can't present a captive portal and might still like to be able to propagate up exception information of this kind to a logger or some sort of user cool thank you thank you okay thank you all thank you"
  },
  {
    "startTime": "01:30:01",
    "text": "thank you dan um thank you i will hi um i'll have slides in a second right so i'm here to speak about um some research is trying to answer a long-standing open question about um dns second internet um this is joint work with austin hensel and nick femster from princeton and chris wood from cloudflare um so uh next slide please so here this is this is not my work this is work by evp nick i guess jeff houston probably uh measuring the extent of dns validation on the internet so as you can see there's quite a bit um that you know this line's a little hard to follow but like you know something on the order of like 40 total validation rates um so that's like seems like good news um except next slide um basically all that validation is done by recursive resolvers not by um endpoints um in fact some of the dsgs is like largely google public dns and people and you know cloudflare people like that um so um uh so no major operating system although do endpoint dns validation by default um window um apple just will just roll this out but i check with tommy paulie and you have to turn it on for yourself um and browsers won't do it either um and this is obviously super limiting um and if you want to roll out any feature that requires like um dns second requires not trusting the recursive resolver um like dane then you need um the endpoint validation um by the way this is like this talk is like entirely about like um endpoints um like consumer endpoints this doesn't apply to like like like server machine server class machines i know there's quite a bit of dns validation for like send mail and stuff like that for mail and stuff like that it's not what i'm talking about here um next slide so um as a browser manufacturer i often get"
  },
  {
    "startTime": "01:32:00",
    "text": "asked why don't you jerks validate um and um so i've i've produced a set of answers for this um so on one answer people often give um and there was um is performance namely you have to do more requests and maybe someone will fail or maybe have some trouble doing them in parallel somebody'll be slower um but the primary reason the one highlighted in in bold here is we're concerned about breakage so um what happens if you have a some sort of middle box whether it's your recursive resolver or proxy whatever that doesn't deliver the dns records or mangles them in some way this is indistinguishable from an attack what you know is you know there's like a ds record and you know things should have been signed but congratulations they're not or these two can't validate it and what you're supposed to do according to the rfcs at this point is to hard fail is refuse to accept the data you've been sent um but like when you're looking up an a record if i respond to that by hard fail that my user sees as they're trying to go to a site and they can't get there and the general sense among people who actually build this stuff is that any significant rate of non-delivery like in excess of like fractions of a percent will create unacceptable failure rates what i mean by that is that if we roll out some new piece of functionality in the client and we see an increase in failure rates that is like in excess of a fraction of a percent it's unacceptable we have to back out um there's really been very little data about this um the uh there's some work by um by my team of years ago um only some people use csd um there was a thing about maybe 10 years ago by adam langley where they tried to retrieve random text records um from the internet and like had like five percent failure rates um so five percent would be way way excessive and so adam has this like famous post that we often refer to about why people don't do this and it like refers this experiment but like that's not dns sec um it's text records and it's a long time ago so we decided to try to answer this question next slide please so this is a very straightforward experimental setup we set up some domains that we control ourselves they have valid records one trusts"
  },
  {
    "startTime": "01:34:00",
    "text": "because we can set them up actually cloudflare shut them up with their automatic you know signing um so we have correct dns records rrsa xds dsq whatever we also have some other less common records um which i'll get to in a minute like https um service b on sma some new records we made up and we use firefox as a measurement platform so what we do is we randomly select select a sub sample of firefox glance and then each client tries to directly resolve the relevant records we don't use the we do use the system resolver to live in a second but not for this so for this we do we do like a udp you know we look up the operating systems resolver and then we talk to it directly with udp or tcp and we ask it for records and we see what the answers are we measure the success rate next slide so here are the queries we do um and we do use like we do some of these in a random order and some of them not um we start with looking using the firefox dns resolve api which just talks to the system resolver and this is just to verify like everything's working so we can't get this then like things aren't going to work it also gives us a control based line of what like a failure rate ought to look like then we look then we ask for a nsa randomly we ask for a records with all possible values of d o and c d um i said some people we um hear me remember i sent mail of the mailing list asking what to set these values to so we set them all um just to be sure um we tried dns key we asked for service b we asked for sma and we asked for four records with all combinations of small and large and code points in expert review and private use ranges um and all these as i say are like correctly populated so they should they should all succeed so here's the next slide this is where the actual answer is question um so red here is just dns sec so the baseline failure rate um i'm sorry i did lose dns resolve um so that's um that's unfortunate um the baseline failure rate is about two percent which is like a little surprisingly high actually um uh dms resolved about one percent a little less um so there's some extra something quite quite right with resolve that's causing"
  },
  {
    "startTime": "01:36:00",
    "text": "like a little diff that we're not sure about um but if you look the um as soon as we start asking and so this um this failure rates are are we getting the correct answers it's a combination of we got like nothing and do we get like the thing doesn't match and so as you can see um if we just do um the straight up are these all off by one these may all be one which is really embarrassing um um the bottom line straight up answer is dad all right i see what's going on okay sorry um so the bottom line straight up answer is that if as soon as we try to ask for our sake we don't get it we don't get it like a third of the time um so and and like we like looked at this eight ways from sunday we looked at the things by hand and like a lot of them are too small so they couldn't possibly have an rseg they don't have truncations yet it's just like oh come on um um if you if your cd equals one of course you don't get it at all which is expected um but if you do do you don't get it basically right um so um dns key you get um so that's fine but you don't get rsa basically um you don't um and then these other records it kind of works sometimes so like if you ask for s my may you get it like percent of time sorry 85 percent of time ask for https you get like 93 of the time um and these other records kind of work sometimes um but um you know uh not reliably um and the bigger the record the worse it is um it doesn't seem to matter whether it's in private use or whether it's in um whether it's in uh non-private use um so this is like what we see um i'd be happy to talk to people offline about this or online about the methodology here um but this is basically what we see here um uh we don't by the way we don't um we did actually ask this is all with you tcp is a disaster like he's gonna get twenty percent right across the board um if you uh just apply a little more color we did try asking for rsx separately that also doesn't work very well by the way um it's not even clear that should work um we didn't as an admission we didn't ask for ds but obviously we don't have our essay this won't work so doesn't matter um next slide so like what's the impact of this"
  },
  {
    "startTime": "01:38:00",
    "text": "um bottom line here is not safe to enable endpoint dns validation over to 53 for like generic endpoint clients um it's probably not fine for server class machines but it's not fine for client class machines um it might well be safe to enable them over doe or dot um we did sort of experimentally measure the public resolvers and they seem to do just fine so if you like you know if you ask like 1.1.1.1 or 888 like they do a fine job of giving these answers um it might be the case that add advertising resolvers do better but of course we don't really know yet because that doesn't really exist but like you might imagine he's been updated just a bit of job um the security posture is a little confusing um it's somewhat practical to deploy other record types so if you have a new record type where it's like if it worked 80 percent time that'd be great like https then like that really is fine um if you have but if you have something where it is where all time like basically it's not safe to do anything that isn't basically like a and c name um and i guess ns um there seems to be some variation like https looks especially good that may just because it's smaller than symme we're not sure um so anyway this is the bottom line for us um uh uh we have a paper that we've written that we're not quite ready to like publish but we'll publish pretty soon that has more diesel methodology i'm happy to answer of course any questions about this people have thank you oh can you go back one side i'm sorry so oh one came back one more thing as i was looking at this i realized what happened the sorting algorithm was running the the the system resolves on the bottom it's just point point four percent is the uh is the figure of system resolver yeah hi eric ray bellis i see so do you understand correctly these are experiments done within a firefox browser um on end user sites yes okay um my own research now which is like 10 years old showed that home gateway resolvers are particularly bad at this side of things so i'd very strongly suggest you try and maybe enumerate whether the resolve you're talking to is on the same subnet as the client so you"
  },
  {
    "startTime": "01:40:01",
    "text": "can actually tell the difference in the stats between on that resolvers and off net resolvers that's a good suggestion i think we can do that but i'm not sure yeah i take a look at take a look at rsc 5625 and the sac uh now if you'd be so good at the email to me because i'm not going to remember those numbers but yes it is of course it's very difficult for us um even um well we're talking detail about whether we can actually use explicit information but for experimental person that's a great idea okay um brian dixon go ahead um so yeah sort of back when you raise uh train of thought um do you have the ability or would you be able to do longitudinal per end user uh collection of information to see for a particular browser is the failure rate consistent and are you able to track that across network changes and can you potentially aggregate those to these customers always failed and therefore you could check uh initially and if you get failures not do the validation but for any of the ones that do succeed if the longitudinal uh collection of data suggests that if it succeeds it'll always succeed that that information i think would be very useful yeah i mean so in principle the answer to like can we take that measurement is yes in practice the answer is that the grad student who did this work has taken a job and so probably the answer is no um if i manage to find somebody else who's going to do the heavy lifting then the answer is probably yes again hazel hi um obviously firstly thank you for doing this um one question i had is you mentioned i think on slide eight possibly that you had done some testing of dot and dough resolvers to see whether to see how they behaved um i was wondering whether you had done enough of that to collect any"
  },
  {
    "startTime": "01:42:00",
    "text": "data whether you have any graphs for that i'm sorry like can you say lessons again uh you said about um don't resolve is that um contacting them with um uh do and cd set um was more successful did you actually did you get enough data to produce any kind of useful statistics on that do you have any charts graphs or anything like that to sort of like to be able to yeah no that's viable yeah yeah no what we did was basically we only realized that this was an interesting question at the very end and so we just like connected from like one of our machines to like like those nut resolvers and check that they did it correctly and our assumption is that because dope protects the data in transit or dot does um we don't have to worry about whatever intermediates are so the view from anywhere is as good as the view from anywhere else so we just we assume that like if cloudflare or google does it correctly once they'll do it correctly indefinitely so i i think they're i don't think there's much mercury on that but i'm not too worried about that part of it sure i guess i mostly meant from the perspective as i think one of the previous questions mentioned that potentially the issue wasn't necessarily the resolver that you're talking to but the middle box is in the way or sort of um right cpe equipment at home environments right whether something was mangling the traffic on the way there and back um and like i mean obviously right but doe and dot will bypass all those mechanisms so we assume it'll be cool yeah it would just be nice to see numbers but yeah i think you haven't got them you don't have them yeah they wouldn't have it thanks uh victor mccartney google uh thanks for doing this it's good to see these things updated again uh and would be good to see in the future after some time it's often we end up quoting things that are five to ten years old so excellent thank you um i was curious whether you've anonymized the sources to a degree that might make it difficult to break this down by goip or s numbers or whatever it would be nice to see how the"
  },
  {
    "startTime": "01:44:00",
    "text": "situation varies across the globe yeah we do have that data um that's in that some of that's in the paper um uh but from memory um it is substantially um the u.s and europe or like substantially some of american europe substantially similar and china and india are substantially worse um and especially worse for like the non-standard things but that data is in the paper and i'm um i think we have it we have it by if i recall isp nas so like if the other analysis people that are interesting we could probably run them hi kendricks did you do plain eating s query no i know if flag bit said beyond if i recall i i could i can like check this for you but if i recall we set all the we said all we always set um the edns uh um uh you know um max record size for all the queries um and like and that's one of the reasons we think it may be worse for the uh for the baseline query um i can share um uh um the code the code for like doing this is in a firefox extension so um if um so i can share that with you yeah yeah um home cpu devices are notoriously bad at everything sorry can you get closer because i'm hearing a lot of stuff from there home cpu devices are notoriously bad at doing dns properly yeah yeah exactly and i think so i mean our our in here our assumption is this is like largely broken home devices um but of course we have to we have to go to like war with the home devices we have basically uh daniel khan gilmore thanks for doing this work and thanks for publishing it um it uh so one thing is just it would be nice to see the breakdown of failure rates by actual size of the packets we have that okay um and then because the size does seem like it's a likely a likely failure yeah so i mean we don't have like a linear you know we don't have a linear but yeah we could give you a scatter plot basically um um i think what you and i assume you mean the size of the expected"
  },
  {
    "startTime": "01:46:00",
    "text": "packet as opposed to the size of receive packet uh yeah though i'd be interested in both because we have we have that too i mean it's like basically when we saw like like you know like all the analysis is errors right but when i first started looking at this i was like shocked at how high the rates were and so i spent some time like are we really i just like our parts are screwed up right and so like i went and looked like binned out how big things were and a lot of cases things are way too too small to contain the record and that's what persuaded me these days is substantively correct um so yeah so we have all the information um uh let me see what i can do to to add that to the paper yeah that would that would be super useful and then the second thing is just this observation that you know the network is filled with garbage and uh you're helping to demonstrate that here and i think we as the as a community who's thinking about how we evolve the networking to think about what we do about that right we can't remove all of the cpe equipment but if we're writing the software that's running on the client we can say if your local network has these kind of failures then we will do something else and i think it's worth looking at this and saying hey encrypted transports are both more privacy preserving and uh more capable and we should start ignoring the garbage parts of the network to do that i just think you know that we should we should send that signal as clear as possible you will you will be taking my support calls right so west vertigo rfc 8027 actually is the dnsec roadblock avoidance document that talks about this type of stuff and talks about you know it would have been neat if you had the chart of like how many resolvers fell into the different categories because one of the things we did is outline tests for all the things you need to do you know to get a dns compliant sort of resolver in front of you and to know when you don't so that you can make appropriate decisions or at least know when you're being you know attacked in some way but one of the things that we talk about that document is you know if you don't have one then you can fall back to doing it yourself that's of course more ugly and you don't get caching benefits and all that other kind of stuff but it would be interesting to do that analysis and then finally i'll note that"
  },
  {
    "startTime": "01:48:01",
    "text": "your table is missing one important record that you should have queried for which is diaz or erseg oh um because you actually can do that so i have so i have so okay so like i am now about to like where are my skis um we do have that data okay um we took it out of the table because the failure rate was extremely high and then we found um an rfc whose number i cannot remember which would led us to conclude that the rsa query was not supposed to work and um i can find a link for you um um and then and then i'll and you explain why i'm right now i don't care whether they're supposed to or not i will say that the ietf resolvers do allow okay yeah and so so we came to the conclusion that the itf thought that wasn't supposed to work sure and so if i'm wrong like we have the data we'll put it right back in um the one we're actually missing is ds and we just forgot is basically the answer all right well thank you for doing the work because it is interestingly and depressing to look at so thank you yeah just very short comment yeah very short comment yes our sig isn't really supposed to work okay great thank you um so people have my email address um if you want to talk more about this and you have questions um you know please reach out to me um if you think i've screwed something up and i think and like this is just me wrong please tell me i'd rather i know now than otherwise um and if there's other things like maybe i could get out this data would help you please let me know but thank you for listening i appreciate it yeah and thank you for sharing this okay thank you so we come to the last agenda item uh peter thomason please come let's all right hi it's peter thomason from d6 so it seems like i'm getting 11 minutes to talk about my two and a half slides um but perhaps it'll be quicker so um you all i guess know rfc 7344 which is cds cdns key records for announcing um your"
  },
  {
    "startTime": "01:50:00",
    "text": "bs parameters in the child zone and the parent can query that stuff and then for example use it for rolling over the s records when you do let's say key change or also for onboarding dns seg for a zone and while working with this in the context of bootstrapping dnsec i encountered let's say an interesting situation that i wanted to point out and i think it needs correction in the rfc um specifically the rfc 7344 doesn't specify how the parent should be doing their poll queries to retrieve the cds records from the child and that can cause a problem if um the cds or cdns key record set turn out inconsistent across authoritative name servers for a given zone um so let's say i don't know i'm the dot de registry for example i'm querying cds records for peter thomason dot de and then what to do if that's different on one name server and on another name server you can say whatever it's endorsed by the dns operator so who cares it'll be right one way or the other and it didn't and it isn't sorry and if it isn't it's going to be the fault of the dns operator but that is not entirely true in multi-homing setups where i think this is a specifically a severe problem because it may lead to one provider in a multi-signer setup and to be able to unilaterally roll the ds record set with intention or without intention i think for example a typical problem could be that there is multi-homing let's say at ns1 and cloudflare for example both of them do signing and now she's announced these are announce each other's keys and there's going to be automation for this and rfc drafts going on and all of that so that's going to be more common in the future perhaps and let's say now one of the providers rolls a key and publishes new cds records and at that point forgets to publish the other operators cbs records and then the parent comes queries that ends up only retrieving one part of it essentially rolling the ds record set and then the zone is"
  },
  {
    "startTime": "01:52:00",
    "text": "broken the delegation broken because the keys of the other provider are going to be missing and that's like a problem if it occurs but also conceptually a single provider shouldn't be in the position to be able to remove another provider's trust anchors so i think this needs correction and this is the first of two slides and the second slide which we're going to now um has the proposed solution so i think would be nice to have a short document that clarifies that if you're querying as a parent these records from the child you need to query them across all of the name servers and may only act upon them if they are consistent so the proposed wording here which is in the draft i uploaded it's an individual draft it says that the parental agent who knows the child zone name and the ns host names authoritatively because they are the parent must ascertain that queries are made against all of the name servers listed in the delegation and ensure that the set of reference keys is equal and then in other words when a cds or cdns record said it's returned by one name server but not referenced in the corresponding answers of another then it that situation must be considered inconsistent and if such inconsistency occurs then the parent must take no action specifically not delete or alter the existing ds record set so um i'm happy to discuss what people think about this and if you think it's a good idea perhaps we can work towards advancing this some way okay thank you mark um let's see you've got a one place you've got a denial of service attack if you require i come a bit closer and i have a return hearing mark andrews you you've got a potential denial of service attack if you require every ns to return the same ds records all you need is a machine that's down"
  },
  {
    "startTime": "01:54:00",
    "text": "and you can't correct you can't recover from that using a rollover and in reality ds records are no different to any other ara type if you're dns if you're modeling home and one of them does something to the zone the s records aren't really any different to any other record type that can potentially be served what's the difference you've got to trust this you're here you've got to trust to the operators that are signing the zone for you with when you've got a multi-sign that when you're running more designer so i really don't see the need for this okay so so my opinion on this is that of course it's just records like any other and if you serve two different a records that's fine the proposal is not imposing any restrictions on how to publish cds records you may publish inconsistent cbs records if you like the proposal is an instruction to the parent um whether to act in a situation that is known to be inconsistent and i think the first time this goes wrong for a large domain we better want that provision uh i think my concern this victory covenant google uh my concern is similar to marx uh there are probably i think non-uncommon hidden masters that are listed in the ns record uh to facilitate various kinds of internal synchronizations but aren't necessarily exposed to the outside world such domains won't necessarily be able to be reached at every name server whether it's a good idea to list the dead name servers or not is a separate question but i would expect it's not entirely uncommon uh and also of course if the only name server with the private key you know fails and you need to do an emergency role and tolerate the outage you know"
  },
  {
    "startTime": "01:56:00",
    "text": "whatever uh then again uh you can be stuck as as mark indicated so requiring every single one is sounds a little risky uh this needs some thought okay i think that's a fair point um so perhaps the wording could be amended so that it's fine if an m server does not return any cds or cdns key records with a proof of non-existence of course but those that do i believe should be consistent because otherwise how do we address the concern that one provider is accidentally screwing it up schwartz it looks to me like there's a missing time constant here i'm missing what time constant because normally the ttl i guess tells the parent how often it needs to recheck the cds but uh if you checked and the answer is that the cds is in an inconsistent state when do i recheck how often how often do i repeat this check so in the current specification the cds ttl does not impose any schedule on the rechecking it's completely up to the parent what to do and i believe there is no registry i mean there's a bunch to implement the scanning i think seven or eight but i think they all do it daily regardless of ttl so yes it could be done through the ttl but i think it isn't okay that's interesting but uh even given that i don't think that we would want the result of this to be well we'll try again tomorrow uh if you're if you're trying to execute a transition like this uh you know an additional 20 you know checking once every 24 hours it could be a very long time before you get the"
  },
  {
    "startTime": "01:58:01",
    "text": "again uh so i think you you might consider you know putting in some time constants recommending that you check back in five minutes to see if in case you happen to be right in the middle of a transition do you mean checking every whatever five minutes for example in the case you encounter such inconsistency or do you mean in general to quickly catch cds updates uh no i mean specifically in the case of inconsistency okay i understand okay thank you thank you peter for the feedback from the audience um [Music] that yeah i think we're done we're perfectly in time we completed the agenda including time permitted i would like to thank the the presenters for presenting their drafts and their work and i also would like to thank the participants in the room for conservative discussions and feedback we see each other either at the next interim in september or in november in london in person any other final words uh suzanne for those in the room it's great to see you in real life and for those out in the remote out in the remote wherever you are thank you also thank you also for being here and i guess that's a wrap thanks guys i think we talked about it together"
  }
]
