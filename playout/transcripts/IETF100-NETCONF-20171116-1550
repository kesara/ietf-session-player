[
  {
    "startTime": "00:00:40",
    "text": "[Music] "
  },
  {
    "startTime": "00:06:02",
    "text": "you don\u0027t have downloaded it since it\u0027s only for this one but once we system and yeah then we have to go back to this one oh you want just get started okay all right we can get started sorry we had some technical difficulties with the projection this is the net come flocking group session in case you\u0027re in the wrong room this would be a good time to exit we will start with the chair slides so take care of to note well this is the updated version that idea of just published any contributions discussions that you have either in this room or on the mailing list are under covered under this node well before we get started some administrative things there is a red box next to the mic you probably would want to stay within that box if you want to be viewed in meat go on the mic please make sure you state your name slowly and clearly for people who are going to be the note takers so that they can record it there are two sets of blue sheets sign in we need at least a couple of note takers so at this point we are going to ask for volunteers for at least two Nate any note takers and one Java scrubber do we have any volunteer so either one of them all right thanks Lada so loud I will do the jabber you need to know take okay thanks guys one more at least please really okay all right thanks Robert with that the link to the ether pad is there I would encourage anyone in this room to add and update the notes in the ether pad you will find the agenda and the any of these three locations on the website status so RFC 6531 through last "
  },
  {
    "startTime": "00:09:05",
    "text": "call went into is see there was a comment that went into further review with the working group I think we are right now what\u0027s missing on the last set of changes we I hope to turn it around and send it back to bid more to progress it through IEC zero touch receive some last last call comments I can\u0027t is working on the last sort of changes he will talk about them today and the submission is planned for December for IEC consideration same thing with Keystone sweeted ramps a to see last call comments and there are further updates probably needed okay and the last call for that is planned in January the yang push suite of truss I had five of them that we had identified that we send into pre last call three of at least one of them received quite a few comments and we will discuss the status of each one of them after the presentation by the authors the NMD s reader drafts were adopted as well group items and we expect that a quick turnaround on them and send them to the last call in December and finally UDP which is the other chartered item is still work in progress here is the agenda for the chattered items we have 80 minutes to kind of cover so since we have already started a little late we want to see if we can earn some of the time back and yeah the non chartered items this would probably be a speed talk for most because they are all five minutes presentations and that we should be move to the neck at the first set of slides I think we have Robert who\u0027s going to be presenting the nmda suite are people able to hear in the back now okay you might want to pull up the mic thank you "
  },
  {
    "startTime": "00:12:08",
    "text": "is that better yeah okay so I\u0027ll be presenting on these three drafts as a set because they\u0027re all closely related and actually onto the more time on the yang library this one that\u0027s the one that has the more interesting questions so it\u0027s on behalf II in the MBA office but we want the next slide Kent Kent thanks so a quick context context recap to remind you of what it is then a summary of the updates the drafts has not that much to say and discussion of open issues so effectively there\u0027s just a reminder of what these three drafts are trying to cover they are aimed at producing updates to Netcom press conf and yang library to support the nmda data architecture that\u0027s going through network at the moment the aim of these protocol updates are minimal extensions only so we\u0027re trying very hard not to add anything else in beyond what is required to support nmda to make it easier to implement and to get the drafts through more quickly the functionality that we\u0027re adding to both net conf andres cough is equivalent so the aim is the draft should look quite similar I think it\u0027s a little more effort to make the text in the two drafts a closer more closely aligned the yang Library changes are more interesting more extensive and in terms of the draft has currently been described the - oh - we\u0027ve had further discussion beyond that point I\u0027ll also be presenting what is a proposed change to that and again another idea of how to incorporate it to cover schema out better as I said we\u0027re on the addressing you complete quickly and we\u0027re aiming for working with last call before the next ITF so hopefully once these issues are closed and we align the text we should be done so just quick summary what change may be made we\u0027ve clarified the origin metadata encoding and align the text between the two drafts and then effectively this encoding is fairly simple one where you report the origin metadata status for each node but if the state is the same as the parents then you don\u0027t need to report it so the intention is that this should be fairly compact and easy to include we\u0027ve clarified that Tessa venom da so basic to check whether you\u0027re talking to an MDA server is achieved by querying the new ylang-ylang library using the new get data operation on the operational state data store so that effectively validates that the device is supporting the new RPC is also supporting operational date the state data store and it gives you information back that you require so Andy has expressed that he would quite like there to be an explicit capability for this as well but it would actually affect it would be the same as doing this operation so the authors are not quite clear whether there\u0027s any great benefit of doing that whether you actually need an extra capability as well because the pragmatic way of testing for this is to do that operation so that everyone\u0027s got "
  },
  {
    "startTime": "00:15:10",
    "text": "any opinions on that please speak now or Orson the 78th us in rests with restaurant we clarified that the with defaults extension does not apply to operational so the operational datastore had sort of a different way of handling defaults the default values are regarded as being informational use only and that the operational state they still will contain all the default values that are in use by the server so in terms of the net contrast there\u0027s this definition of the data is in use and that effectively replaces the use of this with defaults and then finally we\u0027ve clarified we\u0027ve added the additional query filters based on origin and conflict true that are requested for working group adoption of these drafts so the next steps there is actually that much to do here align the structure and descriptive text between the two protocol drafts this is really just a stylistic thing it\u0027s not changing the content and just really need to make sure that we\u0027ve got the same sections in both drafts call the same things I think the net conf draft is missing a performance section moment generally other than that there\u0027s these two burning issues agree to close on the performance questions being discussed and the structure yon library and then hopefully you\u0027re working with call so on to the first of the two issues and they\u0027re quite closely related really is conformance so the nmda architectural document states that all conventional data stores must have the same schema and that the schema for the operational data store must be a superset of all configuration data stores but data nodes may be emitted through deviation so what we\u0027re trying to say here is that you can apply deviations to modify types and that sort of thing applies to all data stores it applies to all all instances of that but the per data per data store differences you allowed are you\u0027re allowed to remove nodes through deviation and you can do that either to the set of conventional data stores or you can do it to operational and you allow to also have features that are enabled in one data store not another but with the caveat that if a feature is enabled in one other conventional data what all the conventional data stores then it also has to be enabled in operational so we trust strike a balance here between not making this too complicated but actually ensuring that it accurately reflects what the devices will do and a key consideration here is we want to be able to relate between the data that\u0027s in running or intended and the data that\u0027s in operational for conflict true notes so on the next slide I\u0027ve actually just one second Rab guys here and I\u0027m going to switch or display unit I think you should work now "
  },
  {
    "startTime": "00:18:16",
    "text": "No so the pitch you can\u0027t see will demonstrate a diagram to try and illustrate the differences between the scheme and the two data stores so that\u0027s our next layer this one\u0027s my next slide please yeah so this is saying the same thing but you know in a diagrammatic or format so what I\u0027m trying to do is show you that the schema between the data stores here on the left you have the conventional data stores running intended candidate and startup and on the right you have operational and the blue boxes represent the regular schema nodes that are represented by the yang modules and then in orange I\u0027ve done a device level or server level deviation so that\u0027d be a normal deviation where you\u0027re modifying the type or you\u0027re adding any properties adding values into the values space and those deviations apply everywhere so that\u0027s that\u0027s the requirement of nmda then I\u0027ve also shown here in the empty boxes the case where you\u0027re allowed to have data store specific deviations and in this case the only thing you\u0027re allowed to do either through a deviation or in the case of using features is to remove some of those nodes so in the diagram on the left for the intended data store you can see that the last node is is obviously not implemented in that datastore or when that schemer in this or schema and the one on the right operational shows you that middle node is not implemented in the schema for that datastore but the idea is that for all the ones that are here blue and orange you can take the contents of the running and intended datastore and you can check whether those have been existing operational so you can do a sort of different in the to a limited sense next slide so the author\u0027s opinion is that the current performance is correct and it\u0027s required I think we are still debating as to whether there should be more clarification in the draft that may help help with the questions that Andy\u0027s been asking there has been some expression that be nice if it was Hospital a simpler performance in particular using the same schema for all the data stores with no no / datastore features or deviations but we think that there\u0027s reason we use cases for both of these the per datastore features are useful when we were migrating is I think 822 3rc that has a route routing config or route ID feature and when you align the config and state trees together into single "
  },
  {
    "startTime": "00:21:16",
    "text": "tree you want to retain the capability of of the device you have to choose whether or not you allowed to configure the route I\u0027d reach ID or whether it is purely operational data and so allowing that feature keyword to express just in operational I\u0027m not config allows that capability effectively in terms of deviations we think that they\u0027re important and partly as a sort of migration path that when people start implementing it it\u0027s not necessarily the case that everyone from day one will be able to implement all of these operations States for the configuration so we\u0027re expecting that there may be some deviations in that case for an interim period while vendors catch up with the the work that\u0027s required here so that\u0027s what\u0027s expected their longer term it\u0027s desirable if there aren\u0027t in these differences it\u0027s not meant to be a way of actually modifying schema it\u0027s just meant to be a way of I just say it a migration path next slide thank you so all this boils down to what the yang library structure looks like and so we\u0027ve been trying to get a structure right we\u0027ve had several gos and the aim is to cover the required functionality obviously with the per dates or and deviations we also wanted to be as simple as poss and in the most recent revision that\u0027s not in the draft but it\u0027s been discussed on net conf and it\u0027s included here is we want to try and MIT optimize it for the mainline case so in the case where you don\u0027t have any per datastore deviations and the case where you don\u0027t have her datastore features then keep the output very simple so what I\u0027m going to do here is going to go through effectively the iterations of the four versions of the young library structure the last one I\u0027ve included is one that\u0027s been extended to effectively support the sort of scheme amount requirements as well his say-so the follow four sides are present first of all I\u0027ll start with the current yang library so our C seven eight nine five tree diagram just to see where I\u0027m starting from then I present the current yang library Biss so that\u0027s the - OH - version tree diagram that the covers the required functionality but is potentially a bit more complicated then number three I proposed a simplified version of that that we think is better and then finally that simplified version tweak to extend it very slightly to mean that it can be reused for schema mountain so so this is a current yang library version the bits and read that I\u0027m pointing out are the fact that currently it\u0027s called module state so we\u0027re going to rename that 80 or the state path and actually just to make it more generic as the new version yang library won\u0027t just cover modules will also list the data stores one other change that\u0027s sort of coming through in "
  },
  {
    "startTime": "00:24:16",
    "text": "these various things at the moment is that the key for the modules in the current yang library is name and revision and because of that the revision is actually Union because you may not may or may not have a revision for a module so you don\u0027t have a revision for module there\u0027s a special case of using an empty string to represent that and that\u0027s that structures because the same module list has been used to represent both implemented and import only modules so you\u0027ll see from the structures later on that I propose that splitting those two out the features here is just a list of features so in leaf list of features and the deviations again they have the same requirements to have used both namely revision to reference those deviations because the main list is in terms of names and revisions the key and there\u0027s a conformance type that says whether it\u0027s an import only unimplemented module so this is the current version in the best draft young lad abyss and effectively this is the one that we\u0027ve been using for a while the key changes here effectively is that the module key now isn\u0027t is an ID it\u0027s a string rather than being named in revision and that\u0027s because you may require with this structure separate module entries if you have per datastore deviations and / based off features so you effectively have to come up with a separate key and what we propose is the key name would be something along the lines of module name slash revision / the datastore version or something like that so you\u0027d put that into the into the key if um if you needed it in terms of that set of modules as you list it under here the entire set of modules of support on the device and then you have a module set the defect or a list of module sets that the build subsets of those modules into particular schemas so whereas you needed separate schema for each data store you can build one or more module sets using that and then then for each data store you report which module set is associated with that so in the mainline case you\u0027d expect to have one list of modules you define one module set that has all the modules that are being implemented and then the data store would reference that single module set so you\u0027d have entries in the data source for conventional startup candidate running they would all point to the same module set and so it operational if you had deviations that differed between the data stores then points all your modules are listed with multiple entries potentially for the same module you then have two module sets you\u0027d have one for that maybe the conventional data stores and a separate one for operational and then the data "
  },
  {
    "startTime": "00:27:17",
    "text": "stores would actually reference those two separate module sets so this is this covers the requirements but potentially is more complex for clients to deal with so the proposed and simplified version of this and there\u0027s two simplifications they\u0027re happening here one is that I\u0027ve split out the list of modules that you implement against the list of modules that import only so given the server can only implement give a single revision of a module then that list can now be listed by the module name and the revision becomes simplified because it\u0027s optional as it is in them in the in the schema and the data type can again can then be simplified as well and separately from that you have also the import only module list now because you can import multiple revisions of different modules then you have to keep the name revision key they\u0027re effectively but I think actually it feels to me that the the more interesting list of modules that is actually being reported here is the list of implemented modules I think that\u0027s what the device has cares about more than more so than the imported list then again by simplifying the name of the modules list it allows us to have it allows like the deviations just to have a simpler reference back to the name of that deviation and the the other two things we have here is it\u0027s not implemented leaf list and not implemented leaf list under the features so this is to say that for a given module you have you choosing not to implement it in some data stores so the default case you did is you\u0027d expect that not implemented in leafless to be empty if you\u0027ve entered in all data stores and again if you had a feature that was enabled in all those stores then you would again have an empty not implemented latest nada yotaka I have one question is this structure extensible enough in terms of incorporating other data store that may have really significantly different schema than this and MDA data stores so and I think that\u0027s the trade-off effectively in terms of if you have one one list here then then it\u0027s there is a compromise the one change that I think we might consider making on top of this is the moment you\u0027ve got this single not implemented in list I wonder that should be a choice statement that either lists the ones it is implemented in or not implemented inside but actually I think that in the version that you are going to show next very heavily a list of schemas then it would be easily possible to have some schema that is this MN m d8 in R which is what we have here and then "
  },
  {
    "startTime": "00:30:18",
    "text": "have the possibility to define other schemas along that that can be assigned somehow to other data stores that appear in this yes or lists okay speaking of the contributor you\u0027ve been thinking about a way to export from a server what are the full list of supportable yang models I mean by that if you would enable a license if you would have all the line cards etc I don\u0027t specifically want to have it in there but is there a way to group things that you could you know maybe in a different language or somewhere tell this device does support us but you now but assuming you have the right license assuming you do the right thing you would have way more so this version no the next version rats presents then almost certainly yes so if if I going to move the next slide I can\u0027t Watson as a contributor going back to yesterday\u0027s discussion that mud working group I\u0027m wondering if this would be extensible to supporting the revision handling revisions assembler I see here you have the revision as a prepping a key key field and and also you said earlier that you know currently there\u0027s an assumption I can only have a single instance of you know implemented at time both of those are no longer true so wouldn\u0027t want to hold up that this work for that but it\u0027d be nice to see if this was not at least constraining us from having there\u0027s flexibility in the future yes I agree I mean that\u0027s I think we\u0027re not that sort of question wasn\u0027t actually close was it as to whether a server could implement multiple versions were module I think it was questionable as to whether that should be allowed or not well I\u0027ve learned at Eric\u0027s on I think for each Center assembler version we should have one date based version so that would just be an additional relief here in this schema in my view so so this is the final proposed extension to this and so the text in purple here is an augmentation so that wouldn\u0027t be directly in young library that would be an augmentation by skimming out so the the only change that\u0027s been made here is rather than having a single list of modules you actually have separate lists so the top level you define lists of schema and each schema has a set of modules as part of that schema so when Benoit raised this issue a question about whether you wanted to do have different licensing or something like that then certainly you could imagine that you could define multiple schema and in terms of what licenses were turned on you could say this is the set of modules would be represented in that schema effectively and the plan here is "
  },
  {
    "startTime": "00:33:19",
    "text": "that in the case that you have if you have just a normal device that\u0027s only it has one schema so you\u0027re not using schema mount then the expectation here is you did have one well-known schema you might call it like primary or or default schema so you\u0027d have a well defined name here yeah what the default scheme would be and for four simple devices or normal devices you\u0027d expect it to only be one and the expectations - when you get more schema and the one that I was considering here in particular was when schema mount comes along so this effectively should have removed most of the requirement of what\u0027s in the schema mount yang module the one bitter hadn\u0027t closed on was the issue of the references to the you are ends and namespaces so I don\u0027t know that needs to be covered so any comments on whether this is better simpler I would certainly support this last version not only because it supports king amount and it is basically aligned with what I proposed yesterday during the net mod session but I think also it\u0027s it\u0027s really more future proof because you know that any Aang if we have a container then it\u0027s it\u0027s really hard to make it a list if you want to do it so even if we don\u0027t do anything else this list means even if it has just one entry it\u0027s still future proof and we can use it for four different sets of modules due to licenses or for support of other later stores of course for support of scheme amount and so I think this really is a very minor change and shouldn\u0027t cost much to make it okay thank you well I think I think your comment you made earlier bounce if your scheme if one of the dates does like ITRs was very different it might be nice as you say to actually define a set of schema fight ORS and then have referenced that by the datastore any other comments at all nothing from the jibber maybe just mistaken in the t3i again but not in comments that the datastore is under the schema yes correct yes sorry it\u0027s meant to be outside the scheme list okay so no more questions or comments from the room but from a cherished perspective I think current in a milestone is for the three drafts to be in two working group last call December timeframe do we still feel that this is working with law school I think "
  },
  {
    "startTime": "00:36:19",
    "text": "I think that\u0027s reasonable I mean I think this is the issue if you closed on this then I think that\u0027s reasonable okay [Music] I\u0027m at Kent as a contributor presenting the zero touch draft [Music] [Music] okay great and the clicker works as well okay some updates since 99 us first and foremost we reverted back to the device always sending a side of ID certificate to the bootstrap server even when it doesn\u0027t trust the bootstrap server the reason for doing this was it was felt that from a security perspective it was better for the device to give its identity to a potentially bad bootstrap server than it would be for the bootstrap server to give the device Pacific configuration to a potentially spoofed device this is called out in the security consideration section so I\u0027m sure we looked at carefully by sector when they do their review secondly we moved the content from it was previously defined as a hierarchy of data accessible protocol nodes sorry protocol accessible nodes in the tree and that got moved into an RPC called get bootstrap data in that RPC we added a parameter a flag called untrusted connection so this is an ability for the device to alert the bootstrap server that it doesn\u0027t trust with shop server that it is the scenario where it\u0027s blindly accepting the bootstrap servers TLS server certificate and therefore the only way the device would accept or be able to process the data the bootstrap server returned was for that data to either be signed or over for to be unsigned redirect information so we always have this provision that redirect information does not have to be signed we did add a you know per martin\u0027s review a another module called the ITF zero-touch device module it\u0027s really it is a standard space module so it can be implemented but it\u0027s more exemplary and that it provides a mechanism for devices to have a flag called enabled whether or not the zero touch is you know the service is enabled by default it would be enabled and then presumably through the bootstrapping mechanism it would download a configuration that would disable it or delete it and since the "
  },
  {
    "startTime": "00:39:21",
    "text": "default is disabled so it\u0027s not existing it would be disabled and then lastly we added some must and mandatory expressions based on some reviews we received so primary couple ask all comments that we received first was that the redirect information needs to support returning partial certificate chains rather than just a single root certificate to support deployments using public CAS this actually came from an operator operator that as using a public CA not necessarily the one on the screen and so from that public CA they were issued a server certificate and but through the mechanisms they could only put the public CA certificate you know the root certificate and the reason why is because the tools that were they were being used not all tools support the notion of partial certificate chain verification open SSL supports it but not not all tools do so what we need to be able to support is for the server that when it\u0027s returning the redirect information to provide that partial train chain this would be the red box at the top so the trust anchor if you will is not Remy we\u0027re crossing out the binary the binary actually represents an x.509 certificate so it\u0027s becoming a pkcs7 which is actually just containing the partial chain of certificates and then of course in the blue box below is what the when you\u0027re making the TLS connection to the bootstrap server and is doing the handshake the the certificate chain that\u0027s in the blue box would be what the Selah server would return and therefore then the entire chain can be resolved and complete certificate verification can be performed ok so this is the from the last comment the only comment I want to make this is we sang it here we\u0027re using a type called pkcs7 in the next presentation on the keystore draft you\u0027ll see that there\u0027s a suggestion that we might want to define another yang module called something along the lines of IETF crypto types so this is in the line of yang types and in inet types so some called crypto types which might define something called pkcs7 so it might be interesting to do that and then have this draft referenced that type rather than define its own type that does exactly the same thing a second comment which was posted to the list this actually from the social on Sunday night we were talking my co-authors and I were talking to the DHC working group chair and it was there stated that we should be very careful about the must that was in the definition of the dhcp option that it it suggested well okay so the bc option currently is specifying allowable you\u0027ll write contents and error handling that can be improved the proposals remove the must in the URI description so that there\u0027s no implication of server-side processing what we\u0027ve been told is that whenever a GC options go "
  },
  {
    "startTime": "00:42:22",
    "text": "for review they inevitably get kicked back if there\u0027s any implication that there\u0027s a requirement for server-side processing so we wanted to eliminate that to route so we don\u0027t have that problem and then to accommodate that we then added additional language to cover how clients handle errors when processing lists of your eyes so you know and kind of go over the fact that the URI has to be of a certain structure and if it\u0027s not of that structure then you know parts of it can throw away and other parts if they\u0027re there then it has to skip over that entire URI and as if it worked present in the first place so again that full proposal was sent a list we view this and actually even the previous comment as somewhat editorial comments so here we are on the final stretch all last call comments have been addressed on list it seems that a simple draft update is all that\u0027s needed now before being forwarded to I use G for consideration and that\u0027s even if we wanted to adopt this I need to have crypto types module if we wanted to introduce a normative reference to that it\u0027s I mean you know it\u0027s just an adding of an import statement and referencing that type rather than a type that\u0027s embedded into the module itself so that\u0027s effectively an editorial change of sorts I don\u0027t think it would trigger the need for another last call of course it would hold up is G processing until all the references could be resolved but that wouldn\u0027t hold up this working group so with that are there any final questions or comments or concerns so chair coming to you and on this document has gone through multiple rounds of discussions and has cumulatively picked up Ward\u0027s of support to go into last call of course every time there were a few more comments to be addressed do you believe at this point that the draft is ready with all the last set of comments questions that people might have I do like again I just believe that there\u0027s these final editorial level changes that could be made as effectively last call updates I don\u0027t think they\u0027re technical you know they are a little bit technical but not to this threshold of needing to trigger another last call I don\u0027t think but I guess I believe we\u0027re ready with this draft okay and the question is for the room here and does anyone have any objections or moving this draft pass last call at this point all right I will observe that there were no objections in the room okay next set of drafts to the keystore "
  },
  {
    "startTime": "00:45:24",
    "text": "and Friends drafts so one presentation to cover them all recap we had there was a last call it was unsuccessful there was only one review from Jurgen thanks Jurgen it was a doozy and in fact it was calling for a major restructuring of the modules which we\u0027ll get into in just a little bit here in fact I think it was because of his review calling for that matrix restructuring and there weren\u0027t any other reviews anyway so for the keystore draft I\u0027m not gonna go over them all but the most significant update was moving of the keys from a and but our associative certificates from being protocol accessible nodes to groupings the groupings themselves are still defined inside this key store module and then are being used by both the TLS client server and the SSH client server modules and actually this does make sense I believe because now the the you know f0 can pointed out that if you actually look two implementations no one is actually having a key store where they\u0027re storing their private keys for Association TLS so while we\u0027re trying to introduce an abstraction that seemed to make sense of the ideal it actually didn\u0027t map to reality to current implementations so I think this is a good change however in italic here you can see does it mean that we need to rename the module since it actually no longer stores any keys right and the most significant not okay them before if something actually somewhat a rhetorical question right I mean can\u0027t have a model called a key store module there\u0027s no keys in it um and you might be wondering well what should we call it well if you look to the next loaded point the most significant non update is defining reusable crypto types this is what I mentioned in the previous presentation a module for instance called IETF crypto types that would define these would be helpful and also there was a discussion about moving algorithmic identities into another module where would we put those identities I mean we wouldn\u0027t put them into a model called crypto types because that\u0027s I think only supposed to hole have types or type types in them not not identities so then maybe we\u0027d have a more generic model called something like ITF crypto which I have both type two halves and identities so I mean that\u0027s not necessarily answering the question of what the name of this module should be but it is sort of you know suggesting that we should move some of this information out of this module and turn into others for the Association teyla\u0027s client/server drafts the most significant update was the inlining of those keys so it\u0027s using the aforementioned groupings that also it\u0027s been updated to use as type deafs to use type types around leaf rafts to the common key store paths this is one of the comments that were made before the fallout of that of course is that that leaf reps are no longer visible in "
  },
  {
    "startTime": "00:48:24",
    "text": "the tree diagrams so from a readability perspective I think you know you may or may not think this is better but it\u0027s okay and lastly we removed compression algorithms and this is because I Gary or my co-author didn\u0027t feel like there was enough commonality across vendors to support the configuring configuration of compression algorithms and could be added in a future date for the net conf and rest cough client-server drafts the most significant update is that now there are containers in addition to groupings for both the client and server modules so we\u0027ve always had the groupings but Juergen felt it was important to have containers as well I\u0027ve continued to maintain I don\u0027t I mean the server container does make sense I understand but the client container I never really understood because I feel that the application itself would probably embed the client grouping into its own configuration model and that would be the container that\u0027s being used not not a global container but it\u0027s okay because when the model module is being implemented it doesn\u0027t have to you know the conformance type doesn\u0027t have to be its implement it could just be there for its if for the access to the groupings coming we\u0027ll send a quick clarification question the containers inside the groupings no no the containers are top level containers that are using the groupings I\u0027ll finally so so yeah if you look at the yang module you\u0027ll see a container it\u0027s like three lines it just uses grouping with a description statement okay and then lastly there\u0027s been several addition I must and mandatory statements added there\u0027s a testament to the depth of reviews I mean these were actually rather deep low-level you know you had to follow the chain of groupings to try to realize that the they sometimes it wasn\u0027t the case that thing had to be present or not present so it was actually a very good review that was received to that result in the addition of these must and mandatory statements so finally most of the action is in a key store module right now and but it\u0027s having a ripple effect to all the other modules in my view that TLS and SSH the net comp and rest cough all those client-server modules are effectively done the comments aren\u0027t really being there are there with the comments are all in the Kiester module so we do need a focus on that as mentioned there may be a need to do some additional refactoring in the keystore module and and but we also need to be aware of the fact that there are a number of dependencies lining up to these out to actually surprise the other day benoit presented a dependency map and it showed there\u0027s eight or nine other drafts that were depending on mostly the TLS client-server modules so we do need to get these modules down soon and I don\u0027t want to hold up the work for much longer come on Jurgen comments on jabber if a client container is useless we should not have it I know that our map needs client "
  },
  {
    "startTime": "00:51:26",
    "text": "configuration if the conclusion is that generic client container is useless then l map has to define its client container I can go both ways okay so I think we\u0027ll follow up on that on the list any final questions how much your concerns alright thank you [Music] it\u0027s gonna stay low I guess hi I\u0027m Eric white and the berliner on behalf a lot of the people who have been working on the subscription set of drafts for those do you\u0027ve been around for a while you know that there\u0027s a bunch of people who have been working in a design team in the previous iteration when you had their first four drafts people like ballasts and others we also have a bunch of new people who have joined and you\u0027re gonna see some of their drafts at the end of the optional section this is really just to thank the people who\u0027ve been been working on this for a while now this is the second hackathon this is just doing a report out on what the hackathon results were from from last weekend the IHF 99 we did a event where we had two different vendors we tended to interrupt this time we did a second plus telemetry with the I can push hackathon element and we had participation from sockem net comp and the core working group and we ended up winning the best cross working group collaboration I think that\u0027s a pretty pretty cool since then we\u0027ve seen other working groups who are interested I to NSF we saw slides from them who want to go ahead and start using yang push and we also have people here from DTN who are interested in yang push so it\u0027s it seems to be having some traction and the fact that we\u0027re using you know production system to do and feed information to another working group is is pretty cool now there\u0027s a whole bunch of drafts and it\u0027s easy to quail under the set of them so I\u0027m going to try to break it down the ones on the left are the ones I\u0027m going to be talking about here and then I\u0027m going to pass it off to other people who have different drafts and they\u0027ll go through them majority those are are not adopted the first one is adopted and you\u0027ll hear from each of the authors there again I\u0027m speaking to give you an overview of the "
  },
  {
    "startTime": "00:54:27",
    "text": "top five on left the one the other draft which I can\u0027t request did a while ago was a overview draft and if you want to get my idea of what the of the drafts are see the draft net comp subscription and notification overview the goal there is just to give people the easy entry to what they are and it\u0027s intended as a as a as a as an entry point even if it\u0027s not adopted it gets people some access now you\u0027ve seen the slide before in earlier versions this just adds the new adopted drafts and the functions that are supported so this is just really color codes what\u0027s in subscribed notifications what\u0027s in yang push what\u0027s in the different transport drafts and so I\u0027m going to kind of not go in-depth here because we\u0027ll be talking about the deltas in the different slides so subscribe notifications probably the most significant stuff on the mailing list has been the changes the draft driven by some excellent comments for many people in the working group on subscribed notifications there been a number of fairly fairly good changes for example you know we had had things become features that were originally mandatory that\u0027s useful for even people over in the core working group where they are not going to need IOT is not going to be needing that kind of stuff we returned to some of the earlier things that we had earlier drafts we had a return to a stream a string for a stream which were in earlier drafts we returned there we had explicit filter psych typing in our earlier drafts we went back and then we returned that there\u0027s also some excellent comments that were made on 52 77 and use for one-way notifications and we clarified that as well so there\u0027s a whole bunch of of cleanup changes the structure of the document stays fairly consistent but now it is definitely more readable and probably more applicable than before the review now there are three issues that are open with subscribe notifications and we\u0027re trying to look at ways of closing them so what I\u0027m gonna try to do is I don\u0027t know how you would do this like do a hum poll or just sort of raise the hands but at least it address potentials this is not definitive but at least gets an idea on the three issues and if people in the room have a preference for one with the other for the most part I don\u0027t really have a strong preference for the one that are still open can go either way but we just got to figure out what we want to do as a community so we will pull the room out for each of these slides you will we will yes excellent so the first question really is will the transport vary for different receivers of a notification and why would people want to do that well let\u0027s say you are using HTTP 1.1 and you have an upgrade you go to an hv to transport do you have to actually create a new subscription and then manually change everybody over to the new subscription working just add the new transport to that receiver so so there\u0027s there\u0027s value in some cases of having an extra configured an extra transport for for receiver rather than a "
  },
  {
    "startTime": "00:57:28",
    "text": "a transcript for the whole subscription now so the current draft has that as a benefit but it is also possible to not allow transport to vary by subscription this makes for a simpler model but it does have extra complexities on the application side and I did send a preview of this issue out to people on the list there have been one or two e-mails back on it and what we\u0027re trying to do is just get an idea of whether people are fine with the receiver varying the transport varying by receiver or if they want to make sure that it that you have a different subscription and only one transport for receiver any comments vote preference people care I mean okay good votes are good I just wanna make sure people have the comments people are waiting yeah Rob Wilson so my preferences keep it simple I mean if there\u0027s a choice here and there\u0027s no strongly clear benefit that\u0027s pushing for the more complicated one go for the simpler model you always make it complicated again later yeah we\u0027ll do the deeply review and Senator comments on the in the muralist and other issues that have all confirmed and all fixed now we have confirmed a muralist a little bottom nokia I would go with the option of keeping it simple as well awesome I love I love closure if people are there I know that Martin has expressed an issue of having both the transport in the encoding be the same so forego the simpler model and we say configure another common martin jabber says and finally the varying perceiver but the issue is around transport encoding in my opinion they go together and if we follow the simple model they\u0027ll go together so it looks like we go with the no and they cut remove from the current draft to the no draft and we\u0027ll make that update okay so I don\u0027t think we need a hum for this unless anybody really wants to fight the other one the please don\u0027t at least for now all right second issue and this is kind of a general one that we\u0027re the first implementer of and I find this one fascinating the question is what how do you represent Sora\u0027s drf for configured subscription yes were the first people I guess that are going to be doing a leaf ref to the network instance model where they have effectively the identifier for a vrf but it requires that you have schema mount and don\u0027t get me wrong I love schema mount but I can imagine that IOT people don\u0027t want to have to have an import dependency on the schema mount if for an optional leaf ref so so the question is the current draft currently has string and you can populate the same field that would be used as a leaf ref but do we want to actually explicitly make a link to a leaf ref to a model that has schema mount so again this really is an issue which probably should "
  },
  {
    "startTime": "01:00:28",
    "text": "be addressed by with guidance by the people over doing the network instance model but I guess the current draft says why not just populate it with the name that would be same as the leaf ref and I\u0027d like to see if people have opinions on this one as well so row bulletin cisco my option my preference is option 3 so effectively I\u0027ve emailed yours on the alias on this and I think the session is you do use a leaf ref but you put under a feature statement in future statements and it might be good if the network instance model defined a feature for vrf capability effectively in its own mobile separate module so you have dependency on that they feature assignment but not have a dependency on schema mount directly and the network instance molix I think it\u0027s good for all these models that might work VRS or not they should they should have a looser dependency and they currently have and this is a fascinating issue for overall general how those yang get managed because we are have a belief that probably the format of another model is the same and do we have the ability to influence how that other model should be structured or not and at this point I\u0027ve been assuming we don\u0027t but if we can get that chain if we can\u0027t get the change what do we do and if we can get the change it probably we can adopt that that\u0027ll be great if we could adopt that I like all four the fourth option actually in that example we use identity to repeat your name my name is Jill Maui yeah we face a similar situation in actually we have a draft NetID young model we proposed in OBS laboratory we actually sync Mary instance model it\u0027s not a stable enough so we instead of use an air instance actually we actually use identity to represent worth so this is another option we we use and this is another reason why we eventually did string because we didn\u0027t see it being stable for the other side and there is that issue that if we populate the string can we use that and match it to the either the identity or the network instance or maybe some other model from another group string does have the most flexibility with what you\u0027re going to be doing under under comparison to other places which is one of the reasons the current draft does have string so I think my general preference is string because of all those issues because I\u0027m assuming we don\u0027t have the ability to change the network instance model or or determine where it goes I don\u0027t know others are willing to comment on this topic so Rob Wilton did you mention before that all the other models that have this issue using leaf rafts is that no I don\u0027t know if any other that mean the network instance model is fairly new I don\u0027t know thirds that are actually referring to it yet there might be some but I don\u0027t know Kent as a contributor Lou told me yesterday that all the routing area working group is moving to using the network instance I think there "
  },
  {
    "startTime": "01:03:28",
    "text": "might be six or seven different models that are currently having deep breaths awesome Network instances but there is one interesting issue here because the routing group does have a very deep understanding of routing and they have more reason that the there versus a management system that just wants to kind of casually have a chance to mention and match the vrf so so the so the need for the deep embedding in the routing working group will be higher than the general case of people who say oh maybe I want a vrf and then they have the linkage so I think that that what I was thinking about I was talking about the casual users who might want a vrf option and that I have not seen outside so you do mention that the name would be the same as in the leaf ref exactly but what is the validation in the model to make sure that that is indeed the case exactly that\u0027s the problem that\u0027s the issue is that you have to do a validation not via via leaf ref I mean it\u0027s that\u0027s so and the minimum would says that validation that must statement or whatever should exist in the model if it\u0027s a must statement then do you must must you also import the other bottle that has the so it does the same thing I\u0027ve actually almost changed the network instance model to be the string with that reference but the ultimate issues you\u0027re still importing in getting all the baggage which many people aren\u0027t going to want Martin and Jabbar says he agrees with ropes proposal oh I would agree with it too if they do it that means they have to change so Rob Wilson so even if they don\u0027t do it then the next best choice is to define the feature in this module of vrf support required and then make again may the leaf condition on feature and if we make it conditional we still have the import so there will be still some dependency but it won\u0027t be quite the same it\u0027d be compiled time it\u0027ll be compiled type not dependency so I think that you know if nothing else we can take it to them to say you know that there should be some guidance on how to do this and if everybody in the whole world has to have a feature for vrf we\u0027re actually mandating a fairly large dependency across all the models that would have there all of a sudden you have to any leaf ref is going to become a feature and that\u0027s gonna proliferate through through our whole modeling environment I just encourage them to define the feature yes I wish they would time time for this is the perfect place we\u0027re talking your feature capability right now all right the leaf wrap to the network instance yeah all right so I know I said earlier that we\u0027d pull for the various options but I think here we also there\u0027s like options three and four that have come up and we can mailing lists in the end I just want to close well salute whatever but but I do think that the the the issue of having a dependency is is not pretty and it\u0027s really beyond this month well I agree and what you said earlier about you know the routing people are "
  },
  {
    "startTime": "01:06:28",
    "text": "definitely you know they\u0027re models of course be it be comfortable and natural for them to have a network instance model there whereas you know a controller type application maybe wouldn\u0027t necessarily have it there so so it creates you know maybe an unnecessary dependency but so I\u0027m sympathetic to this but I do with I think we probably need to take that to list also you know to try to figure out what what is the impact you know to how many applications are VRS are gonna be everywhere anybody who says oh maybe I want a vrf all of a sudden has a very large set of baggage and it\u0027s not ready but angular Ericsson I would like to separate actually the problem so having or not having a feature as well it\u0027s both for string and refresh and then string or leaf ref is a second question all right well take it to the list I don\u0027t think we have to vote but the point is that we had to close and we can\u0027t just tell we just can\u0027t tell you what to do Hugin so blue bar Martin Robert agreed on your model what to do Jurgen asks what is the cost of the import in running coat \u0026 Marting at first he says agree we should ask for guidance from routing working group and then he says I also don\u0027t understand the dependency issue the fact that there is a dependency the fact is that there is a dependency if you need a vrf blueburger that was actually gonna be my question is is is this something that you\u0027re trying to deal with on the for the server limitation or client limitation this is for the push server but even IOT devices are talking about I yang push we have some people in here from IOT and because the yang model itself has that reference and they might not care to have the vrf the import dependency exists to the schema mount so why don\u0027t in your model make your the ability to reference a ver a feature we were talking about that as an option we still have some of that but even if but even if we do do that we have other people who don\u0027t care still having to do the import yeah I\u0027m not sure if it\u0027s if you have a feature that is yeah I guess you would still do the valley.the import even if it\u0027s just the feature that you\u0027re not implementing yes and that\u0027s the problem so most of the tool sets at compile time required the import even if it\u0027s a dummy and that\u0027s what really got me going and that is a thing that I think yeah but that\u0027s just a compile time on your tooling that\u0027s not going to show up on the actual server the server won\u0027t need to have support your development tools will need to have support there\u0027s a big difference if you make it a feature yes so we\u0027re saying ah no if you make it a feature yeah if we make it a feature then we\u0027re pretty much saying that many things that are our "
  },
  {
    "startTime": "01:09:28",
    "text": "references which should probably be done the same way across across all the models that kind of need VR sometimes so if you\u0027re a building a box and you want to support network instances you\u0027re gonna have support for it and you\u0027re gonna want to be able to reference it so you\u0027ll use the feature that has the verb that points the verb exactly anyway if I you\u0027re building a box that doesn\u0027t use verse you just won\u0027t in Port that\u0027d be what you want to implement that feature but that feature can be the your reference in your model all right we have to play my I totally good let\u0027s keep it this one on the list I think that the the proposal rub has is one that we should probably revisit on the list cuz it does it but it doesn\u0027t really make sense to me that if you are supporting a verb a network instance which is your spring a model sorry module whose sole purpose is ver fanned via size to have to find in it a feature that allows you not to sport verse and via size that seems to be really foolish why would I allow why would we want to allow people to implement something that doesn\u0027t support the main function that\u0027s defined in it this is a side function for you it\u0027s a feature for you that\u0027s obvious feature I\u0027m hoping that I\u0027m just yeah I\u0027m gonna go either way I\u0027m just trying to make sure the guidance is there that people kind of implement feature and do it the same way so I agree you\u0027re breaking ground I think you should you know from from my standpoint it should be a feature in yours and you should set that pattern for for modules that want to reference and then it makes sense that sometimes they reference verbs and sometimes they don\u0027t that\u0027s sounds like a feature in that model I\u0027ll salute it becomes don\u0027t you Martin says you can also augment the vrf leaf from a separate very small module to avoid the import and Jurgen says the import is irrelevant for the code on the IOT device and Martin replies but I agree with the compile-time comments this shouldn\u0027t be a problem right so it sounds like we do feature and we do leaf ref and people are okay well sorry as a Kent as a contributor I what Martin was saying was you didn\u0027t augment so it\u0027ll be another this is another option instead of doing the feature I\u0027ll say that that\u0027s the that\u0027s the proposal that we\u0027re using in the TLS client SSH client server drafts as an augment as a separate model to begin with well yeah we simply aren\u0027t defining support for verse initially and you know with the assumption that future documents will add in the verse or augmentin in the verses as needed I\u0027d rather just keep it in there it\u0027s so it\u0027s a few people are having this problem already today this is something that people are hitting so I\u0027ll just I think it\u0027s just probably best to put it in there and make it easy I know I\u0027m not sure I understand really the proposal by ARP so even if the feature is not "
  },
  {
    "startTime": "01:12:30",
    "text": "supported then the device needs this drink right so it\u0027s it\u0027s like like a choice either or so that you would meet maybe to have some choice already in the original code and then maybe augment this leaf ref into it under a different the kind of a hair law we still can do a leaf ref that the point Rob was making in his earlier thing is that if they changed their model then we wouldn\u0027t have to worry about schema mount import necessarily so they could both they could both be made to work all right we have to really close this because we have a number of other topics but it looks like we\u0027re till the guts of an answer actually I\u0027d like to continue the conversation we have some extra time please I think basically the base issue is that we don\u0027t have an a feature on the import segment itself what that would that would solve that so I like the augment solution if it if that is if use of a feature in the model that sometimes references verbs turns out to be a real problem but I\u0027m not sure I understand the problem because it seems to me that it\u0027s a development time just compilation reference check as opposed to actually having to do any extra code because you don\u0027t really have to device doesn\u0027t really have to implement scheme amount it just has to be able to compile this feature that it\u0027s not using and as perhaps I\u0027m missing something um with which quite you know possible but it doesn\u0027t sound like it\u0027s a survived pile time tooling complexities again they don\u0027t override all right but it\u0027s not a device complexity it\u0027s not like we\u0027re forcing a device or a client to support schema mouths agreed yes all right so it looks like we have the guts of the solution there and we\u0027ll try to write it up now there\u0027s one other topic that we actually declared rough consensus on there a lot of people who chimed in on string or integer and for scripture a subscription ID and well I showed two options here just like all the other issues there\u0027s always four options five options so I showed the two most different ones one issue as an integer to avoid collisions and reuse the other one that had the most I guess difference was using string for configuration integer for updates that wasn\u0027t the you know the only way to do it there was another model that that Robert was talking about where you had a combination between the two where you still had some issues in the end I think the issue has worked fairly well on the list and even though you know not everybody you know had the answer they they wanted the rough consensus out there was to use the the integer one all right so moving on to yank push "
  },
  {
    "startTime": "01:15:30",
    "text": "we have version 11 out there based on the revised data stores cut it catching up we went ahead and integrated that model we also had the explicit foot subtyping that we had in version zero to five we returned that based on based on comments to to get away at rid of the the generalization we had attempted scrub we\u0027ll have examples based upon balises comments we had the recent one change RPC that got inserted and the multi-line cart issue that we were originally putting in there we deferred that to the another draft you\u0027re gonna see draft now so so you\u0027ll get to see more about that one in a future issue now there\u0027s still you know one thing about nmda compliance structures I was talking to Alex and we are willing to go ahead and have a nmda model that we want to be able to compare side to side with the curve model early December at the latest and we\u0027re hoping to get all this get ready for a working group last call so we can at working group last call look at the current model and the NDA and the a model just to do a side-by-side comparison and let the working group kind of see what the choices are and the differences are so that\u0027s where we are with with the yang push many changes are questions on yang boys so you said what the differences are I mean if as if there\u0027s a choice I think we have to come out with an N and D a compliant version of this model Dylan and we\u0027ll have the NMD compliant version there I think it\u0027s not explicit must we were we were there early enough where nmda kind of caught up to us and it is a should but we will do it we\u0027re not saying we are gay okay yeah I mean it we will release it with the nmda combined version and the difference can be made by you just looking at the previous draft essentially yeah but it\u0027s good to actually get all the issues down and then convert because we\u0027ve been doing it in one way the whole time it makes easier continuity alright event notifications updates based on 99 honestly yeah so it\u0027s Tim karaoke I just wanted to make sure because it you we were saying this in net mod before but there will be it they\u0027ll still be a state version of this thing right there\u0027s definitely state it\u0027s no I mean a state version like we do the appendix will there be a state version of the model as part of the Appendix I think that\u0027s what we kind of decided on when we\u0027re trying to bring this out to the communities because I know there\u0027s gonna be there\u0027s gonna be some some modules that some implementations some solutions that will be you know still have the separate state trees for a while and so I just wanted to make sure that\u0027s really trying to do that so yeah I think just to clarify I think that\u0027s true for models that have been released or are implemented I guess in this case the question is are there existing implementations that would be impacted "
  },
  {
    "startTime": "01:18:30",
    "text": "if we were to either duplicate collapse the state part of the model so we\u0027re saying and maybe I want to make sure I understand this is that modules that have not reached RFC status right we won\u0027t have a state version of it no it it\u0027s not that they won\u0027t have it\u0027s just we have some flexibility as to whether or not we will do it it was I think in the guidelines it said that based on market demand or something along these lines so you know if it turns out that this is a module or feature that\u0027s needed to be implemented on non MDA compliant servers then that would be the reason for us to go ahead and doing the test state but I think probably more generally speaking that we should defer you know which probably should do the - state versions in general unless there\u0027s a reach out like an overwhelming reason not to do it I would hope so because it\u0027s I mean you know particularly these ones are getting kind of that getting kind of close right you know that that that you would look at that because we certainly would want to adopt the feature but some sometimes it\u0027s gonna be a little bit longer before you can get into that M and DA because they\u0027ve got to change their entire you know solutions that\u0027s right so I you know my preference would be that that you would create state versions for for a period of time as you said as the marking deals with other once one of the benefits of actually doing the conversion lake because we already had the non NDA version so by having them there we have a choice by going down this path versus doing a shift and then going back and forth so we\u0027ll have something that\u0027s pretty close plus we have implementations out there there\u0027s several open source implementations of this it\u0027s already documented there\u0027s production you know production code so we still have it but at least there\u0027s no process requirement I\u0027m committing here that will have them two models well I appreciate that because it\u0027ll make it easier for adoption in various bodies that haven\u0027t switched over yet right so well commit to having both versions available side by side this is one of the reasons we\u0027re doing the state version is the one that we currently have essentially makes life easy all right so Net Kampf event notifications there\u0027s a big big set of changes on here basically I always cared more about HP - but I had to pick up net comp in order to sort of bring this draft up - up to last call quality so we did a whole bunch of changes moving the text and putting all the normative text up front and only pages 2.4 we remove the JSON we put in the call home text and moved all the examples to the appendix including scrubbing them and learning at how that works for net comp so I think that there\u0027s a lot of changes and a lot of fixes from the last we\u0027ve done our best cut comments are very appreciated and "
  },
  {
    "startTime": "01:21:30",
    "text": "welcomes for sure now I guess that we can take a at least a status check on the email list we had said that if there\u0027s three drafts ODF camera what are you book from Nokia just one minor comment on the little confident notifications I think the examples probably would need to be scrubbed one more time because there was a couple of places where one of them needs to match the latest model with the subscribe notifications and then there was another place where it would still using get which in another draft we say it\u0027s gonna be obsolete it I love hearing that I mean I can\u0027t talk me just a few days ago that there\u0027s a yang lint to scrub these examples and I didn\u0027t know that tool existed so I\u0027ve been playing with it manually and trying to scrub it which is not a great idea but that\u0027s what I had so the fact that you\u0027ve gone and done that look is is comforting and well and will make the changes to send them to the list all right so kind of work check you know I can put out an email early saying we have five drafts you know we want to look at all of them and I I guess I stepped on toes unintentionally or intentionally however you want to do it but I see the money being in the the existing implementations right now people have implementations they want yelling push they went to data store subscription and the custom subscription vent streams as a package because that\u0027s what you see in implementations so it\u0027s where the money is I don\u0027t know if we address it now or later but the is a recommendation for at least from the authors to say this is the right grouping because this is the one that matches to what the market seems to care and have the ability to execute on right now yeah when you Erickson I was part of this group and I think these three are good enough for workgroup a school I am quite happy with them and I definitely see these three SS as the first one so based on this you can make something work now UDP and HTTP and all the others are useful but not as urgent okay so I think there\u0027s a few questions here you know a what is the package of drafts we might take to a working group last call that particular question I\u0027d like to wait until after you\u0027ve presented the full set of drafts the second being is when might we taking the last call assuming this is the set that we took already I think you said that there\u0027s a updates that need to be made on both the event notification scribe notifications and yang Bush and in charge event notifications goes I believe there\u0027s also some normal yeah the examples and the normative text that if so there\u0027s some it\u0027s not ready you\u0027re not ready immediately and yeah I think for the two things and oh and then the last thing is that we\u0027ll wait until the end of app you presented to do this code it\u0027s like it was two weeks we\u0027ll get it in alright so other other issues other topics net Kampf risk half note if there were a couple technical issues and and "
  },
  {
    "startTime": "01:24:31",
    "text": "other things that even though I started the other probably one of my first drafts in the space the HTTP two and the rest comp there are complexities that we found as you guys who have been here a lot no I\u0027ve always asked can we have help with G RPC help with your a PC we never really got the help consistently and there are some issues not really with this draft but with rest comp overall with GRP see in that g RPC is more of a match press confuses get not post your er PC uses post so we never really got the deployment guidance that we were looking for to get that through there\u0027s a new draft or a new problem statement on coop transfer and you\u0027ll hear about that later so the idea of yang push for cuddle also means that we have another left in the general space what\u0027s the actual division the one that really got me going and the one that I think is the most important to me is that the issue with DDoS protection there\u0027s a dependency of when you have a configured subscription to stop DDoS on sending a subscription started message which then has an OK reply you don\u0027t send them you don\u0027t send the the updates until after you receive that ok but do you send the updates with the current notification or for this the the new notification messages draft there\u0027s nothing there that says it if we wait and because nobody\u0027s asking for this draft if we wait for the subscribed our site with the notification messages then we can just mandate everybody using HTTP to uses the new notification message and we don\u0027t have that extra complexity so my my big technical issue is I\u0027d love to solve the DDoS issue and this waiting for the new notification messages is a way to do it no implementations yet and at least the recommendation is is make it concurrent with notification messages for notification messages we have updated the and put the text in there\u0027s been a you know that was new as of a very recent ITF yang data was an issue that came up and they in the net mod working group and we\u0027re hoping to go ahead and sync with what\u0027s going on there there\u0027s an issue with how do you discover what the receiver can support which technically we can do but I think we have to work through and there\u0027s fun dialogs coming in with the core guys and the guys over in I to RS and sockem on doing signatures attestation and other ways of validating it so we\u0027re gonna have some good discussions there so this is going to take a little time to work through in order to get it right and that\u0027s where we are with that draft Martin asks probably regarding the previous slide what Jeff ran spots have people implemented well we have h we have net Kampf in Cisco\u0027s production release XE 16-6 so you can subscribe and that\u0027s what we used in the hackathon demos there are open-source "
  },
  {
    "startTime": "01:27:32",
    "text": "implementations out there that are in Python and and Java guys over a while we had a had a implementation they used the Blas hackathon so there\u0027s a number of implementations out there and you guys who work with Netcom right I think you guys use net confident and the hackathon with IETF 99 yes yes their their net comp as well alright so I think I got through most of the notification messages I think the fun topics are gonna come with signatures and add to station\u0027 in other so so stay tuned after that so that\u0027s that great so we have like three minutes until we get to the next presentation so first I\u0027d like we\u0027d like to get some you know the pulse from the room the working group is in the room the first question I\u0027d like to ask is to get a sense of how many people are in the room are active contributors to the design team that\u0027s been working on these drafts so if you\u0027ve been active on these on the design team can you raise your hand please all right a second question is for the whole room how many people have read this these drafts that the three drafts so we\u0027re talking about and that includes the design team so everyone okay good that\u0027s a good number thank you and and then finally assuming that these issues are resolved how many people believe that these drafts would be ready to go to working group last call it\u0027s also a good number okay good and what timeline what do you think you\u0027d be ready we did all the changes in there the ones that are still open give us two weeks and of the issues that we discussed the course all don\u0027t have to be other the resolution that we confirmed on the list yes thank you [Music] next up I think Walker good no I guess okay thank you Oh this topic is about as a distributor data collection next please yeah firstly is here we extinct the net confer for distributed collection and their high Wang draft adopter as with toolpaths where is for multi streamer or Geneva\u0027s the other is for UDP Papa channel so as we discussed on the deal on the design team meeting and we we think it should "
  },
  {
    "startTime": "01:30:35",
    "text": "be splitted to draft so the one is a keeper UDP Papa channel the other one is for multi stream or donita\u0027s the firstly we is udp-based publication and channel for streaming telemetry why you DB beasts the publishing channel firstly the young push separated the mini mentor and a control for our subscription from the transporters that is used to actually stream and the deliver the data and country for young push already mentioned as a existing transport including net comes the rest come for our TV pistol and there are some point we should contain faster it each collector will suffer a lot of TCP connections from many line cars equipped on different devices the second is no connection state needs to be maintained UDP encapsulation can be easily implemented by hardware which we are further improve the problem and because our lightweight you\u0027d be encapsulation high-frequency and better transit performance can be achieved which is important for student elementary and here the solutions at the left the diagram were we the solution will be two main parts as a balloon is for the message earlier and as the upper one is for the content layer for the message earlier there\u0027s really a faster faster in UDP and the same for the security for security TT RS and the message header for the TT RS is where providers are reusable 60 and also resisting function UDP and a for the message header will keep the main important information and before TC writing the notification include for the encoding method like GPB Cabo chasing XML and as a message telling tidyrank timestamp and sequence number may be fragmentation foster fragmentation maybe keep opening issue maybe we can discuss in future and maybe some other option for the 18 same pretty and the same notification message the new message is young push notification the data itself may be included another finishing header the nudity header is defined in the other draft which will occur just now "
  },
  {
    "startTime": "01:33:35",
    "text": "mentioned for the net car for a notification message and then encoded with the content then the next traffic is for the multi stream or the nitrous here for a 4-3 we will begin with the to use kiss the use kiss where is for the device which designed as its tree beautiful main board and ammachi like hard-line Carla if multi-line kinda or all the data as we know all the data may be generated from the line cards and if we collect our data from Lancaster\u0027s they are aggregate from to the with the main border the main border may be easily become a bottleneck so in this case maybe let\u0027s every line card can send as the senator young pushed it on the screen directly to the collector saying that performance may be become better and the second I use kiss is a out EDT collection like is a like the Cooper I think about the maximum for Cooper I mentioned for possible mechanism here for every out in know that we are send us a message senator theta to the polar rotor then assemble the teachers in senator collector but in the traditional device like a rotor switch if we assemble a teacher on the quadrotor maybe the CPU were too high because the rotor itself is a focus on the is business and maybe for protocol so here if we if we send the data directly from the node to the character may be better here is the solution suggested maybe though the solution is not the scooper for this chapter just give a suggestion here in this diagram they have many two-parter where is a force collector and as aim for the publisher for the publisher here argue to note some node for the math node here firstly is no the some node we already stat was a subsequent server on the master zing is a collector we\u0027re knitting with the 16 server the and the corrector subscriber send a subscriber message to the supreme server to establish the subscription and the supreme server we are decided whether we ascend ascend of the collected teacher distributary all aggregate to master if the if it a desire decided to say sanctification separated at this "
  },
  {
    "startTime": "01:36:36",
    "text": "tributary sends a note we are act as a publisher agent and assigned as an affinity rectory to the receiver here is the issues being worked at verse 3 for subscribing decomposition keep track of resource and the associated publisher make decision and the composing the global solution into a multi multiple components substitutions and provisioning composition compose the component and infusion into an substitution measurements may be we either add some recorder or some Magnum the notification related to the substrate decomposition and a component subscription and the Zen notifications and on substitution state changes each component subscription maintains its own substrate state and its response responsible for sending is owen om notifications they may be summer as a potential is really like a synchronization and like security maybe some others next step is because this is the first zero washing so we encourage your comments and a suggest sense and should the workgroup adopts this draft okay sorry there\u0027s two presentations here and their first presentation it\u0027s already a chartered working group item i just want to we don\u0027t have a milestone for it yet what is the plans for when we would be able to move that document into working group let\u0027s go for the first one UDP channel for the first one the transport layer he like correct but when might will even be able to move that document into working group last call what is your expectation sorry we don\u0027t currently have a milestone for that document yeah how long do you think it\u0027ll taken to you\u0027ll be ready for the for the first for the first document yes maybe I think we were for the master maybe we not to confirm now right okay and then for the second document this is not a charter working document item just as you mentioned also has not been mentioned it hasn\u0027t been posted to the list there\u0027s the no discussion on the list just yet so I think it would be premature to ask for a working group adoption at this point we need to have some discussion on list first yes discussing first there this is that it\u0027s time just just fun brief remark actually the mother of swim originators was originally actually in often containing the other draft and also in the notification headers rapper also allusions to that so busy as part of so but it was pulled out as it was very separate more general about general ratio so by assuming we have not discussed this period an individual draft the issue has been there actually "
  },
  {
    "startTime": "01:39:36",
    "text": "for for some time [Music] thank you okay I think yeah [Music] [Music] okay hi i\u0027m saverio from depot and here to talk later about this problem statement of the generalized automation framework so currently we have some related document three related document here Windsor notification.gem Porsche and the Smart Filters based on those already existing documents we are trying to expand that the objective is to make it more generalized from the existing trigger then son notification to a concept event condition and action so the event could pretty much come from existing trigger from the yunkish and the condition will cover some logic expression evaluated based on the data store state and the action will go beyond the current simple notification and the cover of more actions so such kind of event condition action check play the Royal fit into a policy framework so to allow you to specify such kind of policy rules and the generalized action we\u0027ll cover more beyond that simple notification which we have today we also try to do the network configuration schedule IPC and even Lincoln link some sub trees in the datastore so the because this such kind of structure will fit in the framework so in me policy framework so then that we can utilize some polish capabilities so decompose the condition into smaller conditions and do the same for the action such kind of remote work brings "
  },
  {
    "startTime": "01:42:37",
    "text": "us some benefit so we can have more responsible Network we be more scalable and my vision and so that will have more automation so next time I will try to get feedback from the working group and we can start working on some models thank you we\u0027re just at this point going oh sorry yeah please yes comments Frank Shahar sorry I haven\u0027t yet read this draft but I I feel slice I found that you mentioned it a new end condition action practical motor for the policy design so I have a question for you but I\u0027m wondering if you know that there are existing work of the policy design in super working group and in I to myself working group will use you see a model so I I want to know what\u0027s your motor relation with those existing working other working groups so I think we should you know we need to to be aligned to other or there are some relation between then we need to be clarified yes that\u0027s one possibility we have evaluated the super policy framework and that\u0027s my option but depend on how that models going now because I went probably already know this that working group is closed for now so we may or may not use that framework in addition a lot of super policy work is still inherited and extended in the item is a working group we are work where were to tear the information model about ECMO during our capability information motor so you please to take a look at that draft and I think we can sit together and discuss what is the relation between then maybe can you know we can at least yes I think we can now contribute to each other or something yeah you can take a look definitely okay great thank you and it\u0027s Henker coop transfer [Music] "
  },
  {
    "startTime": "01:45:38",
    "text": "the next right so the yeah I\u0027m trying to speak with hampi have hanging up at us and offering me for many witnesses sighs okay so we\u0027re trying to do the telemetry model and over not nikon\u0027s but we have another option here we do the common so he called it a cow cow it\u0027s pretty much core working groups that equivalent vessel and the purpose here is that of trying to hire the more efficient transport so that trying to make it in the most simple constrained environment so they can have binary young transfer and we use that to certain device so the subscription will go through the cope and call me and then the configuration we also used what what we have here the call home and we trying to try to utilize some features and to do the post wrapping capability here and the problem stating that we have this is all sort of problems problem statement to try after we we cannot to work on the list so that we can complete the solution currently the surgeon not complete the code they have some kind of notification but the mechanism is not at the feature as as much as we have here so the we need to expand the hardest thing important in today\u0027s environment so we have identified some curves i need to to some mediant for future expression and we need to do the cop car home and we also need to discover the mackanin for co-pays the solution so the work also be introduced to the core working group so we have collaboration there could imagine that it question first is is this solution "
  },
  {
    "startTime": "01:48:41",
    "text": "only needed for telemetry or is there a more general use of maybe binary encoding in general yes and you said though this work was also being presented to the core working group is right so is it something for the neck off or are you just bringing it to our attention or you suggesting that there\u0027s a need for adoption down the road or oh and so that I think that the tank was talking about having the adoption in that working group and that\u0027s what they talked about there but it\u0027s just a problem statement right now in the end the question of some of the mechanisms that are there being adopted in other places is a is a you know an interesting question that I think the working groups will have to work through okay yeah I was actually thinking that it probably should be in the core working group I suppose to this one so but yeah good thank you and now it\u0027s Alex the last two presentations and you have one extra minute [Music] actually the other one first this one yes thank you okay so that\u0027s that okay so basically this is the problem statement for Smart Filters for push updates so this is busy another extension it builds on top of the yank push subscription drive since above it was listed also in the in the earlier overview um basically to give a little bit background and purpose of this young push filters a lot to allow our clients to select which nodes to subscribe to however many monitoring applications need to have actually a little bit more they would basically they are interested also particularly in values and for instance filtering things based on whether things are within the normal operating range outside of that range shape for instance so points the question so just has a critical range even reached its utilization above a certain percentage in support those are things that you would not be able to provide on the device but pitted by an application at that process the stream filter on values is currently not part is currently not covered in yank push reason for this word should not include not needlessly stretched implementation and complexity and also in many cases you would need to have additional things on top rather than just value based filtering and so forth and anyway so therefore basically this is this is currently not addressed and this smart filter problem statement here is a proposal to address this gap and essentially transition also this way the "
  },
  {
    "startTime": "01:51:44",
    "text": "simple updates to actually really events I think that indicate condition set of interest in terms of how it fits in with the other drafts this is just very briefly the overview well we went through these essentially the smart filters are basically operating missing on top of there or in conjunction with with yank push and they also feed into any automations of all that you want to pursue in terms of what would be included in this modern stateful filters basically clearly filters based on values so basic metric filters comparators those sort of items and also certain selected saiful filters such as special crossings in recent high watermarks where the objects are generally in and out of filter conditions and so those things are basically yeah pretty much boilerplate or basically tables take things that you need to have for service assurance in addition and busy the proposal here is to focus on those and there are of course even smarter filters possible and one thing concerns aggregates forming aggregates over time instance things such as for the simple statistical aggregation computation of maximum averages and so forth we thought that you have to keep it outside the scope however there\u0027s been some feedback to maybe also incorporate that things that we view as he\u0027d be well not within this problem statement are more complex things such as aggregates across objects full ammonification orbit forming the the equivalent of an expression and event nib in the yang space those things basically would go beyond what we are proposing here so therefore busy what is in what is within the proposed scope are basically first of all ready refined unchanged update semantics there was also another reason why we did not want to include it with the plane and young push basically we need to have a distinction whether an object is emitted from an update because an object was sponsored create it or delete it versus that the object is still they had only fill in and out of range so busy for this we will need to have additional update notifications then for the state phone filters basis proposals here to have a few selected stateful filters the one thing basic tool enable special crossing alerts including bitties with the counter here pressure hold that is one thing possibly the you have Multi multi level factor of crossing alerts there are certain requirements from operators for this and then the third one includes recent high watermarks we would basically keep the maximum your baby up update those things but you have busy some exploration so that the equivalent is that of a graphic equalizer that you can use also for performance type of applications when "
  },
  {
    "startTime": "01:54:46",
    "text": "again outside of the scope the more complex things that were mentioned earlier so therefore basically we have review this is a logical extension it builds on top of the existing in push notification work it\u0027s an enabler for assurance application and the building got for Network automation and basically we wanted to assess here the interest of the working group whether it makes sense to define a solution for this problem here well angel Erickson when we go into thresholds and high-water marks I see a strong connection between this and the alarm yang module that we pushed into C camp I think it would be very natural that safe utilization goes over 99% I want an alarm about that quit so I think we should connect them up to that Martin asks on jabber do you have any solution proposal it would be interesting to see do you have any solution proposal well this is busy what we well we have a solution proposed on the back of our head not not in not inside the draft if there\u0027s interest to take it up we believe actually that this is busy in natural extension on top of yank push so basically where we have is it currently filter filter constructs we would basically add a smart filter augmentation into the Eng push update streams this is also the reason why we were thinking of net count as the working group maybe seachem person is another option but this is busy there\u0027s a national natural progression of things here yeah every time you know we people talk to customers they say I\u0027m sorry Eric wait they say well do you have threshold filters for CPU and temperature so I mean it pretty much you have the objects need the filters and is a direct extension for yank push ok good I would actually like to get pull from the room regarding that level of interests in the strata well I don\u0027t know I mean we\u0027ve without a solution we I don\u0027t we can we can\u0027t bring it to adoption and not to mention we have a number of drafts that are in queue that we probably should focus on first but a question at the mic yeah could I just ask a quick question so it\u0027s going through and I was reading the draft and and I just want to make sure is is this is this draft actually tied to the to the event you know the event condition action architecture because you mentioned you know this is something that could be extended off a push I want to make sure that that you\u0027re and that you\u0027re thinking making sure that that you\u0027re saying well it\u0027s push or is it that architecture I think though it is the separate it\u0027s independent of that basically the automation maybe this is also in the automation framework graph as if they want to use this as one source of the events that might then basically be be fed into an event condition action but it\u0027s not really a tire architectural processes just want "
  },
  {
    "startTime": "01:57:49",
    "text": "to make sure that\u0027s like I mean as a contributor I do think I view this is a natural progression to the yang push filtering mechanisms Smart Filters better filters more and better okay um back to the interest level from the room if this is interesting work can you please raise your hand you think okay that\u0027s a very good number okay thank you and you have one more presentation by freaking a question if you tried to blindly unify the alarm it gets right which is yang if I the alarm it blindly assume it works no I have not on that but also I don\u0027t think it wouldn\u0027t have impact to this because the I mean it may be another way to obtain that data but it wouldn\u0027t be via the yang push mechanism like like for instance if you\u0027re subscribing to an XPath in the data store understood it\u0027s just a filter here right so yes there are a couple of thing that our Yang\u0027s mess but you know if you do like those filters these are always the same time type of conditions right involve a threshold below a threshold it\u0027s a resistance Uttara okay so I think when you\u0027re looking to the solution that might be something to consider okay okay all right thanks and were you have one minute one more yeah so I\u0027ll you bring it up I think this is actually a draft that there\u0027s nothing to do actually it\u0027s on a different domain this basic concerns discrepancy detection between nmda data stores and basically that\u0027s a draft engine and Jeff and this baby something whoops this basically addresses something that we feel basic it will be will be useful as an Indy a take takes holes to to be able to to easier for instance troubleshoot certain conditions see and so forth basically the issue is that within nmda the same data can be represented across different data source the question is what happens if there are unexpected different discrepancies that persist between our big value spots in the operational and intended and okay I don\u0027t have much time but basically the death of the chase therefore basically the draft defines an RPC data model within are preceded it allows to compare and in the a data source and essentially allow this way basic to see if there are any discrepancies for it\u0027s not any any unexpected discrepancies between points and intended and operational and that something is either not propagating or maybe something that was intended is suddenly learned or whatever busy those sorts of issues are which would "
  },
  {
    "startTime": "02:00:49",
    "text": "otherwise be hard harder to detect and would require a basic approach of the entire data sort of make the compares that the client is opposed to be able to basically ask the server to do that as at the core of the draft is basically a very simple module defining basic this nmda compare operation we\u0027re busy spend exactly specify a source a target a filter specification as well as a dampening period which basically tells you how long the points in that discrepancy would need to persist in order to be had to be reported the last one is made optional this is a traditional is a feature based on feedback from from Rob and anyway so this is basically whether is it a zero zero graph right now now we believe this is straightforward and useful addition we would like to request feedback from the working group and also yeah we\u0027ll be seeking working group adoption robots in Cisco so I\u0027ve reviewed this draft invite provided some feedback on to it I think it\u0027s useful work I think it\u0027s something that various we\u0027ve been asking for this or to be able to do a diff comparison between the different date stores and I think this this was solution is on the right guidelines so thanks so I think it\u0027s also fair to poll for interest in the room again too early for an option call we don\u0027t have to wait until yeah 101 to do adoption we can do between so if this is interesting work you\u0027d like to see progress within this working group can you please raise your hand it\u0027s a good number okay thank you and with that I think we are done with one one last very last comment interest in the work for sure but you guys are aware NMDA is being done in network right if this is an extension to an MVA then they should be done together good point yes for the net column restaurant working within DA\u0027s done here these are perfect stanchion so this is the Rockies yeah all right blue sheets if anyone has not signed the blue sheet please do it is one still floating around in the room and there\u0027s one up here and we\u0027ll see you in London thank you [Music] "
  }
]