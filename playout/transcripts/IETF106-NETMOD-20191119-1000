[
  {
    "startTime": "00:00:13",
    "text": "[Music] hi welcome to net mod our first session we have two sessions today I\u0027m Lou burger we have Kent Watson who is remote at some point Joel the other co-chair I think he\u0027s going to show up hopefully he does Mahesh is gonna be sitting here with me helping out with the session and with jabber although I\u0027m on jabber as well as usual we\u0027re using etherpad the URL I think is here no it\u0027s the URL is here please do join in and help with our collaborative note-taking it\u0027s very helpful to do that and it\u0027s also a good opportunity for anyone who speaks at the mic to make sure that their name is perfectly captured and their comments are accurately captured so please jump on this URL you can also find it off the tools page or off the data tracker page or off our agenda any number of places this is the IETF which means we have some rules governing what we do here and what said here and what makes it into our minutes and becomes part of our process basically anything you say in this room during this session is part of our permanent record we are using YouTube for video as well as recording audio and so please be aware of that the blue sheets are going around as I mentioned myself and Mahesh should be in a non je ne do see something show up in jabber and we don\u0027t take note of it feel free to come to the mic and to relay a question as you see it the agenda has changed a little bit I\u0027ll get into those details in a moment so we have two sessions the really important thing to note is we have a room change so this afternoon we are not in here why we\u0027re down here I don\u0027t know but just be aware on to document status so since the last meeting we do have one RFC I have to say when I saw that this was since the last meeting I thought there was some that there was a mistake because I felt like we were done with this a long time ago but it does sometimes take a long time from when we finished something in the working group do we actually have the RFC thanks to all who contributed to "
  },
  {
    "startTime": "00:03:16",
    "text": "this it was an important piece of work of course I\u0027m biased because I think it\u0027s useful we have two documents that have been submitted for publication I we I thought we were gonna have a update on that from the author\u0027s I think they decided they\u0027re just gonna speak at the mic and give us a brief update now Adrian I thought had volunteered to do it but I don\u0027t see him in the room Adrian are you in the room you\u0027re not Kent do you want to say anything about artwork folding yeah hi this quickly it\u0027s been in the IC for a little bit and the main thing right now is that Ben could review it again you want to do in sure it was actually about it process compliance Krypton would work on multiple platforms great thank you I\u0027m gonna drop you off come back and Q if you want they did it for me so we also have yang get extension that\u0027s been that\u0027s going through processing I don\u0027t think is anything really that interesting to talk about the post last call we have a few documents the first is module tags that actually left the working group and then came back to the working group that was because it didn\u0027t have the State appendix the nmda related State appendix in it I believe that\u0027s been produced I should know that answer I am at least the contributor may be a co-author on that but really Chris Hobbs is driving that work but it came back to the working group it\u0027s been updated we expect to have a second last call on it next week we have a couple of documents that have been through last call they did get some comments that was a little bit of an extended last call we\u0027re gonna hear from Rob Wilton on that in a moment I clicked the wrong button let\u0027s see if this goes the right way okay one more document that\u0027s post last call is factory default we\u0027ve had I think some discussion on that the last call is was ended on the 15th I don\u0027t believe the yang doctor review came in it was a sign but we\u0027re still waiting on that there is an IPR Poland and Pross going on we\u0027re missing one response at least there was when I wrote that on the slide and it that will of course block submission to the is G but we expect that to progress nicely and Kent who\u0027s the Shepherd is gonna wait until he gets the final revision before doing his his write-up "
  },
  {
    "startTime": "00:06:21",
    "text": "not on the agenda that\u0027s interesting geolocation was not on the agenda when we put these slides together it\u0027s actually on the agenda now so that\u0027s incorrect but we have a couple of other slides a couple other documents that are not on the agenda nmda diff and the 69 91 Biss is there anyone who would lie - OH alex is pointing out I think that he\u0027s now on the agenda and so the only one we have is 69 91 Biss I don\u0027t see the author on in here so I don\u0027t think we have anyone who can talk to it one of the entertaining things about this fist is I happen to be because of one of the geo document I happen to be looking at the reference for our definition of date and time and I realized that we\u0027re pointing to a definition that is several years out of date that came I believe it was from I don\u0027t remember who the standard was but if we reference in RFC that references and maybe an ITU standard that\u0027s literally three revisions out of date Joel made the nice stroke about you know at least we have stable time which is you know it\u0027s good to have a stable time but we probably should figure out how to update the reference as we move forward I\u0027ll mention that to your again the agenda is is pretty tight it says the first item came up we\u0027ve added this schema comparison document that\u0027s noteworthy because is the last sort of building block that\u0027s necessary to satisfy all the requirements from the revision handling and the revision design team so they have a now have a complete set of documents covering the requirements we\u0027re going to spend a lot of time on that because that\u0027s a really important work item for the working group our second session we\u0027ve also managed to fill up interesting font conversion here the the stuff at the top I guess must be more importantly splits larger here so we have updates on a couple of working group documents and then a couple of other documents which are individual contribution we\u0027ve seen maybe it may have seen the list discussion related to ECA and the two different documents I think the authors have been working hard to figure out how they combine their work and I suspect it\u0027s a it\u0027s a merged contribution at this point but we\u0027ll hear that in the afternoon we have one liaison it\u0027s actually a communication that came in since the ITU that sorry since the IETF doesn\u0027t have a formal liaison relationship with that see let\u0027s I don\u0027t believe we have one and it\u0027s really just to be aware of what work "
  },
  {
    "startTime": "00:09:21",
    "text": "they\u0027re doing so if you\u0027re interested please take a look at the link it is posted on the list it you won\u0027t find it in the liaison listing because we don\u0027t have that formal relationship so this is just a communication it it\u0027s interesting do take a look and with that we\u0027re gonna go over to rob okay so I try to give a very quick update on these two models they post working group in law school so first apologies I\u0027ve been quite slow presses in the working room last call comments I was intended to over the last couple weeks but I wanted to get the final versioning draft out to try and get that work to progress a steady pace so that\u0027s why the sub interest draft updates haven\u0027t yet been finished the in taste extensions draft I\u0027ve applied most as well asked or comments there are a few open issues waiting for confirmation from the people who submitted the issues I\u0027m going to cover some of those today just in case anyone doesn\u0027t give any feedback but I\u0027m not sure I believe you covered on this anyway and subbing taste draft are still in progress and so hopefully they should be completed fairly soon so to the next couple of weeks or four weeks a month and then we\u0027ll get those done so into the issues the one-on-ones as Romanies do we renamed the carry delay function so this is a feature that delays normally a hardware state change so that you can allow some other protection equipment to kick in or in the case in his face is coming up you can allow to stay relies before you start running traffic over that so this carry delays the name that we\u0027ve used within Cisco the suggestion may be to change that name couple of possibilities is link flap suppression or state flap suppression I don\u0027t know if anyone has any comments on this or not so you nobody stands up I will continue try to progress on the list otherwise I am I\u0027ll either keep the same or change it there\u0027s no no he doesn\u0027t reek as particularly the the next issue is there was a proposal to add an in disguise overflow counter so this would be a subset of the in discards counter so I\u0027ll add that one and an in discards unknown end caps again that was discussed before so I\u0027ll add both of those counters in the definitions of what those would be I\u0027ve been sent to the list there was also discussion about whether to add in packets and out packets counters so the way that the counters defined today is this split between unicast multicast and broadcast and the expectation is that packets that are okay well-formed and not dropped "
  },
  {
    "startTime": "00:12:21",
    "text": "they fit into one of those three buckets the friends faces that don\u0027t have that split between those three it\u0027s not quite so useful it would have been nicer if they defined and in packets and out packets counters and then the broadcast and multicast were subsets of those but who said that I don\u0027t think this documents the right place to add those and I think further discussion will be required so the plan is not to do that now and if you go into a future revision or ITF interfaces if it was required many comments a Sealand of Cisco Systems isn\u0027t it if if you could if the client could derive those from a you know just adding them together yes isn\u0027t it isn\u0027t it that we don\u0027t add a counter that just if it\u0027s not but if it\u0027s different it depends on the definition whether they\u0027re needed or not I think the case where this came up the concern was for some interface types you may not know this split to unicast multicast broadcast so what which which bucket you put those in because it\u0027s gonna be misleading and I think it was maybe the policy models they wanted to refer back to this counter possibly and their concern was that doing the maths in XPath becomes sort of convoluted whereas you had a single counter that would be easier so there was justification as to why this could be used what I do agree that logically is equivalent to adding these three together I think the last issue and this came up in the last working group last call last ITF sorry and that was to change the name of l2 him to you that wasn\u0027t like surname and that\u0027s changed max frame size so effectively this is reporting the maximum size of frames that you can send and receive over the another physical interface but it could be a sub interface as well before the definition had a tweak to be fairly close in line to eight e 2.3 there now some flexibilities to this size changing by four eight bytes to count for fact your video tags in the packet in the end I decided to actually take that out and make it more generic another change is to include the SES bytes that again we\u0027re excluded to try and make it more aligned with the some other counters that we have and then the last change was to increase its size from you in 16 to you at 32 with the aim of accommodating the Linux loop back in to you which is 65536 so that\u0027s the new text I will get a Loutre II afterwards because he previously mentioned an interest in this yes so again that\u0027s on the list and if I don\u0027t hear back I assume that what I\u0027m proposing is fine and that\u0027s last slide on these any questions Thanks I was just reading okay thanks okay except via blog "
  },
  {
    "startTime": "00:15:26",
    "text": "[Music] oh hello blush thank y\u0027all presenting instance data format which is next slide it\u0027s getting quite stable and I would like by now first of all I\u0027m sorry for publishing the draft just yes before yesterday I think yeah concept is still the same we want a document data that will be available offline server capabilities reloading data a lot of use cases XML JSON encoding multiple new ones can be added add metadata for data set and content it will be similar to what you would get from a racket reply or get operation reply next please so what has changed since the last IDF first of all we had this entity tag and last modified time stamp which was from rest off and number of people thought it useful not other number of people thought that it\u0027s uncertain how it should be used it\u0027s tighter and quite rested on specific after the last IDF there was a email discussion on this and it was decided to remove it maybe later reintroduce if if there\u0027s a real need then just very lately and these suggestions I added this yet yeah instance data version so if the specification ever needs the wrapping a new version then we\u0027ll know that we move go from get version 1 to 2 or 1.1 just that this specification is the first version of the instance data format then there was some discussion about the full inline schema specification method it is now made optional and just the days yesterday maybe and Andy at least came up that he thinks it\u0027s too flexible it\u0027s he wants just yank library but as a base but earlier I think to two IDs earlier there was a discussion on this way it was decided that we want a very flexible solution so it\u0027s not just yank library module format that can be used but any other format as well anyway now it\u0027s a "
  },
  {
    "startTime": "00:18:29",
    "text": "optional based on a feature then I added the based on the last idea of discussions the simplified the online method where for each module you just have to specify a single string like in this example and we will come to an open issue at the end about the exact format of this string next please so and that Martin\u0027s proposal I added the wrapping container about around content schema so to grab the whole choice three methods in the container so cleanly separate them and rename some of the options the blue parts are the ones that changed simplified in line was added in line module and in my schema I think they were just called module and schema before and then we have a feature for in line itself next please there was a yang dr. review I updated the draft accordingly hopefully I didn\u0027t forget anything my English needs some updates as well security considerations so this is not a normal yang module because this is not intended to be accessed online basically it doesn\u0027t have any way it doesn\u0027t involve any way of modifying the server or the publishers behavior it\u0027s pure read-only you know in a sense and it\u0027s of its for files so file handling security should apply then this dot Ian for schema naming I\u0027ll come back to this in open issues and there was statement that I said that the yang what you revision for content defining yang modules should be mandatory because otherwise it modules can change between revisions greatly but it was commented that some modules might not have revision date at all so in that case it\u0027s we don\u0027t we can\u0027t have a revision date that was added next please and this is the example of how we look after all these changes so you see that the geared version one which is always one for this RFC M like we have it 1.1 fixed 1.1 4 7 yang yang 7950 when we have here the simplified methods and next one please this is next maybe this yep we have one major open issue raised by Andy and Martin that in "
  },
  {
    "startTime": "00:21:30",
    "text": "the draft I use the for a simplified inline method I use the format that is very similar to how we named yang modules for example IETF young library the module name the revision date if it exists but I think it would be existing 99% of the case and and dot yang they said that we should not have dot the Angora dot Ian because it\u0027s not a file for me this is a simple short method but I could leave we could as now alternative just say ITF young library date and then not dot dot nothing to me that\u0027s somewhat unusual but yeah it\u0027s can be used or we could use a more complex solution where we have a list with two two leaves or module name and revision I don\u0027t want that because I was specifically asked to have a very simple and short solution so it\u0027s either one or two I can live with two if and the end and the Martin really wants that and I think I covered all the other open issues oh please and I hope it can go to work go past call now robot since this case just a quick comment on the module naming so with the young version work you\u0027re sorta better name young modules using the revision label as well I think I sent a comment to the list yesterday there might be nice to have slightly more flexible so it\u0027s not tighter has to be the date you could for example have a Yank Cimber and there\u0027s that or anything else an extra first of all I agree with you second I would rather make that abyss and not wait for the food revealed provisioning part dependent on this yes we can\u0027t really I don\u0027t know can if I can put in something later revision label may be used instead but that\u0027s rather uncertain at this point I don\u0027t care chair so if I put here sentence that instead of the data revision label which is undefined at this point can be used is that acceptable it\u0027s all state of personal preference I I would prefer that you define what can be done today and then if there\u0027s a new format that that\u0027d be an update to this that\u0027s a personal preference it\u0027s not a chair statement okay so so I think you could possibly get away we just define it more loosely and sorry I think you define more loosely at it because again there\u0027s not a tight format it\u0027s gonna be defined I think necessarily matches or maybe does all right I want to say it here that if you have any kind of revision date revision "
  },
  {
    "startTime": "00:24:31",
    "text": "label whatever available you must put it here so I want to force people to put their the some information about the revision okay a label could be used comment from jabber this is from Andy Biermann he says please just use yang not Yin is he happy with the second solution instead removing both dr. yang Gandhian answer that but he put in before you asked that question was number one is okay he didn\u0027t say okay the other one is bad so okay he just the answer number two is okay - from his perspective okay compromise stick with number one if he\u0027s okay with that yeah I think I think you know once you address what you believe are all the open issues explicitly say that and and say the author\u0027s think it\u0027s ready for last call we can go from there I can\u0027t speak for my co-chairs but I this is a minor issue buys this and okay interesting Kent I just saw this late Kent says he prefers number two with a contributor had on so I think we heard it from Andy one and two is okay and let\u0027s go with two because Martin might prefer to I think as well sounds like a good list discussion oh wait Martin did say he prefers to also so okay now let\u0027s go with two Rob Wilton it\u0027s gonna kick us off with the design team first design team okay so no wrong one that\u0027s yeah I screwed up in the upload that\u0027s it okay so I just an update Navy what the design team\u0027s been doing I just say overall summary of the solution space and then we\u0027ve got individual drafts covering the various aspects of the solution there\u0027s being proposed here so in general terms what the design team sort covering I\u0027m gonna give a quick update and bow is going to "
  },
  {
    "startTime": "00:27:33",
    "text": "talk about the updates the revision modeling draft Joe\u0027s gonna be talking about updates to semantic versioning draft are big thin talk about yank packages Joe we\u0027re talking about the schema version selection draft wish I would have been doing that but he\u0027s not here and then I\u0027m going to talk about the draft that I\u0027ve and we published last week or I should move the weekend on schema comparison and then it may be to be sometime for the next steps discussion in the end so in terms of design and teen update and we\u0027ve been meeting on a sort of semi regular weekly basis in scientific work has been done so I\u0027d like to thank you to everyone who\u0027s been participating in that work there\u0027s lots of various people want our or another in terms of the main output the solution over you draft have been trivially updated that\u0027s not particularly interesting the shape of that hasn\u0027t changed there\u0027s been updates and some rating minor changes to the module revision handling draft that I will talk you through there\u0027s been quite significant updates to the Sebo draft the packages draft in the version selection drafts we\u0027ll talk through and then there\u0027s an early revision of these human heart and draught so to put this all in perspective I think we\u0027re at the stage where we\u0027re going to hopefully discuss whether this complete set could have working reproduction so that\u0027s what I\u0027d like to keep in mind when we discussing these drafts please so covering the individual draft the requirements draft that Blokhin is stable has been no changes since ITF 105 and none are anticipated so and that\u0027s just to sit in there at the moment again in terms of the solution overview the updates that I\u0027ve been fairly trivial just updating the references to the fighter of solution drafts hasn\u0027t yet been updated with this scheme of comparison draft because that was too fresh but it\u0027s worth pointing out here that the the shape of the solution the component parts hasn\u0027t changed in scope or sighs so that\u0027s relatively minor a recap of what the overall solution space looks like so these are the five drafts the revision the updated yang module revision handling primarily that\u0027s about be able to notify when NBC changes have occurred throughout dates in the revision history it allows a revision label to be associated with a revision and that mechanism then used to put semantic version numbers in and it also allows a branch for revision history so RFC 791 a linear revision history of modules but but that I think was the intention the expectation that that was written so this clarifies have a branch revision history you want as I said adds provisional labels so and that sort of the core draft for updating young modules then overlaid on top of that is a somatic version number scheme and that allows the use of somatic version numbers for labeling or for version both modules and it\u0027s also used "
  },
  {
    "startTime": "00:30:33",
    "text": "in package versioning as well the yang package is draft and talks about rather than doing versioning of single modules about sets of models modules together they represent a yang schema so it sort of uplift to work to be discussing larger units of work that then plays into the package version selection draft and that allows service to support different versions of the schemas and allow clients to choose which of those schemas to interact with advice very various mechanisms and then the final draft the one that I was posted on the weekend was the scheme of comparison touring draft so that explains how you can compare two modules or two gang schemas and yank packages to detect what the changes are between those and it defines some annotations to make that tooling work more efficiently so that\u0027s still an early draft there\u0027s still more work to work out exactly what things need to go into there but I think that the aim here is it shows you the shape of that solution and then this is sort of showing you what the dependencies are between the various module doing the various drafts you can see the module revision hanging one sits at the top and the package draft uses that the semantic versioning scheme is optional so if you\u0027re making use of it you have those dependencies the packaged version selection depends on the packaged version schema obviously so that\u0027s what they look like and then as I said so the potential next steps and the outcome of this is we\u0027ve been working on this the design teams overall solution for quite a number of ITF cycles and I think with stage where we would like to know that the working group supports the direction we\u0027re going in so that\u0027s why I think we might be at the stage where working adoption of this set of draft to agree that this is the right direction would be a good discussion to have now I\u0027m not suggesting you necessarily have this now hopefully this time after we\u0027ve presented on the drafts to have that discussion when you said you know you\u0027re not sure about having it now what was it no well discussion about whether we think that ask you for work with adoption of these drafts the right thing to do is the next steps yeah I think that\u0027s a perfect question for sort of setting up the the next batch of slides okay and that we should ask this question at the end okay I think you are actually the UH and I don\u0027t know which if it\u0027s you or blog he\u0027s better present the last one this video should be me so please leave a couple of minutes you asked this again and working guru please pay attention to these with the expectation that we\u0027re gonna ask if the documents are ready for adoption now of course we always confirm things on the list and we\u0027re not making any final decisions but I think it\u0027s important feedback from today to sort of "
  },
  {
    "startTime": "00:33:33",
    "text": "set that direction Thanks that\u0027s the last slot so now we\u0027re on to hello good morning like as just as Robert said that this dropped is the solution to the young model revision update so here is a young model revision update the main updates to the young 1.1 night 7950 the core enhancements to the young one 1.1 is that these updates explicitly said that long nonlinear module development is accepted and also since the it is run nonlinear and there could be like non backwards compatible changes in each revision so this document defines the number or discount or change revision extension to revision that that in each revision it must be specified if it\u0027s non backwards compatible changes and also this dropped defines that it must specify the revision label if it\u0027s it\u0027s a new revision although the revision date is added to specify that each revision revision label is also must to be added and also there are other updates like the revision date or derived used as an import this is the change to the previous one that import you either specify the revision date or it\u0027s optional but this one is given another option the other changes are like this drop to improve the young status no changes make the non backwards "
  },
  {
    "startTime": "00:36:35",
    "text": "compatible is more clear and also this dropped also update the guidelines for updating your modules revisions so here is a recap and the Saints last meeting the major changes is the revision label that when use a revision label then it must takes a young-sam version fernette and the other major one is that each ITF module with a new revision must include a revision label that is confirms to young-sam worship so these are two major changes and the other minor changes is this dropped important revision identify when defining the young and the minor improvements to that the text and modules these are main changes just a quick clarification so and the review the revision label it\u0027s not compulsory you don\u0027t have to have one of those but it\u0027s saying if you do have an original label in there and it looks like a yanked similar number then it must be interpreted that way so to link an interpreter Yanks and the and clients can interpret that way so but it\u0027s still optional as to whether you include a revision label you\u0027d be allowed to use revision dates on your own modules the same unite if modules they had to use revision levels okay so my understanding maybe is that revision labo is mandatory but actually it\u0027s not yeah for ITF modules we\u0027re saying it has to be original label it has to be used yang samba but in your if you had your own proprietary native young models you can use revision dates you wish to do so the men that provision beta is mandatory but the label is optional to add but if it\u0027s as a label then master confirms to the format of young-sam worship so write it it doesn\u0027t have to so again the original oppose doesn\u0027t have to be yang sander but he looks like yang sama it\u0027s interpreted that way so you couldn\u0027t have proprietary original label that looks like a yang sender actually has a different meaning okay thanks then this draft the opening issue to this drop it that disrupt tries to solve their potential 1979 fifties ambiguity of this like when implementing the young models that improvers in existing defining that important module revision is and bigger "
  },
  {
    "startTime": "00:39:37",
    "text": "than choose the latest revision and in is dropped they proposed another different definition that if imported module revisionism because then choose a impotent version rather than use the latest one but it could otherwise when there is no implicit provision than resource to the lady is important abortion so the question to the working group is whether we fix and this module versioning dropped food or that could be and I think this is may not be young library routers right it could be like 78 50s right I believe so this is a question to the working group so and the other open issue that this draft is still working on the the non backwards compatible changes and also the back what will be the backward changes added it more or less that because existing young 1.1 has defined a list of what is backward compatible changes but our draft is thinking we we may add more to to improve this definition so also we are thinking whether we give an exhausted list or we give some generic one that if the unless an√≠bal client then changes classifiers as NBC so here is all the distract questions just a clarifying comment Geo Clark Cisco on this as we discuss this polish in particular has come up with other things that potentially break backwards-compatibility or haven\u0027t been considered I honestly don\u0027t feel an exhaustive list as possible I think we\u0027ll always find some corner cases I think we should probably err on the side of non exhaustive with some kind of clarifying verbiage there to say we\u0027re trying to do the best thing we can for the client and if we aren\u0027t certain better to err on on saying something like NBC Nam backwards compatible so of these open issues which do you think need to be answered before adoption Rob Wilson Cisco neither of them I think we can adopt it and resolve these as part of the regular working group work on "
  },
  {
    "startTime": "00:42:39",
    "text": "this document okay great thank you very much Joe I think you\u0027re up next all right that\u0027s me Joe Clark and this is the version of the yang Cimber work that the design team was doing that Bo alluded to so we had a little module dance here or draft dance we originally made module versioning the related to drop document then we had to go back so now Simba does stand alone but there have been some changes there also been some things that have stayed the same so in particular between 0 1 \u0026 0 0 we have not changed the syntax and rules so the modified Cimber and I\u0027ll recap with an example here in a second that Rob described at a previous IETF I think 104 or 103 even that has stayed the same the notion and the definition of NBC nan backwards-compatible backwards-compatible or BC and editorial changes stays the same we did officially or formally define editorial changes in the o1 draft and modules can still have a semantic version associated with them through the revision label and as Robin Bo we\u0027re talking about that if you using a revision label that looks like a silver or a yang symbol we\u0027ll look at what that format looks like in a second then tooling must treat that as a modified yang Cimber that\u0027s important with respect to vendors because a vendor version might look like a silver quite honestly so you have to consider that maybe you need to precede the the starting major version in a proprietary vendor string of being at Cisco use one of ours like 16.3 to looks like a silver but you might have to call it V 16.3 2 to instruct the tooling that this should not obey the modified semver rules and these rules are here on the slide so we obey December 2.00 syntax and in fact one of the changes we made in ODOT one fully recognizes that syntax so we have a major version component a minor version component and a patch version component that\u0027s all 2.0 and then we add this M lowercase M or uppercase M modifier and you can see there that how "
  },
  {
    "startTime": "00:45:41",
    "text": "those are applied and those are sticky once you add an M or a lowercase M or an uppercase M those are sticky and this allows us to tag our specific branches while we\u0027re using that the the yang module versioning rules in terms of that lineage that that we talked about in the last meeting well the one exception here is that beta or pre-release versions that is if the major version is a zero all bets are off as you\u0027re developing before a release an initial release you can continue to make backwards compatible and non backwards compatible changes you just use a zero for the to denote the major version number formatting aside this is an example this was previously presented I just egregiously stole this from one of Rob\u0027s previous presentations and you can see how the versions are applied the version numbers or components are applied a where NB C changes show up and then how the M and the M of the uppercase M and the lowercase M are applied and those again would be sticky within those sub branches like 1.1.1 M that would be sticky as 1.1.1 X keeps going on so what\u0027s new what has changed since 0/0 if you\u0027ve read this and hopefully you have you\u0027ve noticed that the when we use the word yang module in the zero zero revision we\u0027ve changed it to yang art of I\u0027ll admit my embedded face Oris may not be the greatest artifacts tend to be used in some coding parlance within the industry and the reason we wanted to genera size this a little bit was to recognize that Simba\u0027s can be applied for other things and in particular yang packages which Rob we\u0027ll talk about in a few minutes we wanted that to also be able to be versioned with a yang semver so we changed where where module is needed like when we doing a import revision or derived that we still use the word module but in general we genericized module to be artifact the Cimber construct is no longer a an extension like a top-level extension whereby that is what you directly import from instead it is now a revision label and we\u0027ll take a look at kind of how that transitioned though you can still if you choose to import from a simmer you\u0027re still using you\u0027re just saying I want to refer to this particular revision of a yang module by the semver but the linear or lineage or the lineage "
  },
  {
    "startTime": "00:48:44",
    "text": "import is still how things the tooling will resolve that since we\u0027re no longer updating the the the core tenant of Yang we pulled out the update 7950 we did add full support just just for completeness for the full Cimber 2.0 dato speckled i\u0027ll talk about that in a minute and we formally define the regular expression the type def for what a yang symbol looks like and we restated the rule that we talked about and Bo mentioned if something looks like a silver a yang some ver then tooling needs to treat it as such so those were the the main changes digging in just a little bit to some of them the old 0.0 had this top-level per revision it was a cember module version and and we were saying that that initially was how we were going to do the yang versioning and now we\u0027ve moved on something that\u0027s based on the revision lineage and what we can optionally optionally label a revision with anything we want and this document defines the pattern for that yang silver and if something again looks like a yang Cimber then tooling has to treat it as such in terms of fulsome burr 2.0 support the one thing we hadn\u0027t addressed previously is that cember 2.0 offers this metadata for both pre-release and build and so we just expanded our regular expression to say we\u0027ll allow that if you want to have that as part of your version string but it has no applicability to the yang tooling so gang quill yang tooling will effectively ignore any of that build or pre-release metadata but we will allow it it will it will not break our type def definition and finally the type def definition itself so that is the reg reg X to look for a yang Cimber and I believe that\u0027s the last slide so questions on what we\u0027ve done change kept the same with yang Cimber yeah Charles echo one question you you brought up the example of Cisco using I think version you know do you mind leaning into the my side we have background noise I hear you okay using software version numbers and their yang models that sort of look like some verb aren\u0027t just you guys probably put some thought into this how is that going to be thought about the negative ramifications that because those models aren\u0027t all going to be like renamed to two like a diversion like to add some "
  },
  {
    "startTime": "00:51:44",
    "text": "string I mean those those yang models are out there and they\u0027re going to remain out there and being used for a long time right so what\u0027s going to happen I thought about that so fair point the let me go back to this so the the revision label that would have to be a new property that they would add anyway we\u0027re not saying they need to change the name of the module but if they wanted to adopt this the meaning if they wanted to adopt the whole lineage base a Providence based import and and non backwards compatible backwards compatible change everything we\u0027re presenting here essentially they would have to mark in their in their new revisions that this is this is the revision label that we\u0027re going to use and then they have to be aware that if they were do like 16.3 dot - like I said that and and they don\u0027t want users because this is mainly for users to look at this and say ah okay I understand that that between this version and the last there have been some non backwards compatible changes that they would have to use something that does not look like a Cimber that doesn\u0027t match that regular expression so if they don\u0027t want to adopt this they have to do nothing that\u0027s just bait you you\u0027re keying off of this which you can okay great miss thanks and so Rob Wilson Cisco yeah just derived one clarification to that as well so you could gain it\u0027s a Charles you just put an X like an r3 but one looks zero and yeah comes non-member so you just anything that means that you it\u0027s all for the street formats fine thanks well I thank you Erickson I my first idea would be that you have a revision label called Eric\u0027s on : 3.1 Cisco - 16.3 yeah anything that would break the Reg ups evaluation alright thank you very much Roose just as a heads up right now we are running ahead which means folks who are in the second session might be bumped up to this session but you know sometimes we end up running better the discussion continues if we end up going back on schedule but this doesn\u0027t heads up for those so right now I think we have Rob so the third of our drafts that were presenting on yank packages see so I presented this one at least once before I can even overview what it is again just to remind you what they look like and then I\u0027m going to talk about what we\u0027ve changed in here so this one\u0027s had more significant updates in terms of the details and more things added to it I don\u0027t think the overall solutions changed into the words tried to achieve it\u0027s just more refinements so um what is a yang package we know what a yang module is so yang package is where you take a set of yang modules together and "
  },
  {
    "startTime": "00:54:46",
    "text": "use them to define a schema so what a device might currently report in the yang library via module sets and things you could also report on via yang packages it could define the same thing so so why do we do something new here well there\u0027s two things we\u0027re trying to do one is the ability to define these things off the box so rather than being tied to what the service returning both put into an instance data file and use those well you could still use yang label to do that but the other key change here is that the packages are hierarchical so and the idea here is that you can define packages and then import those in the same way or include them the same way and that you would include modules and build up more complex schema so an example is ITF might define a package for basic route of functionality that defines some modules and then they might find packages for LT VPN or l3 VPN or routing package and those packages would depend on those base ones and you can effectively build together more complex sets as I said they\u0027re the in terms of the package information it\u0027s available both on the box and at the moment it\u0027s automating the angle library I\u0027ll talk a bit more about how it does that and it\u0027s also available off the box one of the other key changes come in here is to add check sums for integrity checks both of the modules and of the packages themselves the idea here is that if you know what the package is a design time from ticular version then your client doesn\u0027t need to download the full set of modules or the full schema from the device and and check whether it matches what you what you need if you know that it\u0027s what you expect the device to have you can just check it has a package that is either what you expect it to be or backwards compatible with what you expect it to be and avoid that sort of more complicated and checking of the schema so it sort of move some of the work that you would naturally do and design time definitely into the into an option of doing our design time so the problems that we\u0027re solving so we\u0027re going to solve some of things we also want to go to version more complete schemas rather than version single modules we want to go to version schemas or set some modules together and one of the main aims here is that today you look at ITF or open config there\u0027s many different yang modules for different features and different protocols and each of those can evolve separately which means that vendors may choose to implement different versions of those models in might implement version two of OSPF and version 1 of is-is-is is another vendor might then implement version 1 language all of OSPF in version 2 of is is it becomes quite hard to then get good interoperability between clients and vendors if vendors are each choosing quite discrete set of "
  },
  {
    "startTime": "00:57:48",
    "text": "modules they\u0027re implementing it\u0027s the idea of yang packaging is by combining those together into more complex schema your the opportunity of putting more linear development flow in terms of how these things evolve other things as I said that yang package is a hierarchical scheme is managed through might be made up of tens or hundreds of modules managing these is a flat list it becomes on wealthy it becomes hard for you to understand the stuff that you care about and what\u0027s what you don\u0027t care about so again the idea with young packages is that you move the conformance up to a higher layer structure and you can use that and check that for example do you support ITF LTP n and that\u0027s what you\u0027re coding your client against and that your client to expect and make your life easier in that way for clients and the last one I mentioned before was the the ability to do or a second last one is to avoid having to download these schemas on the fly to rather having to connect every device download the full set of and yang will either assuming you know what it\u0027s going to be doing or downloading a full schema from yang library and then checking it it matches what you want all the features are implemented as you expect all the deviations what you expect by having these schemas defined off the box and available as instance data files you can move that work off to be done than once so when your client connects the route or it says okay I\u0027m expecting this device to be running package that no vendor at version 2.7 or whatever happens to be and then you can check that yes actually that device is running a package 2.7 that you expect and that\u0027s fine the scheme is consistent or maybe it\u0027s running to your aides and it\u0027s backwards compatible or 2 7 1 it\u0027s backwards compatible either way it works and then the last use of yank packages is to give more flexibility in terms of schema selection and the principle idea here is that given that we\u0027re now allowing non backwards compatible changes in modules as soon as you allow those and our service to be to using those then you start to break clients when you have those numbers collateral changes and this gives you one method of potentially supporting clients that you could bought our versions of the schema so you don\u0027t break those clients and whereas other clients could connect to the new version schema so there\u0027s be Joe we talk a bit more about the schema version selection draft and what it achieves but it\u0027s making use of Yank packages to achieve that so an example here I\u0027ve got an example of ran riot of Network instance a device package and here it\u0027s listing in this package three modules that it implements and a couple of import only modules so that\u0027s listed in that package definition and the definition includes metadata about the package like where\u0027d you find the package where you find the modules what features are mandatory so what features "
  },
  {
    "startTime": "01:00:48",
    "text": "are you required to implement to say you conformed to this package definition it also can import packages there\u0027s not shown in this example shown in the next one and it when it implements modules in implements specific versions or revisions so the idea here is that a package defines an exact schema so whenever you download a package at a particular version you know exactly what every single data node looks like it\u0027s the intention if you know what features are enabled it allows import only modules versions revisions and then the things have been added here more recently our check sums that allows you to know without necessarily downloading on the assets that you\u0027ve got the correct copy of them and also the other thing that\u0027s been refined more recently is more works been done on the import conflict resolution and the basic principle that\u0027s being applied here is that you resolve any conflicts explicitly so the conflicts might arise when you\u0027re building up packages from other sub packages and they are implementing or importing different versions of modules so when you get those conflicts and the point that you are when those occur by importing those two two packages you have to resolve explicit that you say I\u0027m going to choose to use this particular version of the module so you resolve it in that way second example here is a basic routing package and that\u0027s just basically to show how this works in terms of this package the basic routing package has a dependency that includes a particular version that network network a device package so all the modules that were defined as part of that network device package are effectively being used by this and then it says which particular module implements on top of that and it lists more import only modules as well that effectively uses so and this is the case that if it was including multiple packages and they happen to implement different versions so if you for example have a case where one of the package was implementing BGP at version X and another one was implementing BGP version X plus one you say when you pull those two in what does that combined package effectively do what does it use and as it says here any version conflict change must be explicit result so you always want to be very clear when you\u0027re reading the package definition whether or not you are implementing those packages you pull them in faithfully or whether there being any changed so now going over the main changes and since oh one there\u0027s been quite a lot so the ones I\u0027ve put with an asterisk down there the ones I talked about in more detail so some of the some of the train changes a fairly and formulaic that the fact that the work on yang module updates moved from using semantic version numbers all "
  },
  {
    "startTime": "01:03:48",
    "text": "the times using revision labels and semantic version numbers optionally applies to packages as well so if you your company wanted to use just revision labels and didn\u0027t want to use yang cember then it would use it can also define packages using revision labels with a similar versioning scheme in terms of how the modular version so this supports both as support for check sums I\u0027ll talk about in a bit more detail as support for locally scoped packages so previously all the package definitions were globally scoped available off the box this defines packages that are scoped to a single device I\u0027ll explain why they required and why they this improves the performance that I\u0027ll talk through as well and the use of packages as definitions of instance data file schema so so again I\u0027ll talk to this bit more detail but this isn\u0027t just about using a package putting a package in tune into an instance data file it\u0027s using a yang package as the definition of a schema for an yang instance data file so once you\u0027ve got things that define schema that\u0027s one natural usage of them and then finally there\u0027s been quite a lot of minor changes and cleanup to the models and the draft and sort of reshaping it a bit so it\u0027s had very significant work packaging module check sums so this was a request that came in to effectively have some way of knowing that the yang modules that you\u0027re referencing by URL or the packages you\u0027re referencing value RL are actually what you expect them to be so the solution that we\u0027ve added here is to use a sha-256 hash of either the module or the package definitions and to avoid you having to download them each time so these check sums are written into the package definition files when you when you reference a package you can optionally include the sha-256 checksum and and likewise with the modules again when you provide a URL you can also provide a checksum and so that means that with you obviously if you\u0027ve got those things locally within your your processor or server you may not need to download these things again you can be sure that they match what you expect them to be in the case of modules the checksum is calculated on the yang file so effectively this means that it includes whitespace changes and the expectation here is that all instances would match if you had a URL and you can find it from various places and for packages the checksum is calculated on the yang instance data file so the same thing and again that would include any whitespace changes and it include the metadata information at the top of that package yes you actually have something from jabber first okay so this is from Martin is this meaning module checksum because we don\u0027t trust the revision date or label it\u0027s because we "
  },
  {
    "startTime": "01:06:49",
    "text": "don\u0027t trust the URL so in the package definition you\u0027re providing a URL to where you can go and find that package and so you want to check what you actually download from that URL max where you expect it to be that\u0027s one of the cases the other case that is useful is that again when a device says I\u0027m using the package ITF at 2.00 that actually is the checksum of that package so when again is a client you say yes it\u0027s what I expected to be and I can check the integrity of that that\u0027s opposed by the hood I wonder how stable this module checksum checksum is because modules are often extracted from our FCS and different extracting tools just add or remove different amount of white white space so I think it would be useful maybe to to to transform the end module to some canonical white space and then computer checksum because otherwise it won\u0027t be reliable possibly and adds complexity the one I was really hoping to bind it to is the fact that this has URLs at list where those modules could be found so really is the key for me was trying to bind that the files that are downloaded from those URLs match the checksum with them so whether that\u0027s still required I don\u0027t know so next the next change is the relationship between packages and schema so talking about local packages the the aim in terms of what this works trying to do is for each data score datastore schema to be fine by one package so you have a one package definition for that datastore schema that makes it very easy for the device to advertise for each of the data source schema what the package is that defines that schema and it\u0027s easy for clients to know that off the box so an ideally like just really to be useful you want names to be available offline and you want it really to be available design time but there are cases where that becomes quite tricky so one of the cases is that your software itself might be made up of different software components that could be optionally installed and added or removed and hence the packages that you can generate on the device to represent the combination of software components that we install that point in time can change and be more dynamic so in this case you wouldn\u0027t expect to be able to define offline packages for all of those things it might be helpful to define a local package that device that says okay I\u0027m installing these sub packages and it\u0027s those sub packages that each are available off the box and the local package is just the top-level definition to pull those all together and combine them similarly if you apply software bug fixes that change the scheme and that\u0027s another case that we think where you "
  },
  {
    "startTime": "01:09:50",
    "text": "might deploy particularly an advertised particular package for a given software release and say this is the standard version of software but if some bug fixes have come along then the scheme has been changed it doesn\u0027t no longer quite reflects that what\u0027s been advertised as the as the package with that software and so you use a local package to say actually it\u0027s that it\u0027s the same as the package release of the software but it\u0027s got these few changes these two additions to that schema so we have a couple of comments from jabber really on the previous slide about format you can suggest using XML as it is lossless and he\u0027s saying that as a contributor Martin is saying actually the the new RFC text format non paged is lossless for text lieu as Charis Walt says whatever we decide should be in the document yes and again I think well I don\u0027t think they say she has to be sold now before yes I think all these things but yeah I\u0027ll take one board that makes sense well I thank you Eric son in our practice or it\u0027s only the question of removing white space from the end of the line which is a very simple formation so we have one more from Ishod art but RFC\u0027s are the aren\u0027t the only source of yang if we care about it he says it as I mean it reads is a statement but there\u0027s a question mark so I think that\u0027s true we want to make have a solution that works no matter where the module is defined yes that we can compute the checks up in the same way yep I want to be done in a simple way so to be easy that you can to to get those check sums but it really has to be unambiguous whatever it is a great so I think I\u0027ve covered local packages I just to get a notional I said to the end of these in terms of the idea of local packages the two key changes are that the name of the package is no longer global scope so everything in terms of the other package definitions the idea is that package name is effectively globally scoped but here a local package you the device could choose to define its own name for that package that may collide with that same package name on another device effectively so that\u0027s one change and the other one is the offline definition may or may not be available for the device may allow you to download an instance data file containing an offline definition perhaps but that\u0027s not necessarily expected the idea really is it\u0027s just a way of combining package packages together at the top level if required so conformance improvements various proofs we made one is that as I mentioned earlier on that packages can use revision labels or they can use sang "
  },
  {
    "startTime": "01:12:51",
    "text": "cember so that I think works quite nicely and then there\u0027s been some more explicit conformance in a few places when package ii when a package inclusions define both explicitly which package version obviously you\u0027re including but they also now state which package versions they are replacing explicitly so in that case where you\u0027re combining two packages and you have to choose two packages of the same version through this sort of this dependency tree you know city state which ones you\u0027ll get getting rid of whenever they collide and if one of those included packages is modified in an NBC way then you have a flag to annotate that packaging point that is included so the idea here is that when you look at a package definition and you look at all the packages that it includes you should be able to know whether or not the package faithfully implements those included packages so if you had a top-level vendor package that included IHF routing a particular version you\u0027d have to clearly indicate whether or not you faithfully implement the ITF routing as defined by its package definition or it\u0027s been modified in an MVC way perhaps because you\u0027ve got some deviations or perhaps because you\u0027ve included some different versions so the idea here is to try and make that conformance easier for clients similarly when you redefine the module inclusions again the module revision you specify which module revision you are either implementing or import only and it also specifies which revision or any revisions of the module it\u0027s replacing so the case where this is important we had this sort of thing in the draft before for the import only modules you could say which ones you no longer needed but now for the implemented modules you can say I\u0027m implementing module version X and I\u0027m also effectively replacing other module versions of Y so that that really matters for like the import only case where you want to say and they want to have this dependency on an import only module and so feedback on that would be very useful on what we think packages is schema definition for instance data document so this also goes back a little bit to what bal√°zs was presenting on the idea of packages is that they define a yang schema so they\u0027re meant to be a canonical representation of yang schema instance data documents obviously have a schemer associated with them packages I think would be a good way of associating a schema with an instance data document the reason I think it\u0027s good is because the idea is that these package names are globally globally scoped and I have revision numbers and you have a checksum associated with them so you need relatively little information to guarantee that you get the right schema and it\u0027s what you "
  },
  {
    "startTime": "01:15:51",
    "text": "expect it to be the one thing that needs to be resolved with that though is sort of like the bootstrap scenario so if you say you\u0027re referencing up to a package if you\u0027re saying the schema for your particular instance data document refers to a yang package well that yang package itself is defined an instance data document what does it use as its schema is that something that it then has another reference to a another package or a module set or does it is just hard coded that the instance data library understands the instance data documents Beast to understand packages as a native construct or not so that\u0027s one area I think that needs a little bit of worker refinement to make sure that doesn\u0027t get too complicated so that covers the main changes that we\u0027ve made and then I\u0027m also going to cover sort of the open issues and this isn\u0027t exhaustive there\u0027s some minor things we\u0027re still discussing but these are the ones I think are particularly interesting and they\u0027d most interested in feedback on these things so the first one is where the packages should use a different structure for the instance data file representation versus what you get out of the device eg from yang library or or similarly the current approach is sort of try to optimize for readability in the file and optimized to minimize data transfer from the device so to that effect the package definitions are on the device reused the module sets from the yang library so rather than having affecting the same equivalent information in a separate tree for the Yang pakka geez they just got references back to the angle library module sets the idea here being that you could potentially allow those same word row sets to be used define to define the young library schema and also you packages so clients have the option using both so that has some advantages in terms of affected that minimizing the data there\u0027s a disadvantage of doing this though which is the sort of more complexity in structures and the fact that the structure is different differ so one of the bits of feedback from bal√°zs was it\u0027d be nice to use the same structure for both and I think there\u0027s obviously two ways you could do that one is you could try and augment yang library with the packages information I\u0027m not sure that that easily works and I think it fundamentally the hierarchical nature of yang packages I think with them break the yang library I don\u0027t think you can easily do that so I think if we wanted to use the same structure I would instead go for the format is used in the instance data document and use that on the devices as well so you\u0027d have more repetition of this data in terms of of defining the modules that comprise the packages however I\u0027m not sure that\u0027s really a problem because the intentional yang "
  },
  {
    "startTime": "01:18:51",
    "text": "packaging is is that clients shouldn\u0027t have to download this information is there if they need it and they want it but the idea here is that you\u0027re using yang packages you know what they are off the box and you avoid having to download this information that\u0027s that\u0027s one of the key aims here so the fact there is a hypothetical duplication of that operational data may not matter in reality and so currently my I\u0027m leaning towards changing this but I think this would be something again that is one of those issues that we need to sort out it doesn\u0027t have to be done again before the working before it\u0027s doctored by the working group it could be done afterwards it\u0027s not a significant an issue but it\u0027s something that needs to be considered feedback on that is welcome another on this that that we\u0027ve considered talked about is the yang library definition requires that module name spaces be specified in terms of the yang package definitions they allow the module name space we specified if you want to so in terms of the yang structures being used it\u0027s it\u0027s included there but it\u0027s optional rather than mandatory and the idea here is I think that that with the JSON encoding effectively is almost moved to the point that the module names are globally unique anyway they identify the data of the namespace so I\u0027m not sure whether the XML namespace is still that useful anymore for these things so I think in the module name the region label path and checksum as effectively sufficient and by path or II mean like the URIs but you can fix these things from a sufficient to go to identify to pull these things down and if you need the namespace you can get that out of the module if you want it but at the moment that proposes to keep the namespace definition allow it to be specified for people that want to but I\u0027m not sure it\u0027s actually needed again comments on that you sure welcome this one is so talking about the checksum so I was explaining how they\u0027re used so this question here as in the examples in the draft I think use the full sha-256 checksum which is 64 characters long I think that\u0027s right so these are quite long and vabase in the files so one thing I was thinking about was rather than using the full sha-256 checksum you could allow prefixes to be specified in the same way that gets allows you to use prefixes of the sha-256 to identify the particular commit hashes you could potentially do the same thing for young packages the downside with that is that in get it\u0027s really just using the prefix to uniquely identify a file it\u0027s not using it to validate the integrity of that file so I think if we\u0027re using prefixes we would break that integrity check probably so "
  },
  {
    "startTime": "01:21:52",
    "text": "the proposal is actually let\u0027s keep the full sha-256 checksums in the files rather than allowing prefixes but again I\u0027d be interested in one has opinions going the other way use of module tags so the draft allows you to use module tags to associate additional metadata with yang packages it doesn\u0027t define any mechanism to talk to the device to add or remove or modify the tags associated with a package solely the module tags draft allows you to define tags within a module definition and it also allows you to update those tags associated with modules on a particular device you can dynamically modify them so the question here is whether this work should be added now should we add support for doing adding removing and modifying package tags to this draft or would it be reasonable to defer that to future work I\u0027m not sure how displays I might ask the author of the yang one of the author\u0027s if he has any thoughts on this I think it\u0027s a low priority feature so it this is Liu Berger answering as contributor I mean it\u0027s a low priority feature so I would leave it towards the end and if we decide that the group besides that it\u0027s important enough someone will will write some text and if at the end there\u0027s no text I think that that\u0027s our answer it could always be done later yeah sounds good to me packages for schema so this is an interesting one the idea for each package is it represents a schema and it says here potentially incomplete so am i that\u0027s one things I didn\u0027t mention here is in terms of the yang package definitions the schema that it\u0027s representing doesn\u0027t have to be complete it could represent an incomplete schema so it represents say a set of modules that they themselves have dependencies on other modules aren\u0027t defined as part of that package and there\u0027s a couple reasons that those was incomplete schemas are useful there useful in the case that you might have a dependency on maybe I Anna I have types where you don\u0027t binding to a particular version to leave it loose in the package definition and then when the package has been used it would specify exactly which version it\u0027s using and again it\u0027s also where we\u0027re defining things like packages for a bug fix or something you just want to include the modules that been changed in that package you don\u0027t have to include the whole scheme each time so again that\u0027s an example where a package might represent an incomplete schema in the package definition it would specify whether or not schema it represents a complete scheme or an incomplete schema but they actually issue here is to do with nmda and datastore so each data store defines its own schema so as such each data store would have its own yank package definition they might be the same for the same data sources or they could be different but in the destination in our "
  },
  {
    "startTime": "01:24:54",
    "text": "c83 for to the nmda RC it sort of implies the existence of an uber schema that represents a common parent scheme across all data stores what it actually specifies is it says that the schema for the operational state data store must be a superset schema of all the configuration data stores except you can remove some things so you can\u0027t deviate you can deviate remove things to take it out you can turn features off but otherwise you can\u0027t change the data types you can\u0027t change the meaning of notes so I think what that really means is the existence of this uber schema on a device where the schema for each data store must be a subset of that schema so it might have things missing you might have features turned off nodes missing and it might have deviations remove nodes but otherwise everything else is always a subset of this effect this uber schema and this has come up as being something that\u0027s potentially useful in the versions of selection work rather than trying to select sets of schemas for the data stores it might be more appropriate to try and select using these uber schemas identify the schemas across all these data stores rather than individual ones so that\u0027s something that we still sort of talking about of looking at as well as the data tools and advice the same sort idea applies to these sort of schema families so if you had a set of packages representing so the ITF modules or open config modules related modules the same principle applies that for those schemas the schema for the individual data stores may differ but they still logically have the same uber schema that represents all the stuff in all of them at the top level so again we think that these may be useful to describe those things and again it\u0027s really the packet the package version selection draft that\u0027s driving some of this discussion idea as to whether these are useful one things add is that this came up on the alias from Andy that he was saying that it\u0027s quite tricky for a client to know what the schema is because the schema for each data store is different so it is potential an idea that maybe this Buber schema that logically exists could also be advertised in young library saying that this is the schema that acts as the schemer above all the other per datastore schemas but you can calculate it and you can you can generate it by merging everything together so I think there\u0027s a question whether whether that would be useful as well and so that one is still in his open discussion on what we do those and I think it\u0027s really the version selection draft that drives that I think maybe it\u0027s my laughs hopefully my last slide on this one and is once you\u0027ve got these packages one of the principal aims is to try and add some more conformity between what ITF produces so rather than producing this yang modules for inderal individual features can ITF starts produce and sets of yang modules that work together to "
  },
  {
    "startTime": "01:27:54",
    "text": "provide functions for particular services and things not service though yang modules but implementing those services on devices so so I would like the packages work gets adopted we also want to then be thinking about can we try and start defining what these things look like does it work and can we come up these definitions and then there\u0027s a question of how do you manage those packages do we need some ion a registry for those and the other side of that is I would like this package of different definitions to be globally unique so again how do you manage that namespace I\u0027m hoping that simple registry of prefixes on the package names is sufficient rather than using you are eyes that make them more for base but against there\u0027s more thought about this and how we do that and how that works and questions and process and things so this is just all early days and this not really draw I think the draft mentions the idea you need to do this doesn\u0027t talk about the details but again I don\u0027t think this is something needs to be solved for working group adoption it\u0027s just part of work as this work evolves in the working group that\u0027s my last slide on this part great any other issues you think that should be addressed before working group adoption no I think I think they I think the shape the document is describes well what it\u0027s trying to achieve I think most issues are really just working out the details from my course you know I don\u0027t think anything has to be resolved before Dakshina great thank you looks like we have a question I think for not now packages right us were from inclement part do you mind we mean to the mic also say your name please okay whoa from Wahby and here is the question for because I when I read this through that young package I think for implementing is quite useful but right now I think like there\u0027s no standards to define whether how how can we form a young package for the uber package it seems clear but we could like use only two models to form a young package so so right now there\u0027s no standards define in the young package draft so so in that way I still think that if that young pack could get at more text to describe how to like firm use for young package I think that\u0027s useful Thanks okay I\u0027m not be added I think the idea here would be that the I wouldn\u0027t want to define any actual packages within the packages draft has a couple examples but the idea would be to have separate RCS to define an ITF base package for what modules would go into that and one for who our eyes have rats and that sort of thing so but I think yes I think I\u0027ll be tricky to define those but hooked up one "
  },
  {
    "startTime": "01:30:57",
    "text": "idea maybe it might be useful in some use cases to include a PGP signature of the content so that it\u0027s somehow a sure that it\u0027s the right packages that somebody received so maybe as an optional item it could be useful to add some kind of let\u0027s say PGP signature to say to sign the checksum so that it\u0027s real the content that that\u0027s supposed to be there okay all right well thanks for putting us back on schedule and Joe I believe Europe and so Rama\u0027s like in five minutes of your time okay I\u0027m not Rashad I\u0027m Joe but this is his work and he\u0027s online so here we go why are we here what is the goal of yang version selection specifically this is about addressing this requirement from the requirements draft we need to allow for a way that existing clients have a way of interacting with a yang driven server that is is a way in which they expect a way that\u0027s not going to break those existing clients and we also need a way to be able to distinguish now that we\u0027ve introducing yang packages we need a way of being able to distinguish what version of a package we may want to use if a device happens to support multiple packages what do we want that schema to look like so these are the the goals this is the wherefore of the version selection draft in particular the solution here will allow servers to do these non backwards-compatible changes and clients do not necessarily then have to always track the latest and greatest so for example a server could support version two of a given package and version one of a given package so the clients that understand version 1 of that package can select that that is the version by which they want to interact with that is the schema that they want to see obviously then therefore this makes use of the yang packages that Rob just presented and we need to have a way for the servers to advertise this support what packages do they support at what version do they support and we have to be able to say this is the default version and we\u0027ll talk a little bit about how we\u0027re going to do that that\u0027s one of the open items and then additionally then how does the client make that selection how does the client say this is the package the schema I want at the version I want so that\u0027s what\u0027s laid laid out in this particular "
  },
  {
    "startTime": "01:33:59",
    "text": "draft servers are not required this is something that we debated quite a bit on the working group servers are not required to concurrently support clients using different schema versions in reality it may be very difficult for a single server a given server to support two major revisions of a given package two major revisions of a given schema so servers are not concurrently required to support that but they need to unambiguously indicate to the client that they are unable to satisfy a selection request if they aren\u0027t unable to render or support both versions of a given schema at once and servers obviously are not required to support every revision or version of a given schema or given package so for example a server or a packaged version 3.0 may come out 3.00 for example but not all servers need to support that and as well if you\u0027ve got a server that supports version 2.0 and 1.0 there could be non backwards compatible changes there that the server can\u0027t reliably render and there has to be some deviation to indicate that for example we are not going to support a node at string when it used to be int we can\u0027t do both at the same time so there that that onus is not on the server to be able to somehow magically do that but there has to be ways of signaling that to the client that this is a deviation this is how a server is going to handle that changes between zero zero zero and zero one in zero zero we talked about a net cough solution by whereby a client selects a specific version of a schema by using a different TCP port number and then we thought about that and we thought well that\u0027s going to really put proliferate ports as we go and saying proliferate ports a lot quickly is tough to do so we we pulled that out and we picked an RPC based approach in order for Netcom clients to be able to select a specific schema in particular we initially started by saying a client selects this particular package at this particular version but we then started saying and this led into some of this uber schema talk or uber package talk that Rob mentioned how does a client string together multiples of these packages so for example if they have a l2 VPN package and an l3 v how do they bring these together to come up with a overall cohesive schema that that client may care about so we added support and still an open issue for discussion we added support for being able to select to multiple schemas and "
  },
  {
    "startTime": "01:36:59",
    "text": "as I mentioned on the Netcom side on the Netcom side we changed the port selection to an RPC to be able to say these are the schemas that I want so here we go a version schema is associated with as we talked about a it could be a semantic version has a revision label but it is associated to those yang packages and within the pack or we have this this notion of sets of schema that string together to form one cohesive schema that the client is interested in using at specific versions of the the sub packages within that we can do multiple things here we can have the device as Rob pointed out to find these local packages that kind of create that umbrella or group together multiple sub packages to give an overall schema or we can have the the vendor or device manufacturer create packages offline that again specify what schema are using or we can leave it up to the user to be able to say I want to select l2 VPN at 1.0 l3 VPN 2.0 but then we run into some issues with how and Rob mentioned this how do we resolve some of the intentionally inherent conflicts between different schema that might use or different packages that might use different modules or different modules and different versions of those modules but what we want to be able to do is have a way of Netcom clients being able to say this is the set of packages and versions I want or the schema that I want and the same for Netcom or sorry with rest cough and with rest comp we have a offshoot branch in which the client will make a query to be able to say this is the set of packages or this is the schema at this particular version that I\u0027m interested in so different route for rest kampf and the RPC for net conf this is the version selection the yang tree output of that you can see how this breaks down we\u0027ll go into a little bit more details we look at examples specifically of how the RPC works and that\u0027s gonna lead us into some of the open questions that the design team has been having this is for example how a server will advertise support for specific packages at specific versions so this happens during the capabilities exchange so the server will say that I have the capability for these sets of packages so example ITF routing at a specific version or two specific "
  },
  {
    "startTime": "01:40:00",
    "text": "versions a vendor and a vendor package at two specific versions so this could be these are the list of packages that I support this is what you as a client can request in terms of of being able to say this is this is the selection that I want to make for clients to understand this capability so we have a comment from Martin on Java why a different mechanism for net conf and why not use the same mechanism as for restaurants why the the the different mechanism for ruskin versus Netcom we with well the rest comp we had the we had the ability of doing a different URL we thought the RPC we talked about a few different things we thought the RPC seemed more natural with respect to what a net comp client would would expect to do so that is why we again where that is one of the things that we debated most recently is what should we do we felt that the port port change solution wasn\u0027t scalable and we thought that this would be a way of being able to do something it\u0027s more of a handshake and negotiation between client and server where the the server could the server could a Carnac that that is supported for the specific client Maas angle Ericsson I think advertising anything more detailed than the ubirr schema with a lot of complications and lot of how do we support all variations of the support schemas yeah and that\u0027s gonna we\u0027ll get personally I agree with you and we\u0027re gonna get to that in particular with the open open questions in a minute here so this is the example of the Netcom for PC polish just kind of hinted it\u0027s something that we\u0027re gonna get to in a in a second here what happens here if in this example maybe there wouldn\u0027t be conflicts but what happens if the client selects a set of packages or a set of schema that inherently conflict obviously you could just nak this and instead of turning returning an okay you could return an error replied to the the RPC request but it might be better if there was a way of having a single some vetted probably the wrong word but a single definition for the overall schema that the client wants to use hmm so that anima has a continuation on jabber so Martin says config false data instead of special for protocol capability to which Rashad says Martin do you mean to have a different "
  },
  {
    "startTime": "01:43:00",
    "text": "solution from what\u0027s currently in the document and Martin responds saying why not use config faults data instead of special protocol capability I thought RC was using a config fault tree perhaps I\u0027m mistaken perhaps I\u0027m so I\u0027m a little confused at the at the question so robertson cisco so i think that the information of what you could choose would be in config force for both neck get from restaurants is foam in both cases the reason we put it in for capabilities exchange for neck confers we thought that\u0027d be easier for a client when it connects to know what to vote straight away and deburr to choose on that initial RPC beginning i want to choose the schema whereas in the restaurant solution because it\u0027s done on the path based thing effectively once you get the data and then just choose the right paths are things to do with what point of time do you choose the scheme you using and getting early enough in the process yeah one of the things we did discuss thanks robb one of the things we did discuss was wind is when when does the capabilities exchange occur and could the client simply say in its capabilities what it wanted to use but the capabilities exchanged can occur simultaneously so we wanted to it had to happen early we had to have some way of of having in the Netcom session this happened early and and so that was the other thing that we another reason why we went forward on at least on the net coincide with the RPC so I\u0027ve asked Martin if this is something he thinks can be addressed after adoption or does he think have to be addressed before adoption so well here okay Martin says I don\u0027t think that optimization is necessary compare with hello versus yang lip for modules an additional RPC isn\u0027t a big deal so he gave a technical answer to a process question so and he also says there are other problems with this solution so martin again is this something we can work out after adoption or do you think it\u0027s completely the wrong direction and we need a reset here so he\u0027s not sure okay well we\u0027ll go on because there are certainly issues with with this that we do need to we need to as a group work out so this is we\u0027ve already I\u0027ve already touched on a few of these and and polish brought up the fact that when you are arbitrarily allowed to chain together schema the server may not be able you know sorry sorry Dave I can\u0027t let you do that so this the server main a kit because the string together schema that are selected don\u0027t really work "
  },
  {
    "startTime": "01:46:00",
    "text": "together they can\u0027t be simultaneously supported by the device maybe another client in another session has already made their choice and the device can\u0027t concurrently handle both versions say of a particular package or you might be trying to set change this on the fly meaning you did it once at the beginning of the session and now you\u0027re trying to send an RPC again for a different schema version and then the server can\u0027t support that here is a config example Rashad is very meticulous and generating the full example of the of the yang module within the draft but let\u0027s get to the open item since I think we are running behind do we allow multiple schema sets to be selected polish already mentioned there\u0027s a problem with that it might be better to say that we want to either define this maybe at a config time where the the client has to resolve those those conflicts explicitly as Rob was mentioning in his presentation or maybe it\u0027s something where we have these uber schema on the device and there is just one or a set of schema that are supported and maybe for example IETF version one IETF version two and that includes all of the IETF modules and those are a package I should say at a specific revision and version that means the conflicts are resolved it\u0027s clear what the client would be getting in terms of an overall schema and you don\u0027t have this kind of frankensteining of putting together different potentially incompatible sets of packages in terms of recommendations we\u0027ve been I said earlier myself personally I like the kind of uber or or predefined schema that is free of conflicts that we know is going to work I don\u0027t know if we have a design team consensus on that but oh those are that is one of the things we\u0027ve been talking about very seriously in some of the latest meetings i\u0027ve been a part of but that\u0027s an item for discussion too we have Rob mentioned the the datastore relationship we now have a one to end relationship between the datastore and the schema that it could that could be defining it how do we handle those conflicts this potentially goes away if we use those local packages that Rob talked about meaning I could on the device say I wanted to find a package config wise I want to define a package that has these sub packages in it and then I can specifically as part of the config say this is how I handle the potential conflicts between the modules as Rob mentioned or the devices just define a or the gangue servers yang "
  },
  {
    "startTime": "01:49:02",
    "text": "driven servers support a set of kind of overarching or uber packages that define their schema and that is at the level that the client can select so we go back to still a one-to-one relationship so related do we need that superset schema Rob mentioned the ITF open config native vendor that is one way of resolving these conflicts where the vendor pre does it and says that we support Oh a package called ITF 100 and we have offline this yang instance data that shows what that package is and the client therefore knows what to expect same thing with a 2.0 so that could include sub packages around l2 VPN l3 VPN but the client selects from a version selection standpoint that overarching package that overarching schema the other thing is how do we indicate what is a default schema inversion so default package I should say one of the things we talked about was having the semicolon notation and just say semicolon default and say is part of the capabilities exchange this is the default if you don\u0027t do that RPC if you don\u0027t do anything this is what you\u0027ll get likewise something that we need to discuss is what recommendations might we want to give to implementers that says how do I decide for clients that don\u0027t know anything about this so this is a client that understands version selection and what they\u0027ll get by default but what if a client doesn\u0027t yet understand anything about this how do we support that client we can give recommendations that say the default non-selected should be something that maintains backwards compatibility let\u0027s say so for example we could give that recommendation but again we need to discuss how we go about handling the defaults how we go about handling clients that do not make an explicit selection at a time next steps we pass it over to rob banks on the with before it\u0027s RFC actually defines how to handle defaults in the default capability you could we could look at that Thanks okay so I\u0027m going to try and get through this one this is new dress I can\u0027t rush it too quickly but I do want to also try and get to this so we do have an extra 10 minutes in the next session yeah so you can start now and then we take the adoption question at the beginning of the next session I\u0027m just concerned that then we might lose people yeah and actually my co-chair may not be able to "
  },
  {
    "startTime": "01:52:02",
    "text": "wake up because he stayed up late for that cough yeah I\u0027ll try be quite okay so this is the last last draft at the set of five so this completes the solution so what is yang skin comparison so effectively this draft is defining algorithms to compare yang modules and yang schema to determine the scope of changes between different arbitrary revisions and versions so it\u0027s similar to what we all have been talking about a lot about updating and using Yang semver as modules change with NBC changes and backs compatible changes but the idea here is to define the talling on how you do that and the talling would work both between modules that were within their history but also if you have some sort of branching occurring between modules the ability to compare versions between different branches so the reason that this is important is because in in not in all cases December solve all the issues so there\u0027s some cases where you\u0027ve got a branched revision and your client might be updating from one branch version to another and they can\u0027t use the same we\u0027re also to define whether or not less backwards-compatible changes so this is talling to to do that in addition to that we define some extensions optional extensions to provide some more annotations to when changes occur to modules and in cases where it\u0027s ambiguous so where your taurine cannot determine whether or not a change is backwards compatible or not then the assumption will be will default to be an NBC change unless you had an annotation to say that it\u0027s an editorial change or abouts compatible change this is a 0 0 revision was written last week it was published on on Saturday Sunday the in terms of the actual solution this has been discussed quite a long time is really matter writing it down but this is a relatively new draft so why do we want this well revision labels and yang semver work in the mainline case so if you\u0027re updating along a linear provision history then it it works quite well but in the case you get to where it\u0027s branched it is not so useful you can\u0027t you can\u0027t rely on just those similar numbers second the reason this is useful is in terms of actually getting the right semver numbers or the labeling modules with the correct NBC labels it\u0027s useful you got tolling that can actually identify those rather than relying on humans doing it as humans generally get it wrong the third reason this is useful is that clients aren\u0027t impacted if the scheme has changed in the bits that they\u0027re not using so if for example is an NBC change in a feature that you\u0027re not using but you don\u0027t really care about that and from your perspective you\u0027re upgrading your software you\u0027re not if you may see that change as effectively and that backwards-compatible change because the NBC changes don\u0027t break you and then finally be able to define these standard annotations could help improve the "
  },
  {
    "startTime": "01:55:03",
    "text": "accuracy of these comparison tools so by having defined in a in a standard it means that any tools can work with those same definitions in terms of the details with a generic tree comparison algorithm it\u0027s not particularly magical in what it\u0027s doing it\u0027s just doing walking down the trees the schema trees and comparing them the comparison is performed via identifiers rather than the ordering so that\u0027s the difference from what\u0027s then what\u0027s in 7950 this means that you\u0027re allowed to reorder statements okay and that\u0027s not a problem but it does have and some other complexity so and so that\u0027s one choice we\u0027ve made there the Arrogant can either work on the like a yang package level of full schema tree or it can work on individual yang modules and in terms of the definition of the argument for yang packages we\u0027ve just defined at a standard version that sort of gives you the worst case scenario and that\u0027s what\u0027s useful for defining what version you package would be are also defined options to give a sort of filtered version that is tuned to what clients might like to see and then extension statements I mentioned so to refine the comparison I\u0027ll talk about those in a minute and give examples so the filtered version for the full yang schema so this is covering the case where as are saying it tries to answer question from a client\u0027s perspective is moving from one software release to another is okay there might be some number cause collateral changes am I going to be affected by those so the suggestion here is you could filter out some of these aspects because they probably are less interested for clients so if groupings have been changed in terms of the actual names of those groupings will have moved around that doesn\u0027t affect the scheme that\u0027s constructed so probably clients won\u0027t care very much if the module metadata information has changed you product care you can restrict the comparison to the subset of features you actually care about you using them you could restrict this the the comparison to the subset the schema that\u0027s being used by the clients you could feed in some instance data document that says this is the configuration I use or you could feed in some XPath saying this is the trees I\u0027m interested in so that the the results of that comparison actually is tuned to what you\u0027re interested in and finally you could filter it out very toriel changes one of the things that comes up quite a lot is where we\u0027re fixing like description States improving those the taller will naturally flag those up as an NBC change because it can\u0027t tell when you change the description whether the semantics of the node has changed or whether it\u0027s just some sort of minor cleanup of those descriptions as it gives a more refined answer some examples of how these things work these annotations work so I\u0027ve got an example of fixing a description so I\u0027ve gone from revision 1 0 0 to 1 0 1 that\u0027s the standard module versioning update rules and the similar being used there and then at the bottom in that container foo you can see that I\u0027ve added I\u0027ve changed "
  },
  {
    "startTime": "01:58:03",
    "text": "the description from do some stuff with misspell to fix that I did a full stop and I\u0027ve now labeled that that is an editorial change I said which particular revision that editorial changes occurred in so this means that comparison tool that\u0027s comparing those two revisions can then know that this actually this isn\u0027t a non backless capacitor change their flag up by default but actually it\u0027s not of consequence another example so this a different label is a different annotation we the draft defines an annotation for effectively be able to rename a node so it\u0027s changed here from food to bar this change has been done in a non box compatible way so the multiple versions gone from one zero zero two two zero zero but you\u0027ve got a label under the new container bar to say that was related back to food so when the talling is doing the comparison of the two trees first of all they would look for container bar in the old module wouldn\u0027t find it if then C\u0027s got renamed from foo and do the comparison against the food so it allows you to do smarter changes Sparta comparisons whereas by default otherwise you would flag it up as a delete and a crate we haven\u0027t necessarily figured out exactly what all of these things should be in this stage it\u0027s just these ideas of sorts things you do yes cuz the intention it would that be like a UNIX rename like move can I move it from foo it or route 2 inside a container and say renamed at the moment the moments just defined in the same place in the tree but yes it\u0027s actually could be so yeah the way Chris named Chris hubs so I think to clarify I think that\u0027s a detail that we need to work out and we need to work out what these extra annotations should be which ones are useful so the next steps so the so one the question is this defines various extensions to the module versioning things it could be done in the module versioning draft it--if young revisions it\u0027s in a new module within this draft so it\u0027s questionable where that goes we need to work exactly what those annotations are needed and useful again I think that could be done after work group adoption and there\u0027s one question is do we need an annotation to mark something as NBC at the moment only defines its assumes NBC by default it doesn\u0027t know and then adds annotations to say it\u0027s either backwards-compatible editorial and then another question that\u0027s come up is the revision renamed from label allows you to rename it but doesn\u0027t specify exactly when that\u0027s occurred and I think that\u0027s probably sufficient but there are scenarios of you to just swap the two names of two containers or you rename one and introduce a new container with that same old label that you\u0027d have issue so some details to be worked out there but I think these could also be figured out after adoption great rights yeah all right thank you we see that "
  },
  {
    "startTime": "02:01:05",
    "text": "people already streaming out because we are out of time we\u0027re gonna take the first ten minutes of the next session to discuss adoption there has been some interesting conversation are anything place in jabber that wanted to be channeled and we ran at a time so we\u0027re gonna do that first ten minutes so please come back thank you thank you oh and remember we\u0027re in a different room we\u0027ve been around and one last round "
  }
]