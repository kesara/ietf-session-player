[
  {
    "startTime": "00:00:17",
    "text": "welcome to the March tools call people are still joining we'll give a few minutes for everyone to finish connecting if you're willing please share a video we like for this to be a meeting around the table but do remember that the meeting is being recorded and will appear on YouTube later join rate still fairly High we'll wait for it to slow down for those of you just joining I'll repeat if you're willing please share your video we want this to be a group meeting but do remember that the meeting is being recorded and will appear on YouTube later"
  },
  {
    "startTime": "00:02:07",
    "text": "all right it looks to me that this join rate has slowed so we will go ahead and get started everyone should have a link to the notes page that we are using to track the agenda and capture the minutes for these sessions if you don't have that um somebody can paste it into the chat is does anyone have any last minute agenda bashes anything that we need to add that there is not already a uh item for on that agenda all right so under Hot Topics the first thing I'd like to do is is to give a broad overview of where we are for the infrastructure transition and our planning the target date for completing this it transition has been shifted back to the end of April um we had originally been targeting the end of March but there were reasons to um defer um most notably we didn't get things in place before we got into the um busy part of preparation for ietf 119 for the community and we did not want to disrupt that the mailman 3 transition has been deferred until post 119 but we are still doing Preparatory work on etfa getting the actual M3 engine in place um updating the pluming for post confirm in particular um that and we'll be testing that over this week and next week um"
  },
  {
    "startTime": "00:04:02",
    "text": "having it fully tested in staging and ready to deploy and starting the Mayan 3 transition just shortly after um 119 we have a email relay um staged and should be going into production inside our new infrastructure for applications in that new infrastructure to cause email to get sent out to the community it's a internal detail I'm calling it out here because it's been a uh a notable step in getting things configured for the ultimate deploys um the advantage of this relay is that when we do move the entire mail processing system onto the new fabric that is all of the list processing all of the Alias processing um the applications that are using the relay will not have to be reconfigured we will only reconfigure the relay to point to the new mail processing system that new mail processing system is also in the process of um just going into Dev and staging with a goal of it being fully tested by the time we get to the end of 119 with a production deploy shortly thereafter with all of the excitement that comes with moving a um major source of email to a new sending set of ips we have notes and analytics in staging waiting um production mostly on that um mail relay analytics we will probably move during the week"
  },
  {
    "startTime": "00:06:00",
    "text": "during the week next week um or even later this week if we can get that far um notes we will wait till move till after 1119 as it's in heavy use for um people preparing for their meeting sessions already the big applications are being conditioned for deployment the data tracker mail archive the mail processing system itself um wwwi f.org um other things that are currently served by the Apache instance that serves www.f.org um we're hoping to have uh the data tracker into staging on the new Fabric in a development mode towards the end of the week um so that we can start to get uh good performance tests over it to validate that we've chosen the right um sizing for the pods that it's going to be running on in the database instance it's going to be backing it so we may transition services that are not immediately directed in meeting planning um we are looking at whether or not we can transition the new IE website early there are some private instances of of Wiki that may not have high activity that we may move ahead of time um but those that are involved with meeting planning uh we will defer and we will of course coordinate with anybody that uses these wikies before we start moving things and finally I'm in the process of building out a plan working with Alexi for how to um move the isodi map service into this new fabric so it's just a matter of um"
  },
  {
    "startTime": "00:08:01",
    "text": "technically it's going to be straightforward there are several options available to us alexity and I are talking about what the best way to approach um giving it a um enabling license key so all right um Robert has has ISO de got other customers who already use this in the web or or Cloud environment yes and one of the things that Alexi is looking into is whether or not we can deploy this thing in kubernetes or if it needs to be deployed on its own ec2 instance directly and that's he's looking to see what his um um what other experiences um are telling him so okay thank you all right any other questions about where we are on the infrastructure transition and the transition planning I guess there is one bit of information that I did not capture in that update that I will call out now is the um we will be moving um the server that supports the RPC and provides the um origin for RFC editor. org and the related websites at RFC editor. org um on a different fabric um I can provide details on where um off call if anybody is particularly interested uh for the short period of time that it's going to continue to exist until we have the new modernized"
  },
  {
    "startTime": "00:10:05",
    "text": "um applications in place for the um RPC to use instead okay I'm going to move to the next topic um do we have there's Greg is Roman available on the call I don't I was looking I did not see Roman yeah yeah so um I I was waiting to hear back from him I can give a quick update verbally if that' be helpful please uh the short version is that I think the is this is an isg um item to add a um Banner at the top of uh IDs in data tracker to make it clear what their status is and I think there was agreed upon text by the isg um I had um made a suggestion about the pointer to um those included in as part of that html text and the isg has discussed it but I don't think they've come to a decision yet on it um about whether or not to change the uh the suggested pointer to something else and what it should be so um uh I think Roman is the token holder on this so can chase him down and he's you know it's a very busy time for all of the isg but for Roman in particular so we'll move that along when we hear so the next topic um that I wanted to discuss was the resolution of the issue that we introduced when we deployed feat RFC with the sudden appearance of email links that looked like RFC innnn at um i.org um there's an ongoing and longer term discussion about"
  },
  {
    "startTime": "00:12:00",
    "text": "what email addresses we should provide to reach appropriate parties for the maintenance of an RSC but for the short short term we are reverting the behavior to what the data tracker had done in the past for an RFC which was if the RFC came from a draft we provided the draft link um that would reach all of the interested parties um for that draft so it's a a little bit of a back step but it gets us to a spot where we where we're not advertising email addresses that don't work um and puts us in a a the the same state for those email addresses that we were at before we deployed um feed RFC this has been merged and should be in um make it into production at this week's deploy which will be towards the end of this week so Robert the other thing that's related to this is when somebody puts in an Arata the um email addresses that are sent that the Arata has been entered are related to the ones that are at the bottom of the RFC is there any way to grab those and do the same thing the long-term plan for mail from the RFC editor is to pay attention to what the data tracker knows about current addresses for people and those addresses the authors of the rsc's would M map back into people um so that we can use the curated set of addresses all the way to an including not bothering um deceased authors when the um survivors have asked that the addresses is um no longer get"
  },
  {
    "startTime": "00:14:01",
    "text": "email um the Arata system in particular is still in a um state of discussion for what the the next variant of an AATA system would look like um but either way the the direct answer to your question is yes we can do the right thing with these things when we get to the point um and our goal is to make sure that we are attempting to map whatever addresses were in those rsc's into the freshest addresses that we have for the people related with them so yes but not for a while yes but a while might not be as long as a while sometimes has appeared um in other things this is all part of the um tools modernization project and we do really want to have um this done in some small number of months so any other questions on that front all right the next item I want wanted to call out was um because we had some strange mail make it into the archives that was um it was a a Spam message sent from a a well-known participants address and we were investigating it how that particular Spam made it into the system um we found some things that were going that we will be correcting but in the process discovered that mail sent"
  },
  {
    "startTime": "00:16:00",
    "text": "from the data tracker the announce tool the um pretty much any application that's currently running on the existing infrastructure sending mail directly from the infrastructure um both on I the our primary server I IA and on the RPC server our CPA uh the mail going out is not dkim signed and recently this has become more of a problem as the major male silos have been closing their filters um making them more restrictive and attempt to protect their user base from spam so we will be addressing this um carefully after 119 I am reticent to actually make the changes before 119 in case we accidentally disrupt something that causes meeting traffic to not flow well so um that's been identified and changes around it are coming anybody have any questions about it all right this next topic um is one I'm hoping to get uh quite a bit of community feedback on we have a general um strategy as we are evolving our infrastructure to rely on the internet archive for reaching very old things instead of continuing to carry very old things forward ourselves in particular particular we are very close to turning down"
  },
  {
    "startTime": "00:18:02",
    "text": "www 6.i f.org which has existed um continued to be published for um quite some time because there were some proceedings links that required moving into it and artifacts like isg minutes and appeals and um statements the statements I some of that had moved to the main ww website but we've recently moved all of it into the data tracker for use of future maintenance we believe that we are down now to the point where we can just turn ww6 off we're doing a final verification looking through the URLs that need to be preserved to make sure that they redirect to the appropriate content but I expect by the end of April um ww6 will be turn turned off and as we are transitioning the main website um to take load off of the transition we're looking for parts of the current www website that um we really don't need to maintain one of those is the old 2009 we're expecting to turn that off as well we'll find other things as we are moving along and we'll make a note about them um being turned down I wanted to point to this with a very bright Spotlight to get any feedback from any community members that thinks that we might be doing the wrong thing would it be helpful if I gave a little update about crawling those sites into our yes please um so let's see so just for context for everybody um we're using a service that the internet archive"
  },
  {
    "startTime": "00:20:01",
    "text": "provides called archive it um and basically it's a set of tools that allows anybody to set up crawls of domains uh into whatever collections eventually that content ends up in the public Wayback machine um at web. archive.org but that can take a few months so it's uh that is not predictable to me but um what is predictable is is have we done the archive it CW correctly so initial crawls of uh like ww6 uh has uh happened um but the way these crawls work generally speaking is you do a crawl it takes several days to do the crawl because we don't want to like take anybody's website down or whatever uh so you you do several days of crawl and then it takes about a day or two to kind of populate into a local version of the Wayback machine then you go through and you do QA and you figure out how you messed up like what didn't get get captured etc etc the QA process takes a relatively long time because basically you're you're kind of trying to click through every possible link and figure out where the problems are and then you create like okay here's a patch list uh a patch list of seed Earls that and then you go crawl them and then you QA it and then you so you get the idea so it can take um a few weeks to get through so we've done the initial uh crawl that ended at the end of last week um and so the this week I'll be doing the kind of the QA and try to get a patch craw started and then next week I'll do another round of QA that's kind of where we're at right now and Robert obviously I will let you know when I I am relatively confident that we got it awesome anybody have any questions any input everybody is comfortable with the general strategy all"
  },
  {
    "startTime": "00:22:00",
    "text": "right then we will move on to the next um session I'll let Jennifer talk to this one okay um so this is an update of I guess before uh ITF 117 when we first introduced the asynchronous submission end points um we did give a report on how the how they were being used so as a reminder uh the old data tracker up before that uh when a submission came in it was processed before the HTTP reply uh came back um which particularly caused problems with Cloud flare timing out um if drafts took more than 30 seconds they would fail so um we've uh updated the stats on who's using which end points um at this point the UI itself is only asynchronous and the API submission that is used mostly with uh GitHub track uh drafts um has both the asynchronous API that people should be using and the synchronous API for people who are still doing it the oldfashioned way um we've had about 2500 uh the numbers are here uh submissions since uh things opened up again during 118 um and the overwhelming number of those are asynchronous which is great um there were 10 drafts from I think four maybe five authors uh that came in the old way um so I think based on that we're going to be retiring that end point and uh people can you know can get with the program hopefully um I don't think that will shouldn't be prohibitive things still work more or less the same you just get your response in a different way um and we have numbers here uh nice to see that almost 89% of"
  },
  {
    "startTime": "00:24:02",
    "text": "submissions have XML um and most of those are XML only submissions um and it looks like uh I guess we're getting about a quarter of those coming in through cram down um as their origin um I happen to also do a look at the last couple weeks before this deadline that that passed on Monday and of those more 40% were cramdown based which is perhaps an interesting uh split of the data at least people who are doing it last minute are are using markdown instead of straight XML but uh um but I think the big thing is that we're planning to shut down that synchronous API um so I think that's what I have to say I don't know if Robert wanted to add anything else no I think that's that's good we will send a note to the people that submitted using the old API so that they know that their next attempt using whatever tool chain did that will fail until they update the tool chain all right we're now past the uh what I considered hot topics um for the FYI section essentially what we have um left in the notes um I would like to approach these as people have had a chance to read them ahead of time and if anybody has any questions on any particular section in here or there is anyone that has a highlight that they want to call out um to just do so um ad hoc at this time instead of doing a linear read through"
  },
  {
    "startTime": "00:26:03",
    "text": "I guess I will call out since Nick is not here that if you have not been watching the development of the editor for the RPC to use um it is moving rapidly right now um it's pretty got a pretty exciting UI um encourage people to um take a look at it this would then take us to open discussion and any other business hi Dave uh yeah I will just make the announcement I posted to RFC tools thank you Greg for responding that uh my submission to the open source Summit to talk about um uh authoring editing tools where the audience is other sdos uh was accepted I'm one of five speakers and so I still got to do the slides for that but uh and I will my plan is to uh share the slides with uh this group beforehand so you guys can review them uh but it's not until like April so you won't see those until long after ITF anyway uh but if there's anybody else that wants to contribute to those slides or whatever let me know but otherwise just expect that you get a chance to review them uh before the summit itself but right now I'm heads down on ietf so again you won't see that for a month or so but uh at least uh Hey that'll give us some great abilities and uh the the focus will be on authoring and editing so what so basically the fact that people like author in cram down and then XML is the authoritative format there's multiple formats that get generated from that and there's tooling for these things that's the type of focus that I think is going to be interesting to other stos that because there's some things that we do that they can probably copy from if they're uh much newer than say we are so that's my tent that's awesome thank you for all the effort you're putting putting into that"
  },
  {
    "startTime": "00:28:05",
    "text": "Dave any other business if you haven't signed up for the Cod Sprint already and plan to attend please do so I hear someone unmuted yeah uh yeah this is Rich uh I noticed Michael in the chat said hey where would we see the RPC editor and I'll repeat the question oh okay now it's been answered y I assume Michael was asking about the uh software editing tool that Nick is working on I yes put a a link to the the repo thanks all right well safe travels for everyone that is traveling to Brisbane I'll be attending remote um we'll see people online between now and then and looking forward to an exciting meeting well one last thing we typically cancel the tools called that's immediately after an iatf meeting I'm going to leave one scheduled for the moment anticipating that we will probably cancel it but if something particularly interesting is going on with the transition at that time I'll leave the spot reserved so that we can use it so okay well thanks everyone for participating I appreciate you taking time out of your day and for everything that everyone here does to make the internet better for the rest of the world and we'll talk with you all soon"
  }
]
