[
  {
    "startTime": "00:00:04",
    "text": "here and oliver good so we should be ready whenever time comes okay everybody it's 7 a.m somewhere uh this is the cider ops meeting at ietf 112. ideally you're here for the fun uh i'm chris there's cair and natalie as well um i think we have a note well everyone should be able to definitely read that eye chart all the slides should be downloadable in the meeting materials site if there need to be updates we can make those uh after the meeting's over i suppose to the at least to the archive uh we have two things on the agenda plus some updates about drafts i think we have the jabber and we have the jabra scribe covered with the chat thing i think we need somebody to take notes somebody to volunteer"
  },
  {
    "startTime": "00:02:07",
    "text": "ideally the note taking is in the notating tool even yeah uh quick i can i can take notes but i would love if our favorite notetaker can keep an eye over my shoulder george would you george my browser okay if not i can just go ahead i'll keep an eye natalie this is caleb thanks q okay oops oh no speaker working for for george oh maybe we have to be able to him or something okay anyway this tool is very confusing so for all the drafts that are currently uh in flight or waiting for review i'm not gonna go through every single one a couple highlights there's a three pages of these so the 6486 biz is off to the iesg there's one other document i believe that was pushed forward to the iesgo just the lta use cases which i keep remembering is being briefly pushed anyway um the rest of these are kind of waiting there's a couple group last calls which either need to be issued the has no identity i think had an update since our last conversation so that needs to be make sure we're okay with it so that off the working review or sorry work group last call um there's a question about the 8210 biz i"
  },
  {
    "startTime": "00:04:01",
    "text": "think we were talking about sending that to a group last call in the last meeting then there was an update since then i think if the authors can speak up on the list about it we can decide where to go from there sorry for jumping in but for asp documents there will be at least one set of updates for both a profile and a verification procedure before we can just start speaking about working group last call okay cool then uh some rov timing is is has a work group last call pending which i think will decide upon at the end of the meeting time and same for max length rpa max length i think unless anybody has any questions i think we're basically okay with these going once going twice is that a question ah okay all right so i think we're ready to do slides i think the slide makers can present their screen uh provoke video send video okay let me stop sending my video okay oh there's ben um i think you need to allow me to share my screen yeah let me see oh there we go all right again super confusing how to use the video thing here"
  },
  {
    "startTime": "00:06:00",
    "text": "how's the font size small size other people find that unsatisfactory seems good okay um so during the last meeting um in a couple of on a couple of topics um the fact that i had this kind of implementation um for creating and signing and reading various rpg objects came up a couple of times so i thought it would be worthwhile talking a little bit seeing as this call pretty much has everyone in the universe that's likely to care about it on it um so this is really just an opportunity to you know tell everyone what it's about and if they're interested kind of you know what it does and how they can contribute um so a little bit of background it started out life really as uh as a personal itch that i had to scratch around two problems um the first one was that it's always kind of bothered me that in order to read the objects that make up the rpki we either need to kind of crack out of browser um and look at one of the various kind of tools that online tools that people have built over the years or you have to really be a master of the openssl cli tool and the name rpkin mansa came out the fact that i always felt like i was doing some sort of black magic um whenever i was actually trying to read one of these objects using kind of the open ssl cms and asn 1 command line options which i can never remember and i need to spend an hour refreshing"
  },
  {
    "startTime": "00:08:00",
    "text": "my memory before i can read anything um but that's not what initiated actually writing the thing what actually initiated writing the thing is i was um part of the one of one of the authors on the sign checklist draft and we were going through a few iterations of the asm1 module and i wanted because i'd never really done anything hands-on with asm1 before i wanted some sort of tool to add to a kind of a ci pipeline that would allow me to you know and allow me to know at commit time whether i'd broken anything in the syntax and whether the syntax that i was i was writing was resulting in an object that could actually be written out to the disk and first of all so y'all had previously um created a demo object from the first version of the asm1 module that was written for the draft and my initial attempt was basically trying to script what he did and after a few minutes of him explaining what he did and how he did it to me i gave up because there was lots of manual twiddling of text files and lots and lots of different stages which failed in cryptic ways if you got it even slightly wrong and most importantly even with all of those steps couldn't deal with the untouched asm1 module with its imports and so forth um that was going to appear in the craft what i wanted to do is make sure that the one that was actually in the craft was valid um so the second thing i tried to do is i started trawling around some open source rp implementations discovered to my slight surprise that nobody actually uses the asn 1 modules that are published in the rfcs to generate any code um or at"
  },
  {
    "startTime": "00:10:01",
    "text": "least most don't do it at all i believe that fort does do it a bit but with some fairly heavily patched um asm1 modules in order to get things to work and so then i tried to go out and find a you know relatively widely used asn 1 to c compiler called sm1c and it just couldn't cope with any of the dependencies it didn't like the x509 modules it didn't like the cms modules it didn't like really any of the modules that get used in the pyramid on top of which um some rpk is an object to build so that will rather came to nothing and so i decided that i either needed to not have such a tool or i needed to write it myself so the first thing that i needed to do um because i wasn't really in the market for writing an asmr compiler from from scratch because i didn't really need you know a particularly performant as one compiler what i needed was something fairly vanilla flavored but that which was um which was out of the box gonna work with some of the slightly more outlandish syntax constructions that you find in in modules such as uh 5912 and stuff like that um so i did a lot of searching um there's quite a lot of different asm1 implementations out out there most of them are not what i would call an asm1 compiler they don't generate code they expect you to hand write data structures and what they'll give you for free as things like a ddr encoder and decoder but finally i came across this library called pie crate which i'd never come across before and it has some fairly ugly warts um but it was very very feature complete"
  },
  {
    "startTime": "00:12:00",
    "text": "and so i decided that although i'd started this off not really wanting to write this thing in python pycrate is a sn1 to python compiler and so it seemed to be the only way forward and that's what that's that's what gets used under the hood for the sn1 compilation part of this and so with that in hand i began kind of scratching around and trying to build a library that would would meet kind of two objectives i wanted to be in a position where given only a asn 1 module with a content type instance definition and a very very simple class implementation corresponding to a to a particular object i wanted to be able to instantiate any arbitrary new rpki signed object with just those two things to and try and keep them the boilerplate to an absolute memory um and i think mostly i've succeeded in doing those um because python's an interpreted language it kind of works trying to do code generation and python is a bit weird you don't have a discrete compiled time step like you have in you know c or c plus plus or rust or go even where you've got an opportunity to take some sort of external data and generate code from it before a compile step and so a lot of the work that i did was getting um getting asm 1 modules to be discoverable and compilable at runtime which makes the whole thing much much easier to use as a library user as a cli user the downside is it makes the tool terrifically slow so this is not something that you should use should be used in performance sensitive situations"
  },
  {
    "startTime": "00:14:01",
    "text": "the other thing that it does which i believe is unique as far as i've been able to find out there i'm sure someone will correct me um is i wanted to be able to um i wanted to be able to auto discover the different instance definitions that exist in any of the modules that i've compiled of content types so i know what can validly appear in a cms data structure at run time um and that also works which is it requires some fairly obscure and fairly recent python features and so unfortunately rpk answer only works on python 3.8 and above as a result of that but i think it's worth it because that allows us to keep boilerplate to an absolute minimum and it also places the um the onus of correctness much more strongly on the side of what's in the asm one module that you're looking at rather than what's in the python code that the implement is writing so it includes implementations of taca and entity resource certificates um it can also write tal files um it's in the base um package in the kind of the main archicad answer package there are implementations for manifests rowers and ghostbuster records um and it ships with a fairly simple but quite extensive cli tool which has two sub-commands rpkin cam conjure creates a kind of a local publication point on disk um and everything kind of ships with default so you can run literally just that command and get a whole you know get a ta with its publication point and manifest and crl um and then a subordinate ca and a bunch of stuff under that um"
  },
  {
    "startTime": "00:16:00",
    "text": "and that makes for very kind of very quick and easy object generation if you're just trying to spin up a quick prototype to see if someone else can read it i'm sorry i've gone too far and then the second part of it is the perceived command which is um a decoder and data dumper which just dumps signed objects to standard out and it can it can dump it out in sn1 value syntax it can dump it in json it's possible to provide methods for custom formats um like there's one that i've written for the rower object to allow you to actually look at the ip addresses as ip addresses rather than the weird um kind of der bit strings that are used in the actual encoding um and it's got a plug-in architecture that borrows quite heavily from the python setup tools ways way of declaring plugins which allows plugins to declare to declare the the asm1 modules that they ship with the signed objects that they can create and also extensions to the rpko and conjure subcommand and there are existing plugins both of which i've written for the rsc object and from for the for the aspa object and there's but there's a working branch for the aspa object for the most recent um proposed change to the profile that's been discussed recently on the list and which um has i believe been implemented in the most recent uh version of krell so that should interrupt i'm barring a couple of oid changes"
  },
  {
    "startTime": "00:18:00",
    "text": "um now the reason i the the the things that it i think it can be useful for at the moment um as i say the first thing that i wanted to do is while i was writing internet track i wanted to validate the module that leadership i didn't want to you know have someone come back to us after we've published a new version of the draft and say actually you know having a second there's some sort of fundamental invalidity in the sn1 syntax and it's used for that today and fairly successfully by the looks of it um so far russ hasn't come back to us and told us that one of our sn modules don't compile um and similarly it can be used for object prototyping so during the development of a new um of a new object type it's frequently useful to quickly be able to dump an example of that to send to someone to see if they can read it and that's been used successfully a couple of times for signed checklists we've um confirmed that the objects that get created by a rpk announcer can be read by both rpki client and another prototype implementation that tom harrison from ap nick built um and for the the the kind of the work in progress new version of the aspa profile i've confirmed that the objects that i create can be read by by uh micro the other thing that it's useful for is recreating and and finding bugs in rp's and ca implementations and it's actually used successfully for that a couple of times as well which i'll come to um and what i would like it to be a little bit more usable for but can still be used at the moment for this is to do integration testing and and kind of software acceptance testing for um rpm and ca"
  },
  {
    "startTime": "00:20:01",
    "text": "implementations and of course you know the the original literally i had to scratch you know ad hoc debugging of of objects if i none of the because an rp is by necessity fairly strict on what it ingests um it's quite difficult to extract useful information about what is wrong with an object from an rp because it will kind of give up passing as early as possible um and so having a tool that doesn't do any validation beyond the asm 1 syntax validation um and just dumps the contents to your standard out is potentially quite valuable and so far we've found at least two you know actual bugs in the real world using this um the first was a issue that affected i think multiple rp's but i only know of the the actual issue number in fort um where it caused a crash and this was caused um by a um ca's manifest that lists itself on its manifest um which causes a loop and and and in this particular case it resulted in a double free um the second one that we found was during that interrupt attempt that i mentioned a second ago i was sent a demo object which had a common name with common name attributes in the subject name which was greater than the 64 characters that are actually allowed and um answer refused to eat it and that points out a pretty important um a pretty important aspect of the approach here which is that because the only validation that takes place is um is based on the asm1 modules"
  },
  {
    "startTime": "00:22:02",
    "text": "none of the none of the crypto stuff is checked but what is checked quite exhaustively because of pi crate support for constraints is all of the obscure constraints way down the dependency stack that nobody kind of thinks to check when they're writing this stuff by hand and i didn't know before before i rp commander refused to eat that object i didn't know the common name had a maximum length of 64 characters but it turns out it does and that's been confirmed in a couple of places now um and there's a there's an issue open in rpk irs to address that as well i'd like to just quickly um do a walk through um for the benefit of anyone you know who might want to use this at some stage um of what a plug-in for rpkyomancer looks like to implement a new signed object in particular um in this so all of this um all of this repo i've got up on github and there'll be a link at the end of the presentation so that you can find it inside our mounts of poem i've implemented just today a very very simple rpi object which allows you to sign a poem with an as number and this is implemented in rpe commands with really just three files the first is the asm1 module itself which is in rpk immense power [Music] and looks like this um most of this is boilerplate the actual object itself follows that syntax a version that defaults to zero and as"
  },
  {
    "startTime": "00:24:01",
    "text": "number and a poem which is a utf-8 string of a maximum 200 characters in length and a very very small python module which actually implements the object we have to import a bunch of things but what is on screen there is the whole implementation um and all it's doing is telling it which um which uh oid to use for the e-content type what syntax to use for the content what the file extension of the resultant object should be and it needs properties for ip resources and is resources and the reason it needs that is because when you create um an instance of this poem class underneath here it will automatically um it will automatically generate and um generate an identity certificate with the necessary resources in it um and wrap the whole thing up in a um valid cms signed data structure and so really only all the the only kind of logic that you need to add is a mapping from a bunch of arguments which will vary depending on the intended use of the object to a simple python dictionary which kind of bears a direct resemblance to what's in the air someone and armed only with that you can immediately generate it because all of the heavy lifting is done in the um in the asm1 parser and an encoding logic"
  },
  {
    "startTime": "00:26:04",
    "text": "there's then a plugin for the cli tool which is similarly simple um again there's a bit of boilerplate up to the top and a default value for the poem which i stole from the right who is and in order to implement a cli extension one just needs to subclass this conjugate plug-in class set up to find this inner paths for method which sets up whatever additional command line arguments this plugin wants to receive and then define this run method which actually holds the logic for interacting with the library to create an object and as you can see this really is just garbage in and garbage out two command line arguments one corresponding to the as number and the other to the poem and those get passed straight through to the poems object constructor in order to use it um first we need to set up a virtual environment so it's not you know start installing things globally so in fact we don't even need to do that because i have one ready to go so in in this virtual environment i've got a few things installed the only dependency that the plug-in has is rpk manager itself all of the rest of these are pulled in by rp firemancer and it's not a long list of of dependencies fortunately pi crate has no dependencies for the feature set that we that we need for this um it's all"
  },
  {
    "startTime": "00:28:01",
    "text": "written in pure python which also makes it very slow and so once we're set up and ready to go we can issue rpg and can't conjure and you can use dash v to turn up the debugging um but we went for this because it's already a bit chatty and you'll see it prints it it will always print some warnings um that is partly because of some slightly weird constructs that are used um in the picoix sn1 modules that the pie crate struggles to deal with sanely and also because all of the it complains about having to remove all of the default version numbers that are set to zero so you'll see things like this repeated but those are all fine and that exited successfully and it's created a um a directory here which contains a very very simple um directory structure that you would expect to find on the publication point or in the cache of your favorite rp so it creates a towel for a trust anchor called ta.tau and it creates a repo for everything under that trust anchor the ta's root certificate its publication point which which contains its crl its manifest and a subordinate ca and then that ca is um publication point which contains its crl its manifest a rower a ghostbusters record and this funny rsp object that we just created so to look at for example a manifest"
  },
  {
    "startTime": "00:30:02",
    "text": "you can do rpg and can't perceive targets manifest and that will dump the the ta's manifest this is in sn1 um value syntax if we want it in json for example we can just do dash j and we'll get an equivalent thing back out um the json one doesn't um decode the the uh the oids and stuff like that for you which is inconvenient but it's otherwise quite useful if you just want to extract a particular value using a tool like jq or something like that um and then to have a look at our our new object we can just search for any rsp ones because we know there's only one of them and this time we can have a look at the whole um encapsulated um content structure the encapsulated content info value and you can see we've got our new content type our e-content type is a content type instance of type poem and um we've got our asid which just defaults to 65 000 and nigel's poem which is complaining about the uh the state of rpsl and right 181 which i thought was appropriate for this um one last thing to just point out the way that the plugins were implemented um"
  },
  {
    "startTime": "00:32:01",
    "text": "as i mentioned it use uses um set up tools for what are called entry points which people if if anyone has written a console script in uh in python before they'll they'll be familiar with this um but essentially this this plugin simply declares it uses rpg manager.asm1.modules to declare where to find its asm1 module um it uses rpcoms dot sig object to say where to find any signed object types that it um implements and similarly cli.closure to declare any plugins that it supports and so it's all you know it's all run time discoverable and whatever you have installed in your environment is whatever the various tools will have available to them so there's a few things that i still have left on my list to do um and if anyone wants to contribute to they're very welcome um i'd like to implement bgp security certificates um i don't believe that it's going to be true i think it's going to be pretty trivial to do that um it's just a question of finding the time um at the moment the directory structure the directory structure that is um generated by the cli tool follows the directory structure that was used on disk by rpki client from a couple of versions back and what i would like to do is have a similar plugin architecture that allows you to output a directory structure according to what is expected by whatever rp you happen to be trying to read this stuff with um and that's in order to try and improve the the integration testing experience a little bit"
  },
  {
    "startTime": "00:34:02",
    "text": "i'm in two minds whether this one's a good idea but it should be fairly easily possible to generate the necessary xml files to synthesize a rdp service locally and i'd also like to implement something like the diff tool for signed objects because you know looking at them in hex it's not particularly helpful on looking at them in text form it's not particularly helpful it would be quite nice to have a structure aware diff tool for these um so that you can see you know what changed between two instances of the same ca's manifest for example um and then the other thing that i'd like to do is is just make a template available to people who are implementing plugins so that it's easy to get up and running because as i say there's i've tried to minimize the amount of boilerplate but it's certainly not boilerplate free um and as i mentioned any help and suggestions and pr's and you know criticisms are welcome specifically i think the areas where people can help um that i'm not in a great position to to do um for anyone that's implementing a ca or an rp um i'd love to have some feedback about what a good what a convenient way of serving the data that this tool generates to a running a locally running rp um looks like is it to spin up a kind of a dummy rsync server or rrdp server on localhost is it to output files directly to some cache directory in a particular structure um is it all of them um you know do people want to do people have significantly different code paths that it matters whether something is retrieved from the network or just on disk when when startup happens um that's feedback that i'd really like to have and if the latter it would be great to have um"
  },
  {
    "startTime": "00:36:02",
    "text": "own the plugins for their own directory layout so that for two reasons firstly it's difficult to guess as a you know as a as a third party when or why something might change and also it prevents you know that having to become part of the public api and make stability get guaranteed you know if if rp implementers are simply shipping a new version of the plug-in for each new version of the rp then nobody cares about the stability of that layout and you know what what what does a good test harness for this look like more generally um whether on the rp side or the rpk manager side um what can be done to improve that integration testing experience and in particular one suggestion that i have is that none of the rpgs from what i've seen have particularly useful or machine readable logs to work out why something's gone wrong and i think that would be that would be a helpful area to do some work on for authors of internet drafts that define new signed objects write and plug it um i've just shown a very very simplified one but the checklists and aspa ones are maybe a dozen more lines of code than that that dummy poem one that i just i just showed um and in the case of checklists the um the the plugin that implements that signed object lives in the same git repo as the internet draft itself and that has the advantage of being able to keep the version of the plug-in and lockstep version of the draft and it also has the benefit of being able to unit test the module that gets shipped in the draft using the plugin um and so that that tight coupling works quite well and if you have a look at"
  },
  {
    "startTime": "00:38:00",
    "text": "that git github repo you can you can find an example of how to set that kind of integration up that's all i've heard sorry i've been waffling on for so long um but if anybody has any questions then please ask either now or by the issues of the mailing list or wherever else you can find them thanks yep good hi i think this tool has been incredibly helpful uh and it has helped discover numerous bugs so thank you very much for putting in the time and effort to create this you're welcome as a separate comment i you know i think being able to look at the difference between manifests and rdp lists and or diff files and content for particular objects over time is certainly very useful so it will be nice to see that so the being able to do a diff between two um two kind of known versions of a given object should be fairly easy to implement i don't expect that to be hard um the difficulty with making that kind of a temporal view is how hard it is to store snapshots of what the rpkr looks like over time um for a while i had a github repo with a bunch of automation that would run an rp periodically and just store the cache um and kept that for for a while but that thing within a matter of a few months grew so so big that i started getting"
  },
  {
    "startTime": "00:40:01",
    "text": "emails from github telling me to turn it off um so i don't think that there is that that's the missing piece of being able to do that easily and at the touch of a button um and i'm not sure that i have a good solution for that and it's certainly not this tool okay perhaps some thought process and uh meaningless conversation about how to keep historical track of the rpki like we do with bgp and a couple of different places would be useful so i i've got some ideas as to how one might achieve that um the the issue is you end up with a phenomenal amount of object duplication because of manifests and crls rolling over all the time um and i think that being able to track it on a more granular level than the file itself down to the entries in the data structure would eliminate that duplication um and probably give you a diff for free actually if you got that right but that's that's a much that's a much bigger that's a much bigger job than you know a a small python library like this is is going to solve it so it's a bigger bigger question yeah sorry what i meant was um oops i guess job says rpgiviews.org but what i was suggesting is something along the lines of like the route views and ris storage mechanism for bgp data some thing to hold over time there are pki system as well but also yes i think that's what job has built basically with rpk views um"
  },
  {
    "startTime": "00:42:01",
    "text": "which is a kind of a series as i understand it's a series of archives um but what it doesn't tell you a lot about is what the uh um it tells you about the snapshots doesn't tell you what happened in the middle um which is unfortunate because usually it's the middle where something broke yep i think we maybe one more question after this and we could skip over to oliver if we want to track what happens in the middle i think we need certificate transparency that's perhaps a topic for the next ietf meeting look forward to some slidewear um and discussion uh i think unless there's any other questions uh so if maybe i could just respond to these comments in the in the chat quickly um i i i agree in general i think having hooking a a test rp to this um this data is probably easiest to implement if you use one of the the retrieval mechanisms like rrdp or rsec running locally the problem is that because you need to embed the urls in the actual objects themselves you end up needing to kind of spoof your own machine's dns in order to do it um which can be done totally but it just kind of feels like a little bit of an overreach and it feels a bit clunky and prone to breakage on you know cloud ci systems which is but that's exactly the feedback that i was wanting to get um i suspect that being able to do both is probably going to end up being the right answer but the question is how and who looks after that code"
  },
  {
    "startTime": "00:44:03",
    "text": "tees yeah ben i agree there it's fiddly to set up and it's kind of an investment but once you get it going it's really nice i agree that you probably need both yep okay thank you thank you very much man that's cool oliver so share my stream or screen share starting so you should be good to go i rotate the screen now i have to move this one over [Music] did the rabbit hole move here we go okay so you see the main screen or the slide screen the slides looks good good okay so this year we decided to go a little different route than what we do in the past and participate in the hackathon and in particular what we were looking into is to develop tools and data sets for testing the route leak mitigation techniques um so we the the project was relatively small because we said okay let's we want to have a project that you can start on monday and finish on friday uh in parallel to all the other work what we had to do and so what what we what we what we did was or"
  },
  {
    "startTime": "00:46:02",
    "text": "so yeah we as you know we have the nest bgp srx software suite what we back then developed for first for the origin validation and then later on also for the digital path validation and now where we are talking about the asba verification we also added this implementation to the software suite we implemented the aspa verification 8 including some algorithm corrections that sriram presented in iit f110 um we also implemented the [Music] is three very basically uh yeah bring all the aspf information down to the router or in our case to our validation enter so what we wanted to do is we needed some scripts that allow us to generate simply and easy large-scale test data to what can be used for interoperability tests and also later on maybe for some some research kind of part so if you look at the data flow so basically that data flow take it with a grain of salt so you register your aspa object and then the validation cache gets all this stuff validates it and shoots it over to the routers and that is the area of our interest so we we have the validation cache test harness that does not do um the 509 validation and all this kind of stuff it basically takes data what we assume has already validated and sends it over to the to the router because our our part was more interesting always on this side so that was like a test time is what we use the"
  },
  {
    "startTime": "00:48:00",
    "text": "validation cache has a fairly nice plea where you can basically even script experimentations with timing and what have you so we wanted to create input data for this cache test harness and input data on on the right side of here where we get route use update data and [Music] then have all this stuff validated um so our first task was basically to create the aspa data set and the cache test harness take them pretty much in an ascii version of the 8210 best three pdu we used as um as input data the cada cada data that's a very nice data set where they go out and look into the internet topologies and try to infer peering relationships and then we create our test input looks basically at an aspa with this particular av my customer is and then the whole bunch of providers behind that and then of course also create sample bgp updates uh from router use three that was the collector what we used so what did we do we created a script that takes the cata data and formats a little bit in a different way so that we can work with that and we created around 72 000 plus asp apdus and they contain around 100 a little less than 150 000 customer provider relations uh last friday when we when we presented that to the hackathon i i had a little calculation error i said we did around 180 000"
  },
  {
    "startTime": "00:50:02",
    "text": "i i over the weekend i went one more time over the stuff and it's it's more like 150 but you know um the part is that you don't always want to test this uh this huge set of data you want to be able to down select that so we created we created uh tools where we can down select the uh aspa data depending on the updates uh what we what we found or what we will play into the router and then we created um an output what we believe is um easy to to be used if you want to make comparison between different implementations they should basically have the same outcome um so we and i showed that later so we created a couple of data sets one was 100 updates 500 updates 800 1000 10 000 20 thousand you can create whatever you want uh and and then the tools go out through the raw data and generate you a nice set of um the asba input and the bgp traffic that that fits to that so the first thing what we did was we we looked at okay how do we do one do the peering so we as i said before we took the mrt data from route 23 and we selected the table data not the not the update stream um and then we said okay we for our bgp secio traffic generator we have a slight different format than the data so what we what we did is you see here on the right is basically printout we have the prefix then our our b4 basically means generate only bgp4 updates because"
  },
  {
    "startTime": "00:52:03",
    "text": "remember it was this traffic generator originally was built to create bgb's hack updates and we are not interested in bg sec right now and um and in the past to create bgb4 updates we just didn't have any keys and had us fall back bgb4 um but this is just resource waste so we added these before that i can say okay don't don't even deal with b2b sec so it creates [Music] a bgb4 update with this route we removed so and and what we did was we basically created a file for each peer that sent their data to this particular collector but then we also removed the peer out of the data stream because so this one here for example is a data file of 701 so normally you saw 701 all the time in front of it but we created the file 701.text and then these are the updates and then bgp sec io will take on the role of 701 and play these updates and of course it puts its own as in there so so um yeah so that's basically what we did on this side and uh then on the other side the cada data so what we did we said okay let's go through um through the uh bgp update um and we we only generated aspa data kaida data or no sorry we only generated the aspa input data for uh containing um custom is what we saw in the bgp traffic um of course you could say you know what i don't care about this i shoot all 72 000 asp data to the router and everything works fine that's perfectly good in guessing but sometimes you don't want to have everything sent directly you wanna you"
  },
  {
    "startTime": "00:54:00",
    "text": "wanna you wanna reduce a little bit the data set what you work with it might be for debugging purpose or or other things and then we created the test traffic so as i said before you you saw pretty much this file but it had real real prefixes and in acep we are not really interested in the prefix we are more interested in the past so what we did was we went through the if i want to say i want to have 100 000 or i want to have a thousand or ten thousand rounds uh we are interested in 10 000 unique routes so we we prune the the egp traffic data one more time we throw away all the prefixes we and then we said okay we only take the unique as path and then eventually because we need prefixes we just um we we generate some synthetic prefixes they start by zero zero zero 0 0 1 0 24 and then just run down with a counter and it could go up to 255 to 55 to 55 the why are we doing that because we want to make sure that that we get every path in because even if we don't do that we would for example keep the the original prefix is what we have in there and we would say okay select uh um the first as path what you of of of this lookalike the chance could be that you have duplicates if you don't take the table dump but if you took the bgp live stream and because we don't want to rely on the that you use the table down we said okay let's let's deal with that that's easier way it's it's true i know but it it does"
  },
  {
    "startTime": "00:56:01",
    "text": "the job and then we uh we again this whole thing is done automatic so i always like plug and play i always like you have one or two scripts and it does everything in the background for you because you don't want to waste your time generating the data you want to waste your time or you want to spend your time in and testing yours your software or creating data for analysis so we have a tool that is called generate data and then i give my prefix in this case 701 and i say give me the first 100 unique as path and then it goes in its file based database thing or it's not really database it's basically a directory where you have a file that calls 701.tx and then it generates out of this all the things and uh it creates the the the import file we call it dot uh the extension bio that is for the bgb sec io that contains the updates we have one file that contains all the unique asn's um we could have deleted it again but we kept it in there just sometimes you want to know what are the as is what you use you don't want to have the unique number of that and then we created out of the big 72 000 uh [Music] aspa data we created a downsized cache file and the the files are generated like the s number what i use the number of updates and then it tells me what it is so this is fairly straightforward um then we started our oh yeah and then from the scripts there are two ways how you can start the experiment you can start it from a from a console where you just remote login or you can start it preferably from from all on linux"
  },
  {
    "startTime": "00:58:00",
    "text": "from a windows system if you use the terminal only then uh what it basically does it just starts all the all the modules the um the cache test harness the srx server for the validation the quagga router the bgb sec io and runs everything in the background it redirects all the output to standard io and arrow into into log files and um and it works fine but the problem is sometimes you run into issues and then it's really a pain to or you or you want to manipulate a little bit our cash test harness has a cleave where you can uh add and remove data on the fly and uh you cannot do that when you run it there so therefore we have the gnome terminal that is a preferred one so you start it it it moves or it starts every module in its own tab and you can easily then switch between one tool and the other one so that looks pretty much like this so i start the stuff and here i have my start service i say use my terminal the minus w basically means uh in case one of these crashes normally the the tab would immediately disappear the minus w says we ask for a key input before the windows closes so that you have a chance uh to look into this window and see what what happened that is especially important when it comes to the traffic generator because sometimes uh when bg when quagga srx is not uh ready yet and we start this too early it uh it tries a couple of times to connect but if something goes wrong in the connection this one just stops and then you want to know what's going on and you might have to do something in timing or query for open ports or what have you and there it's nice to see okay did it actually run or did it crash if something is not coming out the way you want um"
  },
  {
    "startTime": "01:00:00",
    "text": "again so you start that and then it it automatically uh configures the router so that's a nice thing the uh we have we have templates if i say i want to run it with 701 then it configures bgb secure to conduct to to act a 701 it configures quagga that 701 disappear and so forth it starts everything we have some timing in between and at the very end it asks you if you if you you press r for the resource then what it does it it makes uh it's like a 10 attack where we go into quagga and we make a show ipv gp and then with some regex we modify the output so that this one is basically the output the result output what you want to have the validation state and the as path and you can also start the service pages search service and then the parameter i think it's called minus view table or minus minus view table and then it does this one for you as well you run a little bit into problems there that when the data set is too big that we lost the connectivity between quagga and the tennet session uh we did some timing there but there's something else going on so i don't know if this one is the the best way of doing it but for smaller data sets it is very nice uh maybe it was also just my system that cracked up there a little bit so we have we have to look into this one going forward but for the time being that's that's a very good way of doing that um so what do you do we basically use the large-scale isp in the data from cada from october 1st 2020 we use the october 1st 2020 data because scada right now is rewriting all the algorithms and that's the latest data set they they provide we created a subset of unique routes and we take the cata data we performed the asbi validation and um we set the iot as a private is i think"
  },
  {
    "startTime": "01:02:02",
    "text": "the 65000 what we use and then we run it against it that versus the results but again you have to take these results with a grain of salt because one thing for example we we run it where the isp is a provider that's fine then the isp is a customer but i would would assume that if the isp is a customer it might would send me different data than if it's a provider so again you have to you can choose other other isps but we just want to show though so even even though that is um that i would be careful with analyzing this really really deep right now um you see if it's a provider you have most of it is valid uh and just a small of embedded so it just gives you already some some ideas and then this was a relatively small data set uh depending what data you it would be there nicer to have or or to to play them the whole i think the 8 000 prefixes ended up to be a hundred thousand unique routes so so but don't don't take my word for that might be that it's even a little bit more um so uh it makes more sense to to then really um look into what kind of data you want to put if you want to if you want to make serious uh research on that and again it was this data we generated in the middle of the night before the hackathon presentation so i was happy to have at least something so the code itself we will put on uh on github we don't have it on right now i don't know right now we will put it on the hackathon github part or if we make a part of the the egbsrx github um once we but it doesn't matter at this point because i still want to clean up a little bit it's a little crude so i want to have it in a way that that if you're interested in looking into that that you actually don't have"
  },
  {
    "startTime": "01:04:00",
    "text": "to fight for two three hours to figure out how the stuff works but that you have all the information needed that you can do it relatively quickly um and then we will send an email out to the list or even have it on our at least on this bgb srx github page we will have a reference to where the data can be found or if you want to have it in the status right now just drop me an email and i just wrap it up and send it to so there's this incentive now why did we do this we said it would be nice again to to maybe start using hackathon for these simple small scale projects and maybe finding others who are also interested in in doing that and tackling just a simple problem and trying to get the standards in the seven days or the five days of the of the hackathon and bring it out so uh one thing is what what i'm personally very interested in is taking really a larger scale set and then um [Music] playing with gradual deployment so for example our cache test times what we can do we can say if i have let's say a thousand aspa objects i can say play the first 100 then wait five minutes play or wait until i press a key and then press the next 100 and so forth and with this one then you could basically run and always see what is the validation output and see how would it look like if we have gradual deployment that is one thing and of course if this one is automated even better because i always prefer as little user input as possible necessary but i also want to be have the chance to give my user input if i want to so um then the other thing is um currently we made it with one peering session um maybe extending the the the scripts and this all shall all linux telescopes extending them to allow having multiple peering sessions if you want to start looking into performance uh"
  },
  {
    "startTime": "01:06:00",
    "text": "testing of the router then you have multiple peers and testing scaling scaling scaling we run in some issues in our implementation with scaling that's why we we couldn't run the full scale we found one segmentation fault unfortunately what we will now start looking into and hopefully we have it fixed within the next week or two um and then we maybe can look into really uh uh at the full table you know um we were loading the 72 000 asp objects into our cache test harness that worked uh was very quick was very nice we fed them into the srx server that went very nice as well um we just have to see that we manage our memory correctly in every little thing it might also be that my vms just run out of memory i don't know i didn't look into that yet i at one point i said you know what let's do this after idea where we have more time and can really carefully look into this stuff um then maybe it would be really nice to test this also with other implementations so you can take our shell scripts and instead of starting the srx server and quarkxrx you can start your own implementation and run it against that and it would be maybe nice to have other other reference implementation implementers to partake maybe next next hackathon to to work a little bit on that another interesting part is looking at the other side of it so taking the cata data and not only generating the the data for the pdus for the 80 to 10 videos but um creating the input data for the validation caches so that you can take your validation cache and and test it and then you don't need our test harness but we could run it against the real validator that would be something really nice too um i don't know if if i can do it right now"
  },
  {
    "startTime": "01:08:00",
    "text": "because i didn't i didn't work enough on this side of the of the project but maybe others can join that would be something really cool and i would really look forward to doing stuff like that so that's it for now um so if you have any questions speak up now or down there we have or send me an email if you have questions in general what we do in this field um we have like a a group email for us this itrg contact list mr.gov and just send it there and then we will find the proper person who can answer your question and with that i think i'm done thank you i think there's two questions for you oliver okay i think rudiger maybe was first and then alexander yes oops so am i being heard yes yes okay uh well uh nice nice nice project uh one stupid simple uh syntax question uh the syntax that you give for the aspa test data ends in an asterisk you know and ends in a plus sign and i guess it should be a plus uh i have that now it is a plus did i put an asterisk and that might be a a function of create your slides in the middle of a night i would say no there's a plus it's a provider yeah well okay yeah but uh kind of uh that precludes uh empty a as provider sets which i hope still are in aspa oh if they are in there then"
  },
  {
    "startTime": "01:10:01",
    "text": "maybe i am again so i let let me check if there if it has to be a star i hope i normally our implementation should do that i thought it was always a plus um yeah then this will be changed thank you so much thank you so first of all thank you for the report so it's good to see more testing evaluation around sp but i have a two questions the first one is about your source of the data since you were using cada as far as i understand and as far as i remember k the peering relations is based on some kind of expert set of t1 providers and the thing is that these tun providers are not in the customer to provide a set so you should have explicitly create what is called now is p0 or mta spay uh have you done it because i'm very surprised with the number of unknowns that are in yes again that's that's why i say take the data with with a grain of salt um we we did not really the only thing what we did data data we looked basically into the um the peering what they gave we didn't really spend time into really adding other things or analyzing the data correctly because for us the main the main issue was for the testing that we wanted to go scale test and having some data set that would make some kind of sense rather than just randomly generating something and um"
  },
  {
    "startTime": "01:12:00",
    "text": "again this data currently is not meant to go out and start making real in deep analysis why is it now like that or like those uh you also have to see what kind of as you you choose you know i mean here right now we we chose 701 but you know i mean if if the if the peer is your customer you would expect a slight different kind of update stream what you receive than if it is your provider um so so again i uh the the main part is this even though if the data what is is in its in its uh greatness or not that greatness the main part is that if i have two implementations that take this data as input the output should be the same so so with this you can at least assure that the validation itself the validation algorithm in the implementation is um deterministic okay uh i understand uh but i will just highlight my point that you should check how you are processing data from cada because once again what is called to1 providers will not be in this data because they are in the source of the algorithm but the data shows only autonomous systems that do have providers and the second uh comment is about the kd data itself because um everybody ever knows that it is kind of noisy you may have both kinds of uh false positives you may have the uh the uh systems that are named to be provide providers when they are not and the more important one when there are missing providers and uh such"
  },
  {
    "startTime": "01:14:01",
    "text": "a situation may turn into the invaded outcome my suggestion is to not uh using the big set of outdoor system relations but to use top bottom approach to start with simple sp records for t1 providers that's what we did in uh in our detection system and it just works there is very rare noise in this uh kind of system and uh com combining this kind of of pro of approach with uh proud use what you just did i think it can be very interesting research yeah but anyway thank you thank you for your work i mean you agreed you know it's always but for us the main part is we wanted to have a very quick way to generate large large scale data sets and we have we have handcrafted um asba experimentation what what comes with the software suite what we have in there we have a complete experimentation framework um and that that's all fine but i mean okay that was what is what is right now around i know it's not the perfect set but it always depends for what you want to do if you have other sources though that uh that we can use that maybe give more or a better data set even for looking into uh into working with um i'm more than willing to to to use them as well uh again it's it's i think it's better than just having a random number generator to generate aspart or or customer peer relationships um and yeah"
  },
  {
    "startTime": "01:16:01",
    "text": "but thanks uh yeah um so this is free ram um so answering some of uh alexander's questions uh one of them is like small data testing with us some small data we do have uh oliver didn't mention it i think but we do have um like small data also which is synthetic with experimental as numbers uh so so we can also make that available in in which case you can just do a quick test and see an output that is only 12 aspa or us 12 updates or something like that with few few asps and you can do a quick simple test so so that's available and i think oliver can make uh is planning to make no no this one is that is already available right yeah that's part of when you go to this github page oh yeah and it's part of the experimentation set yes right okay so another observation that alexander made and he he does have a sharp eye on one of the results slides which are of course like oliver said needs to be taken with a grain of salt but alexander did observe very correctly that the number of unknowns is large and that indeed happened also for the reason that alexander speculated or guessed and that is when you have tl1ass we we realized it over the weekend after the hackathon was completed uh we realized it uh we realized that for tier one ass or any ass that don't have providers uh we should have an aspa"
  },
  {
    "startTime": "01:18:00",
    "text": "as0 aspa uh so so that's going to be fixed it's very simple to fix it uh we identify the uh the um the tier one ra says that don't have providers and we would add aspas uh that have uh asps 0 in them and that would fix that problem and alexander you will see that once we do that the the unknowns will not be so many they will in fact become invalid invalid because something like the way the experiment is being done is a little bit unnatural uh the 701as which is a tier one in in the second set of experiments the iut considers 701 as a customer which is unusual and many uh so once we fix this if it's considered as a customer many of the uh routes will become actually uninvalid uh and so there will be a shift from unknown to invalid once we fix the asps to include the tier one asps with as0 in them and the third point uh picking up on uh rudiger's comment uh there is no such thing as alexander correct me if i'm wrong but from the draft there is no such thing as empty aspa there is a zero uh aspa uh which which we you would create for uh tier ones or pro or asses that don't have providers but there is no such thing as empty aspa right uh about your last uh point yes uh you're you're correct so at the moment uh the draft says that the empty should be represented with asp 0 likewise it is done with ross the in the medical east uh there is still ongoing discussion of the subject we'll see how it ends up but the syntax is correct in the slides so"
  },
  {
    "startTime": "01:20:01",
    "text": "and thank you for giving comment for my comment okay thank you thank you so much yeah maybe one thing what i i also want to say the the syntax basically is based on the draft a's 82 10 bit three and the pdu that gets sent over to the router i think has a plus in there but again i will verify that and if if i'm mistaken then we will of course make the modification um [Music] or maybe the ace right then then you will deal with okay any more questions there are some chat stuff about why and where for the data came from which seemed to clarify some of alexander's questions as well i think lacking any other people showing up at the mic line i think we're at the end of today's agenda and i would uh thank oliver and ben both for some actually pretty cool talks and i think that's it i think we can all go off to do what other things we're supposed to do thanks a bunch"
  },
  {
    "startTime": "",
    "text": ""
  }
]
