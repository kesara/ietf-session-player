[
  {
    "startTime": "00:00:01",
    "text": "model model? You hadn't transcribed that one? Didn't Yeah, but it didn't do the the the Thank you Thank you All good? Coming down from the 34th for a while. Yeah, I had a similar issue Howie?"
  },
  {
    "startTime": "00:02:01",
    "text": "should have I should have asked the lawyers during the work group chairs session that are keeping recordings and transcripts of everything we say are we licensing it to be fed into an LLNist training down Did you restrain yourself? or? I didn't think of it until I found the right now I'll have another opportunity opportunity We're going to have a, they're going to be talking to the IAB tomorrow morning. So you want to ask? Oh, that's true true Hello everyone We'll get started in just a moment. It's already 102 so we do need to get moving. So if you take your seats If you could take your seats david schinazi, take your seat Thank you Um, can you present by a chance? That's what I mean, yeah I think so, yeah Okay welcome to the second session of a HTTP we have a fairly full agenda today because we had to switch some things around. First up, we have security considerations for optimistic use of HTTP upgrade by Ben, then HB's server, secondary cert off by Eric Those are both active drafts. And then we have four other topics lined up one that we had to shift from Monday from David Then we have the no very search from Jeremy, revise and cookies from Johann and HP Resource versioning from Michael And the agenda bashing Okay"
  },
  {
    "startTime": "00:04:01",
    "text": "we need a scribe for this session Does anyone volunteer as tribute? We do need a scribe. Hi, Martin We need a scribe That's everybody's excuse alan frindell I'm glad to my questions. All right right, I feel that. That's fair. I feel that that Someone else, I see a lot of presenters and what kind of cuts them out. Well, they can always trade off. Collaborate yeah, I suppose Sure. So we have yeah, I suppose. Sure. So, so we have a volunteer of david oran early in the session, if we could find someone to do later in the session David, tell us when you need to stop scribing and we'll find someone else. How's that? When I start with that. Remind us. Thank you. Okay well, that's sorted at least for the time being Please sign into the session with the Datatracker so that we know that you're here And imagine that there's a large slide on the screen that says note well. If you haven't by this time in the week, you should already be familiar with the note well terms. These are the terms under which we participate in the IETF regarding things like your behavior, privacy, intellectual property they are very important. So if you're not familiar with them, please search for IETF, note well, and understand and read that material and what's linked from it And if you have any questions about that, you can contact us or any other IETF leadership So let's go ahead and get started First up, we've got security considerations for optimal use of HGF upgrade. Do you want to? Yeah, I won't slides, or would you let us to? OK"
  },
  {
    "startTime": "00:06:01",
    "text": "Okay, lovely. Take it away away Okay, hello HTTP Biss. That's the name of the draft draft The this was adopted relatively recently and I haven't really made any changes to it except to add a mention of Connect IP, which should be on contract maybe. There's not much to say about Connect IP There are a bunch of proposed changes that I wrote up to try to represent feedback that I got from the working group mostly during the adoption call but also then I got a lot of good comments on some of that, and I've adjusted some of those proposals so I will be presenting the latest version of the proposed changes that my best attempt to reflect possible consensus of the work group. So first I want to talk about the HTTP and TLS upgrade token. So I went and looked in the Iiana registry and the Ianna registry says there's an upgrade token called HTT and it takes a version suffix, which can be 2.0 So you can do upgrade colon http slash 2 2.0, and that is like a registered defined thing. If you want to know what it means, you should see RC 910 section 2.5 which is the thing that defines HTTP version That's the section of adversions and then there's also actually another one for tLS then that right references RFC 2817 and so I mostly want to talk about the HTTP upgrade token. There's a lot of story here, but I think the consensus of the working group at a minimum is that this is not a thing that anybody is actually doing As far as as anybody can tell, this is not a deployed thing anywhere on the end"
  },
  {
    "startTime": "00:08:01",
    "text": "internet. Nobody's told me otherwise. And so you might say, like, well, whatever, it's just some like weird text in the IANA registry, but nobody's really confused about that and who cares And I think that's mostly true, but I did find this one thing, which is called http.dev. It's a website with advice about how to use http and it's suggests that to get htp2 or htp3 3, you should use an upgrade header and specify htp2 not too 2.0, just 2. To upgrade And I would love to be able to email these people and tell them oh, and by the way, this is not like ancient history from before HTTP 2 able to email these people and tell them. Oh, and by the way, this is not like ancient history from before HTTP to existed. This page was last updated about a year ago I would love to be able to email these people and tell them they're wrong but I'm not actually sure that I can do that because I don't have any standards to would love to be able to email these people and tell them they're wrong, but I'm not actually sure that I can do that, because I don't have any standards text that says that they're wrong. In fact, I have standards text that says there's a token called HTTP and you can upgrade to it So I wrote a poll request for this draft and it says it says basically this. I'm not going to read it The bottom line is it updates the IANA registry to put in parentheses the words obsolete token token There's been a lot of comments about this, maybe mark thomas comments about it. Are you ready to? Yeah, so let's, I'm happy to stop here and ask for feedback on this change. martin thomson and some others have commented on GitHub about questions of like I'll give you an example, questions about should we even be talking about this in this draft? Can we somehow punt this into a separate draft? draft? So yeah, feedback welcome. So just"
  },
  {
    "startTime": "00:10:01",
    "text": "speaking personally, even though I'm sitting here, I'm just lazy lazy First of all, I don't think we should be acting based upon what random websites put in document about HTTP. Long experiences taught me that we will never be able to control all of the text about HTTP written on the internet and we would cause ourselves great pain if we try to do so. Having said that, I think that a convincing argument can be made if they're receptive I've had reasonably good luck with sites like the Mozilla, what you call it? the MDN, thank you, whatever they call it these days MDN seems reasonable about the things if you engage them, and I think that's generally seen as authority by folks who look for stuff on the internet other than in the specifications themselves. I should possibly clarify that that screenshot I gave you was the only case I could find via search engine of anybody talking about this. Right Did you ask AI? Bad tummy Second, the I tend to agree with Roy the registry you know, should not be updated in a revisionist fashion to reflect current practice. It needs to be a historical document as well There are many registered things that are not used and that's okay And third, I do tend to agree that if we do this, it should be a separate intentional thing, not tied to the failure of this draft. But that's just me personally Okay. Thank you Mark. I do want to point out that this is a sticky issue because uh in my view this is a quantum superposition of standard and deprecated and nobody seems to know what state it's in, but by talking about it at all in this draft, I am going to have to force it into one of those categories it's either a standardized thing with certain properties that I can discuss"
  },
  {
    "startTime": "00:12:01",
    "text": "or it's a deprecated thing that doesn't exist The only way I can avoid that is by not mentioning it all in the draft And we can do that. That's another option. We can just skip it But if we mention the HTTP upgrade token in any way, then that I think forces it into a lost slide control. Oh, no mike bishop Just responding to Mark's comment Oh, is my mic too hot there? Responding to Mark's comment, I don't think we lose the registry as a historical document by indicating that something is obsolete or was reserved and never defined So, okay and I think we should. Okay I think we need to move, move to the next slide, but I've lost slide control. How do we keep? I can remove both I can share the slides try to reshare maybe Okay, the next thing is extending the draft to cover HTP Connect. So the draft, this the draft is security considerations related to optimistic upgrade. That's literally the upgrade header in HTTP 1.1 but HTTP 1.1 also has connect and that turns out that's another case where the HGP usage of the TCP flow"
  },
  {
    "startTime": "00:14:01",
    "text": "ends and some other protocols starts using that same flow, and so you have the same problem And that isn't that surprising? that this draft sort of comes out of connect TCP but mike bishop pointed out that we could cover that as well. One interesting thing about that is that it is actually a deployed thing. And so they're are actually known clients out there that do optimist, unsafe, optimistic upgrade with HTTP Connect today which means server-side mitigations might actually be warranted in this case to try to paper over the bad behavior of existing clients So that's some proposed text there I guess the most interesting thing about this is that it's a big change in scope to the point that I think you need to change the title So yeah, I wouldn't want to make to make this change without some clear consensus, Mike So I just wanted to say the the stack that I thought did this I am told does not. So I wanted to ask the question, have you found others that do or is that based on my thinking that there was one? I can say that as an occasional proxy client implementer, I've certainly considered doing this as a performance optimization And it didn't at that time occur to me that it was going to create any kind of security problem I don't know of a specific implementation that does this today. So that's may also be a question for the room. Can anyone else come? to the mic and say if you have seen this? happen or if your own stack does this? Yeah, yeah, we're"
  },
  {
    "startTime": "00:16:01",
    "text": "yeah, H1. Okay so, yeah, I really appreciate some more input from the group of this is a big change to the draft draft Recommending get. So I highlighted the only normative part of this so this is most of this like draft, this is only speaking two ourselves. This is telling us how to define our own future standards. And it is saying, you, we should restrict our upgrade tokens to only use get basically in places where the method isn't even meaningful it's just some kind of placeholder we needed some method this is what mask has done. This is what WebSocket has done. And so basically this is just saying, look, for consistency going forward, this is what we've been doing, we should actually remember to keep doing there's certainly an argument that this is unnecessary that we can just do whatever we want in the future when we decide to do it But there was some feedback on this point Tommy. I'm also going to be lazy and not go up to that mic. tommy pauly, I will I like having this in there if for nothing else, than the fact that like when we did this in mask, it was something that we had to discuss and figure out what is the right thing to do, and it would have been convenient to have some we had to discuss and figure out what is the right thing to do and it would have been convenient to have something to look at it and say like oh this is the obvious thing to do so having it written down somewhere that we could reference in the future would be nice. And I don't see a reason it couldn't be here Can you identify? yourself yeah uh michael tumum yeah um it couldn't be here. Can you identify yourself? Yeah, Michael Tumum. Yeah, on that note, we could change the language to recommend instead of should those are theoretical equivalent in IETF. If you think it flows better, we can certainly do that and I got on cue"
  },
  {
    "startTime": "00:18:01",
    "text": "to say that generally people frown when you place RFC 2119 requirements on things other than implementations. And in this case, you're placing on a future specifications. So perhaps this should be non-normative and that gets around the problem of, well, maybe somebody wants to do something else and they have good reasons okay thank you individual for now okay uh martin thomson sorry yeah Yeah, so I think you're right. This shouldn't be normally but I think it is still good advice Should not I do not want this to be normative We were just teetering on the edge of a rabbit. The word, the word, the word appeared in that sentence So I think... Go there there's two parts to this There's, there's the fact that it doesn't have a request body, which is key and then there's the suggestion that you might use a get. And if you can sharpen that a little bit appreciated, I think, because I think people reading this will go, oh, that they're recommending that we use get but I've got this thing that it doesn't make sense for I think the key is to really put a signpost on the request body part and say that unless you have unless you have other reason to do so, get is totally appropriate So it's just rewording Okay. And if I can just build on that I think if, you know, if it's a recommendation, if we can capture the reasoning behind the recommendation in all of its nuance, then an informing decision can be made in the future We shouldn't just say, don't do this or whatever. So there is a sentence at the end on the slide that attempts to do that, but I'm happy to... Yeah, let's just shop on that"
  },
  {
    "startTime": "00:20:02",
    "text": "up a little bit. Lucas points out in the chat here that get can has body but really not It's sort of pretty much not. Just don't don't Okay. Runs off cue, yeah Okay, TLS upgrade token So originally I said we should get rid of the HTTP upgrade token because it's not a thing and we should get rid of the TLS up upgrade token because that's also not a thing But it turns out that I'm wrong, I guess. And there is there are some systems deployed printers that use the TLS upgrade token maybe So instead, we have to talk about the TLS upgrade token, which I believe is safe to send optimistically, because because because it can't be distinct, can't be mistaken for an HTTP request. If you disagree with my reasoning maybe we should just take it to GitHub home. I don't think this is particularly important, but I don't think we want to make really strong claims here about this one. It turns out in the past things that have looked at streams of TCP and whatnot have taken liberties I should say, about the interpretation of that stream of bytes to the point that they can squint at it and infer things that they really shouldn't have. So how do you feel that the HTTP 2 preamper? in that context? That was a piece of a defensive piece measure. It's not necessarily considered a definitive defense against that sort of thing The primary defense that we have in HTTP2 is"
  },
  {
    "startTime": "00:22:01",
    "text": "the use of TLS and ALPN, right? And this is just a defense in depth, if you will This, again, is probably sufficient in a large number of cases, but we have evidence of middle boxes in particular that would scrub through looking for an end of line and then a get on the following it and if you had those four characters, they would do things and they would do interesting things as a result so don't make strong claims here yeah it turns out that you can do that and it turns out that TLS doesn't protect you from that sort of thing Yeah. Okay that seems like good advice, and we can rework this to say something else, which is probably, don't do this Anything else in this draft? I think you're done. Okay, thank you very much. Look forward to seeing more progress in that one. Thanks Next up, we've got Eric with HTP HTTP server, secondary cert all seeing more progress in that one. Thanks. Next up, we've got Eric with the HTTP server, secondary cert off. Do you want to drive your slides? Or shall we? If it lets you? Okay, let's see how it goes doing reasonably well in time Yeah, but my request So is that on the uncheck tool now? The onsite tool? Either I'm one of full client. I don't know where to do it Why don't we draw this? You don't have a couple of things one of the client i don't know where to uh do it why don't we when we drive this slide you don't have a couple yes go ahead of you kick him off off Yeah, I'm doing. Hi, everybody. I'm Eric and today we'll be talking a little bit"
  },
  {
    "startTime": "00:24:01",
    "text": "about the secondary certificate authentication of HB servers draft So the current status of the document is that it has been recently adopted by the working group yay So it's still early on in the editorial process. There are a few non-editorial issues that I thought we could use a little bit of time here to talk about if anyone has any opinions So they are listed here and we'll go through them. Next slide So with that, the first issue that has come up, is about the ability for clients and servers to agree upon the used certificate for a given request I have heard in this discussions that there might be some use cases for the like if the server needed to know which has it was wearing, or the client thinks it's wearing There is currently no mechanism in the document that actually specifies how this could happen or even enough information to make it happen so if this is something that the working group was interested in, and so that's one of the questions is that we could reintroduce some cert ID field, which was in the old secondary search document, which, you know, essentially the server can provide either in the frame or a certificate request context on the exported authenticator And then the client could essentially tell the server, this is the certificate I'm using by for example providing the certificate ID in a request header So the big question here is, does anyone actually have a use case or think this is interesting?"
  },
  {
    "startTime": "00:26:01",
    "text": "Is this something that should be in scope? in the document? So we can follow up on the GitHub issue if anyone does think this is important. Otherwise, we could potentially not do something about it. Tommy's in the queue Yeah, just as an individual here. So speaking for the use case, which I know you also where we would be using secondary certificates from a proxy, like to be clear, like that is a case where the server is sending the secondary cert based on learning something about previous client requests but it's not needing to use, like, a separate frame like a cert ID. It's like, oh, I've detected, like I've seen that you've been doing connect requests for this thing By the way, did you know I'm authoritative for it? That's a case where like there is signaling, but it's not a new frame and like it's already present. Right. So I think one of the interesting questions would be like, are there use cases that people actually have that they want to build right now that need the explicit signaling you could also add the explicit signaling later yes So it doesn't seem like we any is volunteering any use case here So Eric is in the queue Eric Nagrin Akamai I think the thing that is particularly helpful to know is when a connection has been coalesced onto Because there's a lot, and I think there's a blurry line there of whether it's, I'm not sure if it's that you need to know the cert that was used as much as the path you got the so that could be something in that indicates like which alt service or origin frame"
  },
  {
    "startTime": "00:28:01",
    "text": "or other thing got you there um Because I think the important thing from a lot of service side, is to know I got here versus DNS versus I got here versus some other mechanism Right. And I guess if it was directly correlated with the particular certificate, I guess I don't, I don't currently have a good answer as far as wow we might do that, but we can also discuss offline and, you know, bring into the GitHub issue as needed. All right, next slide So the next issue and I thought we would talk about is providing the ability to send export authenticators in multiple frames for H2 The thing is that if a export authenticator is sufficiently large enough, there is the possibility that either it doesn't fit in the maximum frame size for H2 and even if we were to give it an infinite you know, effective frames size, we then block the not just a control stream, but effectively the whole connection So the question is what we would like to do about this Continuation frames themselves actually cannot be used because continuation frames are not supported on the control stream for which the certificate frames go So one thing that has been discussed is just simply a simple to be continued flag for the certificate frame that, you know, if that frame is received with it um the following frame is a continuation"
  },
  {
    "startTime": "00:30:04",
    "text": "previous exported authenticator fragments that you've received up until the flag is not set We do have to be a little careful here. You know, things like continuation flood and whatnot mean that we probably want to hash out any possible, you know, implications of adding a mechanism here So, yeah, it would be good to figure that out I see Martin is in the queue martin thomson. I have a strong allergic reaction to anything continuation shaped Mistakes were made, and unfortunately we can't unwind them. Let's not make them again Although I'm about to suggest potentially another mistake I'm thinking about this a little bit more So for something like an exported authentic the key sort of challenge is okay so the signature is going to be big, but the signature is probably not going to be so big. It's not going to fit in a single frame It, no a single signature, which is the primary thing that an authenticated contains, is probably small enough to fit within a single frame That may be reasonable I think. But there's often a whole bunch of other stuff that comes with that signature. Right certificate chain. I'm wondering whether or not we can employ one of the many compression techniques for those certificates that would allow us to sort of break out of this in a way that's a little more accessible and one of the things you might imagine happening here is that you have a set of authenticators that all change to a common intermediate certificate. And so in a typical export of"
  },
  {
    "startTime": "00:32:01",
    "text": "authenticator case, you would be sending the same certificate chain multiple times typical export of authenticator case you would be sending the same certificate chain multiple multiple times so there may be value in having say a small store of this is the the set of immediate or these are the higher level certificates on the chain that you might reference from an export of authenticator Of course it increases the complexity of the thing, but it potentially means that you can replace the entire certificate with a pointer to it That might be something worth exploring here rather than doing this. It means you have to set up, you know, I'm willing to take this many certificates and they can only be this big and all those sorts of other complexities around this. But I think it might have been much more efficient and simpler protocol at the other end of that Please do sir. Hello Mon about the security issues with continuation like design I've got no more to add than that. I'm not strongly opposed to it if we'd really need it, but I'd want to make sure we've gone through like all of the possible ways this could go wrong and document them if we really decided we needed it. Thank you mike bishop, Akamai, I will point out that since this is an extension, an extension can change anything about the protocol it could also change continuation frames having to be for headers and not being allowed on the control stream so that is completely within our power if we decide that's what we need to do I would really like to avoid meaning to do that but it is an option. As far as martin's suggestion goes i"
  },
  {
    "startTime": "00:34:01",
    "text": "avoid me to do that, but it is an option. As far as Martin's suggestion goes, I really, that sounds like a great approach to take if we were building exportability authenticators and new i'm unclear whether we can do that with exported authenticators as they exist without going back to TLS. So I don't know that this needs to go back to TLS or we are able to use what already there Slow everything down david schinazi, I had a clarifying question for martin george the room Why we're continuing frames a mistake? I have some gut feelings, but I think it's might be worth discussing really to clarify the decision that we need to make here Mike, you want to give that one a go? You're in a few. Yeah, I mean the short answer is that we defined a maximum frame size and the ability to negotiate down the maximum frame size for a reason, which is that in H2, you're on a TCP connection and if you're going to make any progress from multiplexing, then you have to be able to switch between things And the headers plus continuation effectively gives you an escape hatch out of that maximum frame size and you can send an arbitrarily large frame that blocks everything else on the connection until it's all there But unfortunately, the nature of each pack being a continuous block that must be processed all at once, you have to send the end a continuous block that must be processed all at once, you have to send the entire thing no matter how big it is, or each pack doesn't"
  },
  {
    "startTime": "00:36:01",
    "text": "work This might be a little different in that exported authenticators is not blocking any individual request that's already in progress So we don't have to have the same requirements of nothing else in between. But that would mean we're building some, that would be like the two be continued flag, I think So the other thing about continuation frames is that we currently have, I think it's 16K is the maximum frame size that's that's that flag, I think. Yeah. So the other thing about continuation frames is that we currently have, I think, it's 16K is the maximum frame size that exists by default in an most implementations, but we have a 3 byte length field on every frame. We could have said that the maximum frame size does not apply to headers and push promise for anyone using that which would have been far simpler but functionally equivalent to all the complexity that we added for continuation We didn't do that, so here's where we are This is another option for you in this space, which is to simply say, ignore that limit. It's on the control stream That is an option It does potentially cause stalling and various other problems like that but yeah. Right, which I mean, essentially we would be essentially keeping the language of the draft as it is you know maybe add that the maximum frame size for these particular frames could be ignored and then you can just have a sort of infinite length thing and yes right so the head of blind blocking issue is the main concern there, but it might not actually be a huge problem in practice Yeah, so the limit would then become 16 megabytes which is pretty pretty big. Yeah. If you wanted to go"
  },
  {
    "startTime": "00:38:01",
    "text": "crazy, you could make a stream for sending these things and then you avoid all of those stalling problems And you stream type is something of an undertaking in HTTP 2, I imagine, but it's an option that's on the table as well Thank you if the main reason that this was a footgun is H-pack and that's not going to be necessarily relevant here the to be continued flag seems kind of in play based on a if the main reason that this was a footgun is H-pack, and that's not going to be necessarily relevant here, the to be continued flag seems kind of in play based on all these solutions Seems definitely much easier than a lot of the other ones, unless there's a massive footgun that I'm not seeing So it seems like, um, there have been a few options to discussed here. I guess we can follow up on the GitHub issue and the mailing list as far as which way we want to go here But it seems like there's certainly more consideration over this point to be had All right, next slide So this is the last thing. This is maybe a little bit of the bike shed But the current frame name is a certificate, and it seems to make sense to rename it to server certificate or something that is more aligned with the scope that has been in intentionally chosen for this document and kind of open the door for extension elsewhere. So any objections to this? or should we just do it? The queue is closed He said bike shed, so david schinazi, bike"
  },
  {
    "startTime": "00:40:01",
    "text": "shed enthusiast no just do it we know We know. Cool. All right, we're done by david schinazi bike shed enthusiast. Just do it. We know. We know. Cool. All right, we're done bike shedding. All right. All right We'll be a everything for now. All right. Any other comments on this draft? Fantastic. It's good to see the progress Thank you, everyone. Thank you Okay. Next up, we have David Skarnasi, provided you don't turn into Pumpkin as scribe after your session All right, go on Can someone take minutes of David's session? this bit and this bit only? Okay We'll take that Are you driving on? All right, happy Wednesday, everyone Hey, I'm david schinazi and let's talk about the wrap-up capsule Next slide, please. So, uh before we dive in, let's talk about Intermediaries. This is HTTP. We all love our intermediaries So a common type of intermediary that I'm choosing to call a front end, which is not quite the terminology in the HTTP spec, but it's the one that we commonly deploy is the one that is it has the TLS private keys for the origin. And so from the client's perspective, the front end, is the origin. They speak HTTP 3 to the front end, and then the front end turns out behind it actually talks to another server but that's within the like operator infrastructure. And where it's relevant to this presentation, it in the case where there's a bunch of gets on an HTTP 3 connection it sees those gets and then it forwards them not necessarily all on the same HTTP connection to the back end, but it sees everything that's going on It's inside the encryption conceptually"
  },
  {
    "startTime": "00:42:01",
    "text": "and another type of intermediary the one that I care most about these days, because we're building a few lots of them, is a privacy proxy which in the HTTP specs, we call that more as a forward proxy where the client has some level of local configuration, which decides to talk to this privacy proxy to well improve its privacy in our case the idea is it talks to some other websites to hide its IP address from the websites. But in this case kind of the relationship is between the client and the privacy proxy, as opposed to in the previous one, the relationship was between the front end and the back end. And so in this scenario, you, the client establishes an HTTP3 connection to the privacy proxy, does it connect GDP? request to their privacy proxy turns around, opens up a UDP socket towards the origin and then the client establishes an end-to-end HTTP connection between itself and the origin and then through that it could send get requests in this case the privacy proxy has no idea that there's a get in here because it's conceptually outside of the encryption of the orange arrow here it just knows that it has an encrypted flow of bytes going back and forth Next slide, please. Okay so if we look at the first one, when we have a front end, a lot of the time, the client is making a bunch of a get requests. And the front end then forwards them to the back and at some point, let's say the front end is going down It's doing a software update, or is that a resources It doesn't want to break anything on the client but it wants to like go down pretty soon. So next slide, please It sends a go away. Pretty simple And what that tells the client is you can finish these in process get requests, but if you wanna make new ones, you gotta make a new connection and practice you'll probably land on a different front end"
  },
  {
    "startTime": "00:44:01",
    "text": "That is part of the same infrastructure and the important why is this important is that if you're client's a web browser and your browser, has requests that are mid-flight, especially if let's say, like, a post, and they break mid-flow, the browser can't retry them. So what's important here, to prevent those failures to become visible to the user what you do is the go away tells the client like finish the ones that you have in flight so everything works correctly, and then for new ones, then take the somewhere else, please. And that works great. Next slide please. But now let's look at what happens in the case of a privacy proxy So same as kind of diagram as before we have a bunch of these requests through this end-to-end HTTP 3 connection to the origin but then the privacy proxy has to go down because they let the manager write code and we have to do an emergency software update again What do we do? Because we have the same problem where we don't want the client to like break mid mid get request. Next slide, please Because from the perspective of the HTTP, of the privacy proxy here, all it can do is pull the plug on this ConnectUDP udp and request stream and on the UDP socket. But if it does that next slide, please, things explode sad browser, I'll and on the UDP socket. But if it does that, next slide, please. Things explode, sad browser, all... No, no, no there's something else in this line. That's weird. I'll have to I think emojis don't render properly Anyway, all right, that's good. And so sad browser. Normally, there was a really frowny face on the client, but you can imagine it. Next slide please. So, what am I here to talk about? It's a solution to this problem So the idea is now when the privacy proxy says, hey, like, I need to"
  },
  {
    "startTime": "00:46:01",
    "text": "like, load shed. I'm going to probably be going down Hey, client, can you please finish what you're doing? doing? But for anything new, talk to a different privacy proxy Well, it sends a wrap-up capsule capsule Yeah, and so anyway, the things work on the browser, and another example of why this is useful for us is that on our privacy proxy, we use kind of anonymous token for the client to authenticate to the privacy proxy and those tokens allow you to send some amount of bytes to the origin and we don't want that to be infinite because we do rate limiting based on those tokens and so if the privacy proxy reaches a point where the client's running out, similarly, we don't want to break things in a way that would cause user visible failures in the browser. So we send a wrap-up capsule And so a well-behaved client will wrap up move over, and a misbehaving client, we would still yank the plug once they reach the hard limit And so we get the security properties we want without actually breaking eyeballs Next slide, please So yeah, sorry, my AI game really is eyeballs. Next slide, please. So, yeah, sorry, my AI game really isn't on point, this IETF. I haven't had as much time as usual But how do we encode this? It's really simple. You put a wrap up and a capsule. I don't know, Gemini I came up with this But, and so why did we pick our capsule conceptually between the privacy proxy and the client, as in the ConnectUDP? proxy and the client, you could have intermediaries there and for those we want this signal to go through and that's exactly what we built capsules for and that's why it kind of feels like a natural use to send this signal. And then it's a capsule where the, that has no value like the length is zero all it says is wrap up Next slide, please"
  },
  {
    "startTime": "00:48:01",
    "text": "And that's it. This is my last slide. Do people think this information is useful? Is this the right way to send it? And anyone interested? in working with me on this? And then another discussion point is like, we saw this for Connect GDP. We have Capsule and ConnectDP dp works well there's a separate draft confirm And then another discussion point is, like, we saw this for ConnectDP, we have Capsule and ConnectDP works well. There's a separate draft currently in this working group about Connect TCP where if we add capsules to connect TCP that would also allow us to use it So from my perspective, right now we use regular unextended connect for TCP proxying on our privacy proxies And if we decided to have the option of using capsules, to connect TCP, I can say that we would implement would implement connect TCP to get this feature that's what this would be what we would want I chatted with benjamin schwartz about this and I saw him run out of the room so I can say that he said anything I want him to say but he seemed open to the idea of allowing capsules with Connect TCP All right, and that's the end of my slides. Let's discuss Peter. Piotr Piotr Chicaa Pierre. So I think this is great and it solves real issue that's happening with, you know, mask and everything Three questions were one comment I think, yeah, we should use the opportunity of connect TCP and try to get capsules there since it's you know so this one question that you didn't mention I think the draft says that only, you can only sense capsule down downstream right so from servers and proxies down to the client, but I think it would be potentially useful if you also send it upstream right? Because then potentially servers could send go away, especially in multi-hop scenario right when you you could yeah that basically server to send go away on the inner stream Oh, that's, oh, well, no, so the issue here is that if you send it upstream, you're going from"
  },
  {
    "startTime": "00:50:01",
    "text": "the client to the proxy, the entity sending the go away inside the stream is the origin You don't have a control channel to send this between the process proxy and the origin Because at that level, you just have TCP or UDP inside which is all encrypted Well, potentially it could be much mask-aware application Let me think about it, but maybe no. Okay and the second question is, would it be useful to distinguish between relays that are not terminating? and terminating idea you know on? client side, on server side, again, in multi-hop scenario? because then potentially you could send wrap up and connection could be migrated without being interrupted on the end to end I see, so you're saying to make sure I understand what you're saying is that in this scenario you have client and in intermediary and then the privacy proxy and the intermediary could send a signal to let's say you have like you know kind of like private relay with like not not even two more like three proxies so the issue there is in the example of private relay or IP protection like we're building we're doing these nested privacy proxies but it's the exact same problem as what you suggested with the origin is that you don't have a control channel from the proxy to the proxy bomb, because from its perspective, it's just raw UDP. You don't know what's on there that happens to be quick or it's just TCP which then has TLS All right, let me think about it. Thanks Thanks. Yeah, that's definitely worth thinking through. We have a somewhat long queue, just so folks know we have a bit less than two minutes for each person in it, and assuming no one else joins. We're going to close the queue pretty soon So next up is Mike So in discussing implementation of a mask, on the last"
  },
  {
    "startTime": "00:52:01",
    "text": "time we built something like that, one of our requests, on the client side was to treat a go away on the proxy connection as also a go away on any internal connections so stop using it and move on And I feel like if we just wrote that down, go away is sufficient signaling because that's effectively what the proxy is saying here unless you need a per stream, stop using the stream but you can do a new stream and that would be fine Is that the case or are you doing this just? because you want the end to end in case of intermediaries? So the, what you described solves one of the two use cases. So in the case where you're proxy is going down for maintenance, that works because you're conceptually sending this on all of the streams. So sending a go away to the client, that totally works. And maybe writing that down is probably a good idea, too if that's something folks are doing But in the second use case that I mentioned, where we're putting a limit on how many bytes you can send per request in particular per authentication token that's on the request then you need a per request signal. And that's what this is Thank you for that distinction. Thanks Next up is Eric Noggren. Okay, so you're done Eric had the same question as Mike I'm just repeating. Yeah, I also was thinking about the same question at Mike had I think if we're doing that, maybe we want to have like a different name or semantic for this. Like, when you think about saying like hey i'm telling you that you kind of need to re-up with the token or like you're kind of running out of your credit maybe that's focus on that use case and really explore the motivation"
  },
  {
    "startTime": "00:54:01",
    "text": "there. But in general, I, I, I, a reason that we can have capsules in Connect TC because I think it's just like a useful way to extend the protocol So, like, let's use something around this as the first step there Cool. So on the first point, we can always rename. I'm always down for a good bike shit amongst friends. On the other, yeah, the right now the capsule is as simple as possible but the nice bit is there's a value field in there and you could imagine having some kind of data in terms of, let's say, for example, as this is a byte limit, you could put the actual limit in there. So I've thought about that a little bit I wanted to keep things simple as first, but we can definitely do that if we think. I think it'll come down to implementing this on the client and see if this information would change the client's behavior Because I thought through it and in case I think once you receive it, you just stop whatever the limit is because you don't really know how many bites that get request is going to be but it's worth thinking through for sure because there's something that can be done there It's, yeah, Lucas Yeah, I understand the problem. I think it is a problem and this is a solution to it. We discussed on the list maybe other ways we could augment things client signals or not. I think we don't need to resolve any of those discussions until this is like potential an adopted working group draft and, you know, if those things make sense but they're too complicated then we do them as a follow-up work I think we can have all of that discussion but to Piotto's points I do see use cases for non-privacy proxies where we want to have controlled planes between proxy hops. One of the problems we have now is that the signaling between those things isn't great So if one of them goes down for some reason, planned or in plan maintenance everyone else has to kind of sail oh, was that which downstream of me? went away which upstream did um goes down for some reason, planned or in plan maintenance, everyone else has to kind of say, oh, was that, which downstream of me went away, which upstream did. But also specifically in the proxying, but"
  },
  {
    "startTime": "00:56:01",
    "text": "privacy proxing cases, we do speak to an origin for example but we don't speak to it directly we offload that work to another service and that service might have its own reasons for maintenance too so i think yeah, I think there's a lot of good work that we can be done here and i'm willing to contribute toward that Awesome, thanks, Lucas. Yeah, let's sit down and chat about those use cases, I think it totally makes sense to fit them in here I think this is a good great proposal. I think it has real benefit The only thing that I wanted to say as the I'd be a little sudden if this works only for a capsule because there are other HTTP things that are long-lived and probably benefit from this like chunk-to-h-HtGTP for itself So I'd prefer a frame-based approach Thanks. So the reason I didn't use a frame is getting through intermediaries There is a potential solution where the way we will add capsules to connect is by defining a data capsule where conceptually you put the data stream inside those frames now that are ordered because you have to exchange it somehow And it turns out that that solution could potentially work for any other method. I think whether you want to use capsules, for other method, I don't know if that's great or not like the extra layer of capsulation that doesn't really matter because you already have three more. I'm doing capsules for other method, I don't know if that's great or not, like the extra layer of encapsulation that doesn't really matter because you already have three more underneath you of the HTTP 3 level and the like the extra layer of encapsulation that doesn't really matter because you already have three more underneath you of the HTTP 3 level and matthew quick level and so on but that would be one potential solution to that where you still get the intermediaries and make it potentially usable with other other methods thank you alan frindell One of the thoughts I had was similar to what Kizuho had, which is I've seen this problem with, we had some pre-web transport"
  },
  {
    "startTime": "00:58:01",
    "text": "things that did long-lived stuff over HTTP streams and having the ability to send go away like behavior is interesting. So I think if we can work on something in that direction, I think it's good Since I mentioned web transit that was the other thing I wanted to say is I think we have a similar capsule in web transport might be nice if we use the same one although I think maybe somebody was trying to get rid of the one in web transport, probably him But anyway, this is interesting. Thanks for bringing it Thank you And if we end up with the same data exchange, right now, Drain Web Transport session has like an error code and a narrow string. So if we ended up with the same data, it would make sense to unify. I'm not 100% sure, but I'm not opposed to the idea Eric Nagar and Akamai. Yeah, I think this is interesting i think it also is worth as part of this looking at other other use cases go away, doesn't cover well today. And like, like, maybe that having a a new frame that's like the kind of is the that is an encouraging go-aware or wrap up that covers existing on the stream for long-lived connections which has got slightly different has could cover some use cases it could go away doesn't handle today Awesome, thanks so chairs what do you recommend we go from here? It sounds like there needs to be able to more discussion perhaps, but it sounds like promising discussion. So I think have some more discussion with people, maybe modify your draft a bit have some, you know, contemplation and then we can talk about adoption in the not too distant future, it sounds like. Awesome. Thank you Sound good? Okay i do need a new minute taker now both of our previous ones have expired. Anyone willing to serve the group Lisa thank you so much it's it's a a new minute taker now. Both of our previous ones have expired. Anyone willing to serve the group? Lisa, thank you so much. It's at the note link in the agenda. Great thanks. Now we're going to shift"
  },
  {
    "startTime": "01:00:01",
    "text": "gears a little bit away from the the stuff that's close to the wire and further up closer to this semantics. So first off, we've got Jeremy with no very search. Jeremy, are you here? Yes let me see Can you approve this? Okay There we go. It should be coming up now Hope that works. Yep, go for it Hi, so I'm Jeremy. I'm on the Google Chrome team I'm here to talk about new very search So basically some background. This is going to be familiar In practice, the URL's search component is treated as a list of key value pairs even though some specifications are treated as an opaque string. This is how it's typically released on the web Just as HTTP responses typically don't depend on all of the request headers that were sent by the client. They also often do not depend on all of the URL of query parameters that were sent. Catchers work best and there's a particular catch that I'm interested in, but when they know is much about as possible about which cash responses can be used to satisfy a request, because it might mean that you're able to serve a response from cash when otherwise you couldn't And matthew quick survey suggests that existing cash software, catching proxy CDMs and whatnot do support ignoring query parameters in so way, even though HTTP doesn't provide across cross-bender way of the server indicating this to its client, which may be a cache and proxy HTTP does, however, provide a mechanism for the request headers in the form of the vary header which lists the request headers upon which the response depends or star. Today, Chrome now does support a way of varying on the query parameters for navigation prefetch and pre-fetched and pre-ender, which is the novary search headed"
  },
  {
    "startTime": "01:02:01",
    "text": "that we implement in that cache. But the concepts seems generally useful to HTTP cache implementations browser caches, proxy CDMs so it suggested that we bring this to HGBIS So why do clients send these meaningless parameters? that the response doesn't even depend on? There's a number of reasons One of the common things is that the server processes, the query parameters but doesn't care about the order. It treats it out a map or a multi-map. So the order so A equals 1, B equals 2, and B equals 2, A equals 1, have the same B-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E-E Thank you same meaning to the server and therefore it doesn't actually matter which order the client sends them and it would be acceptable for the client to use response where the user is in a different order Some parameters affect server processing, but not the same semantic meaning of the response. For example, they're might request load balance into a particular backend instance enable debug logging, or change request priority. None of these semantically changed the response, but nonetheless, the server is interested in receiving them when it is processing a request And on the web, it's common for parameters to carry data that is actually intended for client client-client-side processing. For example, by analytics software, or to initialize stuff on the web page that sometimes is used in the ref but can exist in the query as well And those don't affect whether the response is suitable because once the client processes the response, all the client said process from that point is going to be the same So what we implement as a header called No Very Surges It's inversed in sense to vary because the because very basically does the opposite thing. The default for HTTP is to vary on none of these request headers and the default for query parameters is to vary on the entire query along with the rest of the URL It's a structured header"
  },
  {
    "startTime": "01:04:01",
    "text": "You can indicate that your response doesn't depend on query parameters at all, doesn't depend on their order or it doesn't depend on the values of particular parameters You can also say that in the opposite sense These are joined by commas. So when, you know the automatic concatenation of HTTP headers works basically as you would expect expect This is something that we implement only in the prefetch and pre-fetch caches for navigations, not in the normal HGP cache and Chrome right now. That shipped for pre-fetched in Chrome 121 and for pre-ender in Chrome 1 127, which is relatively recent. One-21 was about six months ago It's implemented in Chromium, which means that it's likely that other Chromium browsers are shipping it, but I haven't checked other browser, chromium-based browsers shipping schedules. The specification is a community group draft within YCG, within the WG other chromium browsers are shipping it, but I haven't checked other browser, chromium-based browsers shipping schedules. The specification is a community group draft within YC within the W3C, but if this is something that's interesting and if it makes sense to move somewhere all that specification text into the IE scope within HG working group, that is basic what was suggested that I open the possibility of What we remain to be done I've done some work to reformat as an internet draft, but there's different conventions and whatnot between the two groups The specification currently discusses how to parse this header and what it means, but that doesn't actually tell HTTP implementations how they should modify their cache behavior, which is going to be RFC 911 So if we wanted, you know, standard HTTP caches to respect this header, some sort of this is how that affects in RFC 911 would be required, including how multiple matching responses should be handled in general that's treated a bit of a hand wave. But any issue?"
  },
  {
    "startTime": "01:06:01",
    "text": "that we didn't encounter that have pre-fetched cashbo, which would affect other parties' implementations, would be interesting as well as any extensions to express a minute that people think are important but which aren't yet captured in, you know, the, the, uh, as we currently envision it And then I've just got some links to the explainer on YCG, the Internet draft, and the YCG draft. The YCG draft, the YCG draft, the Internet draft, have the same text Modulo, the need to fix up how links work and that sort of thing. So those are the same document You'll find yourself very bored if you read both of them that's all I have prepared Great. Thanks, Jeremy Any responses, comments, questions in the room? I'm curious, Michael Timum, I'm curious if you have any data on how this improves caching performance in practice So the primary use case I'm aware of basically we wouldn't have caches hits at all, and so it sort of is a fairly radical change I don't have any data about other applications. So like, for instance, Google search uses this in some cases because we want to mark our prefetched request especially for different server processing and without the ability to say that that doesn't affect the response, the prefetches never match And so the cash hit rate is zero and the prefetching are completely wasted for that use case so that is, you know, a zero to a very high cash hit rate, right? But in terms of the general applicability across other kinds of requests, I don't have any data I just have vibes. But you're using this for Google search, you're saying, so like when I'm typing in a search and it's live updating results, is that the case? Yes, so there's basically some cases where one of the"
  },
  {
    "startTime": "01:08:01",
    "text": "um you're you're typing in a in a query and we will prefetch a search result page for that. And then when you ultimately click on the query that we think you're going to make, then that response is used Does that answer your question? Yeah, it's really helpful. Thank you Anyone else? But we do send a second purpose header indicating that request is a pre-fetch the reasons why this is search at least wants more granularity than that for in internal reasons. I'm not sure I'm at liberty to share in detail The other applications, obviously having knowing that it's a prefetch isn't necessarily enough. But yes, we would expect that there are some applications where people are satisfied with the request here to answer Martin's question in chat and just speaking as myself I'm very interested in this because it's very common practice in proxies, caches and CDNs to do this in a proprietary way and so having one way to do it is nice and having one way to do it that's a line between intermediary cache and browser caches is very attractive, I think. So I'm quite interesting I think we're suffering from the fact that most of the folks that I'm looking at in the room are more interested in the transport side of things, and especially don't have a lot of cashing people here in the room. And so what I might ask is the people who are representing especially CDNs if you could go back to your caching teams and see if there's interest in adopting something like this, that would be really good information I'm going to go and talk to my caching team I suspect they will be interested"
  },
  {
    "startTime": "01:10:01",
    "text": "And then we can also take it to the list, perhaps, and get some I know a lot of the, especially the independent proxy cash vendors are on the list and we can get their feedback But again, I think this is very interesting and a pretty obvious way to take forward Any other feedback in the room? Okay, so now with my chair had on, I think we'll probably take it to the mailing list, have a bit more discussion and if things look promising I don't see any reason why we can't do a call for adoption and formalize that pretty soon yeah yeah okay Anything else, Jeremy? That's everything. Thank you. Great Thank you very much. So stay tuned. We'll get back in touch Next up, Johann, also remote revised cookies Hey, everyone. Can you hear me okay? Hi The audio is not great Is that great? All right, let me try to fix it Not sure Yeah If I say Meetecho, can something magical happen? I don't know MEDECO, META, MECO. We can just turn down the volume. Yeah? Yeah, yeah you do that. I'm not touching it. Yeah it And Johan, do you want to share your slides or should we share them? for you um can i control them if you sure sure that that sounds a bit better. Okay, go ahead and make the request and we'll How do I? Yeah, I can also just share them if you How do I approve this?"
  },
  {
    "startTime": "01:12:07",
    "text": "It's taking a little while Oh, yeah, no, I see it now. Thank you. You need to confirm Okay, there we go So asking again, can you hear me OK? Do I need to like? change my input device or something? That's better now, thanks OK, cool. Thank you Yeah, so hello everyone I'm Johan work on Google Chrome And over the last, I think one year or so, I've been working with Annenkestrian looking at cookies again and, you know, as part of the sort of bigger architectural change that we call cookie layering. And I think it's at the point where we sort of start want to start talking to this group about it and want to sort of you know work on the draft that we have together with this group And also, obviously, I want to get to adoption of the draft with the group. And so I wanted to give you a really high level overview of this effort and you know, the draft that we have and happy to, you know, dive into details as needed You know, please feel free to ask questions afterwards. All right so uh yeah as you know we I guess I could go next slide, yeah um we just put uh 6265 biz into workgroup last call. We just got through a work group last call after I think 10 years of overall development which is really really great So I really appreciate all the work of all the editors. And now we want to do another one Why? is, I think, a very valid question at this point And so 6265BIS you know, added, obviously, you know, over its lifespan, added a lot of really great innovation"
  },
  {
    "startTime": "01:14:01",
    "text": "particularly just, you know, from my perspective, really in the browser security, you know in privacy space, you know, same site, none, or just generally you know, same site attribute and all that, you know, it came with six to six, six, five biz The way it implemented those things, unfortunately, you know, is done through what we call layering violation right? So in a lot of ways it tries to describe browser behavior in a way that theoretically, sort of like in a way that browser specs would but it's not a browser spec, it's an RFC And so that's, uh, causes, I think, sort of a bit of confusion in browser land today And also it, you know, we really feel the pain of that trying to standardize a bunch of other things that have come up in the recent past and that were working on such as the pretty cookie blocking uh the cookie store API, storage access API, the partitioned cookie attributes, and all those, right? All of that should have components in in fetch and the HTML specs that, you know, control how it works on a browser level Unfortunately, it's very hard for us to integrate with the Cookies RFC because it doesn't really provide that kind of surface. It's really right now, like, you know, the, best option that the browser specs have is to more or less hand wave and hand it over to the cookie spec which then tries to, you know, do things like you know, walk a document tree or something like that without actually being a browser spec And I, you know, I can imagine it also, like not being a not non-browser user agent developer, but I can imagine this also, you know, creates some unnecessary you know, over for non-browser user agents overall. And additionally, we have a bunch of deferred issues still from 66 to 65 biz. I think that's, you know, obviously at some point we need to make a cut"
  },
  {
    "startTime": "01:16:02",
    "text": "and publish a document. I think it was a great state, but we had some, you know, remaining disagreement on things like same side none, which we, you know, in the to work out. And I think another document is a great way to do that so how are we going to do this? Just the rough plan here and you know again I want to keep it high level but happy to dive into details as needed it we want to define sort of a concept of a cookie store in the cookie spec And if there's already a concept, I think, but sort of make it more explicit that that's sort of the main surface, main normative surface that the cookie spec, the Cookies RFC describes and then we have some shared with some operations in this cookie store such as adding a cookie or deleting a cookie or something like that And then we make expectations around how specs and implement should deal with these operations and how certification like that. And then we make expectations around how specs and implementations should deal with these operations and how sort of they should use them in network requests And for browser user agents, that practically means that we defer largely defer to HTML and fetch in order to use these operations and, you know, store, delete and all that depending on the, you know, the network request that the fetch or HTML specs are handling And for non-browser user agents, the cookie spec would remain pretty concrete in terms of steps to do and how to handle said cookie or cookie headers specifically specifically Yeah. And so this is really it. Not a lot. We have a draft that we would love to get adoption you know, eventually, get it up adopted eventually. And, you know, in the means we very much welcome you all to take a look and review it and read through it"
  },
  {
    "startTime": "01:18:02",
    "text": "There's some rough edges for sure, and there's also stuff that you know we haven't fully integrated yet that's on the to-do list, such as the cookie store which is sorry, the cookie store API We called our concept also cookie store, which is very confusing We're considering changing the name, but there's Cookie Store API shipped, I think, only in Chrome right now, which has, again, like a lot of these sort of hand wavy formulations around like how things might work with with cookie spec that we're trying to fix up specifically around what watching, observing cookie changes in you know, in JavaScript, which is the thing that this, you know, API exposed So some of those things aren't fully specified yet but I think we have the main architecture sort of laid out and the main structure of the specs if you want to take a little it's definitely sort of definitely big change already to 6625BIS. We're working on top of 6 to 6 to 6 to 6 5 bits and I think we're planning to I'm not a hundred like, I'm not super familiar with the idea the process, how it works, and like the, the, right, some semantics, but I think we'd be sort of replacing whatever number 625 bits will get once it gets published Yeah, and that's it mostly for me. Great thank you, Johan. So any comments, questions? clarifications in the room? Expersions of interest, doubt heartbeat, fear, yeah yeah think people are digesting. Ah, Stephen steven bingler Chrome, and current 6265 bis editor Speaking as an editor, I think this is a great step forward and directly addresses a number of issues that I ran into working on the current draft"
  },
  {
    "startTime": "01:20:04",
    "text": "Yeah, my impression is there's been a fair amount of background discussion about this and then recognition that it's necessary and perhaps thankless work So, you know, we'll have to have some chats with people about who's going to put energy into it, which is one thing I guess the question in my mind is when we did 60 65 bits, we created this process to make sure that any large which is one thing I guess the question in my mind is when we did 60 to 65 biss we created this process to make sure that any any large changes to the spec especially new features were vetted as separate drafts first. I'm not sure that's not necessary at this point. The nature of this work seems a little bit different This is more maintenance, I think as I'm understanding it. So Johan, it sounds like you're not quite ready to say you want adoption of the draft, but you're trying to give people a heads up. Is that reasonable character? I mean you know, we definitely want to work on the draft in the working group, right? So happy to get adopted uh ask or request adoption at any point really, but yes, you know any point, really. But yes, this is the first time I was showing this to the group, right? So I'm not expecting people to just, you know, say, say yes right off the bat. Happy to continue this conversation. Also, you know, happy to join virtual calls, whatever, to talk more about it, all on all you know, I think what we expect to continue with work know, happy to join virtual calls, whatever, to talk more about it. All in all, you know, I think what we expect to continue working on this, you know, in the working group. And I don't think we'll get more ready personally, but yeah Okay, that's good to know. And I think Tommy now will have a chat, and we might reach out to a few people and figure out next steps from there yeah thank you for doing this and we'll discuss more i think one of the main things I'd be interested in is how do we? do it strategically so that it doesn't take quite as long as our previous cookies updates to make sure that we get the right energy and attention and also make sure that the scope that we start out with is something"
  },
  {
    "startTime": "01:22:01",
    "text": "that is achievable and relatively relatively short time, right? know, less than that. Yeah, that's a great point and i think we should talk about that for sure um relatively short time frame, you know, less than... Yeah, that's a... That's a great point, and I think we should talk about that, for sure. I'm not interested in the other 10 years And, yeah Stephen, go ahead. It's actually almost eight years I have a meta comment to whoever's taking note it's 6265 BIS Oh, yeah, Steven, go ahead. It's actually almost eight years. I have a meta comment to whoever's taking notes. It's 6265 bis. Thank you Yeah, that's my fault. I began calling it 6265 at some point and it is 626 6265BIS. I don't know began calling it 6265 at some point, and it is 6265. I wanted to just briefly remark that there are some new features we would be introducing, I think, as part of the drive for example, the partitioned attribute, right? which has its own RFC, or it doesn't have its own RFC, it has its own ID by Dylan Cutler just wanted to just for complete this sake, mention that, but I think apart from partitioned, most of the things aren't necessarily new. That was my impression too, yeah okay so it sounds like uh we'll have a little more discussion and try to make some progress on this. And thank you for putting energy into it because yeah, this is important work but it's not necessarily glamorous, so thank you Glamorous. Thank you all great so i think we're done with that And our last scheduled presentation for today michael sweet got HTTP resource versioning. Do you want to share? the slides? Yep, yep, that's one And then you can scroll through and find yours and click confirm Yeah, hey we're last live"
  },
  {
    "startTime": "01:24:04",
    "text": "Thank you guys great, so today, talking about version for HTTP resources and so for those of you who were in Prague two meetings ago this is a continuation of work towards adding synchronization features to HTTP So we have so in the Braid group, we found that we can from the user's perspective, give every URL the functionality of a local first Google Docs So you can have multiple writers editing it, you can have collaborative editing, you can have multiple writers edit things offline, everything can merge It's really nice and it requires these four independent extensions to HTTP. Each of them is independently valuable And so today we're just going to break off the versioning extension and talk about versioning and how that is independently valuable for HTTP and what it looks like So first, I'll take you through the problem. We'll have some discussion on the problem statement Then we have a spec with a proposed solution and we can talk about the feasibility so here's a basic problem HDP resources change over time It's a state transfer protocol Rest is representational state transfer and that state changes. And each change to the resource creates a new version. And we don't have a term of a version yet like in the specs of HTTP, but it'd be really nice to have this idea of a version so version history can be linear like on a single computer, but then over the network, it branches and merges like a DAG. So when you're doing Git stuff, like you have a version DAG So the basic ideas of what versioning looks like"
  },
  {
    "startTime": "01:26:01",
    "text": "And this does exist in HTTP, you know, sort of a smattering of ad hoc uses for different, that serves some subcases of the purposes, so the last modified header, you can, it gives you a timestamp for a resource. And by the way, when we talk about versioning, talking about versioning for resources, not for age APIs. So like an API will have a whole set of your and data schemas, not talking about it that. We're talking about a particular resource changing over time So last modified gives you a timestamp. Now that's really useful, but if your resource changes, multiple times per second, there's no way to distinguish because it's only at the resolution of a second So e-tags give you a more precise way of marking a particular version of a resource but they are specific to the content and not to a moment in time So if your content changes from food to bar, and then back to foo, it might have the same e-tagging are specific to the content, and not to a moment in time. So if your content changes from foo to bar and then back to foo, it might have the same e-tag again. And neither last modified or e-tags give you a version history They don't tell you which versions happened before or after other versions, and that's something that you need for a lot of purposes, like collaborative editing and in practice programmers want to be able to store and retrieve a whole version history of resources and they just put them into new URLs And so you'll have a URL per version for this or that and there's been a bunch of approaches to try to standardize or formalize the semantics, how you're doing that with web dev, memento, link relations So here's a matrix of our current smattering of approaches to versioning We've got the different approaches as columns and then a bunch of qualities of them as rows. So we want to be able to store and access history, okay?"
  },
  {
    "startTime": "01:28:01",
    "text": "Last modified in ETAC don't give you Samantha of history what happened first or later. You can't store and access them We want to mark time, not just contents. We want to have support distributed time so that multiple clients or servers or peers can modify it at the same time. We want us to be into independent of allocating new URLs. We want to have the semantics of a resource changing over time not multiple resources for each change. Although you could also add that later, but that should be a separate design decision And then there's this, I'll talk about customizing. But so if we get a general version a general functionality for versioning that can fill in these gaps. I'm going to show you some examples of what we can get in HTTP So we could have incremental RSS updates so that when you're pulling an RSS feed you can say, here's the version that I have. Now just give you the diff. Instead of if you are, instead of RSS updates so that when you're pulling an RSS feed, you can say, here's the version that I have, now just give me the diff. Instead of every time the feed changes, I have to read re-download all of the feed entries all over again including the new one. We could also host Git repositories directly on HTTP There already is an HTTP Git repository protocol but there are two versions, there's a dumb version and a smart version. And if you want to get increment, updates, like you do a pull, and you want all the new stuff just the new stuff to download, then you have to use a smart version, which goes outside of HTTP in order to negotiate that we could bake that directly in CDN so like the internet arc to download, then you have to use a smart version which goes outside of HTTP in order to negotiate that. We could bake that directly in. CDNs, so like the internet archive could archive website and resources and then proxies and caches could store all those old versions and that'd be really nice patch requests so there's a bunch of issues with patch put and post that"
  },
  {
    "startTime": "01:30:01",
    "text": "when you're trying to patch when you're sending these these requests they need to know what version they like if they go, if there's a race condition or the request go out of order, then they have to specify an if match or if not match e-tag so that you know which request, so the request only apply to the right version and you can't if you could if you got a versioning built into that, then they can apply to old versions not just the current version and then the request fail And so this could provide your basic mutation requests could function automatically behind the scenes and just run and do the right thing. And that's what you need for collaborative editing. So first question is, is this problem interesting to the working group? Here's some design goals that we'd want to have in a generalized versioning architecture And how much time do we have? We have a reasonable amount of time. Okay, cool thoughts or responses? So I think earlier you mentioned can you identify yourself? I'm sorry, josh cohen cohen I think earlier you mentioned web dev. So like I mean, in general, like, how would you compare this to that in terms of? like the problems that it solves? Solves I mean I remember when that was done I mean it's very complicated. All the difference scenarios. Anyone else here from WebDab days? Lisa to so. Oh my God, it's you Oh, great nice to meet you, Lisa. Wonderful, yeah Yeah, it's attacking a very somewhat dressing a similar problem as web deaf, but proud of part complexity of WebDAB is that you have separate URLs for"
  },
  {
    "startTime": "01:32:01",
    "text": "every version, and there's even a separate URL, I think, for the version history And if you want to find out, for instance, what is the current version of the resources You have to do a problem find, pass in some XML, get something back, and then do another requirement and then get something back. This is a much simpler, you just do a head request and you get the version ID So we'll see that in the second half also I guess I'll add myself to the queue and speaking only as myself. I guess I wonder about the use cases here in that yes, Git is a thing. And if you have a Git repo, you have this full history in all the different versions. And you can do it interesting things with them. So if you have a Git repo, this is just kind of surfacing that. But for a lot of other cases, you're not going to have that fullness of the version history You're going to have a selection of it because there are practical considerations you like a cache somewhere or even if I have an RSS feed on my website, I probably don't want to keep every version of that feed document overall of history And so then some tradeoffs need to be made and your protocol becomes potentially a lot more complex. So then if I look at the lists of use cases through that lens, I'm wondering how many are really going to be compelling at this point Okay, so maybe this is a question of how feasible it is to implement it without additional. It's true that these use cases do require some additional things too Okay. Just a flow use cases do require some additional things too. Okay. Just a floating thought, really. That's great. Yeah. And we do have a lot of implementations to, for most of these things we have implementations that work pretty well on it. Share some of that practicality Eric Nagar. I just kept tossing in use cases I don't know if these are worth the complexity, but other ones I've seen are for CDN partial object cache"
  },
  {
    "startTime": "01:34:01",
    "text": "It can be, it's really messy to get a consistent object, like you want to be able to have multiple consistent kind of multiple large objects both cache simultaneously and be able to fetch those from origins having a model here might be able to help with that and a similar one is for objects storage systems like the S3 interface where you might have different things upload different versions, being able to have a versioning model like this could be potentially useful there natively within the protocol for resolving conflicts Yeah, great. What is the partial object? Is that like you've uploaded? part of an object, but not the full thing? So in a CD, so for CDNs that have, I'm downloading a very large object, oftentimes they'll only store, range ranges of that object in cache yeah and will be a range request to fetch it from, from the origin. Yeah. And but the challenge becomes when that object changes on the- origin you're fetching from you don't want this frank an object in your cache. And when you're serving it out to clients, you'll also don't want to be serving clients to frank an object So being you being able to cash different, and when you have something that's kind of an inconsistent state, you can, like, e-tags will help there but you can still get into messy situation where you don't converge towards a single object and being able to have a versioning model so you could get a within some short period of time, both objects as two distinct things but with separate versions and being able to talk about that more usefully could be helped Excellent, that's a great point, yeah And that also connects. I'll show a resumable uploads example, which I should have put on this list also. And because you can also, let's say that you have mutated your odds object, part of it's cached, now you make a you patch another part of it but Samantha that we have here will let you keep that same cache of the old part and know that the new parts"
  },
  {
    "startTime": "01:36:01",
    "text": "does this new a new patch with and that doesn't invalidate the old part that hasn't changed So I'm hearing some interest and also some thoughts about how is this actually going to be feasible in practice I guess I'll move on. Cool. Okay, let's just go So, okay, so here's what we want. We want, um, very that mark points in time, not content. Okay? We want time that can branch and emerge We want a history of versions that you can store and access And something I haven't mentioned yet, we want to be able to have different time formats, because a lot of distributed systems have specific algorithms and optimizations that we can encode there. So I'll show us some of that. And we don't want to require additional URLs. We don't want to but we want to be compatible with that So here's the proposal Pretty simple. We have two main headers. We have a version header and a parent header. Both of these headers store a version. Parents have stores multiple versions and that's how you get the DAG. So, but in the same time a version is the defined as a set of version IDs Okay, you can think of these as events in physics. These are the raw mutations that happened on some computer at some point in time And a version ID is a has to be a unique string And then a version type can define the format of that string. And that's going to enable a bunch of fantastic optimizations that are made CRDTs practical these days. So we're just going to take those headers and we're going to now extend the regular"
  },
  {
    "startTime": "01:38:01",
    "text": "methods, get put, post, and patch with the version semantic So here's what it looks like So top example, we can put a new version with some parents, and there's the body and that is going to add a new entry in the history. And so you can just take a regular put. You can add the version in the parents. Great next example is getting a version So I want to get a specific version. I just add the version header. And then the last example, this is something that's very hard to do with web dev If I want to know what is the current version, I can just say head, and the version header tells me what the current version is I think there's an example, I think, I read this up on Hacker News one the web dev equivalent is like I don't know, 50 lines or something okay so the semantics for these headers there's a nice symmetry to it for both get and put. So we're going to look at get and then we're going to look at put, post and patch So if you just do a regular get, it says, the semantics is give me the current version If you say get with a version, the semantics says give me this version If you say get with the parents, it says, give me updates since that parents And so that might be just a patch, but it's up to the server to determine how we can have additional semantics later to constrain that And if you say get with version and parents, that's it says, give me the updates from these parents to get to that version. And so that can give you a range of history Likewise for putposts. Oh, do you want to speak, Mark? Just a question clarification. If you can go back to the get slide so it looks like on that second box, the version header is modifying this semantics of get. Is that the case? Yes OK Okay. So how is that backwards compatible with the deployed? web? If the server does not understand that semantics, then it returns a research"
  },
  {
    "startTime": "01:40:01",
    "text": "without the version header. Right, but it might be going through a cache Yes. Cash won't understand it That is, okay yeah. Yeah, so it will return the representation that it has in cash. Yes. Is the client expected to notice that the version is different? Yeah, I think the client, if there's no version header in the response, because the cash won't return a version response. Well, it might. It just might be different version. Oh, yeah, that's true okay yeah so then the client would recognize that but so it's responsible for for rectifying that, effectively. Good point yeah yeah and i'm just i'm a little worried that it does have a way to force the issue, as it were. Okay, interesting And I'm just, I'm a little worried that it doesn't have a way to force the issue, as it were. Okay. Yeah, so just interesting. Okay. Great. Thank you Okay Okay, cool. So semantics for put um if you you can put as normal without a version You can also put and say what version it is You can also put and say, if you put with a version, but you don't specify the parents, then the server presumes its current version is the parents. If you put without the version, then the server can make up a version ID for it Or you could encode everything This is kind of a side topic So this is multi-response is a different proposal that I haven't written up, but just to sketch out if you want to request a version history then you need some way for the server to return multiple responses or multiple things. So this is just a sketch for that. And there's also I haven't mentioned there's a current version header where when you're returning multiple responses, or if you're putting a bunch of versions, you can say which one is the current one that you have which can be useful Okay, so now the can say which one is the current one that you have, which can be useful. Okay. Okay, so now here's some crazy things that you can do with version type So distributed systems have a lot of different ways to represent history"
  },
  {
    "startTime": "01:42:01",
    "text": "and they can do different kinds of optimizations for different cases based on that. And because each version idea is an opaque string, we can now have a version type that describes the contents of that string and gives you all these abilities So on the list, we had a bunch of comments from CRD people who were saying, this is not going to be optimized, but it is going to be optimized using this stuff. This is how you optimize it. So, for instance, some systems use a vector clock If you have a known set of peers like Mike, Steph, and Greg, then you can encourage your version as a vector clock. And this like you compare any two versions and know which one is newer than the other without looking at the entire DAG And you can do that in O of one time, which is pretty nice But you have to have a constraint set of peers The second example, this is you can have a run length encoding version type And run-length encoding is what lets CRDT become pragmatic. So this type you that each version is an agent ID like Mike and then the number which is the clock for Mike, and that tells you, so now if Mike, this, this optimum because when we write text, for instance, we tend to go left to, right and insert huge strings of characters and each character is going to have the version of one plus the previous version which means you can infer them and you can throw away all those, all that intermediate metadata and know what it is which gives you huge compression And the last one, version type Git lets you say that the version is going to be a hash of everything, and then you can verify the hash So there's all these different properties that you can encode How much time do we have now? Still plenty of time. It's 15 minutes. Oh, great okay. So this is relevant to the group since we're just discussing resumable uploads. We can do a version of resumable uploads, at least the basic version without any additional headers, by just to find a new version type. And this is taking advantage"
  },
  {
    "startTime": "01:44:01",
    "text": "of the basic symmetry between time and space that when we are uploading, we are uploading, let's say, one bite at a time And so across time, basically time is proportional to the length of the file that is being uploaded So we can have a version type called a byte stream, which says that this resource is append only sequence of bytes. And now we know that at time T, the length of the resource is t-bytes and so we can use that on the client. If let's so the we're doing an upload, and the upload gets interrupted Then the client can just ask, well, hey server what time are you at what's your current version of this resources And the server can say, well, I'm at time 500 The client now knows that you've got 500 bytes And so taking advantage of the symmetry between time and space for a stream to upload we don't need a new header. And there are other nice symmetries for CDNs as your talking about large objects, we're changing large objects, especially with range requests. We can reuse a lot of ranges from patches at various points in time so that's kind of nice nice We talked about GITs. Okay we've got a lot of implementation. So, for instance, this is a Chrome window on the left with collaborative, this is a text file, add a URL that is also a collaborative editor because we've added collaborative editing to the region itself. And so you can open your Chrome Dev tools and you can see a version history with a DAG, you can go off editing to the resource itself. And so you can open your Chrome Dev tools and you can see a version history with a DAG, you can go offline and edit it, you go back online, all your edits get merged. And we also have this working in regular web apps where all of the state is hosted in this way. And so then you get"
  },
  {
    "startTime": "01:46:01",
    "text": "you don't have to use tons of layers of different JavaScript packages to stream your state over a web socket So just some examples it's working. We have a, we have working feeds. You can recal state over a web socket. So just some examples, it's working, we have a, we have working feeds, you can reconnect to the feed and get the latest stuff and back to questions so that's that's a sketch of the solution, sketch of the problem. These are the questions, are we interested in this problem? Do we see? feasibility in the solution and long term? What does it take to get towards adoption? Okay, so let's open it up. Any discussion comments I am seeing some interest in the chat in in in problem space Lisa seems to want to say something, I think. No? Okay, Chris chris lemmons Yeah, I'll say it to mic when I mic, some of what I said in the chat. I have interesting the problem space. I think you're solving a real problem And I think there's value in having a standard solution to this problem as opposed to what we currently have, which is a lot of different ways that genuinely work, but they don't interrupt And importantly, they don't operate well with caches in the middle. And so I think it's going to be important that when we look at this work, that we think hard about what caches in the middle does to it And I think, you know Mark is, uh, made some observations about what the current implementation is likely to do to caches, and I think those are spot on and we're going to need to think about that if we want to move this work forward But I think thinking about the intermediaries is super valuable here because that's what that's what the standard is"
  },
  {
    "startTime": "01:48:01",
    "text": "buys us, in my opinion Great points yeah and just to piggyback on that i think personally um it's the integration in h HTTP is where this group can provide the value to make sure that it plays nicely with the rest of the ecosystem in all the different features. I mean, this is always the problem when you introduce a new HTTP feature It has to mesh with all the other defined features in some fashion In my mind, the big question here is, how do we chunk this work up so that it is, manageable and of the appropriate grant? other defined features in some fashion. In my mind, the big question here is how do we chunk this work up so that it is manageable and of the appropriate granularity, and I'm thinking probably a there's a versioning framework and one or two versioning schemes that you'd want to ship to start with and there are other further out things like the byte ranges stuff you talked about scares me a little bit because we already have a way to do that And the multi-status stuff, there are some scars around that that have been, we have a lot of lived experience around and maybe we don't want to repeat that but but that's those are separable discussions I think. And so if we can focus on the framework I think this has a lot more chance of viability because I am seeing a lot of interest in it in the general problem area so next stop, please Coheuena of Google, one thing that might be a tangent to the spam is that we struggle with JavaScript resources. So in particular, we try to cache compile resource while it's maybe difficult to make it tie to the previous versions of the compiled resources since we don't really know, like, what has changed since, and so we end up completely discarding them and compiling the new one again and since JavaScript resources tend to get larger and larger we hope to reuse the previous one With this budgeting scheme, it may be a step forward"
  },
  {
    "startTime": "01:50:01",
    "text": "in that direction, where the data is much created and we can thus only compile the delta and apply it to the latest version Okay, so to clarify, it's a little bit hard for me to hear, I apologize, but I think you're referring to compiling JavaScript resources Yes. Okay, yeah, and then as one updates, you need handle the deltas better if you have conversion Yes Thank you Tell me, Polly mainly speaking as chair here I just kind of want to echo what Mark was saying about kind of keeping the pieces separable and also just like, thank you for, in this presentation, kind of focus on a particular flow and having it be very clear I think this type of approach compared to some of the other iterations of this, like it's going to be a easier for the group to digest and understand And I think there's aspects of the framework we need to do, but also teasing apart what are the individual building blocks and how can we do the technical work on those building blocks separately and clearly will help make it successful So this is a good step in that direction Cool. And then what I'm, I think what I'm hearing I think Mark's comment is definitely resonating in my mind finding something that's going to work with existing problems And I think if I could probably get some help on just getting a good framework for testing I think that would be really helpful. Maybe if we had that demonstration for the basic get-put patch of versioning that would be a good step I suspect that people need a little more time to kind of let the draft soak in and think about the implications and that could be a little bit of a process we need to go through okay it's it's and I know that's maybe a little frustrating because you've been doing this for a while now, but let's"
  },
  {
    "startTime": "01:52:02",
    "text": "everyone take a look at it and maybe give some more comments but keep the discussion going on the list, definitely. And we can also have a background discussion about what the path forward might be cool any other discussion Right Does it have to be on this part? Anything on this? On this draft On this presentation. Yeah, this is the end of the press This is the end of the presentation, so anything thoughts are open I was looking at the draft and the subscribe notify stuff, like the subscription stuff Yeah, and that's out of scope for the versioning, but it is interesting love to hear about it. Okay, I will can talk offline. Sounds great All right, well, thank you, Michael And thanks everybody else. Thank you to our various scribe And we'll hopefully see everyone or at least some of you, in Dublin, I think Yeah. Thanks And it'll put the right last time, but this was good. We had a nice amount of discussion and I feel like There is still some things that we're looking at adoption here but the number of big chunky ones is cookies is going to be like chunky ones and if we do eventually adopt this, that's going to be relatively chunky, too, I think, to get it chunks that go together. Sure, sure, sure, sure like a series yeah yeah Or many chunks that go together. Sure, sure, sure. Like a series. Yeah. But I want to make sure it's not quite there, but it's not quite there, but it's definitely showing promise yeah yeah I think this is nice decomposition I mean the problem"
  }
]
