[
  {
    "startTime": "00:00:15",
    "text": "all right welcome to the sex dispatch working group this is not your intended destination so your last time to get off the plane this is the note well note it well should\u0027ve seen this at least a couple times by now but these are the rules governing your intellectual property right tonight\u0027s hip contributions so you should be sensitive and comply with them administrative tasks blue sheets are circulating one of rich Saul\u0027s and Joe halls Joe Hall is gonna be our Jeffers scribe and rich Saul\u0027s and Geoff Hodges of kindly agreed to be our note takers so thank you for that reminder of our dispatch process we are here to dispatch things not to actually do work so we\u0027re gonna have some presentations about proposed new work for the security area and we\u0027re gonna decide what to do with that we might drop that into an existing new existing working group who might have a new working group to handle this often pre focused working group got to get specific piece of work done we could recommend that a tease that an ad ad sponsor this if the ABS are willing and we can kick things out for additional discussion or say we shouldn\u0027t do work here so that\u0027s kind of the spectrum of possible outcomes we have here so keep those in mind as you\u0027re hearing these presentations for proposal to new work because we\u0027re gonna try and decide on one of those for each of the pieces of new work that\u0027s proposed today so here\u0027s the agenda we have five things proposed for discussion today we\u0027ll try and make a dispatch decision about each of those any agenda bashes at this point okay of course sorry about that I would like to introduce our at the end of our slides here yeah so as one final item of chairs business I would like to introduce our new chair you may recognize her as our former security Adi Kathleen Moriarty so let\u0027s have a round of applause welcome grim [Laughter] yes and I\u0027d also like to thank Grubman for all this hard work as co-chair overly it\u0027s last couple years and you know much as I\u0027m sad to see you go I\u0027m happy to see you step up as a D so thanks again for your service all right and miles okay so miles I think if you can join the queue queue I should be able to give you a speaker power okay okay hello is this working my herb thank you is the audio "
  },
  {
    "startTime": "00:03:16",
    "text": "working yep works fine you can go ahead perfect then I\u0027d like to know right now that I\u0027m joined in the remotely by mica who was another one of my co-authors so at certain points he may also want to interject I\u0027m not sure how we\u0027ll get that to work thank you for the to the chairs for giving us this time today we\u0027d like to speak about key lists which is our proposal for a system to distribute open PGP key public key identities across larger organizations so if we could go to the next slide thank you so the problem that we\u0027re trying to solve is that large organizations typically need to keep their potentially tech illiterate employees internal key stores up-to-date across all devices at the place that mica and I we use PGP for email and for other purposes - such as code signing and we have a lot of people coming and going all the time so keys are frequently added or updated and not all keys identities or the emails associated with them correspond to one single control domain so we have certain employees whose preferred keys correspond to a gmail email others MIT others the intercept and so existing solutions such as web key directories aren\u0027t always workable in our organization next slide please so in an email to the open PGP mailing list where we\u0027ve had some existing discussion one of them a micro wrote what are the main purposes of Kilis is to reduce the amount of work required to ensure that everyone in an organization or anyone who wishes to communicate with that organ members of that organization have the correct public key for everyone else so really the purpose of the key list is to ensure that a certain number of keys are constantly kept up to sync in the internal public key stores of either members of that organization or people who\u0027d like to communicate with those people notably we are not distributing the keys themselves we are just distributing pointers to those keys currently in the form of key fingerprints which then allows the clients to leverage existing open PGP key distribution systems such as key servers or even web key directories to actually pull the public key content next slide please thank you so our solution is that organizations publish what is effectively a list of public key fingerprints in the form of a signed key list if someone would like to subscribe to that key list their PGP client will "
  },
  {
    "startTime": "00:06:16",
    "text": "refresh and or import all of the keys on that key list at a user-defined interval that would be set up for you know certain employees when they are on boarded for example and the result of this would be that users automatically have the latest version of their colleagues public keys this is a system that is currently actively in use at first look media the intercept and the freedom of the press foundation using a tool called GPG sink which Myka is the author of it\u0027s completely open-source and it works incredibly well at least for our use case next slide please so this is an example of a key list you\u0027ll notice that there are number of metadata fields and that it resembles some sort of key store so you\u0027ll note that a key list is signed and that is an important element so every key list can be authenticated with an authority key so for example the the tech administrators at an organization have full control over the key lists but individuals can still push updates to their keys without having to change the key list and ask the administrators to update the key list so the actual keys themselves remain in full control of the original owners where and and and the tech administrators just ensure that every other person in that organization is constantly updating those keys next slide please so there has been some discussion as to whether this is actually suitable to be an internet standard and and it\u0027s it\u0027s a valid this it\u0027s an absolutely valid discussion this is very much sort of an application discussion but the reason why we\u0027d like to turn this into some form of internet standard whether it would be experimental is still absolutely up for discussion is that we want multiple PGP clients to be able to import and subscribe to the same key lists right now GPG sync only interfaces on top of GPG and lots of other PGP clients especially those that exist on mobile currently cannot be used with key lists so what we\u0027d like is for there to be a single format standardized format for key lists that allows other PGP clients to subscribe whether they are traditional clients such as GPG or newer mobile clients such as for example pro tunc male next slide please we\u0027ve also gotten a fair amount of discussion as to why not such-and-such why not such-and-such and really helix are trying to accomplish something else so with briefly two web feed "
  },
  {
    "startTime": "00:09:17",
    "text": "dictionaries in x.509 PTI we don\u0027t with key lists want to actually manage the distribution of keys themselves we want to keep the actual keys in king but the we want to keep ownership of those keys in control over those keys in the hands of the actual key voters so what key let\u0027s do is it simply points to the keys and then individuals can push their own updates to key servers themselves so that\u0027s one fundamental difference between those two existing systems and then finally Jinyu PG key rings so the same is above apply and also as far as I know Mike have pointed this out GPG hearings are not actually an internet standard and so key rings for GPG two are incompatible with key rings for GP g1 and what if I don\u0027t thing to note about why web key dictionaries for example do not work in this case is that all of the keys fall under different domains not all of which are controlled by the central administrator so key lists allow an organization to distribute keys across from several different domains within an organization even if they don\u0027t themselves have control over those domains next slide I spoke with this briefly earlier but this is a system that is not just a theoretical prototype it\u0027s implemented GPG sink and it\u0027s been an active use in first look media at the intercept and the freedom of the press foundation with collectively probably more than 300 employees for the past two or three years or so so this is a this is a system that has been sort of tried and true within our organization and that other organizations are actually looking to to potentially use themselves next slide please there are a number of areas that we\u0027d like to further develop so we\u0027re considering adding a well-known location and there\u0027s some discussion over this is whether this is potentially a security vulnerability because the authority key needs to be inputted separately but that is a discussion for elsewhere we would like to consider additional functionality for key lists we\u0027d like to better analyze the potential security implications do general improvements to our draft you you may know that Micah and I are relative newcomers to the ITF community so what we are grateful to all of those who have engaged with us and provided us useful feedback so far and we look for even more of that and then finally we\u0027d also like to just achieve wider adoption by existing PGP tools and much of the PGP community also is in the ITF community so those are the five major points of development and of course eventually we\u0027d like to push this out either as an experimental draft or an "
  },
  {
    "startTime": "00:12:17",
    "text": "Internet standard and we\u0027re hoping that we can be dispatched today thank you that is all thanks miles we\u0027ve got a couple folks in microphone queue here so high school I have a potentially dumb question you say um so as I understand it the keyless contain fingerprint of the key to the public key right and you said you can update your key how do those work together right so the key list contains a fingerprint of the public key so let\u0027s say I have a private key yeah if we go to that slide thank you if I so Mikey you\u0027ll notice it\u0027s it\u0027s the second one what I can do for my if what I can do from my computer is because I have the private I can update my key and then push it to the key servers and that fingerprint will not change so what be a HUD sorry I just offer a clarifying thing that I think may be accurate missed which is that this is a fingerprint of your open PGP when people say key here it\u0027s confusing because the PGP terminology is confusing it this actually represents the fingerprint for an open PGP certificate which is also known as a key block also known as a transferable public key which is an Internet Center which is arts and 4880 which can be updated by adding subkeys by Chinee expiration dates that sort of thing so this points to and identifies a certificate that can be updated without changing the fingerprint it\u0027s the root yeah effectively if you want to change what which you have to you have to update this and sorry miles I just think that I think that wasn\u0027t clear for folks who you thinking about keys from like an x.509 perspective because you have got your mike is there a need for hash agility here this is dkg at the Mike I do not so there\u0027s been a lot of back and forth about the way that open PGP fingerprints are formed this is not the right this draft is not the right place to do that we should use whatever this if this draft goes forward here at value temp we should make sure to use whatever the OPC fingerprint standard is mr. Kinnick 9k tech so I was taking notes a few questions first you sort of mentioned that web key distribution is not usable for your use case because you\u0027ve got all these people that have keys at different domains and the domains don\u0027t have a common administration and you also mentioned you have lots of other organizations that are looking at this so it\u0027s kind of curious if these other organizations have the same property where you know they have these multiple domains the keys and so they can\u0027t use wkd if I may I could let Micah answer this question because he has been at first look for a while longer and "
  },
  {
    "startTime": "00:15:18",
    "text": "oversaw the initial development of GPG sink so Myka if you could raise your if you could join the discussion yes I\u0027d see I seem like an acute Robert Myles I\u0027m gonna have to eliminate you from the queue I think yeah do I have that that is fine thank you uh hey can you hear me yes okay so with first look media that was wasn\u0027t really an option because we have like several different domains internally we have like first look org and the intercepts calm and feel division org and stuff I think that that might be appropriate for some other organizations like something like there are many organizations where everyone has email addresses at the same domain but a different issue is that if your organization starts growing and getting to be bigger it still is kind of hard to scale people posting updates to their own keys so if you have like 50 people and someone\u0027s key expires and they want to update their expiration date and push it with wkd they have to you know update their expiration date send a copy of their new updated public key to the administrator to post it to the wkd directory so yeah yeah perfect thank you so my next question sort of relates to what exactly is being signed so if I can pull the example back up I think was in one of the slides I miss you got the metadata with the signature and then the array of keys has key fingerprint but also a field for the name the email and a comment and so like are we claiming that this the signature from the authority key is like being authoritative for the actual name and email address as opposed to like the actual PGP key or has its own signature over the name and email address like I was supposed to trust both of them or you\u0027re supposed to only trust what is in the what\u0027s in the actual public key block the name and email address in this are optional and this is basically just to make it easier to read the key less what\u0027s actually being assigned is this entire JSON block so this is like a file that and then there\u0027s a detached signature file and you have to have both of them and if you want to you could add more metadata to this file but if you want to the only required field for each each item in the keys array is the fingerprint field yeah thanks this is dkg again I\u0027m resisting the urge "
  },
  {
    "startTime": "00:18:18",
    "text": "to like get into the details here I think this is an interesting solution to what I do think is a real issue for organizations that want to adopt protective mail so I would like to see work on it I just want to observe from a dispatch perspective that this seems to depend heavily on the existence of the public key server Network and as someone who operates one of the key servers in the public key server network I can tell you it is an unspeakable disaster right now the key server network is not healthy it is not designed to withstand attack and has recently been attacked by people who are desperately trying to prove that it can\u0027t withstand attack and it can\u0027t and so I don\u0027t know that the key server network will persist in its current form and so if this if this work depends on the key server network which is not an IETF standard and is possibly not going to be functional even two years from now then we need to make sure that we think about that for how we dispatch the work and before you get up the stage since you receive your linkers do you have a preference on how we dispatch it I am X co-chair of the open PGP working group which failed to get a revision to the open PGP spec sighs so that would be the natural home for this but the working group got closed for failing to get that done so I don\u0027t know how to dispatch it sorry and I think that\u0027s a good point about relying on the key server network but like miles was saying that really it just needs to rely on some type of way to fetch keys so it could be from wkd or other methods as well but yeah there is it but like if the t server the key servers are really how is the only way that GPG singh implements it and yeah there needs to be some way of a public way of fetching key it\u0027s like they could be from you know key base or whatever but there needs to be something yeah so that might that whatever that other something is if we decide that the key server network is not an option for making this viable might might make a make us decide the different thing about this batch that\u0027s all so to reiterate it they\u0027re kind of the back and forth of this conversation that\u0027s pretty much saying if the the initial feedback is interesting on the draft but we have to pull more threats than just this draft if we were to proceed forward we need to discuss the robustness of the key infrastructure of the key servers I think the draft as it stands does depend on the key servers or as Micah said you have to get it from somewhere and the only somewhere that\u0027s been defined is the key server network and I\u0027m just giving you a warning and it heads up that I suspect the key server network will not survive um I mean it seems like that attention while giving up a lot of flexibility that could potentially be resolved by by actually just stuffing the keys so that was know as Ecker "
  },
  {
    "startTime": "00:21:19",
    "text": "saying that and I wanted to can we go back to the side that said why we don\u0027t just stuff the keys there was a couple different arguments for why so your final argument we I can\u0027t address I think that\u0027s correct the the and the top one obviously just doesn\u0027t work for the use case you\u0027re talking about as far as good new PG key rings there is an internet standard there\u0027s a cross compatible internet Center which is just the transferable public key itself RFC 4880 actually says transferable public key packet sequences may be concatenated to allowed transferring multiple public keys in one operation so that is what the original open PGP the original group eg keyring was is just a series of transferable public keys and so that would be possible that we away around and that\u0027s actually something it\u0027s not in our draft yet but we\u0027re discussing making an optional to include a copy of the actual public key inside the key list basically making it required to include the fingerprint but optional to include a copy of the public key but but you know so it\u0027s with a the first look media key list we have about 230 fingerprints on it and there\u0027s a lot of people who fairly routinely update their keys mostly their expiration dates but occasionally add user id\u0027s or other things like that and it would basically you know require way more updates to the centralized key list each time any little change happens if we if we like had to do it so we\u0027re not doing that now but I mean if key servers are completely broken then you know that that might be a way to move forward this year which girl again I\u0027m trying to wrap my head around the semantics of this object mainly it\u0027s signed what is the suitor attesting to okay so this the purpose of the signature is to make it so that okay so this object is at a URL somewhere and people\u0027s key lists clients like GP g-sync download this URL download the signature verify and then import it their purpose of the signature is so that if there\u0027s like maybe the server on the internet that you\u0027re downloading it from is what is hacked and an attacker tries to replace this key list with a different key list it prevents the attacker from being able to put an arbitrary set of keys on everyone\u0027s computer basically it like the idea is that centrally managed and this proves that you know whoever has the authority to manage it you can post updates yeah Ben can look again so I guess my take on this is that what\u0027s being testitude is basically these are keys you should be interested in and like they\u0027re probably people who are in your organization but you\u0027re gonna have to use the actual keys themselves for trust "
  },
  {
    "startTime": "00:24:20",
    "text": "decisions about your communications and code signing whatnot and dkg is reminding us that we should not have the actual like technical discussion hearing so I\u0027m trying to refrain myself so well I\u0027ve got the 80s at the mic um you know yeah well let\u0027s try and take a cut at making dispatch to sit in here so this seems like a pretty well consolidated piece of work that doesn\u0027t make might not need a whole lot of iteration before publication is this something that seems suitable to folks for 80 sponsorship just to propose an outcome here do we want to ask people are interested in having an ITF do the work first sure anyone else in the room want to speak up in favor of you or favor of doing this work in the IETF or people who are concerned this is not good work to be doing the idea not seeing anyone like anyone besides the author\u0027s interested in this work okay see four five or six hands okay this that seems positive and it seems like a positive indication of some interest in this work and I\u0027m not seeing anyone saying it shouldn\u0027t be done here so that brings us the question of how to do the work here so we\u0027re back to discussion of should it be working group should be ad sponsor it well someone who would not be having to manage their working group or ad sponsoring yes um I mean that does not seem like enough critical mass or working group at this point so I think maybe disagrees like his father so dkg is pointing out without standing up to the mic that there\u0027s discussion going on on the open PGP list which is pretty active and like this getting actual discussion there I agree with that here that yeah yeah activity here is not really enough to spin up a working group just for this but it seems like we might still be able to get enough for you to still publish the document all right so um I agree with open a little bit too bad because there are a lot of subtlety and sort of this this kind of bag of keys approach like around hard what\u0027s the semantics around expiration and how long are these valid right what does it mean when a or an entity used to be part of the list and now isn\u0027t right stuff like that has applicability abstract model has applicability in a bunch of places and it\u0027s actually being tried in the IDF before there so so the approach for managing PPP bdp keys that\u0027s sort of "
  },
  {
    "startTime": "00:27:20",
    "text": "similar in semantics actually so you have a bag of a bunch of keys and sign them right so I think if - it would have been a good idea to have a working group actually look at this a little bit more generally than just PDP but I realize it\u0027s not gonna happen right now incoming ad Roman Danelle us incoming AJ I\u0027m with Ben I want to see a little more discussion about the edges of this work on the so Roman you just said on the open PGP mailing list on the ITF open PGP at IETF dark that\u0027s the mail so you say you said either good yeah so is it exporting group so let me suggest the summary of this um that the authors continue discussing the open bgp mailing list and refine this proposal and when they feel like it\u0027s officially baked to be publishable as a that as a you know standard they bring it back to the eighties or to sec dispatch for on publication that are say all right that sounds good to me no takers thank you for taking that time all right Mike and miles thanks for your time today thank you very much and next up is in yes some of you have already heard this specific this talk so maybe you can sleep a while it was a little bit interesting to hear about this PT list because they mentioned detached signatures in my opinion that means that they probably need chemical ization as well because how can you do that without okay if you want to try something of what I am talking about today you can you have a URL this mobile peak why you can play around to see that it exists and how it works can test the boundaries of it and you can of course read the internet draft okay once again we\u0027ve been hacking it\u0027s something wrong with chemical ization I\u0027m sure about okay you\u0027re back can we have a full screen as well okay almost mm okay "
  },
  {
    "startTime": "00:30:23",
    "text": "chemical ization can be done at different ways one is to do it on text level only I skip that idea because essentially get two streams when if you would first first you would pass using your regular JSON tubes and then you would have another line where you do the chemical is Asia that is technically possible but it would integrate very poorly with existing tools the idea with this system is that should integrate into existing tools with ease this picture shows what conical ization does it changes it normalizes numbers when I started with this long time ago I thought that must be very easy to write a number I was totally wrong it\u0027s rocket science fortunately I met a few rocket scientists that fix this problem for me which I definitely could not do fixing string normalizing strings that is over completely trivial sorting keys in lexicographic order is also trivial that\u0027s nothing what you end up with at least if the fall of the draft is a limitation jason numbers must stick to a cheaply double precision that doesn\u0027t mean that your application is limited by that but using the canalization system you must stay in that limit ideally you should characterize all data that means that people put things into JSON strings like date types and big numbers etc but in order to do that you would have to define a strict serialization of all these data types that falls outside of what we were interested in we are only targeting cryptographic operations and for that you don\u0027t need to - to do full cannibalization we are satisfied with doing hashable Jason however it\u0027s not this is what this picture shows essentially the only snag I am aware of and that is that if you pour something like this date stamp and then you serialize it it must be serialized as this in the same way and that may affect some applications it may affect some horses but if you know this you won\u0027t have any big problems at all using this system the motivation behind this was as a gue better first page is to not have to put everything in base64 one of the "
  },
  {
    "startTime": "00:33:26",
    "text": "reason is that I\u0027m working with financial applications and the financial people they are sort of unused to basics before they don\u0027t want to see payment requests or things like that in base64 they don\u0027t have to and my mission here is to see if there\u0027s any room for this in ITF with a goal of making some kind of standard that is essentially what I have to say about this it has been a multi-year research effort and I think it\u0027s concluded now it\u0027s no its implementation adoption and standardization that\u0027s left I\u0027ll just like to note that asn.1 in the canonical representations should be be serializable and be serializable before you verify the signature in a x.509 certificate yes in practices it\u0027s completely untrue yes you have to keep the raw bytes around and verify a signature on the broad bytes of the to be signed of certificate yes and so I\u0027m very very worried of this actually working I understand I understand well if you stick to what I called native Jason that means the literals the true/false known the strings and the yes number I will say it\u0027s completely a solved problem it actually works it works like their encoding it should work in asn.1 but I know that they were our Asian one it\u0027s a much more complex system so I think that the slide that I have now shows the only known problem with this system matthew Miller I mean so we did have some of this discussion earlier today in dispatch for people that were there so this is this me partly reiterating so my concerns from there like I think for what this is doing it\u0027s it\u0027s probably not harmful but there are there are implications to doing canonicalization and I think if we\u0027re going to do this work we need to we need to understand the shape of those before we try to try to standardize anything around this oh yeah you are near so um anything you can canonical as you can then hash and send a signature without sending the raw bytes and it\u0027s all great but why is it here I mean you kind of know if you cannot analyze email you get a shovel email due to HTML you cannot get a shovel HTML I don\u0027t really expect all these things to come into sec dispatcher and in general the security area work so while we it\u0027s like this that\u0027s rather than it\u0027s dispatch I have no opinion exactly where this "
  },
  {
    "startTime": "00:36:28",
    "text": "belongs but to me it\u0027s it\u0027s designed to support to be a building block of secure protocols so therefore they and the application is harsh obligation rather than canonical Jason that\u0027s the reason and so it is mainly an interest for people working with secure systems at least as it stands today I think the current use case that you\u0027re looking at is will may be really good security but canonicalization and hashing is far more generic and there are lots of protocols around sure sure it\u0027s it\u0027s not a cryptographic protocol it\u0027s just a building block that could be a part of cryptographic system you you know you jbs container for instance you can you can open up that and make that readable using this system I have done that and it works fine you don\u0027t have to have a base64 encode the container for signatures right Jordan thanks for doing this I fully support this work I know a lot of different groups that are dependent and on some of this working and being made available I think it\u0027s very important that it gets done here in the IETF so we maintain control over it and it it\u0027s just it\u0027s really great and I noticed that you\u0027ve done a lot of implementations in various programming languages and they all work I\u0027ve gone through it and tested some of them so yeah I think this is very important and I would really like to see this take place I know there\u0027s projects that I work on in the IETF and outside the IETF that could make use of this and then meet this so thank you thank you Eric rajala no formal status here yeah I\u0027m not not super enthusiastic about this for several reasons I said some these things earlier and dispatch to me I\u0027ll be sure they\u0027ll be shorter this time first of all on the motivation of this seems extremely innovation as far as I can tell us people don\u0027t like the BC to run calculation that\u0027s accidents like a very good reason at all but deliberately would last several times we tried to design secure container formats we\u0027ve deliberately avoided canonicalization to deliberately go on with um I\u0027m you know with with data preserving transformations instead um precisely because every time we try canonicalization has been a debacle it\u0027s not just a matter of pick whether you can canonicalize or not it\u0027s that people sign the knot and commodifies format a lot of the time and then you end up having to not having not use the canonicalize format because even though the conversation may should be trivial you end up with non-qualified signatures on second um it\u0027s actually not desirable to have the DB brought the raw data and this injure intermixed because it encourages people to make use of the data before the student validation hasn\u0027t happened so that\u0027s actually quite undesirable so um is it so generally i "
  },
  {
    "startTime": "00:39:28",
    "text": "think probably this is not actually something we should be doing that\u0027s it hi this is Sean Turner um so I think there\u0027s dragons here from a number of perspectives one Jason I think it\u0027s not really on Connie isn\u0027t economy is not of the ITF there was a JSON working group there was a JSON this working group that was there there\u0027s a whole bunch of history for people who like talked about canonicalization I think that consensus of the JSON working group what they didn\u0027t want to do canonicalization so if you\u0027re gonna do this and take this on you\u0027re gonna have to talk to the archives to make sure that we\u0027re all on the same page um otherwise I think we\u0027re just going to be an endless disaster with people killing each other of these defense and that probably can add a lot more context to this so it\u0027s a matthew Miller one of the former JSON and JSON best chairs I wouldn\u0027t say that there was consensus not to do it I would say that there was there was there is absolutely no consensus to name any way shape or form to do anything because there are so many dragons and nobody could agree on what what could be done about them I understand well you have to trust me that the work has been done to sort of research the issues around this and they are clarified in the draft you are free to find homes in the specification of course that\u0027s very important but I claim that the problems and the solutions are here they are already occupant \u0027add they were not available the time the Yossi group started nobody have done that work before and actually I got the information from people in the Yossi grow about numbers which was the most complicated part it was said that it was impossible but it\u0027s not it\u0027s a public algorithm that does this the folks who keep comments briefer come up time here alright so I wanna have my voice to Eric\u0027s point I don\u0027t think it\u0027s a question of whether you can do this you can do it safely I think either it will encourage people to use data before validation which is sort of where a lot of the bugs in XML signature comes from I say because moment as I have said in The Dispatch already this is three different things one is define the data model of JSON which is not defined at this point in time which is a really good thing to do second define a consistent or deterministic mapping from the data model to take strings which once we figure out that we really have it is also a good thing to have I have use cases for that and the third thing is define security protocols that make use of that in one or the other way and I know several security programs I don\u0027t want to do on this based on this but I really want to have the first two things "
  },
  {
    "startTime": "00:42:28",
    "text": "but those are we application area things not really security things yeah I think thanks for that segue Carson we\u0027ve discussed a little bit among the chairs here as this discussion is going on it sounds like there\u0027s pretty good agreement in the room here that this is not sec area work we shouldn\u0027t take on any work and set carry for this but there may be some in the art area is an interest so like so yeah we like screwed this up I\u0027ve had in the wrong dispatch group so yeah I\u0027m gonna say most people object that our dispatch outcome here is no work in the SEC Arya but feel free to try with this patch okay sergeant sorry oh sorry I got you the runway runs thank you and David Ross hi my name is David skin\u0027 ozzie I\u0027m at Google and I\u0027m here to talk to you about masks so first off apologies for the very convoluted acronym but I thought an acronym would look cooler so this is very much not a full-on oh we have all this work that we\u0027re absolutely doing we want to do it in the ITF this is very much more early stages more of a thought experiment of something that I\u0027ve been toying with for a few years and kind of thanks to a few some people in Bangkok came up with a way to do something and I wanted to present it here to have people tell me either on one end of the spectrum that they\u0027re interested in that this is useful work or on the other that I\u0027m horribly wrong and that I\u0027m gonna do terrible things because at the end of the day I\u0027m kind of rolling out my own crypto and it\u0027s totally fine i follow cryptographers on twitter so I know what I\u0027m doing alright so what\u0027s the idea here as I\u0027m sure you all know internet censorship is on the rise there are more and more instances of governments and other parties getting clever and clever about ways to either block access to things or to detect that people are accessing things on the Internet and I personally feel very strongly that we should fight that or give people or ways to go around that especially you know journalists and people doing things that we think is for the ethical good so in particular a lot of these systems will go out of their way to filter what you can access by like IP or S\u0026I and will block any kind of technology that works around so though detect IPSec yes and I go and block these and the the way "
  },
  {
    "startTime": "00:45:28",
    "text": "these work is that any traffic that sticks out that doesn\u0027t quite look like web traffic is just either completely blocked or logged and you know then you get a nice visit from government officials that have your best interest at heart one of the ins that I think we still have is that the Internet is still be very very big place and so it\u0027s very hard today to mount a whitelist like as a government of what websites are allowed so most informants are blacklist based or a heuristic based so the goal here is to come up with a way that anyone who runs any web site can run this kind of VPN server on it and that won\u0027t necessarily land it in the black list because it is hopefully very hard to detect so that\u0027s where we\u0027re starting with this to Capo a where I can run a VPN on my personal website and with and I want to really raise the cost for a government actor to detect that I\u0027m running this VPN so in terms of a threat model the main threat are passive observers so let\u0027s say this government has a tap in the ISP and can see all the packets going back and forth so we can\u0027t have any information plane test because they\u0027d see that but also we want to protect against active attackers meaning there already have been papers showing prior research showing that some of these entities will actually do active probes so for example let\u0027s say they see traffic to my website they will fly VPN protocols to my website and then if they detect that I connect then they\u0027ll block my website so there\u0027s been a problem with for tor for example so we want a way to prevent those kind of active probes from detecting anything so I\u0027m gonna go through a list of requirements for what specifically I focused on and to kind of bound the scope of this so kind of went over this a little bit it needs to really be hard to detect this so nothing that sticks out so the s and I needs to be the same as the ones that you\u0027re talking to but also the LPN because both of those are being sent in the clear today and I know yes and I is a good solution for these things but because of the way s and I is currently designed you can block it by just detecting that extension in yes and I in and of itself sticks out today so for the same reason is that we can\u0027t add any tight Els client extensions because those would stick out or any quick transfer parameters because those are also sent in the clear in the client hello another requirement is this impossibility to probe so one of the why this makes it a little harder is that a lot of graphic protocols rely on like signing and knots and therefore a lot of "
  },
  {
    "startTime": "00:48:29",
    "text": "them you start by getting the server to tell you announced a new sign it\u0027s but you haven\u0027t authenticated yet so you need to do this without having a way to ask the server for not so we\u0027ll see all that factors into the design later and another one is even today and not for necessarily censorship means a lot of networks block quick more just any kind of UDP so this needs to be able to fall back to something that looks like HTTP two instead of HTTP three when you\u0027re doing that performance will be worse but that\u0027s a fact we can live with as long as it doesn\u0027t stick out and is not noticeable so now going more into the mechanism and how this works so you start off by having your client initiate an HTTP 3 so HTTP over quick connection to the server and do a regular handshake validate the sort of certificate so for this you can validate the other web PKI but you can also in the client since this is running a custom client pin the cert and so we don\u0027t need to build a new authentication mechanism for the server we all hire II already have a very robust one for HTTPS let\u0027s just use that then because we can\u0027t request a nonce from the server the clever trick there is to use a keyless key exporter to generate a key between both sides and you actually use this key as a nonce so this is very similar to token binding in terms of the mechanism but very different in terms of what it tries to achieve because we saw it grabs this shared secret between the server and client without any explicit communication between them and then you have a nonce that the server has de facto added entropy to so it\u0027s not replayable between TLS connections but that the client can still use and then basically you sign this knots so use then send an HTTP connect request inside the the encryption and you connect to a specific mask protocol this is very similar to how WebSockets work for HTTP 2 so instead of connecting to a further host like Connect was designed for an HTTP 1 you say connect to this protocol which is just allows you to switch this byte stream to a different mode of communication between a client at server and as part of this you can pass in a username that\u0027s and an OID for which kind of either a symmetric signature you want to use or HVAC if you want to use symmetric keys because I had some people reach out to me that their deployment model couldn\u0027t usually symmetric keys so this also works for set your keys in hashing and your basic ste for encode all these things good HTTP is great that way and you just send that up to the server and then if the server validates your keys everything looks good it sends your 200 error code everything\u0027s great and then you have a byte stream to be able to talk mask otherwise it does the same thing that any HTTP webserver will "
  },
  {
    "startTime": "00:51:31",
    "text": "do if you send a connect to something it doesn\u0027t understand it sends your 104 protocol not support it and so if someone tries to probe you and they don\u0027t have the keys don\u0027t get the exact same response that if they try to send it to a server that doesn\u0027t support mask then once you have this channel you can negotiate what kind of mask features you\u0027re using so for example one the simplest one and I think probably the most widespread is a connected proxy so then you\u0027ve just enabled proxying on that web server you can connect to any other server on the Internet and that just creates opens up a byte stream to port 443 and then you do end-to-end TLS so effectively you\u0027re doubly crypting but then I the important part here is the mask server doesn\u0027t sees the clear text so for example on my personal website I could run this service as like just a favor to some friends in another country and I get them a way to get around the censorship where they live but I don\u0027t become someone who has access to the private information I still they can talk to this like news website that they\u0027re now out of Texas at home but I don\u0027t know what they\u0027re looking at I don\u0027t know their usernames or password and of course that works for any TCP base protocol they all can also use it to SSH or whatever another feature is domme you can just say okay mom my web server is also a doe server so that gives just private DNS all the way to the client for it today in today\u0027s internet part fir from clear techs UDP almost all the uses of UDP are connection based in the sense that you end up sending a bunch of packets to the same 5-tuple back and forth so in order to make that a little bit more efficient you do something which is similar to how Sox v5 proxies UDP you negotiate saying I want to be able to send you to be back and forth to this IBM port the server says here you go and then it gives you a net identifier and anytime you send a quick Datagram which is a quick extension right now with this identifier the server just takes that UDP and sends it to that server back and forth and that allows you to do quick inside quick or WebRTC DTLS any UDP based protocol that has a lot of traffic no a lot of sorry a lot of packets to between the same five tuples and then finally if the previous features didn\u0027t do what you want you can just bring up a full-blown VPN here where you just send data grams with 4 IP headers and then server treats some the same way as let\u0027s say an IPSec server handles like the inner IP header it just either forwards them or you can nap them in such a way that it becomes like the end server like the really further one doesn\u0027t know that there\u0027s necessarily a mass client somewhere it could all look like it\u0027s coming from your server so try try "
  },
  {
    "startTime": "00:54:34",
    "text": "traffic so traffic analysis is like the main answer I\u0027ve had from people about oh but you can saw like you\u0027re in trouble and I totally agree so this proposal really focuses on the bits that are obviously stick out as in clear-text and I would love help from anyone who\u0027s actually really knowledgeable in traffic analysis and the prior work that\u0027s been done there too like if they want if you want to contribute to this I would love to because that is the main risk here and probably the main thing that sensors will be using in the future and these two things I will skip I\u0027m presenting this in transport area later this week we\u0027re also be focusing more on the transport items so like I was saying not really asking for any kind of specific adoption or what to do just please tell me if you think this is cool or dumb or what should I do with this Thompson you should cover it in coming and throw it out the window and say what happens to it I don\u0027t know Thank You Martin that\u0027s helpful I don\u0027t know it\u0027s a weird question to ask you actually mentioned a whole lot of things in here and it seems like there\u0027s there\u0027s a number of things that you might work on in this area there\u0027s a lot of existing work in this area I think we\u0027re gonna be talking about using quick cos general substrate from multiple protocols at some point mixing them together are we talking about that donation fee to context as well and I don\u0027t know to what extent you\u0027re proposing to invent these things and put them under the same banner or whether you\u0027re just talking about one thing I would suggest concentrating on a number of things and doing some more work on this before we talk about dispatching this proper it\u0027s because there is there\u0027s this authentication piece that you\u0027re talking about which is kind of interesting and maybe a little more thought-out than some of the other aspects of this there\u0027s the integration of that that with all of these other features you\u0027re talking about is like a VPN you can have this you know this and this all of those require extensive protocol work yep and that\u0027s work that\u0027s going on elsewhere is in other drafts and other people working on these things I would encourage you to work with those people and work out what it is that those things can do before we get to the point of saying okay now we want to stitch them together and put put together some sort of package that has these sorts of properties I\u0027m seriously concerned about the encrypted traffic analysis of aspects of this one that is going to be you can\u0027t just go are you and we need to work on that one yes that\u0027s not what I\u0027m saying what I\u0027m saying is I don\u0027t know anything about that please help me out not we can\u0027t solve this like yeah and in trigger oh absolutely and to your earlier point I "
  },
  {
    "startTime": "00:57:35",
    "text": "totally agree I just like have had a list of requirements and motivations and finally came up with a way to kind of tie them in together but if we were to bring this and actually work on at the ITF I totally agree that it would be work kind of all over the place but I wanted to present the kind of stitched together picture of I think we can make this work here some please be brief ya know Khan Gilmore I think there\u0027s a lot of really interesting ideas here I would love to see these ideas go forward within the idea possibly in different places as Martin said I I love Martin\u0027s claim that you can\u0027t just throw your hands in the air about the traffic analysis and move forward but it\u0027s not true like the ITF has four years throwing their hands in the air and said we don\u0027t do anything about traffic analysis I\u0027m not saying we should keep doing that but like clearly you can and but but I think that this kind of thing is going to actually impact the decisions that are made on discussions like this bend it in quick because they will represent radically different traffic patterns and so I want to make sure I think that in terms of breaking it out I think you should break out your authentication piece as a first byte but the traffic flows that are likely to come from this kind of work are going to be very different from the standard traffic flows that you see over quick today and so I think there are gonna be impacts elsewhere things like that thank you Cathleen Moriarity just picking up on you saying the only risks and Martin\u0027s comments on honey it seems like you know how would I trust the specific servers that\u0027s right for some government agency or whatever to get at the exact point you\u0027re you\u0027re looking to prevent to stand up one of these capture the server you know the traffic and and then the points has been lost absolutely and that is true of any VPN and you don\u0027t need to convince me yes that most of the people signed those don\u0027t improve your privacy but yeah okay that\u0027s a good point I want that to the document you\u0027re correct okay so it sounds like dispatch outcome here is needs more work discussions thanks David all right thanks everyone for your time and next up we have yeah I was sort of thinking to myself you know if we made up a dedicated feeling list to discuss this like a show of hands if who would actually join that list and participate oh that\u0027s more than I expected thanks Ben all right I will take the action item of creating a list for interest and all mention it on the sector specialist thank you no I think the finesse is if you want an ITF last weekend help facilitate creation oh sorry that\u0027s what I meant of putting effort to try to get an ITF one thank but thank you "
  },
  {
    "startTime": "01:00:42",
    "text": "so hello my name is Fernando and I\u0027ll be presenting a document we co-author with eben arson we had actually presented this document a while ago I think it was at this AG meeting in Buenos Aires let\u0027s briefly discuss what we\u0027re talking about here this document is all about numeric IDs on America identifiers now if you look at different protocol specifications over time for the last 30 years or something many of these identifiers we got wrong think about for example predictable tcp seconds numbers or iessons predictable transport protocol numbers the ephemeral port numbers we also did predictable ipv4 and ipv6 fragment identifiers we also did predictable DNS transaction IDs and we have also had predictable ipv6 interface identifiers in most cases over time what happened was that eventually the security and or privacy implications of of these suboptimal identifiers if you want you know came into light quite a few times there were implementation specific fixes for these issues at times they were suboptimal and you know the bad thing about this is that the lessons that we learn from some protocols were actually not applied to other protocols one example simple example is you know fragment IDs we had predictable for amenities in IP before and essentially we had the same thing for ipv6 and that\u0027s actually still the case I don\u0027t remember when was the last protocol that I had checked about this but there was a Brady Advisory like a few months ago don\u0027t remember off the top of my head what protocol was him but same kind of thing okay so for some of these for example Steve bellevigne had done work on TCP seconds numbers I did an RFC on how to randomize the transport protocol numbers there there was some work on them but it was always fixes to the safe load skin schemes to pick these IDs what we think is the the cause or the root cause of this problem well in some cases you have specifications that actually under specify the requirements for these numeric identifiers so essentially the spec says okay you need an ID there but it doesn\u0027t really tell you you know what are the properties that you know such ID should have that\u0027s the case for example "
  },
  {
    "startTime": "01:03:42",
    "text": "with the TCP port numbers on the NS transaction IDs just to mention a few and we have also had specifications that actually did the opposite so they say well you need an ID here and well it should be okay if you use a counter for example instead of actually specifying what are the you know in terrible interoperability properties that you actually need for other for that ID and then also you you have some cases where the spec does the right thing but then the implementation does not so what we try to do in these respect is not try to prevent future protocols or implementations to actually fall into the same trap okay so essentially well if you wonder what we use or define the you know the term you know numeric ID sometimes it\u0027s what you also call a protocol ID essentially it\u0027s a data object that you use that is employed in a spec - you know distinguish one protocol object from another this ID normally have like different interoperability requirements in some cases it could be uniqueness so you just need a number that is unique in other cases the requirements could be things like there is Ben monotonically increasing and there are others in which the idea is required for example to be stable within some kind of context also associated with these numeric IDs is a failure mode so the idea here is okay I might have aspect that has some sort of requirements for some numeric ID now the question is well what happens when those requirements are not complied with well in the case of some protocols it\u0027s there\u0027s a soft failure meaning well there\u0027s something that fails but it\u0027s very easy and trivial to recover to them to that from that failure whereas in other cases we have what we call a heart failure meaning that no things break badly and all takes a lot of time or effort to actually not recover from from that condition so when we were trying to you know find a solution to this problem one thing that we did is to try to now look at different numeric IDs that we have four different protocols I mean here we are just mentioning a few with these are actually in the in the ID and try to figure out what were they on one hand what were the interoperability requirements for the IDs and also try to figure out where was the failure mode for each ID okay this is well the list "
  },
  {
    "startTime": "01:06:44",
    "text": "that we we came up with or the ideas that we analyzed and the reason for which the we did this analysis is that we try to let\u0027s say produce a figure ease of numeric identity no Merrick identifiers both in terms of the interoperability requirements and in the failure modes so for example here for the ipv6 flow level the interoperability requirements are are that you know they should be a unique you but you know if you happen to let\u0027s say generate an ID that collides with a previous one well there\u0027s not much of a big failure so that\u0027s a soft failure then you also have cases like in the category number two where the requirement is that of uniqueness but if there happens to be a collision well the failure can be actually bad so that\u0027s why you know we market those as heart failure well there are others that know category number three the requirement is for for uniqueness and you know for the idea to be constant within some some context that for example the case of ipv6 interface identifiers and the last one we did was category number four which you know the requirement is that it should be unique but they also should be monotonically increasing and if you fail to come up with proper IDs there\u0027s a heart failure now the point of you know producing these categories was to then you know come up with possible algorithms this doesn\u0027t mean that is the only way to do things or the best way to do things but let\u0027s say for each of these categories we phone what are what some implementations are doing and are go rhythms that are kind of like okay put another way if you you know have a protocol spec you identify you know the interoperability interoperability requirements and the failure modes and you cannot come up with a better algorithm well at least you have some algorithm that that you can employ [Music] another thing that you know our document does if you know we besides the analysis that we mentioned before is to try to come up with requirements for protocol specifications when it comes to numeric IDs meaning that if your protocol has a numeric identifier well the spec should clearly state what are the interoperability requirements your ID then it should also do security and privacy analysis of that ID and then recommend an algorithm we are providing you know some sample algorithms for those it doesn\u0027t mean that you know a new protocol spec should use one of "
  },
  {
    "startTime": "01:09:45",
    "text": "these but if you cannot come up with something better then you might end up using some of these algorithms that that we propose we polish this document of this a couple of years ago I think if I remember correctly you know at the time there was work on 35:52 B\u0027s and at the time we got positive feedback from the at the site meeting among the comments that we got is like you know this document had you know different kind of stuff for example where we were doing analysis of the numeric identifiers of existing protocols and that was kind of like you you know could be useful as an informational document then there was also some other part like you know the algorithms that we were describing that could be like a BCP kind of thing and then their requirement when it comes to the security and privacy analysis of numeric IDs which at the time the idea was to wait for the 3552 visa for and then try to you know see if those requirements could fit in there but all 3552 B\u0027s as far as I understand was installed so that didn\u0027t happen and the other parts of of this document were stuck to so now we you know we keep the original document as a whole thing we also split the original document into three parts and we are curious you know about whether folks find this useful and if they do you know what they think might be a way to actually work on this stuff benkei duck so I mean I think it\u0027s true that there are definitely several different parts in here that you have different qualitative nature of content to them and like there\u0027s a lot of sort of historical interest in how did we manage to share ourselves in the foot so badly in the past and then you mentioned that there might be some parts that are also more of a BCP level and the main question that I had was like what do you see is the target audience for these different documents like because like I know you could write a BCP that says you know this is what you should do when you\u0027re generating your identifiers but who do we expect to actually read that and take action upon it well there\u0027s this part for example which I think would be a BCP okay and I guess that at the very least when you have protocol specs that you know work with you know no merica these these requirements how "
  },
  {
    "startTime": "01:12:48",
    "text": "do you prevent at least some of the issues that we normally see when when it comes to numeric IDs so I think one of the questions that you\u0027re asking vendors that we typically write BCPs for like operators of these like implementers of protocols or operators of the services that run these protocols right and in this case I think what Fernando is offering here is best current practices for protocol designers so a little bit meta but I think it\u0027s actually really useful for us to think about the ITF themselves as the consumer of the of the best practices document I mean sure but we have some things like that that RPC piece for protocol designers and we have a lot of cases when people don\u0027t know about them and you know they got it maybe they get surprised by it is G evaluation time then maybe nobody noses and the best practice is just ignoring like we\u0027ve had this come up with PCP 190 a couple times recently we\u0027ve got your PCP 201 BCT 107 I\u0027ve had no idea I bet you and most two people in the room know what all three of those are sir the key off that a little bit in why I came to Mike is what I was intrigued by the draft is how it took a historical perspective of you design issues kind of threats how things were exploited and then reflected that into protocol kind of guidance and then kind of thinking big hat what else is happening in the IETF or in the kind of the broader community is the smart work and it\u0027s a temp yes there\u0027s a lot of talk about malware there but there this idea of taking kind of research and best practice and try to distill that into kind of protocol so I wonder what kind of the audience kind of thinks about about a lining war in the IRT F um given this era scroll I mean this seems like useful I guess I\u0027m trying to figure out why isn\u0027t serving the function needs to serve presently by the internet draft I mean generally these I mean these BCPs that we often do like are intended to like constrain the protocol designers and it says there which is documentation and advice it seems like people could prefer we did the drafts repository so I\u0027m trying to figure out what why why MIT why is that mission accomplished don\u0027t really meaningfully expire anymore the dots are like to do Watson lab copper I really would like to see in RFC that protocol designers don\u0027t need to think they just cited say generate this identifier according to section law of law I think that would be a very positive step and whatever process get signed on here should end up with that dkg just respond to Eckhart like if we want to have a discussion about the fact that expiration doesn\u0027t mean anything like that\u0027s a meta meta discussion and "
  },
  {
    "startTime": "01:15:49",
    "text": "and I don\u0027t think that we can say don\u0027t worry about this fashion is because we decided offhand at the my client that we\u0027d actually don\u0027t care about expression I\u0027ve heard multiple times yeah that drafts expired you shouldn\u0027t use it in other contexts so all right that\u0027s just not yeah yeah so I\u0027m what I heard at the mic line is generally there\u0027s kind of elements elements kind of interest it\u0027s you know we talked about how to kind of codify all of that but what I didn\u0027t hear other than kind of in kind of one places that we have to package it up and kind of publish so there\u0027s good kind of design guidance I didn\u0027t hear next step though anyone want to fight me on an alternate direction did I mean I did hear Watson literally say I want an RFC where the section that I can point to you in the next draft that I wrecked so so if you say that you didn\u0027t hear that maybe Watson needs to say it again I wanna Darcy I could put you the next draft by right all right we heard that so then it so again if you pull that thread does anyone have a proposal beyond ad sponsorship about where an RC like that could be published yeah how do we get there effectively maybe CFR Jean all right your people saying no but no I mean I mean again I mentioned smart you\u0027re mentioning CR C FRG yeah III was just saying this chairs you know the private scene Hansen\u0027s research group could also be other candidate group for this it does to me seem to be a little bit more of that research in character and of general considerations character one is she was putting it in a research group is that often there\u0027s a sense of research groups are not expected to put constraints on IETF work yeah yeah what we think is at least part of the document should be a BCP so that when you hear get a protocol spec that has a numeric ID there\u0027s a document that you can direct people to and say well how did you come up with these IDs what are their requirements did you did the proper analysis and if you are doing if you are employing some other algorithm well why are you in that instead of the algorithms that have been already used for that well ad sponsorship seems the easy path why are we looking make something more complex "
  },
  {
    "startTime": "01:18:55",
    "text": "thank you like again so I guess I have to confess I was trying to do some research on this stuff earlier today and there\u0027s like four word wrap for documents floating around that I tried to look at and I ended up sort of a little bit confused about which pieces go where and which things I\u0027m supposed to be looking at so at least for me personally it might be simpler to like publish the timeline and like is informational let\u0027s sort of just get some facts out there and easy to check and then we could be able to leverage that to focus them to the other parts of it yeah my perspective isn\u0027t coming ad I mean to respond to kind of Daniel I\u0027d want to decompose what\u0027s there back to the properties that are BCPs the properties that are others so I struggle to take it kind of asses Steven province so I I don\u0027t think you necessarily need to get hung up on PCP stuff because you can always just put it in an RFC that\u0027s informational and with Narcy 1984 we made it pcp 20 odd years later so you can just I mean just if you want to publish gnar and see if you can find somebody to sponsor their ghost EIC then you can get an ROC and if there\u0027s text in there that the community want to turn into a PCP they can just do that as a actually so excuse me you\u0027re suggesting to publish this as an individual submission no I\u0027m saying that there\u0027s an option to just not worry about which bits need to be PCP or not and just find a path to make it an NRC if you if that\u0027s you want and you can get people interested so I put I suspect if you\u0027re gonna try and make it a PCP you\u0027re gonna have a hard time yeah so there\u0027s like a couple things going on in what Stephen was talking about so one of them is that you know you can have an RFC almost you published this one publication stream class and that can be changed at a later date just by ITF consensus and iesg action I think and so like you can have a if\u0027 stream informational document that later gets reclassified as pcp we have done that before it\u0027s a little bit confusing but it is an option and then you sort of other aspect is that you know notice that Watson was up here asking for an RFC he wasn\u0027t asking specifically for BCP he just wants it to be like written down and archival version of this is the procedure you need to do and you know the documents that want to use that procedure can refer to that and say this is the procedure us do as a normative reference and like it doesn\u0027t matter if it\u0027s a PCP or not or those individual protocols are saying this is what you do and the procedure that you follow is well specified and so like that\u0027s another reason why the specific PCP label is not necessarily as important as "
  },
  {
    "startTime": "01:21:58",
    "text": "it might seem at first Daniel then you come up to the Mike or am i wrapping up all right so to kind of wrap up it so to kind of pull the thread on the follow up discussion here there\u0027s there\u0027s some interest I talked to Ben what we\u0027re going to commit to you it\u0027s kind of sit down with you to figure out what it makes sense do we do we think one document we think not and we\u0027ll help you shop it kind of in different places and we\u0027ll work towards kind of getting something out there yeah yeah one thing that we like to have a Creole idea about is that you know the last time that we try this but we have this single document then we were we split it and we got nowhere so we don\u0027t mind okay so which way to go as long as we get somewhere okay so I\u0027m not trying to kind of some of that history but maybe we get that back can be part of the conversation and I are committed to working that out okay thank you yes you can see I\u0027m wearing glasses and custom woman I want to talk about concise IDs and I want to do this quickly because John where is John oh then it\u0027s going to use the other half of the slot to talk about something completely different except that it almost looks the same so I have to do a little work on keeping us separate so you may be a way I want x.509 certificates because these have been around for a while and they are used for lots of authenticating assertions and what we have now as a new thing is CWT zebra web tokens that are authenticated assertions and they\u0027re used for various things and again and again the question comes up I\u0027m using certificates right now to solve problem X could I be using CW trees as well so the obvious question is can I do that and actually it turns out that we have the RFC\u0027s in place to do this there\u0027s one in the RFC editor too I think so the next question is yeah shouldn\u0027t we go ahead and write a document how to do this and I\u0027m not even sure it\u0027s really knowledge if it could even be informational because it just profiles what we already have and says how to actually build things that traditionally have been done using x.509 certificates but also can be done using CW trees and "
  },
  {
    "startTime": "01:25:00",
    "text": "of course Cosi has a security scheme so there is this pretty rough draft out there which calls these things concise IDs because that\u0027s the main application we had in mind in the sense of our c49 online identities are sets of attributes and once you authenticate these you have concise IDs so that\u0027s an indication of the direction where you want to go home but certainly not the document that we have in the end so that\u0027s one trick and I quickly want to separate this from another trick that\u0027s also very interesting which is you could we encode an X of x.509 certificate in C bar and you would gain something so this is not really compression it\u0027s just the fun of concise serialization so the advantage is everybody already knows what an x.509 certificate is the disadvantage is you get all the baggage of 30 years of using and abusing x.509 and you actually need to convert the thing to ace and not one before you actually can sign it or before you can verify the signature so it\u0027s the one thing that in the we in the constrained environment don\u0027t usually want to have I should have said that we\u0027re most interested in this for small devices not just small but using very little energy having very little processing power unfortunately there are only about 5 people in this room who want this and everybody else is sitting over at 6 low but ok so this is interesting working John will talk about it but this is the what I\u0027m talking about I\u0027m talking about profiling CWT for authenticated assertions with a view of having this available in places where traditionally x.509 certificates have been used we had a site meeting about a month ago on WebEx I don\u0027t know where we discussed where could we put this work and so the problem here is not that we have no idea where to put it we have too many choices so the cousy work you could work on this this see.well working you could work on this and so on but in the end after the discussion we came out with two working groups that are kind of credible one is in the security area that\u0027s ace which is looking at authentication authorization in constrained environments these people are the owner of the CW T\u0027s which kind of was offloaded to this working group "
  },
  {
    "startTime": "01:28:01",
    "text": "because if we are solving a different problem potting our earth - constrained environments but anyway ace owns CWT so it would be one natural home and the other natural home would be to do it in a work that actually has the requirements for using these things and and this is the core work and again there\u0027s not really another existing working group that would work here we could do a new working group not sure that\u0027s worth it or we could not do this at all and proclaim that x.509 rules yeah you probably want to use chunks work then but there is another problem people are just going to go ahead and ucw T\u0027s for these things and if we don\u0027t have a central document from which these applications can benefit we will get much more diversity and then probably also many more security problems so I really would like to do this work and it would be interesting to hear whether people think it should be done in a so it should be done in core do we want to take questions now or do we why don\u0027t we rent or both of them and then then break folks to the mic okay why don\u0027t we do both presentations yeah yeah so this is this work is based on a research paper from the Swedish Research Institute the draft is from Erickson\u0027s with his research institute and Swedish certification Authority Nexus Carsten went to a lot of the reasons why we need is x.509 is big it\u0027s heavy to process da da da da this work was presented in Tingting RG IDF one country last i def quite well received I would say and what this dropped proposes is to do a lightweight x.509 certificate encoding / lossless compression algorithm the potential benefits and applications of this is to do it should not be it\u0027s wrong erosion of the Gateway the Gateway should be gateway to endpoint so gateway to endpoint compression when older versions of TLS is used this is what they do currently in six low signal paths this "
  },
  {
    "startTime": "01:31:01",
    "text": "is not a long-term solution hopefully people will switch to DT TLS and DTLS 103 but device is deployed today would probably be used for 10 years until the battery runs out maybe even longer the second thing it could be used for is Steelers client - TLS server compression when TLS 103 is used we think we can achieve better compression for this type of very constrained ECDSA certificates and the third application long term would be as a migration path would be to completely skip asn.1 and just use this as a seam or certificate format and how does this mechanism work it has two steps one is a very strict profiling over X zero five zero nine certificate and the second is to take this profile x.509 and andrey encoded as seaboard right swedish research institute says that these certificates they see in use priority is five hundred bytes the profile versions 340 and the sebring who this is half of that so you get very compactness you get compability and a migration path from x.509 you get smaller footprint for the compression than general lossless compression our wits and the disadvantages is that you you still need to support a SN bomb at least medium long term until you switch to a c words that we get format but yes the best would be switched immediately but then you have a hand in the eight problem devices will not adopt a new certificate format unless cas implement this and see us we\u0027re not implemented unless devices implemented so this creates migration but i think this is mostly complementary we do see about WT thing i think they should be discussed together i think it\u0027s important that it\u0027s done in a iot century working group the certificate thing would be security the compression is probably mostly not security okay interest at the mic and while the the proponents here are both said that they think they should go together if "
  },
  {
    "startTime": "01:34:02",
    "text": "the folks at the mic line want to suggest yeah so if you want to if you want to provide advice to dispatch in different ways by all means yeah first let me tell you a story of a scrappy little dedication format designed about 25 years ago it started out which is like some names and like an you know skis and a and ability period and it grew up in don\u0027t we call x.509 v3 so so I think I\u0027m not necessarily persuaded that just like removing all the fields is gonna like end up being smaller except temporarily but um more seriously um I think it\u0027s a this cannot go into some existing working group this is like some it\u0027s is like this is the reason we have working groups is to like actually focus on things that they they know about and not to have this be stuff things like some like working group that\u0027s like doing something else so um it without so I taking pictures on merits of this on like maybe maybe the super encoding can be done like in some small corner the profile the seat of beauty is like a real security work and like it\u0027s the same order of like PKK\u0027s it has to have someone working group with senators a kind of it\u0027s somewhat related whatever just said my name is Gary monument Qualcomm both of these many presentations focused on the encoding aspect of it but you know what I\u0027ve heard a lot from internally in my organization that externally is that that a lot of service providers would still be reliant on the existing infrastructure and may not want to actually change how they deal with certificates and would like to stick to 509 encoding formats so if you have for instance resource constrained devices that are producing encoding formats like this what we mean to actually standardize how intermediating proxies would work that would accept these formats yet produced something that\u0027s more in lines of a 5 a 9 certificate that that in existing relying party you see this pocket would be using that\u0027s my question to the author well I have a personal review on this which is that this is not trying to replace X 5 or 19 places where it works well but it\u0027s it\u0027s trying to solve problems that x.509 can sort of in places where x.509 does not work well so I I wouldn\u0027t expect a giant Gateway infrastructure to spring up before for the current size ID work of course Jon\u0027s answer will be completely different in "
  },
  {
    "startTime": "01:37:06",
    "text": "that case people who want this compression they would update the gate weights it would be a new compression algorithm and potentially a compression algorithm boy could you go back to the previous slide to the one before the yeah so I didn\u0027t understand this last three bullets or one of the bullets was that there are devices that are already deployed that use 1.2 so if I\u0027m going to update the certificate format why can\u0027t I just update to to a newer version of the library yeah sure you can update hi Sean Turner um I\u0027m gonna maybe amplify a little bit what ever said about that every time we make a small protocol we then turn around make it that um because there\u0027s extension points that are still even in the profile and I I just feel like we won\u0027t be able to stop ourselves from extending it in and adding things later on the other hand like many years ago I probably would have been up here falling on a sword about coming up with a new encoding scheme now like yes and when I was like five encoding schemes just do them for C for this one for JSON there\u0027s one for XML and for whatever like just make a new encoding format so the question is whether or not the DR thing is required and people can get into that argument but I\u0027m like yeah as far as the profile goes and dropping out the fields I think it\u0027s so much easier if you just stand on the shoulders of giants as opposed to trying to selectively pick what field is there and what\u0027s not filled is there because you\u0027re gonna rehash all the reasons why those fields are there in the first place and you\u0027re gonna end up probably putting them all back yeah can I just answer one thing Jason if we do this right it might you who work with Jason yeah now so I mean my whole idea is if you just want like a new encoding scheme but there\u0027s basic encoding world English encoding rolls it back decoding rolls so now you see we are recording role like just make a new one of those like whatever that\u0027s not the idea so if you are if you are completely happy with the information set that x.509 gives you this work is not for you so first of all good look so yeah I think XIV 9 is a Potter craft that\u0027s videos but it\u0027s very hard to do significantly better than us but you\u0027re gonna actually grip the places or displace it even in smaller areas so I\u0027m really not clear that this at least burned while I read the current draft is significantly better although then in terms of encoding sites so it\u0027s not "
  },
  {
    "startTime": "01:40:06",
    "text": "clear to me that there and for example I think to be significantly better you probably need to have some different semantics really so I think there\u0027s a bunch of work to be done before you\u0027d know if there\u0027s something worse trying to spin up a replacement of x.509 activity if anything ever did happen like that it definitely should be its own working group and I think PKK\u0027s lasted 18 or 19 years or something before it got killed all right the line is cut Richard Barnes like a couple of observations here first I don\u0027t know why everyone\u0027s hating on Anson one like it\u0027s it\u0027s my favorite allegedly canonical format that you can take an object and you can deserialize it and Yuri serialize it you get different bytes I was expecting it Watson to be up here because he made this observation this morning in this patch some other context oh well maybe it was here on time plot you having fun yeah so I\u0027m kind of amazed that you think this this process will even work like this idea that you would recreate the asn.1 from the seafloor and have that be faithful like why don\u0027t we have empirical validation that that that actually works yeah for sure but it\u0027s it\u0027s a very first you need to apply this profile to the said it gets it doesn\u0027t work on any certificate it works on a profile certificate so it would be a deployment that that knows they want very slim said it gets so they ordered these profile certificate from they CA which is still the x.509 City gift it works with a basically similar to the current profile for aut somebody it\u0027s a little bit stricter than that but very similar yeah you\u0027re probably aware there\u0027s been some work and see us working group on certificate compression which is using like off-the-shelf so you\u0027ll ever partly or whatever have you have you done like the comparison and see how your performance is here versus that because that seems like much more simpler yeah I think this is doing better compared to this figures I have seen from the compression people but I that would need to be done before such comparisons should definitely be done not only decisis but also what are the code and memory requirements on the devices I don\u0027t have any clear answers research institute of Sweden say tell me that this is lower I believe them but I don\u0027t have any numbers yeah thanks um hi the sink so first of all this is not about rehashing the work of 30 years that then then I would have to repeat the Carson\u0027s comment if you find where that this is "
  },
  {
    "startTime": "01:43:06",
    "text": "not for you we have new scopes and intends all these assertion mandals and every prominent one is the identity we\u0027re talking about so I\u0027m not going into what is an identity here it\u0027s 25 I hope but the the purpose here is is this we start the work and there is actually there are entity out there like a CA or Ganesha CA that is very interested in this so just saying nobody wants this because it\u0027s changing and hard is I don\u0027t think a ready palette argument also there are stories where I have experienced that trying to use a s and one again for a different purpose didn\u0027t work while it would have been work from the start when we would have used see board so there is also basically the entire example for this that it would have been better to just not use age and from the beginning and don\u0027t use the CMS extra four nine representation so that that\u0027s just just all side information on this topic we are starting with concise identities because people I think have a relatively good understanding how to deploy them and work with them but in the end these are signed assertion bundles and not everything here is about identity this is a first step or a thing that we want to establish here and it is again to repeat Carson\u0027s comment about the constraint no environments and for future use and not for rehashing every extension that has been made or created in the last thirty years because and now I called an editor of the divided raft they are insanely arcane Laden and I regret it so that\u0027s that max pala CableLabs um one comment and one question the comment is just hearing what he just saying and what you said before seems to me that this is not just a a change of encoding but is let\u0027s remake what we did before I think new possibly new features etc and this could be seen as let\u0027s reopen it in another way if we can work through the question that I have for you is certificates is one thing in a PKI there\u0027s many other protocols many other data structures that needs to be updated as well are you considering to do all this work and if is not an encoding are you planning to replace current mechanisms with new ones well just an example sign a request to CSR is a CWT so we don\u0027t "
  },
  {
    "startTime": "01:46:09",
    "text": "really need a separate data structure this just falls out from from doing the signed data structure as well so in some cases we can just reduce the number of variations we need but if you\u0027re talking about things like timestamps and and and OCSP and so on yes so in some cases those will be needed but I think in many cases they will just take a different form because we have things like like authorizations hours of education servers that that we do these things in a specific environment so I don\u0027t know whether we actually would ever come up with a generic timestamp for word but we might come up with something that allows an authorization server to issue some some freshness indicator that can be used but by a client or a server to continue working okay thanks just to fall on that Carson I think for you\u0027re here for edification to know about knowing his background um there\u0027s also co-op est right so there\u0027s this or there\u0027s other things that are going on in that space that can handle these additional protocols already you know cylinder some quarter of the seaboard certificates draft and I\u0027m listening to to the discussion here and I have a general question to the security community about how I mean these are two examples of people want to make more compact certificates for the purpose of IT and we we haven\u0027t got there plenty of use cases where people want to deploy lots of devices using some automated mechanism with based on manufacturing certificates enrolling new certificates while in the network and so on so so there is this is a concrete problem and it seems that these people are not happy but not delighted or enthusiastic over these proposals some ideas are present but I\u0027d like to ask the security community way at the IDF where do we go what do we do I mean how do we address this problem if someone has some guidance here or what we should do because this is a concrete problem and even if you don\u0027t like these proposals what\u0027s what\u0027s your proposal for addressing this problem yeah so thank you for the Segway urine I think what I\u0027ve been hearing at the microphone here is that people don\u0027t think this is appropriate for kind of the fast path options we have for dispatch you know baby sponsorship and things like that but that there\u0027s there\u0027s enough meat here between these two proposals and you know the general topic of doing more compact authentication Xin Meredith off I\u0027m so I think that\u0027s probably the course we\u0027re gonna recommend um so it\u0027s just the propensity to work with deities and see kids and mix it up alright and "
  },
  {
    "startTime": "01:49:11",
    "text": "any comments on that outcome further discussion alright call it the steps thanks guys so so that Dennis Dennis the last of our scheduled agenda we have about five ten eleven minutes left I had one request for an AFV slot from Joe Hall so Joe what\u0027s the name of your craft again oh yeah we\u0027re the blue sheets you someone bring those forward please we\u0027ve got some folks in the front who need them so I have no slides I didn\u0027t even know this working group existed until about three or four hours ago I\u0027m apologize the ITF 91 and Honolulu we started some work very descriptive work on an internet draft to talk about techniques that global sensors were using against protocols we had a pretty drafted draft at that point we\u0027ve worked on it since then it\u0027s now split up into three things prescription identification and interference you could read the prescription is how you decide what the block interference is how do you identify what the block and weight identification is how to identify what the block and interference is actually performing the impairment or blocking doesn\u0027t have to be exactly blocking it can be slowing down and stuff like that it\u0027s now busted out in two layers and this is with collaborators at Princeton and folks at CDT the whole purpose of this draft is to be purely descriptive for people who write protocols or implement protocols that if you care about people in places of power and using your protocols in ways you didn\u0027t intend or whatever you can read this draft and get examples of how protocols have been used against users and so there\u0027s a similar draft that I know Richard had worked on that talks about the effect on the Internet architecture of blocking and impairment this is more about the effect to users and the actually availability of information it\u0027s pretty so it\u0027s been four years part of what happened as I did I asked a 2.0 in the middle of that and I haven\u0027t had staff excuses excuses it\u0027s pretty mature at this point there\u0027s a couple of open issues in the repository we\u0027re hoping that this is one of the series of descriptive drafts that are meant for IETF the IETF community or any protocol designers to identify themselves simply as references to see how people are doing certain kinds of things and we\u0027ve had some interest in traffic analysis there\u0027s some very fledgling pointers at that kind of stuff too at the time we had sort of tentative "
  },
  {
    "startTime": "01:52:13",
    "text": "ad sponsorship from Kathleen and Steven when they were a DS obviously that\u0027s been very different from now and in fact ad sponsorship itself may have changed but anyway and as people have said this is very useful as an ID as itself we thought about doing in an RG in the HR PC research group at the time people felt like it would benefit from being an IETF document for IETF folks and going through the IETF process and consensus it\u0027s anyway so I can do a more polished presentation later in the week just got to give me a sorry I just didn\u0027t even know I was going to do this I could do it at sagger for people if you want me to pull you aside and sort of talk about what\u0027s going on here there are a couple of open issues that were stealing still dealing with like for example all South Korea blocking yes and I that\u0027s something that definitely needs to be in here I think it is sort of in here but not you know in a perfectly polished way with that I won\u0027t waste any more of your time than happy to take questions yeah Joe real quick I\u0027m sorry I came in late and just completely all of your presentation but it\u0027s very short this seems like something would be good for the drg the privacy enhancements assessments research group we\u0027ve recently took on work describing data collection mechanisms and best practices around there and this is sort of in a similar vein so I would encourage you to essentially consider something there and Trust for presentation that\u0027s that sounds like a great idea I don\u0027t exactly remember the the rationale people were saying it should be and I in IETF rather than IRT F as you can imagine I anyway but happy to do whatever even happy to keep it just like this - because it\u0027s the kind of thing that changes quite a bit and not so sure something that an environment that changes is dynamically a censorship a RFC may not be the best thing as an endpoint for that and I think we doing the authorship to sort of realize that because we have to change in every field for four or five months to respond to the actual global Internet environment but it yet I think that\u0027s a great idea I\u0027ll start all right thanks hey comments all right thanks for the heads up Joe I think that\u0027s probably not no further action at this meeting but please feel free to send me a little list and we\u0027ll see if we get this dispatch there all right um any other any other business in the remaining five minutes of course yeah Kathleen reminds me that I\u0027m in in our personnel announcements earlier we\u0027ve got to thank our outgoing 18 mr. Eric Carle oh all right that was the last business on yet so uh oh yeah so hold on one more second so yeah okay just so wrap up what we said I\u0027m reading kind of from ether I\u0027m reading from ether pad here as we talked about the "
  },
  {
    "startTime": "01:55:14",
    "text": "the protocol for PGP key subscriptions we\u0027re gonna the recommendation was to go to the IETF open PGP mailing list and grow the interest there as it relates to JCS the feedback was to bring to the art area as it relates to mask I think efforts are in flight to already create a non working group ITF mailing lists to further that conversation based on the interests on the on the work related to identifies the adsr gonna follow up to think through what the right venue is and for the last two things we heard two concise IDs and seaport certificates the breadth of the topic suggested Boff recommended and to the very last talk that we had were going to just bring that to the man list or further the conversation and that to figure out what the next steps might be if dispatches for free all right with that thank you everyone [Music] [Music] if someone has the other blue sheet would appreciate if you could "
  }
]