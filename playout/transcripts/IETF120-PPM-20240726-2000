[
  {
    "startTime": "00:00:03",
    "text": "And clearly as well Like, I didn't hear Sam say, hi I saw him way, but there it is not specific did say hi to you. Right Hello. The audio and the video of Right. The audio and the video are very badly synced. Oh, that's not good oh no it caught up again again used apart from this Okay, it is one hour afternoon in Vancouver, British Columbia. That means it's time for the meeting of the IT PPM Working Group. That's privacy preserving measurement We are going to need a note taker to take minutes for this session We can't proceed with the session until someone volunteers to take notes All right I note that you are also listed as a potential speaker Okay, thank you to martin thomson for volunteering to take notes for this session watson ladd in the chat volunteered as well"
  },
  {
    "startTime": "00:02:00",
    "text": "Great great. If you can both take a look at that collaborative document, that would be great help, probably easier for both of you Many of you have been here also week, but maybe not all of you, so you might have missed the note well This is the reminder of various legal obligations and other kinds of obligations that you incur by being present here if you're not familiar with this document it is very important that you familiarize yourself with it because it is legally binding on you So if you haven't had to document, it is very important that you familiarize yourself with it because it is legally binding on you. So if you haven't had time to review this, please do so If you aren't sure, if it applies to you, you might not want to participate until you've had a chance to review it This applies, of course, to all sessions at the IETF. This one is the same as always In addition to the note, well, which principally covers IPR, legal obligations, there are also some other things that you should know We do our best to interact respectfully and we participate in the IETF as individual So whatever you may feel about organizations to which other people in the room might be affiliated, please don't project that onto them They are real people who are here talking to you If anybody has technical difficulties, it's possible that this slide might provide you some use tips, but other please feel free to reach out to any of the different support channels to try to resolve those issues during the session"
  },
  {
    "startTime": "00:04:00",
    "text": "session Oh, that slide is out of date Okay, this is our proposed agenda for the session The next thing on our agenda is the Agenda Bash, if anybody would like to argue for a different agenda than this one I'll take one second to give people a chance to look at this proposed agenda and tell me if they'd like it to change in some way Okay, seems like maybe it's all right. The next item are on our agenda. Oh, thank you Tim? Thank you. The agenda is fine. I just wanted to remind folks who are in the room out of consideration for remote attendees please try to identify yourself when you come to the mic because we can't always tell who's speaking Thanks for that reminder, Tim Chris Patton, I wanted to, this will probably come up with Ben's stuff, but we should probably have a brief discussion about the feedback from SAG yesterday, which is well we kind of talked about what to do with with have a brief discussion about the feedback from SAG yesterday, which is, well, we kind of talked about what to do with fancy crypto, where should it land, is it the CFRG? responsibility or not. I think having a short discussion about that would be good. Not any, you know, not any make decisions. I don't think we can right now but just see what people's thoughts are Okay uh thanks for that note hopefully we'll have time toward the end of the session to get into that topic Okay, the next item is to talk about the heavy hitters consensus call. So we had an interim session Many of you were there, but not all we had an interim meeting and some folks in this group laid out a range of options about what we might do in relation to the heavy hitters problem In that session, it seemed like"
  },
  {
    "startTime": "00:06:01",
    "text": "there was consensus to drop heavy hitters support from the DAP document at the point in the version that ultimately will become RFC for the first time The chairs confirmed that consensus on the mailing list and called consensus for that change. The current version of DAP now implements that change. That means anything you've heard about Poplar 1, for example, no longer appears in that draft and some other elements of the DAP protocol that were needed to support that have been removed I want to emphasize that this is not a charter change for this working group The working group is chartered to think about the heavy hitters problem That continues to be the case. So if you have proposals that are relevant to that problem, they are potentially in scope for this working group but they at this point won't be in the DAP draft Tim? Thanks. Nick in the chat pointed out, I suppose this is in response to Chris from a minute ago, rather than heavy hit hitters. Perhaps once we're done with this heavy hitters, slide, it'd be helpful for those of us who weren't there to summarize whatever happened at SAG that Chris just alluded to Okay, that seems like a good point and I think we will get there But now let's move on to the DAP active Issues segment with Chris Patton The version two, I asked to present"
  },
  {
    "startTime": "00:08:00",
    "text": "that would be this one I hope it's this one. Let's find out Sweet Okay welcome everybody. So 45 minutes, that's a question time. I don't think we'll need all that time I want to go through the remaining substantial issues that are against the DAP repository We definitely have a lot of editorial work to do I think what the editors would like to do after kind of sell a landing on what the proto's call is going to be. We're going to spend a lot of time like cleaning up and making it very pretty But yeah, I think we feel that we're basically at the end so the issues i'm going to talk about are what we feel are the issues that the real architecture questions that remain If, of course, anyone is implementing this right now and wants to bring up concerns, you're welcome and encouraged to We don't feel this is necessarily like the best design protocol in the world. We've been dependent it. We've been experimenting it for a little while now and we feel pretty confident that once we get through these things, we'll have settled it. But please, keep providing feedback So changes since 118, we did not meet in Brisbane We had an interim, as Ben mentioned, but I wanted to quickly highlight the major changes since then So in draft nine, so there is, this concept of a query type which kind of defines how reports can be participated and then what is the scope of a back that is collected collected"
  },
  {
    "startTime": "00:10:00",
    "text": "So we have two types that are specified in the draft It's an enum so you can specify more if you wanted to We have time interval, which is, you know, I think hopefully obvious, and then fixed size, which is, I think, less obvious by the name. But the semantics of this is you have a minimum and maximum batch size and the lead gets to pick any batch at once, any batch of reports as long as they're not overlapping in draft nine we made the max batch size optional because it seemed less used than we thought initially In draft 10, we got some feedback from mark nottingham that we incorporated. Thank you, Mark, if you're in the room I think that we probably still have to do HTTP Durr again but we'll see as things gets closer to the end. In draft 11, we merged the change that removed support for the heavy hitters mode of operation for Poplar. I do want to clarify one thing, Ben. The draft still mentions it You can still use Poplar like technically, but it's not a very exciting thing to do. You do have, like, a sparse histogram that you can aggregate with like, some specified histogram buckets, but you can only do like a very limited amount of stuff with that. So we might end up removing that in at the end of the day but that's where things are So multi-collection is no longer possible I also wanted to mention there's this been an issue open for a while about drill down. This is when, you know, you have a bad and you want to kind of split it up in some ways, do a little bit of investigation, look at different like subsets of clients and so on. We decided to close this without any action. The reason being there are ways to address this kind of at the VDA layer rather than at the DAF layer. So if you wanted, you can define a variant of PREO that kind of preserves the semantics that were discussed in that issue. And then also we at the VDAF layer rather than at the DAF layer. So if you wanted, you can define a variant of PREO that kind of preserves the semantics that were discussed in that issue. And then also we're developing another VDAF called Mastic which provides"
  },
  {
    "startTime": "00:12:00",
    "text": "much of the same functionality, but in a much more privacy preserve way. Okay I'm going to do these in order of the PR. So each of these issues, I think they're five of them we're going to discuss. Each, how a PR associated with it. This PR reflects the feedback that we've gotten just from talking to people Folks, some of you might know we have like a weekly sync where some of us implementers get together and talk about issues Anyone is, by the way, welcome to join that And yeah, so these are kind of resolutions to existing issues So the question for you all today is, which of the are we going to merge? Which are these are we going to close andres olave in a different way? So the first question is about the maximum batch size. This basically this turned out to not be useful So the the proposal and it's also kind of annoying to implement. So the proposal here is to just remove this altogether And we also have some related nitpicks in terms of naming things. So we want to rename query type to batch mode. This seems to align better with the intended semantics. I think there were some misunderstanding about what, like, mechanically is happening based on the name query. So we would like to resolve that And then we would rename fixed size to just leader select which is like, you know, the kind of the only condition that matters for this particular batch mode We also made an observation about a civil-like attack in which the leader can attempt to partition reports you know, all coming from the same client. And so this is, this is you know, you still have to fulfill the batch size. So this is not a super strong attack But it's, you know, it's worth noting because we're, we discussion of civil attacks. So"
  },
  {
    "startTime": "00:14:00",
    "text": "that's that. It's mitigated by for example, using OHTP or something like that we can do for other civil attacks. Go ahead, Martin I'm trying a minute, this martin thomson So it seems to me like the security consideration here is kind of the key. And one of the reasons why the original was kind of appealing in a way was that it didn't give the leader as much control or it seemed like it didn't wen lin fact it probably did. I actually don't think that's true. I think the only change here is that there's not a cap on the bat size. Right. So this type, this query type sorry, this batch mode really does give. It's really the same think the only change here is that there's not a cap on the batch size. So this type, this query type, sorry, this batch mode really does give the leader quite a lot of control. Yeah. I'm sort of wondering whether we need to start we need to consider new constraints on the lead mode of operation here for numerous reasons. One of the things that Ben and I have been discussed about the anti-replay aspect of this is that it is much easier to implement the anti-replay side of things when you do that based on time ranges rather than individual record identifiers and if whatnot. And if this were a time span of query, then it would be much stronger from an anti-replay perspective because you could say for the reports appearing in this span of time potentially you could just say, I'm not going to take it stronger from an anti-replay perspective because you could say for reports appearing in this span of time, potentially you could just say, I'm not going to take any more reports during that time window And you're done in terms of anti-replay, which is a really powerful thing. So I'm thinking that might be a better way to sort of start to frame this one the goal here is to give the leader the ability to control the size of batches, provided that meet the minimum, and we can do that with a time span type system. The only problem is, of course, the submission timestamps can be a little bit ragged at the, like, the frontier"
  },
  {
    "startTime": "00:16:00",
    "text": "but that's that's probably manageable so the the this The only problem is, of course, the submission time stamps can be a little bit ragged at the frontier, but that's probably manageable. So the fixed size was introduced at a time when some people were making observations about operational or potential operational issues with time interval We'll have to go back a few IETFs in our mind to remember what those are. I wanted to comment though, on the replay protection bit The requirement in the draft is that replay you never process a report more than once across an entire task. And then we have buying two tasks So I agree, I would say that in my experience, replay protection is much, much easier for time interval than it is for leader selected, which is I would like to solve that problem. Personally, I would, I would like to solve that problem exist from the perspective of my implementation, but I don't think it's a security issue, I think it's an operational issue Yeah, so I think time interval is specified is perhaps too rigid, and that's why we have this and if if this were lead us a selected time interval, then I think we may actually be in the sort of sweet spot there maybe that's the way to suggest it so let me ask you in terms of procedure. Are you okay with Procedurally, this is a step forward. Okay, so go ahead with this. So, so your vote is merged, but let's revisit. Yeah, that sounds like a good idea to me Any other feedback on this issue? Tim, go ahead Thanks. Okay so let's see. First off, I support taking this change as previously discussed. Second, I wanted to respond to a specific thing that Martin just observed Sorry, I'm going to get the details wrong if I try to explain it. But for a while now, like we've been batting around something that we have called in some context, time bucket"
  },
  {
    "startTime": "00:18:01",
    "text": "fixed size, which I think probably hits the balance that martin thomson was getting at in the sense that its leader selected batches, but there is there's essentially like a guarantee about the space of time that a batch will fit into, such that I think you could do the more efficient anti replay checks the marketing was getting at um so i think so again, that's like orthogonal to the question whether to take this change. I think we agree on moving forward with it. So I think what I will do is try to put together some of the materials that my colleague Brendan Pittman, has put together over the last couple of years about this and like circulate them, I think, to the list so that we can all take a look at it So I'd say let's open up a new issue and start discussing this We probably have discussed this on other issues before, but cool. Anybody else? So I'm gonna, I don't know how you want to do this procedurally, but I want to... If there's no objection we'll merge this change Is that okay? I don't, yeah, I don't think we need to apply working group process there. Okay, cool Let's move on. Okay this is a bigger one. So strap in I'm just kidding. Okay, so we have some deployment experience with draft nine The, the so strap in. I'm just kidding. Okay, so we have some deployment experience with Draft 9. The way, just to give some reminder, the way aggregation works is a leader picks a subset of reports it wants to aggregate for the batch and it sends those in an HTTP request and it in its response the helper completes aggregation. So actually, how many rounds is required depends on the VDF, but for now let's just pretend there's just one round, it's PREO"
  },
  {
    "startTime": "00:20:01",
    "text": "for example. So the leader sends a put requirement like run this aggregation job. The helper replies with a for example so um i have the leader sends a put request like run this aggregation job the the helper replies with uh i've aggregated these i've rejected these and we're done So this step required a couple of things that might be surprising for new implementers First of all, the VDAF preparation work. This is like the work that you're doing to do input validation for each report That's, you know, it's cheap but if you're, if you have a job of like a thousand, then you have you, you really benefit from parallel But in some situations, for some users, this might not be possible. So here we have a C of like a thousand then you have you you really benefit from parallelism but in some situations for some users this might not be possible so here we have a CPU constraint that is that we really can't ignore We also have a strong requirement for consistency So the helper is expected to commit the state change to its long-term storage before it can reply commit the state change to its long-term storage before we can it can reply so this is like you know like right the aggregate share to a database, write the set of report IDs that for anti-replay So this creates resource contention, especially if you have a large number of aggregation jobs running in parallel. Because really, this is what we want to be able to do. We want to be able to, the leader to just horizontally scale by just spouting out a bunch of aggregation jobs for the same task the same batch simultaneously and expect the helper to keep up There are, you know, so our P99, in our first kind of experimental deployment was like, like 10 seconds, which is really like, not very good. But the know, so our P99 in our first kind of experimental deployment was like, like 10 seconds, which is really like not very good, but it's, it's, you know, we're interacting with a database And sometimes that database gets slow, especially if"
  },
  {
    "startTime": "00:22:00",
    "text": "you know, in my company's situation, it's a very multi- environment. We don't always have the CPU and or sort of back-end communication resources that are required to resolve the request at the given time So I think the other people have had a very similar experience We want to basically to give the helper some time to do the work it needs to do to respond to the request. So the proposal is to allow the helper to process jobs asynchronously. So basically, the helper gets to respond to a put with a 201 created and then immediately after process, after parsing the request and doing some little bit of pre-processing And then the help, the of the leader is to send a get request to get the result. And so this is kind of like polling very much like how collection works today. So this is an optional behavior, the helper is a allowed to respond with the actual payload or not. It's up to the helper So yeah, so we have a PR for this This has gotten a lot of feedback from, I would say, a pretty big group of core people. I wanted to hear if there was any objection to taking this change. Anybody have concerns about this? I see someone in the queue watson ladd Akomai. I'm not quite sure I follow what's being contended on. Is it? just that you might need to take a while at the end what to say? okay, these reports have never been used before? I'm wondering if making it a scene asynchronous, yeah, it lets you hide that latency I'm not, and retry not sure it necessarily gets you the results in effect"
  },
  {
    "startTime": "00:24:00",
    "text": "asynchronous, yeah, it lets you hide that latency. I'm not, and retry, not sure it necessarily gets you the results any faster. Maybe the right solution is as you're going through the reports, say, okay, we're never using this report again But Noah's in the queue maybe has a response Yep. So the, there we go, enough do we have, do we have my stuff in the queue and your... It is clip is too loud loud Is that better? Is this better or? A little bit louder. There we go Yep, good. Perfect. All right. So what the issue here are fundamental is that the replay checks in particular, like, basically require a serialized transaction across all, like, basically on every request for a new aggregate job and like that is like under like most architectures that is actually fairly expensive to do just a request for a new aggregate job and like that is like under like most architectures that is actually fairly expensive to do just in general like serialized transactions are expensive with like or just basically just generally expensive. But especially what we found is for like those running networks that are that are distributed across sites or across geographic regions and that are trying to receive aggregate jobs across us site like this is particularly troublesome not just like not necessarily so much from a compute perspective as it is from like an availability perspective and like handling that network partitions and things like that this gets much easier if you can like if for example all you have to do in response to getting a new aggregate job is just queuing it for eventual process somewhere. Because like, the, like, operational considerations based aggregation job is just queuing it for eventual processing somewhere. Because like the operational considerations basically with these serialized transactions, we can shard them a fair bit and like trade some of availability since now like any shard going down could break things. But"
  },
  {
    "startTime": "00:26:00",
    "text": "like that does that moves basically that moves the failure risk out of the critical path Watson, does that answer your question? I think I'm a I think it does it's basically your saying that because you had to respond why the leader's still on the line to say that yes, all these things you've sent us were good You don't have enough time to do this big transaction We can discuss more on the list. I'm not quite sure I still got by any, but thanks for the explanation. Yep That's was all I really had to say was just responding to that that So martin thomson, I'm wondering if we could engage with Watson's suggestion, which was to suggest that the helper take the tasks, maybe on the assumption that the leader has performed already some amount of replay detection and whatnot So in most cases, you won't have to do anything further beyond that point but then develop an exception process for identifying those records that were in indeed replays according to the helpers understand of that and having a way to strike them from the register So we looked at this that we had, yeah, so the I think this, if I understand the suggestion, it's, it's, implement eventual consistency by allowing the helper to reject replay at some point before collection is finished. Does that sense? like the suggestion? So one, I think one downside we found of this approach is it requires the helper to start storing report individual reports. Whereas today, and I believe even with this PR, it can store just aggregates of things that it has committed to aggregating"
  },
  {
    "startTime": "00:28:03",
    "text": "So I'm not suggesting that that's necessarily the case either. So if you think, if you look at this process, here, if that two or one creates, was a 200 okay? And if the failure mode of this scenario invoked a new process rather than the success mode invoking a new process then we have some new process, rather than the success mode, invoking a new process, then we have something that I think is much closer to what I was suggesting than what you described Does that make sense? Yeah. So it's something like if I, if I don't think I could finish this in time, just abort and then retry? No, no So it is the leader sends a job to the helper The helper says, okay, I've got it And that's all the 200 response would be in that case It then proceeds to do the processing that would do, which is checking for replays. And then if that's successful, it adds to the aggregate No more action on that side. However, if it detects a replay it is the responsibility then of the helper to say to the leader, hey, you sent me this it's a replay. Yeah, when does it do that? When it detects that something is a replay. So the helper's a server so it has to do that in response to some HTTP request, right? Right, okay. So which are you suggesting that it does this via like a web hook? or via like another input that can be gold? Well, but no is question is a good one. Yes, why not? Or you could have some regular process whereby the leader check in with the help us to say, look, have you got anything that's? broken? Let me know about it. Now, of course that's problematic from the leader's perspective, potentially, because the leader may have done the aggregation as well I, yeah So this creates, in my, if I understand the proposal, it creates, the requirement that both parties store individual"
  },
  {
    "startTime": "00:30:01",
    "text": "reports until the batches final collected. Right, which is, which is why I don't like this replay design, anti-replay design at all Maybe we should be talking about that instead I actually don't see how this is too different from the current async proposal we're like the leader effectively polls after that to see like the status of that request was Yeah, the difference might be that you can batch in the other case the other case where you could say in the in the in the report that we've submitted to in the last half hour sure I go and scrub any of them for replays Because in most cases the answer will be nope and we carry on, right? Yeah, so I think there's always an option to, like, I'm only going to do replay protection, guarantee replay protection for the last 30 minutes, beyond that 30 minute window you just drop the report on the floor because you don't know whether or not it's replayed. And that's, that implementation is a available to, to both parties The requirement is that, I mean, the requirement since the beginning, has been no report is ever replayed What, no individual report Okay So, yeah, so I guess if someone, I guess we'll have to hold off on merging this, Martin, if you want to talk about this a little bit more So yeah, I guess we'll wait to hear from you I mean if there's time we can talk about a little bit more Okay, does anybody want to either? raise their hand in, you know, like, what is, I, like to hear for more people. Does anybody feel? strongly that this is the only way to? this is, this change needs to happen?"
  },
  {
    "startTime": "00:32:03",
    "text": "No? I'm pretty strong in favor of this happening. I think we need to if we are good we need to either get rid of these sort of like serialized transactions requirements or if we're going to, or we have to keep them, we need to very, at the very least get them out of the critical path and get them into somewhere where they can be like nicely and easily retried over time I'm not sure I understood that actually. Did anybody else get that? Do you want me to read? my mic quality bit off, or? Sorry, I didn't know understand the question It's hard to hear you, Noah is this better a little bit How about this? It's hard to tell until you say, maybe speak a little bit slower Okay. All right. How, how you bit. How about this? It's hard to tell until you say, maybe speak a little bit slower. Okay. All right, how easy or hard is this to call? or hard is this to talk? Turn down your gain if you can yeah it's yeah the room you're very bouncy, so it's hard to, it's hard to hear everything All right, I'm going to switch to another headset and rejoin Tim? Thanks. For whatever it's worth, curiously, we can hear Noah pretty well remotely. I'm not sure what, maybe there's something up with the sound system in the room. In any case, yeah, as marketing notes in the chat, room acoustics are terrible Okay, in any case, excuse me right, okay, so Martin brings up a lot of interesting points. I think we need to take like a lot a hard look at anti-reaching stuff. However, I think that this change is this change to make asynchronous, excuse me, make aggregation jobs, asynchronous is a good one regardless. For the reasons that Noah was just trying to explain to us, which is that it removes,"
  },
  {
    "startTime": "00:34:00",
    "text": "serializable transactions from the hot path of the protocol. Basically, my thinking in that even if we go do like smarter anti-replay stuff, that Mark is sketching out, we probably, that this change is probably still Yeah, there's not just the replay protection, there's also the CPU stuff you have to do there's CPU bound work to do as well yeah And also I want to double down to the point that you made, Chris, earlier, which is that if we're not careful about how we articulate anti- work to do as well yeah um and also i i want to double down on the point that you made chris earlier which is that if we're not careful about how we articulate anti-replay and particularly the the the recovery mechanisms then you easily end up with cases where, yeah the helper has to store individual reports or individual prepared reports. And that's makes the storage requirements for the helper, like, vastly bigger than they are now, which is a problem on its own, but especially given that a design goal throughout DAP has been to make the operational requirements on the helper lower because we want it to be relatively cheap I mean, DAP is pretty expensive to matter what, but we want the helper to be relatively cheap and easier run Amir? We don't hear you. It would help if I unmuted myself, sorry. I'm here with ISRG, I actually wrote this PR I wanted to clarify that as written, and I think what is already true, even before this PR, that there is still one argument small requirement of a zero serializable transaction for the aggregation job ID because imagine, if you will, two as simultaneous requests come in for the same aggregation job ID, but different request bodies"
  },
  {
    "startTime": "00:36:00",
    "text": "The way this PR is written, you have to be able to detect whether a duplicate request is coming in because otherwise it becomes ambiguous as to what request that the subsequent get request should actually respond to so more broadly i what request that the subsequent get request should actually respond to. So more broadly, I think if we're trying to solve the problem of moving all serializable work out of the hot path, I think we might have to go design a little bit more and maybe to what Martin was saying And then even broader than that, speaking for myself in a little bit of who I work for, there's no urgent need for us to adopt this if we need to go and iterate on it more, no big deal Uh, Noah? Yep, is my audio better this time? Yes all right perfect so uh what i was trying to say earlier is that i think we should I am strongly like in favor of this I am with a mirror that we should also if we're doing this, we should also try and get rid, though, of the other serialized, like, transactions that are required So we should revisit the, like, aggregation job check for example, and how that works. But yeah, I know, like, I'm pretty strongly in favor of this in getting anything that has to be serialized out of the critical path and that said like if we can find other ways that like to make this more eventually consistent then, like, I'm all ears to those proposals as well martin thomson, if people will indulge me,"
  },
  {
    "startTime": "00:38:00",
    "text": "I'd like to up level just a little bit The, an anti-replay design that works without having to track every single job is one that is the time window one that I talked about previously If there is a span of time bound to the task then things that are in that window will be accepted Oh no, that's not right. Sorry I'll go back and put my brain back in again we can get we can go all right Bench work schwartz as individual participant these two options seem like different HTTP API bindings of equivalent functionality I feel like I could write an HTTP front end that converted one into the other statelessly without ever touching, well, okay, not quite stateless but without touching the state that we're working about here. So I think it might be helpful to disentangle that a little bit and talk about the maybe changes in dependency between different kinds of information that you're storing separately from the question of how you actually represent that in the HDTPA API. Because, for example, there's no obligation that the leader only has a single HTTP request open to the helper at any given time. So all of these things are highly parallelizable. Yes. So the parallelism is what leads to resource contention. So imagine you've sharded your database, but you have multiple aggregation jobs hitting the same shard at once You're queuing a bunch of work against one shard And the problem and it's an operational problem, is that cue gets so full that you can't handle requests in a timely manner, and there are impacts"
  },
  {
    "startTime": "00:40:00",
    "text": "observable impacts in other aspects of yourself system. So I think that's exactly the same here right? You create this thing and and if you have a bunch of these puts all at once then you'll build up a cue and it'll take some time to drain that queue. And in the meantime, these gets that are polling are going to keep returning some sort of weight state. I think it's equivalent The way in which it's not equivalent is if you have Nat bindings and HTTP middleware that doesn't play well with HTTP requests that take 30 seconds to come back so that's exactly yeah so that's exactly my organization's problem. We're bad at holding requests open for a long period of time because a lifetime of a request is generally like a shot tops. Sure. I think that's a fine sort of a HTTP API design question of what is a reasonable limit on how long a client should expect to wait in our system Yeah. So is that in favor or what what outcome would you like to see for this PR? if I? I view that as kind of a trivial question there's there's really it's not difficult to write an HTTP API that avoid those long delays. And we should just do that should just do that and disentangle that as much as possible from the question of intertask dependencies that and deeper questions about how the database works. So I would love to see that suggestion put up in a PR All right okay, I'm just going to take a note and then we'll move on All right, that's the one that I was hoping we could solve today, but that's how it goes. Oops, I accidentally stopped sharing. I'm sorry This is like my ten"
  },
  {
    "startTime": "00:42:00",
    "text": "was hoping we could solve today, but that's how it goes. Oops, I accidentally stopped sharing. I'm sorry. This is like my 10th idea. I should be better than this Okay, moving on to hopefully one that's a little bit easier, although I don't think the professional IETFers in the room are going to like how we want to resolve this So there is an ambiguity in our document the way we use TLS representation language. So we will write some of the like what is shown on the yellow highlight line there. We say like, here is the struct. So basically, the text that precedes this is a the party constructs a message with this format and we're saying, like, set this value to two, this for the first field to two and then report ID report share error these are set these are like implied by surrounding text this is a so this syntax is used in 844 to indicate a constant. This feature does not change. And for the, for the for this is because TLS 1.3 had to be written in a way that is backwards compatible with TLS 1.2. So it's a historical artifact that needed to be spelled out in 8446 So we're very, we're overloading this, which is a problem. There's kind of two ways you can resolve this. One is we just don't write text in this awkward way, where it's like convenient to do this The simpler, hackier change is to just modify TLS representation language and we do so as follows we add the word variant after the word struct, and this indicates that we're specifying like a more specific variant of this struct the word variant after the word struct, and this indicates that we're specifying a more specific variant of this, of the struct here. So what do people think about merging this?"
  },
  {
    "startTime": "00:44:00",
    "text": "Does anybody object? Yeah. It's here hear This is an editorial question and it's up to you. How do you want to present your protocol? I don't think we need to discuss it in here. Cool Any other opinions? Oh, Watson, go ahead Um, I would nitpick struct variant. Me Um, I would nitpick struct variant. Maybe variant is all you need Okay. We'll take that suggestion unless there's, unless Tim, who wrote this PRJ doesn't like it. Tim, do you want to say something? Yeah, well, one, I support this change because I wrote it, two. Actually, I quite like Watson's suggestion Indeed, we don't need both struck-end variant If anything, really, if we were going, like, deep on this, we would want to, like, make the notion of an enum more powerful like that's where the notion of variant comes from is because where we use this typically, we're describing a variant of an numerate um anyway long story short i think we should take watson suggestion i will attack a commit onto the PR in question to achieve that okay All right, moving on We're going to merge that That's the decision there. Okay, so this one, I was hoping Chris would would be in the room. Okay so this okay, so, okay, so context so context. The, oh, Tim, do you want to interrupt? No, I just want to get in queue to say something about this after you've finished explaining it Though if you want me to, I can speak to the slide Okay, I'm just going to continue. So today there is a feature of the protocol that allows you to configure tasks configure per task, HPKE configuration"
  },
  {
    "startTime": "00:46:00",
    "text": "So as a reminder, HPKE is used to protect report shares in transit to each of the aggregators. So when I'm uploading a report, I, first shard, I do the V-Daf shard step, and then I encrypt each input share to the intended recipient And that way, they're confidential in transit So what key do we use? This is obtained by hitting this endpoint called HPK config that is specified by each of the aggregate So yes so today you can specify a task ID parameter in your request and that will integrate to the server if you have a particular HPK config you want to use for this task let me know, and I'll use that. And this is, yeah, so this is meant to provide key separation So this is a slightly problematic for applications that might need anonymity. So imagine you have R running DAF over OHG which is turning out to be a common deployment scenario Aggregators know basically what task a claim is participating in, and that might invalidate some of your security properties So there's, and just as another note, I'm not aware of an implementation that is using this feature today So there's two ways we can resolve this one is add more security consideration text describing the scenario and what to do in that scenario Or we just nix it because we haven't needed it so far. So the PR removed this task ID parameter and basically forces you to use the same set of HPK configs for a given endpoint. So does any ID parameter and basically forces you to use the same set of HPK configs for a given endpoint. So does anybody have opinions about whether we should take this or not? Go ahead, Tim, you're still in the Q"
  },
  {
    "startTime": "00:48:00",
    "text": "queue. Thanks Okay, so I think Chris explained what the change is about, and another key motivation in our analysis, like divv you up that is or bioseergy surgery, was that when you do DAP in conjunction with OSGTP, which we think is going to end up being a quite popular configuration um it you end up very strongly wanting globally we call global HB configs or non-task specific HB is going to end up being a quite popular configuration. You end up very strongly wanting what we call global HBQ configs or non-task-specific HBK-E configs because per task HPKE configs is a privacy leak. It reveals which clients are going to participate in which tasks. Additionally, there task prov the sort of automated client-driven task provisioning protocol, which is not adopted, I don't think, but it is a working, whatever, it's an idea in the working group. TaskProv also basically forces you to do global HPCA keys because you the client doesn't yet know what the task will be when it's generating reports Okay, so the one nuance I wanted to bring here, having like written this change and still unbalance supporting it is that uh the real motivation for why I did this was that I saw an opportunity to take some text and let some conditionals out of this protocol And I feel like, you know, simplicity is always a good principal, a good North Star to follow But when I went and actually made the change, I realized like this doesn't actually delete that much text. And it's also like extremely easy for implementations to just not do per task, HPKE keys Point being, I don't think it costs us very much to continue to allow per task HPKE configs in depth So I think that should be born in mind when we consider whether or not to take this. Cool. Thanks, Tim Just one clarifying thing. TaskProv is not a working group drive Yeah, sorry Does anybody else have anything they want to say? I think I've heard one opinion offline about this agreeing with Tim that this is not necessary to take Does anybody object?"
  },
  {
    "startTime": "00:50:00",
    "text": "the decision right now? is do not merge Nick, go ahead We're short on time and we do have one more to talk about, but this is more important than the next one I'm also interested in the next one, but it seems like a privacy improvement, so if it's also simpler that would be a vote of support for taking it Thanks Go ahead, Amir I support taking it because I'm not sure that has anybody come up with a tangible use case for the task ID parameter? that isn't covered by a global key I don't believe that's the case, so I would be in favor of striking it Cool. Thank you Martin, you're next Yeah, martin thomson it seems like this can go and I'm not hearing anything in favour of keeping it So in the spirit of making everything better privacy, simplicity making it easier to deploy, All right All right. I think Tim and I will talk offline, but if I could convince him, I think we'll merge All right next one and this is the last one Sorry, Chris Brubon oh go ahead Tim sorry you guys not see the yeah um on. Oh, go ahead, Tim. You guys not see the kids? To be clear, as much as I argued, against the previous change I think we should take it on balance. So I suspect"
  },
  {
    "startTime": "00:52:01",
    "text": "Let's move on to the next one. All right, merge, great Okay so there have been some suggestions from a- about it might be useful in certain circumstances to add versioning to the content type. So the content type is a field of the media, of the HTTP request that tells you basically how do I parse this message And so the media type has to match the type of message that's being transmitted and because we are making breaking changes in the protocol sometimes it might be useful to actually there's other ways of doing versioning potentially, but some have asked for being able to do versioning this way. So you might indicate the draft number and or something like that. So, this is a May in the draft This is like may use it this way uh tim has noted in the past that this is something that it HTTP semantics already lets you do So it may not be necessary to have normative text at all about it this. In any case, this would be deleted before RFC because the text is specifically about drafts. Noting that if we do have DAPV2 post-RFC, we'll have to update the media types anyway for that new draft. So does anybody have opinions here? Tim? yeah I don't think we need this for the reasons you noted For the reason, the other reason that like, this is text that we would add, and then it comes in the proposed change, it comes with a note that says delete this when we go to RFC which we hope that's me knocking out wood will be soon anyway. So I don't see why we would take text that we plan to delete imminently. And finally, the concrete like proposal for how to do this version that's in the change is to add, I forget what it's called but like you put a semicolon at the end of the media type and then you add"
  },
  {
    "startTime": "00:54:01",
    "text": "some like version equals this, which is fine but yeah, like HTTP already allows you to do this I don't think it's fruIETFul for a protocol that is built on top of HTTP. Like explicitly and with a normative reference to enumerate the things from HTTP that you're allowed to do So, yeah I mean, I don't think we should take this Okay. Do we have time to drain the queue? Yep, cool. Cue is open Sum on We don't see you, Sumon Now we see you We don't hear you I'm sorry First time, 88, yeah I'm just curious. How, what HTTP? is providing out of the box without having something like this? I understand that a major DAP versions can have a different mine types altogether. But I'm just heard some comments about HTTP uses I understand that a major DAP versions can have a different mind types altogether. But I just heard some comments about this out of the box. So I just want to understand better better I'm not sure I understood you, Summon I think that was directed at me, so I'll respond to that Yeah, I mean, the specific feature is the ability to tack on, again, I forget what it's called, but this is what's proposed in the change, right? Is that, like, at the end of so the example on the slide is the media type application slash data aggregation JavaNet request. So you would tack on the a semicolon, and then I think it was like draft dash version equal something, some number, right? So the point is that like that syntax you would tack on like a semicolon and then I think it was like draft dash version equal something, some number, right? So the point is that that like that that syntax of semicolon separated like fields in a media type um that's something HTTP already defines already lets you do And like without this change, to DAP any DAP server should already be able to handle those fields. Or like that"
  },
  {
    "startTime": "00:56:00",
    "text": "that's not standard, right? in the media time you could pass anything this here we are explicitly specifying what to pass there well the change as is doesn't though like it just said like, hey, you could go do this, but it doesn't actually instruct applications what you do in the presence of a given um version value So I think we need to close this topic and move on in the interest of time so I would encourage the editors to consult with some some HTTP experts outside this room who might have some advice on the best way to use content type with draft versions. Thanks I think I love X Yes Okay. Oh, do you want to do test proff? first or the other one? TaskProve. Okay, so this one should actually be quick. I think we had like 50 minutes for this, but I guess I said that about the last last topic. Okay, so this is a this one? Task problems. Okay. So this one should actually be quick. I think we had like 15 minutes for this, but I guess I said that about the last topic. Okay, so this is a draft that we are seeking adoption on. This specifies an extension to the report extension to the DAP protocol that has a couple of different tasks sort of a couple of different objectives The feedback that we've gotten from this on this from from previous ITS is some people are enthusiastic about one of them and then not the other So the feedback has been to split it up So that's what we've done. So let me quickly go through the"
  },
  {
    "startTime": "00:58:01",
    "text": "goals. The first goal of this draft is what we call task binding. So executing adapt tasks requires each party in the protocol to agree on the task configuration. So there's many parameters that everyone needs to agree on like the batch size, which VDAF to execute. What is the query type, well, batch mode will now start get used to call it And things like this that matter to execution There's no mechanism that tells you that everyone actually agrees on this. So what we want we're trying to accomplish with this extension is when a client uses this extension, the aggregating who support this extension will check that the client and that aggregator agree on the parameters And this is accomplished simply by specifying a serialization of the task configuration and deriving the task ID with it so what the aggregator does is I see this report extension If I don't recognize it, I reject the report That's already, that's the semantics of the protocol today. If I do recognize it, what I do is I make sure the task ID matches the task config and if they agree, then I accept it or I drop it right there So, yeah, so basically what this is supposed to do is say, like, if I've executed the task appropriately, it's because I agreed on the parameters. Or rather, if I don't agree on the parameters then I'm not going to execute contrapositive. Anyway, you know what I mean. Okay Another goal for this draft is actually provisioning a task. So what the draft says, is the task is configured out of VAND There are these parameters that all parties need to agree on We don't tell you how they agree on them So this is what the draft initially did was specifying this mechanism where basically you can advertise the task config in an HTTP header"
  },
  {
    "startTime": "01:00:00",
    "text": "and you either opt into the task or opt out If you opt out, then it's basically an abort and the protocol stops if you opt in then you have, then you're like agreeing to take those parameters and continue processing So the security considerations for this are the same as the CoreDAP protocol We don't intend that this changes. Of course, this is a tax surface and we have to account for it. And we've done some things to do just that The other aspect of this is like how many tasks are you running through your system at at any given time? If you have a fixed number of tasks, this is just not necessary, but if the set of tasks is large, and maybe changing over time, this can be useful So I will say we've been experimenting with this. We have this deployed and we're comfortable using it at least So whether or not it gets adopted, it's going to get deployed Okay so changes to the protocol mainly to address feedback. The main thing being splitting task by binding, and provisioning into two different sections Provisioning is completely optional task binding, and we've actually renamed the extension to DAP task bind instead of DAP task prov. And yeah what else did I want to say about that? Oh, and we've reduced the advertisement rate so so you know the client doesn't have to send it with every request. That was a good observation. We took that And we also been as individuals pointed out some potential privacy concerns with fingerprinting And we kind of address that by acknowledging it because there's not anything we can do about it and we, yeah, we're not certain the risk is that high anyway so yeah happy to talk about any of the"
  },
  {
    "startTime": "01:02:00",
    "text": "things We want to ask for adoption. We think this is ready to go There's a few changes we can make after things. We want to ask for adoption. We think this is ready to go. There's a few changes we can make afterwards. One of them might be just rip out tasks task provisioning bit section completely We're open to that. So it's just a task binding But yeah, what are people's thoughts on this? Anything else you want to see before we, before we this? It would be great to hear some comments on this draft because the authors are pursuing adoption in the working group Okay, great. We have some commenters I'll just go ahead. So I think I grew Chris in the sense, particularly in the point that this has been deployed and implemented and deployed in a couple places now So there might be merit in adopting it in this working group for that reason. Just so we can all work together on producing the best specification of it that we can nick doty, CDT, I believe that I'm generally supportive of in-band task provisioning because it provides an opportunity for interoperability rather than out of proprietary, you have to already have just some existing agreement between the client and the aggregators but also it seems like it gives us an opportunity to address the transparency question for the user. I know it's not the"
  },
  {
    "startTime": "01:04:00",
    "text": "only way to address transparency, but it does seem like a good mechanism to give the user through their client an opportunity to investigate the properties of a task, decide whether or not to participate, and that seems like a good approach to the sort of privacy and ethics of participation and even if it can be done in other ways, it would be good to standard one way to do it Tim, you're still in the queue? I am, or I'm in the queue again, to respond to Nick. In that it should be noted that test prov doesn't resolve the issue of need a pre-existing relationship and arrangement particularly between like aggregators, because there are at least like some secrets that you have to share and other parameters we have to share between participants before a task gets provisioned. So this this, like what we have here doesn't get us to a world where like anybody can just rock up to any aggregator and start doing DAP, like, completely seamlessly you still probably have to establish business relationships I also think we should be incredibly careful about talking about transparency Because I'm sorry, this is not strategic transparency. It doesn't do the same things It doesn't provide the same security guarantee but other transparency solutions do so let's be very very careful about like gesturing at those concepts Yeah, so, um, transparency, but I think, yeah, I think that's absolutely worth calling out um it's not like it's you know there's there's a couple things first of all Tim's right, there still has to be some preexisting relationship between the aggregators, between the collector and the leader, between the leader and the, and the, what do you call, the clients"
  },
  {
    "startTime": "01:06:00",
    "text": "And some system has to arrange for that that communication and that's not fully specified here there in particular like this there is the shared secret from which we derive the VDAF verification key that we use for the, for the, a given task but I do think we improve like, the state of affairs like I like the the client gets to decide if it likes the minimum bad size and I think that's a good thing so it's not you know, it's not, it's not transparency in in the broader sense of IETF but it's it's it's better if, yeah it's better the, the client knows more about what's going on, at least if they can inspect their device So, Chris is there a poll that you would like to run? in the room for this draft today? Sure, yeah. Yeah, I, yeah, yeah sure. We could run a poll to ask if people have read the draft. We could run a poll to see if people would like to adopt the draft Do you have any thoughts on what sort of poll you'd like? to see? I'd like to know who's who's, yeah, I'd like to know who's read the draft Yeah Appreciate you okay So I've opened the poll Please open your on-site tool or if you're off-site, please note the button to switch over to the poll view. If you haven't used the Q1 code to connect your on-site tool to the room, this is a great time to do that. It is obligatory And, uh"
  },
  {
    "startTime": "01:08:00",
    "text": "we'll, we'll puzzle over later what it means to not have an opinion about whether one has reviewed the task proposal Um going to give 15 more seconds for that Then your business, mother, I'm right right so I think it would be great if a few more people could review this draft And I do think that we're going to, I think we should expect to see a call for adoption in the near future. Thanks All right one more for me and then and I'll make this really, really quick. This is just a idea I want to put in people's heads So, um think that one of our jobs is a working group at IETF is especially one that is working on fancy crypto is to help direct academic research towards problems that matter to us So there is a couple recent papers that basically add some efficiency or at least like shift around some things"
  },
  {
    "startTime": "01:10:00",
    "text": "in VDath execution that lead to different trade-offs for like a protocol that uses it So I want to talk if I want to see from the room if I just basically wanna have an on-the-record conversation of, is this something that academic should be pursuing? So yeah, let's get into it. So basically today, the way processing works in a V-Daf is we are doing some processing per report So this is the input validation thing. We call it preparator for reasons that we don't need to get into But the idea is that I'm running, we're, so this is on like, think of this as an HTTP request, an aggregation job For each of a set of reports, I do some computation and I send the result of that computation to the helper The helper does basically the same computation and produces a response. And at the time it's produced a response, it knows whether each of these reports is valid. And then there's one more message to send to the server, and the server will compute the same thing So this is kind of an abstraction around like the zero knowledge proof thing that we're running in PREO So we do this per report and then, you know, like for each report, if it's valid, we reject it. If it's invalid, we reject it So there's a couple of new papers that allow this process to be batched. So basically for a sequence of reports, I can do the same thing I produce this value called a tag that applies to all of the reports and the aggregation job. The helper produces the same thing and then with these two tags, it decides is this batch of reports valid? So if, all of the reports are valid, then then knows it's valid. If one of them is invalid, it doesn't know which one There may be more than one. So it's a yes or no, like, is everyone? good or is not everything?"
  },
  {
    "startTime": "01:12:01",
    "text": "good? So, um there, so one paper that does this, like uh, from, uh, S&P, security and privacy this year, Whisper, this is from Henry Corgan Gibbs, the designer of much of the crypto that we're already using So they figured out this way to make a fully linear proof system This is the thing that we use in Preo, what they call silently verifiable. And this basically, basically a this batching thing to happen. And I'll skip this details of how exactly it works. What we end up with here is the, we have the same bandwidth overall except the client is going to be sent more bits than the aggregator. So during, aggregation, we end up sending substantially less So, and instead of the leader sending that to the helper, the client sends it to the aggregator So we're just shifting around who pays the cost for the bandwidth. Do clients pay the cost for bandwidth? or does the aggregator? And there's another paper that does this similarly and with similar functionality is popular So there's a there's a draft PR up that shows how we would incorporate this into DAP And basically this would be like if the thing is not if the batch is not valid then we fall back to per report validation. This is an option for both of these constructions so the the thing is is this, this this doesn't add any have any bandwidth savings for our protocol as it is, and it has to do with the leader upload architecture that we pay picked back at like IETF 113 if we had gone with split mode, where the client sends a report share to each of the aggregators individually, then we would get this benefit of, again, the communication cost is the same, but the client pays the cost. So the question for everyone is is this a desirable property? It has a"
  },
  {
    "startTime": "01:14:00",
    "text": "some potential downsides. We have like a denial of service thing that we have to engineer around, but some parties might be able to make this work. So I'd like to hear what people think of this feature You have five seconds Tim? Thank you Well, I have no more seconds left. I think martin thomson elsewhere has made the point that another important tradeoff to be waiting here is moving work between the servers and the clients Like, we have to be mindful of the fact that there's a lot of clients out there, right? And if we push more compute work into them, like we're using a people's battery life and we're burning a lot of cars and we have to be like we have to be making responsible use of people's CPU time on their phones and laptops and so on Okay, otherwise I like the degree to which like it's cool but agree to which his working group engages with, like, what did Chris call it? Fancy Crypto, or, you know, cutting-edge stuff from papers that have just been published, like, that's good. But one of my takeaways from the DAP process is been that we really should, I think we should be motivated by like concrete not quite industry but like industry use cases more so than, you know, there's a paper out there So rather than speculate that like maybe somebody has a use case for this, I think the use cases need to come first before group commits time and you know protocol tech to looking at a problem. Totally agree I'm not I'm not suggesting we work on this this is the point of this is there are academic that are spending time on this problem and are they spending on the time on the right problem that's that's thing that I want to ask Because if they're not, we should tell them Sometimes silence is an answer Let's move to the next topic"
  },
  {
    "startTime": "01:16:06",
    "text": "Ben's getting up and getting situated. Is there anyone willing to take over notes for this session? Because I feel like I might have some things to add Watson has reiterated his volunteering to help out with no taking. Thank you again, Watson Thank you yeah So, yeah Good All right. This is Ben Case at Meta and joined by Mark and Alex here in the room Some other folks from our teams have contributed to what we'll talk about So we'll talk about binomial D.P in the room and some other folks from our teams that have contributed to what we'll talk about. So we'll talk about binomial deep noise generation and MPC and a little bit of from our teams that have contributed to what we'll talk about. So we'll talk about binomial DC noise generation and MPC and a little bit of... You're going to need to be closer to the microphone Thank you All right, so as I was saying, Ben case from Mata and joined here by Martin and Alex we'll talk about DP noise generation and MPC and also mention a couple of other topics that may be of interest to this group So Martin shared on the, see if I can get this remote thing to work On the email chain three new drafts that we would like to see this group express"
  },
  {
    "startTime": "01:18:00",
    "text": "interest or feedback on The first one is the one that we'll talk in most depth about today, which is about generating central DP noise inside of MPC using a binomial mechanism. And then the other two, are related to doing three-party honest majority MPC so I think in the last ATF in Australia Ben Savage was here also from META and he and Martin sort of gave a little bit of an overrepe of this direction of three-party MPC And then another tool in chris box is a pseudorandum secret sharing, which is way a little bit of an overview of this direction of three-party MPC. And then another tool in chris box is pseudorandum secret sharing, which is a way for MPC parties to sort of sample sort of a shared secret state So we're going to talk most about the first one and maybe there can be some discussion as well toward the end about, you know, this group's interest in sort of continuing some of the things after that. I think the first one is potentially interesting in even the DAP setting or two parties. The other three are the of continuing some of these things after DAP. I think the first one is potentially interesting in even the DAP setting or two parties. The other three are, or the other two are perhaps more future All right. So as I said, we'll spend most of the time talking about the DP noise in MPC And first we'll kind of talk a little bit about the motivation for the So I think the motivation for DP itself is pretty clear like we want, you know, DP can help you a lot with output privacy. But then the question, is, why generate the DP noise in MPC? There are other approaches. The most natural baseline would be for each help party or worker to add D noise independently. And as we'll see, that's that sort of gives you a baseline for how much DP noise is added but it it tends to be more than what is sort of ideal Another sort of alternate approach here to sampling in the NPC would be having clients add local noise and shuffle That approach has some benefits as well but it also has a tradeoff that it sort of the noise can be dependent on this"
  },
  {
    "startTime": "01:20:00",
    "text": "size of the query quite heavily, whereas this approach can tend to separate the two other question you might want to motivate is why binomial noise if you're familiar with the DP literature, normally people would sample from a Laplacean or a Gaussian And I guess the approach here is more of a pragmatic one like we would like to use something which is, you know, as close to the Gaussian as we could, but, you know, binomials are simple. And they're also relatively simple to generate inside of MPC And at least for a lot of common parameters, they might give us a meaningful improvement that we're happy enough with. So we'll at least start here So talking a little bit about the binomial D.P mechanism itself. So suppose you have a function here that computes something on a private database D, and it maps it to the integers. An MPC, what you would do here is you would sample sample X from a binomial np distribution and then you would output the function plus the noise, the recipient is going to get that output you know they'll get it from the mpc noise, the recipient, is going to get that output. You know, they'll get it from the MPC, so they'll have to recombine shares to find out with the actual result is. But then after that, they will also subtract off the mean of the binomial to have an unbiased estimate of the computation on the database okay this is the basic protocol or mechanism here You can make it a little bit more involved here, where the function D has, or so the function F is sort of D dimensional outputs And so here, of course, you're sampling, you know, D, I, I, samples from your binomial and adding one to each component You can also add this additional factor here, S This is a scaling parameter. There's a paper I think I'll come to it in a slide by some researchers at Google that sort of analyzes a lot of the different parameters for binomials in the DP context"
  },
  {
    "startTime": "01:22:01",
    "text": "and this scaling parameter can help you to sort of to a slide by some researchers at Google that sort of analyzes a lot of the different parameters for binomials in the DP context, and this scaling parameter can help you to sort of tune to get a good sort of privacy utility trade-off in more parameter regimes So then if you're the recipient, you're going to need to subtract off your mean, and then you need to sort of rescale things So generally here we're gonna be making S small. So S is you know, one over some natural number Okay, you can write a different way, but I'll skip that all right so now the main question that this draft considered and it's based on this this twenty here, is if you want to get a proximate DP, which is, you know, given some epsilon and delta, you define your private with those, how should I say? the parameters of my binomial? What is N and what? is P? So that's most of what this draft currently looks at Thankfully for P, the answer is simple Set it equal to one half That it's optimal for some reasons the paper discusses, but it makes for a nice feature for us in the fact that sampling uniform random bits is also the easiest thing to do in NPC as a coin flip protocol Determining N from your epsilon and delta, is a bit more of an involved process. There are two constraints, one that depends only on delta, the other one that depends on both epsilon and delta. And that one is a little bit more heavily dependent on the epsilon factor than the delta. And basically what you need to do is do some calculation to find the smallest and that satisfies both constraints simultaneously these are the constraints in the paper discusses them in more depth there. I won't get into the details. You can see it's a little bit involved in the math for if you're familiar with Laplashians, normally you look at the L1 sensitivity of your function, Gaussians, you look at the L2 sensitivity"
  },
  {
    "startTime": "01:24:01",
    "text": "of your function. If you're looking at binomial noise, you have to consider both the L1, the L2, and the L infinity sensitivities of your function to sort of properly get your Epsilon delta parameters set. Okay, but needless to say, that these are just some some complicated equations, but we can solve them either a little bit analytically with a chondretic and n, or is I did when I first just started trying to implement this, is just do a binary search to find the smallest end that satisfies both and once you've determined that yeah, then you'll know sort of know how you need to set up your sampling The other sort of parameter here is this quantization scale So decreasing your quantization scale is going to reduce your error, but it's going to increase your N. So the N increases the MPC costs So this is one where you kind of have to play with it a little bit and right now it's just somewhat of an exploratory process maybe we'll have some better analysis in the future that will sort of tell you what you want to do a little bit better here But basically you want to decrease this as much as possible subject to your practical MPC cost constraints So, as I mentioned, I think I may be missed a slide way back or that was just talking about what we need to do here in terms of generating the DP noise here. So once you're, let me just see if I had it Yeah, I miss this binomials here So an MPC, right, if you're going to sample from a from a binomial, if you remember a binomial is just the number of successes in and for nearly feature trials. To do this in MPC, it's it's you need two things. First, you need sort of a coin flipping protocol to run these trials, and then you need some way to sum up the values and if you sum up the values that will be the number of successes. So it's pretty straightforward protocol here that allows you to sample"
  },
  {
    "startTime": "01:26:00",
    "text": "binomial noise and MPC So now moving back to our pretty straightforward protocol here that allows you to sample binomial noise and mpc. So now moving back to where I was though, setting the parameter The thing that you want to try to minimize here is the error that you're introducing into the actual outputs And so this is where we can start to look at what are the basis and how can we compare, how well we're doing the binel to other approaches so in an ideal world, you would like to sample from a Gaussian, a single Gaussian distribution that like exactly satisfies your epsilon delta constraints, and you would do this inside of MPC, and it would be perfectly generated, okay? So that would give you sort of like the ideal minimal amount of noise inside of MPC and there's an error formula for what that is In a binomial case, you have a different error formula. And the other approaches would be to have error that is added independently by each of the helpers. So if you're in the two-party case, two of your workers add DP noise. If you're in the three-party case, honest majority, all three add noise. In the three-party honest majority setting, you end up with 50% more than is ideal. So more than this like single Gaussian case. Because in honest majority, if you think about it, every party is going to add sort of 50% of what noise is sort of needed in the end so that every two of them are trusted to at least add their noise. You end up with 100% of what you actually need to guarantee But that means that for, you know, your, you're typically output party that receives the outputs they've actually had noise added by all three parties. So you get sort of 150% of or of sort of the desired noise. So 50% more than what's ideal In the two-party case, you end up with twice as much as ideal, because each party needs to assume that the other party, might not add their noise. So each party needs to add it fully"
  },
  {
    "startTime": "01:28:00",
    "text": "So these are sort of our baselines. How can we improve a upon 50%? How can we improve upon 100%? And how can we do that in the binomial? All right, so I have some tables of parameters here for different epsulons, deltas, the sense sensitivities, different dimensions, different scaling parameter, and that gives you what's this n? Okay, so I think that the two columns of most interest here are probably the ones on the right. N is like how many samples do you have to add? together inside your MPC system to achieve the percent here of error? that's worse than the ideal setting. Okay, so on this side, I'm targeting ends that are less than a million here So I think the main takeaway is that for a variety of different epsilon delta parameters that maybe you're interested in for your DP system, if you sample a around, you know, 200, 300, renewly samples and you add them up, you can bring your sort of worst case error down from like 100% of what it would be if you did it sort of with two parties down to around 50%. So you can sort of like improve your privacy utility performance by around 2x by sampling, you know, two or three hundred thousand All right the next table looks at sort of a more generous sort of allowance for N So here we look for N's up to 10 million here And that's sort of for a variety of different you know epsilon deltas you can see allows you to sort of bring your error down to around, you know, 10% kind of ballpark So in, say, for example, the three-party setting where you're trying to improve a upon a baseline, which was already 50% worse than the other setting, you would need to probably be somewhere on this slide here in the three parties"
  },
  {
    "startTime": "01:30:00",
    "text": "where you're trying to sample, you know, a couple million bits to sort of bring your error down to about you know, from 50% to 10% Okay, and then the next slide is a bit more aggressive setting for N. Maybe if you're trying to support some really large queries to be to about, you know, from 50% to 10%. Okay, and then the next slide is a bit more, a more aggressive setting for N, maybe if you're trying to support some really large queries to begin with, where you're able to sample, you know, 100 million sort of binomial samples and add them up And this is where you start to be able to get your error down to pretty close to the ideal, you know, two or three percent worse than what you would ideally have Okay, but obviously, you know, the cost is going to scale with with N. That will depend a little bit on your MPC system and sort of the may depend on little bit on sort of what you want to add over to the base query itself. So this is kind of a summary there in the two-party setting if you want to improve some like 100% down to 50% worse than the ideal, you need your end to be approximately two or three hundred the base query itself. So this is kind of a summary there. In the two-party setting, if you want to improve some like 100% down to 50% worse than the ideal, you need your end to be approximately 2 or 300,000. If you want to improve upon the three-party baseline, 50% to 10%, you need probably N around, you know, 1 to 10 million. If you want to improve further you're going to need to end to be, you know, up to 100 million All right. So one thing you may note on some of these, tables of parameters here, if you look really closely, is that we sort of lack really small epsilon deltas with large sensitivities So that's a harder sort of parameter setting to support We'll kind of do a little bit of future work to try and see if the are other ways to scale when you trying to go to those parameters, the scaling parameter doesn't really help you. The paper only analyzes a scaling parameter which is like less than one. So there may be some more, maybe some future work that we can do to sort of improve supporting sort of those types of parameters So as of course I'm sure you'd be interested like how much is this going to cost? of improve supporting sort of those types of parameters so as of course I'm sure you'd be interested like how much is this going to to cost so in the three-party honest majority settings so this would be the setting that these other drafts,"
  },
  {
    "startTime": "01:32:00",
    "text": "talk about, where we do honest majority replicated MPC similar we have built for IPA and the W3C, we implement this, and we sample 2506 samples in parallel. So this would be as if you're dimension is 256. So think about like you have a histogram with 256 buckets and you want to generate noise for that all simultaneously and adding the same amount of noise to each bin. One million binomial samples is going to cost you about 95 megabytes of bandwidth and latency is around three minutes on our current implementation Go to 10 million, it sort of scales up roughly linear. So it's going to cost you around about a gift to do 10 million in bandwidth and more along 30 minutes for latency. They just sort of go up linearly from there. So I think we're the bandwidth expense here is I think manageable I think the latency we still feel is high and we have some future work with a different aggregation protocol that would hopefully will hopefully improve the latency here So some of the things that we're continuing to work on for this, is work to improve the analysis for small epsylons and large sensitivity parameters As I mentioned, that's just like a harder sort of parameter space to support. It's also maybe interesting to just sort of get feedback from the group on which parameter regimes might be most interesting to support, like, you know, how large a sensitivity do we actually need might be an interesting sort of piece of feedback Now or? Sure So I, I've been thinking about like, fair large ranges of sensitivities lately but like initially like you know, we don't have to go crazy. There's a lot of use cases that we have that are just, like, histograms, you know? Yeah, I think that's one thing we have seen is that"
  },
  {
    "startTime": "01:34:00",
    "text": "for a lot of a lot of queries especially large scale, ones counting is very common So counting queries are at least a good place to start there with low sensitivity As I mentioned, the other thing that we, are still working on a little bit here is the three-party aggregation that would hopefully decrease the latency sum here And then as this draft here is uh we have some work in there to sort of sketch out this is how you do it in three-party setting. This is how you would do it in other settings But the draft is currently sort of empty in how you you do this in a two-party setting? How would you initialize? or instantiate the MPC to sort of sample and add up your bit? in the two-party setting? So this is, I don't know, two-party setting? How would you initialize or instantiate the MPC to sort of sample and add up your bits in the two-party setting? So this is an area in particular where, you know, help is welcome especially if folks find this to be interesting in the DAP setting and we'd like to explore this Definitely sort of you know, please, please contribute in this direction. There's been some ideas that have been discussed to use an OT in this direction but plenty of work to do here. And then the other thing I think that we'll just kind of consider to continue to look at is, you know, this binomial sort of supplies us one way of doing central DP noise. You know, there are other ways that you could approach this in dp that would be you know more involved circuits. And, you know, we can see if those would improve our binomials at least binomials here, gives us another good baseline to have Okay, let me just stop for any more questions on the DP binomial draft for now And Martin, if you have anything to add go ahead too watson ladd Akamai very interesting work I wonder if there might not be as a whole"
  },
  {
    "startTime": "01:36:00",
    "text": "if there might not be, you know, there's a whole literature out there on MPC for various evaluations Not sure that at the side there isn't some way to get that down under performance down, just applying stuff we know. We'll be interesting to see how expensive is evaluating Gaussian really really Yeah, I agree. It would be nice to have these sizes uh down a bit more and the total cost. And so I think, yeah, it sort of is, it gives a nice, at least sort of baseline to compare other approaches against Yeah, martin thomson, there are other papers in this area that talk about evaluating binomials in MPC one of the things we found that was interesting about the paper that we're looking at as the basis of this work was the very clear P equals 0.5 being the optimum for both efficiency and getting the these error numbers down But there is other work out there that's talked about using binomials with different values for P and working through all of that. So they have some numbers on that that compare those with, for instance, evaluating Laplashians and Gaussians and they have algorithms for that that I think are interesting to look at. One of the things that concerns me in that area, though, is that there's, like I said, in chat, there's a rather large corpus of work on differential privacy that looks at attacks on differential privacy that come as a result of the approximation you make along the way. And one of the things that I think some of these papers have been very careful with and others I'm not as clear on is the approximately they make. So if you're looking to do a binomial with the probability of 0.3 point three, your approximation of that point three in the computation of that one does have very direct"
  },
  {
    "startTime": "01:38:00",
    "text": "relevance in terms of the security of the output and so we need to do a little bit more work here personally I'm not very happy with the numbers that we have here. I don't think these are anywhere near as good as they could be, and we need to do a bit more work Ben and I've been talking about some of the various ideas, and I've got a few ideas of my own to help sort of streamline this. One of the things that we haven't done, for instance is we haven't looked specifically at histograms or vector sums and tailored the computation to that And it may be that we can do some tightening in that regard and it may be that we can also do some other things around this scaling parameters that will allow us to get much much closer to the to this ideal yeah thanks, Martin. Yeah, one thing you mentioned that some of the papers, maybe look at like Gaussians. I think it is important to be careful there that like your approximation isn't sort of degraded and you don't sort of actually consider it. So it's some of the papers maybe look at like Gaussians. I think it is important to be careful there that like your approximation isn't sort of degraded and you don't sort of actually consider it. So it is nice in this paper that they look at the binomial directly and analyze its properties directly what you get from that. So you kind of avoid a little bit of that potential approximation error. Thanks, Chris Chris Patton. So I I'm not nearly as terrified, but I'm also like not super exposed to this stuff. Thinking about like the, just the latency, like three minutes, it's a long time to do work on a computation, but if, if this, if we're doing this once per batch, like, you know, in the DAP setting, for example, I can imagine tolerating that latency Of course, you know, we want to like see if there's other paradigms for this. So, um yeah, in general, I'm very enthusiastic about this work and very, I'd love to see this be built on top of DAP in particular And I'm game to help out"
  },
  {
    "startTime": "01:40:00",
    "text": "I think I think I just want to hear I think Martin said this before but I think it's worth clarify for the group, why can't we use three-party on a specific? I just want to hear, I think Martin said this before, but I think it's worth clarifying for the group, why can't we use three-party honest majority in DAP today? The naive thinking being that you have a collector, you have a leader, you have a helper, so you have a network of three parties. So why couldn't they do this computation? Well, I think it really just relates on what's your trust model for the leader. Now the leader needs to sort of fit within the, you know, trusted not to collude with any of the workers. If you're willing to make that additional assumption, that at least from the trust model setting, quick, terminology, it's the collector-leader-leader helper that it was talking about. The leader is not, we have a different concept of that in our system and it's really confusing it's the collector that's the one that is not part of the threat model there and yes you're exactly right right Yeah, so it is a different threat model for working in there. Tim? Yeah, I was going to respond to the notion of why can't we use the collector as a third party Well, I think it's because DAP already allows DAP generally tries to maintain privacy in the face of the defense of one of the aggregators, right, or only one aggregator needs to be on it for the privacy guarantees to be upheld. But we already allow for one of the aggregators to collude with the collector based on the observation that like frequently in deployments the collector and one of the aggregators will be operated by the same group So, yes i don't think that given that threat model we could achieve a like two out of three honest we all because we already assumed that the, excuse me, we are could achieve like two out of three honest. We all, because we already assumed that the, excuse me, we already assume that the collector in one of the aggregators are polluting Yeah, so martin thomson, Chris Patton and I have talked about an interesting idea that I thought I'd float here"
  },
  {
    "startTime": "01:42:00",
    "text": "and get people thinking on, and that is in DAP, we also have the entire population of climate that are submitting reports as a potential source of the third party, as it were. So the idea that we sort of jointly came up with, I think was to have the bits submit by the clients. So you have every client submit some number of bits that could be your random samples, and then the job for the aggregators would be to add them up that's free if those samples are in the prime field that you're operating in already. So the aggregator don't really have to do any additional work aside from selecting which one of those samples would be there We have the whole idea about sort of hedging which is a little more complicated but that's a that's kind of an another angle that we have on this one potentially Yeah, that's a, that's a good point too, that there's the thing that you'll have to trade off there is that you know, it's not going to reduce the size of N you need itself. So if you don't have enough clients, you're gonna have to do more work to hedge, I guess, as you say, you know But yeah. Yeah, the suggestion there was that there the clients don't necessarily get to choose which ones are their bits enter into the system But every client could provide some number of bits. So if you needed to provide 10 million and you only had 100,000 clients, well, you might be asking those clients to submit 100 bits each, which I think at the point we're doing 10 million, I think something's gone wrong A hundred million seems a little much but let's see if we can work on getting those numbers down to something more reasonable. Three minutes is fine three hours is not All right, if no other questions on the DP draft or feedback, we'll just touch on"
  },
  {
    "startTime": "01:44:00",
    "text": "briefly these two other drafts over share shared on the mailing list. One is a three-party MPC and the other is a pseudorandum secret sharing. So these are work that it's mainly Ben Savage, Martin, also Daniel Mazni at to have contributed to. So most multiplications for Boolean circuits and the three-part MPC is something we've done a lot of work on recently particularly in how to upgrade it to active security or malicious security. So just like a high level over of sort of what this gives you here is you have a semi honest multiplication is going to require one bit of communication for helper and then sort of one computation of an AES encryption to generate shared randomness you get your malicious security upgrade in the three-part setting honest majority setting a lot more efficiently than you do in most other MPC settings So it tends to be like a, yeah, it says a little a lot more efficiently than you do in most other MPC settings. Okay, so it tends to be like a sublinear sort of overhead in total for here We have done some VAT work on validate in batches, so 50 million multiplications around 6 megabits of network bandwidth 100 rams of 100 megabits of RAMs to store the intermediary. So your memory costs sort of changes with your batch size or your validation but otherwise the amount of a bandwidth you have to spend on this is pretty small and then the other thing the other draft which Martin should really, you know, tell you about more about is the sometime is this a random secret sharing, which is a method for sampling sort of shared secret randomness. It's sort of a building block of these MPC protocols every time they need to do an mpc they need to sample some cedar random sort of shared state to use for masking their shares that they're sending around to other helpers in this three-party replicate setting. Okay, so we, we aren't going to talk in sort of depth and technical depth about these latter two drafts, but maybe we would like to"
  },
  {
    "startTime": "01:46:01",
    "text": "sort of open discussion as Chris alluded to earlier about you know, what do you all think about sort of this? this direction of sort of three-party MPC in the PPM setting I think there have been some presentations on a couple of past IETFs to sort of a tease this work as future directions but now we have some documents that we've gone and written down some of the building blocks So, yeah, I mean, I would say I'm most enthusiastic about the binomial stuff especially getting it integrated into DAP That is figuring out a two-party competition way to do it That said, this other stuff is, I don't know my impression from like looking at like, you know, other, other standards, bodies, and also just talk to like MPC people I've gotten to know over the last few years is this is like, this is the, probably the few future of this is the thing that's going to be practical and generalizable to new problems so I think we should work on this I think it would be helpful It probably would help us if we can clarify what our use cases are. I think we have like with DAP in particular, we've tried to be very specific about the class of problems we're solving So I think, yeah, maybe maybe you want to speak to some of the immediate use cases you have for let's say the three three-party, honest majority, MPC stuff. Yeah thanks, Chris. Yeah, so on the first part, I think that that's great, that there's interest, I think we expected initially like, yeah, like the DP one has the most relevance in the short term. In the longer term, for use cases, for these other ones, like we are mainly motivated by the work that's happening in the PATCG"
  },
  {
    "startTime": "01:48:01",
    "text": "as some of you may be familiar with the private measurement standards work there for advertising And we can, yeah, we can definitely do a deeper dive on the use case and sort of how those protocols are shaping up in the PATCG. I think we've made some pretty good progress on some of the shapes of some of the APIs But two main things that come to mind in terms of the use cases of aggregating history for advertising measurement And the other one is sort of aggregating sort of feature of vectors for ML on top of sort of adds measurement data so those two sort of use cases both fit very nicely and like we need aggregation and we also need to add some DP noise to them There's also a little bit more that could sort of be needed to fully support that use case. There's some other sort of like PSI work that's also needed to join things up I think what we try to do is sort of break down that problem into sort of, you know, individual components here. So right now it's just sort of introducing like the general MPC that we would need to support some of the computations and maybe in the future we can we can bring another draft that's more oriented toward the PSI aspect of that use case. So what is the delta between those two use cases you mentioned? So what what DAP not provide that you need? for a histogram aggregation in your use case? and model training? That's a great question. So for model training, I think there's a concern of shipping features to the device The design in sort of the latest ones in the past CG, they allow the breakdown or feature vectors to be sort of, we call it late binding, so they can be added late on the server side to a report that the client sends. So you don't need to send sensitive features onto the client that maybe maybe leaked that may be business sensitive data And so you don't have to, and it's also, as Martin is,"
  },
  {
    "startTime": "01:50:00",
    "text": "just trying, we also are promoted by minimizing client server bandwidth So one of the things that we have seen in the use cases that we want to support one is that, yeah, when you're doing a feature vector, you know, that's sort of like a dense, it's a dense object The dimension does not need to be extremely high, you know, 64, 100, sort of dimension with some percent precision on a float in there but shipping all that through the client is going to increase bandwidth On the histogram side, a lot of times you don't just want to sort of aggregate a single histogram. You want to aggregate a single histograms lot of times you don't just want to sort of aggregate a single histogram. You want to aggregate sort of many different histograms and the histograms are generally like small in size, not too many bins But even so, like that's going to increase the clients server bound with a lot if your shipping meant you know a hundred different histogram bins back and forth to the client so I think we're conscious of that. It's also there's I think there's several sort of scenarios where this, this late binding of the breakdowns is helpful It allows for a little bit more sort of dynamic where in the system where you can query the system on one set of breakdowns you can sort of see the result and you may be able to change your sort of breakdown scheme and sort of query it again subject to the, you know, the correct sort of way of doing the DP budget for your reports. But those are a couple of things that come to mind that we have sort of thought to add additional kind of functional kind of functionalities currently in the DAP itself Chris Ryshek, just some questions on the second one, because you're doing three-party honest majority and I skimed through the draft. I haven't read it in detail, but I assume you're considering the militia security model as well, but I was wondering how you're thinking about the about such setting, because usually most MPC models assume that if the if the militia party just drop out then"
  },
  {
    "startTime": "01:52:00",
    "text": "you basically have the computation stops you don't necessarily leak the output to any of the parties, but computation stops So depending on who the three parties are in your specific models, that could be disruptive, especially if, for instance, run times of three minutes or 30 minutes even if they're not malicious, something happens, they drop out So are you considering that setting? Like how? does that fit in here? Yeah, great question, Chris. So we are considering the sort of malicious security with abort so that if party does do something incorrect or malicious, the query will just hold and that's mainly for performance reasons as I understand like you get better performance in that MPC setting than if you're trying to guarantee output even in the face of malicious party. And I think we've just made the trade-off that sort of it's worth trying to reduce the computation cost in total. We hope that parties that participate in this are incentivized to participate And so I think there may be other means that are like not related to like the core cryptographic guarantee here to sort of guarantee like, you know, SLAs and availability of these helper parties and that can be handled in more you know contractual or other agreements to sort of make sure that people are actually participating and running the query. So I think we've at least tried to separate that and sort of take the win on the NPC performance in the setting Tim? Thanks. I just want to express my support for this work in PPM. I think this is very promising in particular like Chris Patton said, using the differential privacy stuff in a setting like DAP app would be good. But it's not hard to imagine a bunch of different things that IETF even beyond the PPM working group that would like to have you know, a well-standardized I don't know if it's fair to call it a primitive, but a well a well standardized and specified like mechanism for jointly doing different"
  },
  {
    "startTime": "01:54:01",
    "text": "privacy. I'm not sure it's ideal for PPM to be the place where like differential privacy stuff happens at IETF, but, uh, I think we've been around on this a bunch of times and there doesn't seem to be a better place So, yeah, I think we should do it Thanks, Tim Yeah, martin thomson, I want to put a little bit of colour on the on the client side side constraints. Some of the things that we're looking at here are sort of around in the order of a hundred byte of submission from the client on a per health basis. So a couple hundred bytes is the sort of order of magnitude that we're talking about for doing the sizes of histograms where for something like DAP that would probably be in the order of kilobytes rather than hundreds of bytes and that's a pretty significant change in terms of the client side and obviously it scales a whole lot better if you have larger histograms, you pay one bit per buck or very small numbers. Whereas in depth the proofs get bigger and the If you have larger histograms, you pay one bit per bucket or very small numbers. Whereas in DAPs, the proofs get bigger and each bucket requires that you provide a value that pretty significant in size Yeah, thanks for mentioning that, Martin. Yeah, that's a one of the performance sort of benefits of this three-part honest majority setting of different tools available for robustness or then having the client generate large proofs for you Okay thanks to everybody for their comments on this topic. We wanted to reserve a few minutes at the end for anybody who had thoughts to share about observations from the same session I think Chris Pett might have volunteered to share thoughts about that"
  },
  {
    "startTime": "01:56:00",
    "text": "Yeah, that was my first time at a SAC meeting. I don't know how those usually go but it was like everyone at IETF like, you know, disagreeing not understanding the shape of the problem, not knowing what to do next, but there does seem to be this question of IETF working groups are working on fancy crypto by fancy crypto, we mean cryptography that doesn't fit into the boxes of primitives that we have, say, for building TLA or, you know, anything like that. So not like AEAD, not chem not blah, blah, blah. We have blind signatures, partially blind signatures, VOPRFs VDFs, all of these abstractions that are used in like to provide fancier security properties. And CFRG has been the place for those before, but like, you know, there's a question of like whether that's the right place for us to take like new MPC things. The V-Def draft is our already there, see if RG is already committed to reviewing and and finalizing that draft uh so the question for us is like what do we do be beyond that? I'll note that that's been several years in the making and still in process at CFRG So I'm deb cooley, SEC AD So the goal is not, was not to address fancy crypto? Yeah The goal, because I think that stuff generally happily has done CFRG and other places the goal was to address crypto primitives and apparently we possibly didn't state crypto primitives quite succinctly enough I mean, this is a work"
  },
  {
    "startTime": "01:58:00",
    "text": "in progress, right? It was not meant to be a fully fledged I mean it was a proposal but it was not meant to be a done deal, right? So it was meant to get comments from the community, which we certainly got But it was meant to address crypto primitives. It was meant to address things like, national crypto requests and I guess what we tend to call vanity crypto, where we talk about not, it's sort of crypto primitives, not not not fancy crypto, right? so not the like blea call vanity crypto where we talk about not it's sort of crypto primitives not not not not fancy crypto right so not not the like bleeding edge new stuff it's the bog standard old crypto primitive stuff that comes up. It's the you know, the U.S wants their crypto and standards I mean, there's various countries that want this and there's also there is vanity crypto um And it's only going to get worse with the post-coron amount of algorithms coming up so it was meant to address that it was not meant to address fancy crypto. So thank you very much, Martin, for bringing that up because I had not even, like, not even cross my mind. So that was like, he stood up and I'm like, oh, whoa, I didn't even consider that It was definitely meant for the we get asked to AD sponsor all sorts of things If you look at the current request, to AD sponsor, the East Lake draft that's an interesting document. I wouldn't call it cutting edge at all. So it's stuff like that So it's what do we do with this? Do we publish it? in the, does the IEC publish it? Do we publish as a sponsor do we let people get their code points based on drafts? Do we? And it's more targeted at code points, which the fancy crypto is not, right? You're not looking at code points"
  },
  {
    "startTime": "02:00:01",
    "text": "for VDAS, right? It's definitely code point oriented so there will be a summary we will work on the notes, so you can go back to the notes and look to see we'll clean them up. We had somebody right notes. I haven't even looked at it. I haven't even actually gotten a chance to look at the entire chat of that because as you're listening to something and answering questions it's hard to also look at the chat So hopefully that helps martin thomson I think there's a bigger conversation that we potentially need to have about the relationship of the IETF and see if when it comes to, I don't know, fancy crypto I don't know one of these terms mean the sort of things that we're talking about doing here And it's become sort of clear to me that if we asked the CFRG to do everything that the IETF wants to do in terms of new cryptographically related work, they're not going to be capable of doing that We do too much So that's a topic that I'm more interested in than the vanity stuff. I do appreciate what you're trying to do and I think ultimately the direction that that was head was great But I'm more interested in the bigger problem Yeah. Yeah. As speaking as chair of PPM and also chair of privacy pass I can say that there is a there's definitely a consistent process here with where the scope of CFR is not really in line with some of the emerging needs that are appearing in those working groups. Yeah, I think we're going to have to have a series of difficult conversations about each one of those different things, I think in Privacy Pass, for instance, you're talking about BBS"
  },
  {
    "startTime": "02:02:00",
    "text": "or variations thereof It seems to me that BBS, or at least the core part of BBS is something that's very much in the CFRG wheelhouse. But some of the sort of work around the fringes of that is IETF work And working out the split there is very difficult this is this MPC stuff is another one of those examples I deliberately brought it to the group on the understanding that this is not particularly fancy crypto This is like high school math stuff I mean, I recommend the words, which is good It's pretty, yeah yeah, as Chris said, it's all just polynomials anyway, so, right but but, so we are over time I think these are turning into some great hallway conversations uh thanks thanks to our everybody for coming to our PPM session and enjoy the rest of your IETF Friday Friday Friday"
  }
]
