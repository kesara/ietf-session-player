[
  {
    "startTime": "00:00:10",
    "text": "hey folks as you're joining see if you can start video and audio me okay sorry could you say something give me an audio check on the bridge nope nothing jay i guess you could be the other person if you're hearing me could you try to say something back sure can you hear anything now no maybe hang on yep there i've got sound now okay good um and just let you know i added something to the hot topics um let me take a minute just about the um project manager for the tool for the rpc tools and my ability to find someone to do that"
  },
  {
    "startTime": "00:02:00",
    "text": "[Music] okay liz i'm wondering if the link in the calendar didn't get updated people are pinging me for the link and you're right robert the link in the calendar does not work for me only the one you know i did update it but um not too long before the meeting so maybe it didn't refresh soon enough for people sorry about that yeah calendar subscription refreshes are done by the client and they're generally configured at one week by default so um they're not brilliant that way yeah and the the medical links are different every time for this so um there's it changes every time maybe we should change the calendar to um say just to say just go look at the notes page and the notes page changes every time so we couldn't put a link there but we'll work it out so i'll give people an extra couple of minutes to work through"
  },
  {
    "startTime": "00:04:00",
    "text": "that bit of impedance before we start so we've got a lot of ground to cover we'll go ahead and start we'll probably rearrange things i know i'm expecting glenn and i want him to be here before we jump into the new production server topic so we'll skip we'll ride around that for a moment and come back to it um i received email from alice out of band that there are no blocking issues um she provided input to the cmt to consider it their next meeting for changes that we might prioritize over others so i think just capturing no blocking issues and the notes for that topic today is sufficient we'll skip the deployment of the production server until glenn has a chance to join"
  },
  {
    "startTime": "00:06:02",
    "text": "for moving the rest of the services off of tools.itf.org and redirecting it we did set up a temporary bap service so that the rpc has something that they can use until the um functionality about can be integrated into author tools um the long pole in the tent for finishing the tools migration remains the web xml service which we'll talk about next for that you'll see that there are some nearly 50 remaining issues almost all of them are about data quality um not about actual application functionality we think that it's in a deployable state at this point and we're going to try to bring up an instance either on our own fabric either later this week or next and begin the acceptance process taking the data repositories and moving them into a organization that we control if people haven't been watching the way that this thing has been being developed there is one aspect of it i wanted to put in everyone's head the web service relies on data in github repositories for um what it serves and there are github actions in those repositories there's a repository for each of what traditionally had been bivximo 1 bibxoml2 etc aka big bib xml ids bibx and lrcs bigabit xml ieee there are actions in those repositories that go feed from"
  },
  {
    "startTime": "00:08:00",
    "text": "the canonical sources and make both relative formatted entities and bibx web xml formatted entities that just live in those repositories um and the web service feeds from those caching as it needs to so when it comes up from scratch it just goes and gets things from those repositories so the um important connotation is that our service will fundamentally at this point rely on github so as part of if part of its running now if we ever need to move away from that it would just be a matter of putting those things in a get repository somewhere and changing what's currently in those github actions to be cron running somewhere so there's a it's it's not a a difficult thing that we're being locked into but i wanted everyone to be aware that it existed do we have glenn yet i sent him a text so hopefully we'll see him shortly you have a question from um carson in the chat um i'm looking i just was curious whether i can just clone the repository and get a backup instance yes it's repositories but yes you certainly could no i lost you uh not from my end i don't think the answer is yes i'll type it also as well but the the actions are in the repository are they um the ammo for the actions is yes so you can see what um oh yeah of course"
  },
  {
    "startTime": "00:10:00",
    "text": "right got it thanks yeah i'm sorry i just lost all the media why did you answer my question so i haven't uh heard you yeah i i sent the answer into the text yep thank you sounds great again that's great which is going to be good for the next topic um yeah i don't see glenn still there's a question about whether or not we're doing the right thing with our api design at author tools and at the bib xml service we designed these things from the beginning to require a data tracker personal api key the notion behind this was that um we would be following a pattern similar to what cloudflare's api shield follows and other um entities that are are protecting apis that you have um a an allow list you know basically a a somebody comes to you with a thing that says yes they get to use this and the api key was what we were we were planning to use but this has tension with um the apis that have existed in the past at places like um xml rfc.tools.itf.org that we're just free to use by anybody anonymously um carson in particular is running into this with kdrsc which if it doesn't have a local xml rsc will [Music] use currently the um xmlrc.tools.itf.org to run xmlrc on the the the tools behalf and if we stick with this api construct"
  },
  {
    "startTime": "00:12:02",
    "text": "um to get what we would want out of it the users of carson's tool would have to go get an api key from the data tracker and tell the tool about it before it could go do this thing and carson has said several times that the that introduces too much impedance too much um trouble for the user and he doesn't want to take it down that path one of the things that alternatives that we discussed was creating a key just for the application but by the nature of the application that key would be accessible to anybody that bothered to look at which point we shouldn't we don't have any advantage for having keys at all because random robot could go grab that key and start doing stuff so the question is do we stay on the path that we're on right now and try to find some other way for tools like kdrsc to get the kind of access that they that they need um or do we um back off of the position that we want this positive identification kind of mechanic and just open the apis for use and then make changes later if we ever actually see abuse jay go ahead please thanks um isn't the impedance not in having a key and getting a key but in the process by which one gets a key you know so if getting a key is trivial then there is very limited impedance but if getting a key is complex um and you know requires multiple email round trips all that kind of stuff then it is um then that's where the impedance is yeah so it's creating a data tracker account and then asking for a key from the data tracker account so you know can that be automated so that can we provide"
  },
  {
    "startTime": "00:14:01",
    "text": "an api for that or something so that um carson could you know call that api i don't know how you feel about this class and i'm giving you work but you know so could we simplify that in some way rather than um you know rather than sort of abandoning this yeah that certainly can be done now before we go down that red hole um the question really is do we need this api key right now the web interface for author tools um actually uses a second api that doesn't need an api key and of course i can simply use that second api uh for for the uh cardi rc uh access and then i don't need an api key it's not apparently not a published api but it's not hard to reverse engineering and i did it um so uh do we really need that um the the reason why why installing an api key is a little bit pesky is that um kdifc is used for from a lot of make files different repositories and so on and it's not entirely clear how where someone would put the api key to make all these various instances and installations actually work together that's easy to do in unix but i i hesitate uh making sure that works in windows as well cazard we actually have a second api i thought that the what the ui was using was exactly the same api but with its own api key as a secret um yes we do have a second api it's a reverse proxy to the actual one"
  },
  {
    "startTime": "00:16:03",
    "text": "[Music] it has some restrictions restrictions but someone can like just fiddle with things and use the command line to submit one if it's not from a browser it doesn't check the domain whether it's coming from all the tools or not yeah so it sounds to me like effectively we've got just an open api anyhow with just a little bit of work so let me guess i just asked is there even though we had these hopes that we would learn a little bit about usage and have a better knob for controlling abuse if abuse ever appeared by requiring these api keys um is there a fundamental objection to just falling back to where we were and saying you know this is an open resource that anybody can use we're just providing free compute for the these translation services in the in the case of author tools and um wait and deal with it if it turns out that it is you know something that gets abused but there is i mean i tried to get the secret key as well for doing some api access honestly i spent 10 minutes on it and then uh i gave up simply so they have friction now if we are more concerned about abuse we can still do the rate limiting on anonymous queries right and then we fix the problem and we only of course cause a problem for everyone using the anonymous if one is bad we put all the people in the same basket but it's safe that the resources right all right why don't we go down that path for now so we will accept a key if it's"
  },
  {
    "startTime": "00:18:00",
    "text": "available and track it and and if we get to the point where we need to change levels of service based on the key we at least have that available to us all right one possibility is to embed you know mint a special anonymous key for xml rfc probably more work than its work and then you can just crack that one instance but there's some other way for first time to indicate this is you know the script but whatever wait until it becomes a problem well it does require a little amount of reverse engineering to find out how this api to work so whether people are going through the trouble to mount a distributed denial of service attack on the itf servers i think our attack services surface is pretty small there yeah it's compute it's really all it is okay um if anybody has any other thoughts about this later after they've had a chance to think about it please weigh in on the tools discuss list otherwise we'll we'll just we'll take this alternate approach um zulup is up and running i don't plan to bring it up again unless we run into issues or we get to the point where we are um actually looking at um the details of integration of medico and zulu jay do you want to talk about the rpc tools"
  },
  {
    "startTime": "00:20:04",
    "text": "thank you so um just a reminder from the notes here um i need a contractor to work with the rpc for what will probably be about a two-year project or from now until the end of nature i imagine um their role starts off as a ba role largely doing requirements analysis um for a full replacement of their tool chain um and they're the the then working with the rpc to help them decide what nature of replacement they want because currently they do a lot of things on command line tools and do they want um to replace that with a lot of command line tools or do they want to have some kind of workflow system or an off-the-shelf package or something like that um and then um supporting the production and the running of a tender on that and for someone to develop those that tool chain and then project managing the the the client side project management of the implementation of that and the delivery of that and i have spectacularly failed to get anybody to do this both through the rfp and through shoulder tapping people and through going to church and through rolling bones and all that kind of stuff so i am now i'm going to do an rfp again unless anybody here can come up with a really nice way where i can get somebody or something or has any suggestions or any help for me well assuming my current role winds down as when expected i could probably do it perhaps you should chat offline and see if that makes sense yep right okay thank you does anyone else have any other suggestions or anything in the shoulder tapping um capacity there are a few other people i could tap"
  },
  {
    "startTime": "00:22:01",
    "text": "to at least get them to look okay great fantastic thank you yeah okay until he signs up to do this but you could also ping other people who've responded on other rfps in the shoulder tapping area which i'm sure you've done yeah that's a point i hadn't particularly thought about that because it's been a while it's the one that came to mind who was sorry ribose revolves right okay yeah okay great thank you rich thank you back to you robert all right um working down things we have a design team that's going to start in on describing the it infrastructure services um an initial call um to get um some experiences from west hartekers um automation of the b root as worked examples of of things that we can of mechanisms that we could use and start talking about what our infrastructure should look like and how it should behave when we are on a different target that is more automatable the rest of the workshops jay i think you were going to have somebody put together a poll for priority i don't think that poll has gone out yet no apologies i haven't got around to that sorry covered in holidays i'll get to do that soon yep as you've seen the completed projects the svn track migration is complete we still have some track out there for people that have just been using the"
  },
  {
    "startTime": "00:24:00",
    "text": "wikis that we plan to have a crowd-sourced migration into wikijs that's waiting for us to finish the data tracker wikijs integration which is behind many of the other things we're going to be talking about later in the call i'm bringing it up here so that we you know know that it's still on our plan but we haven't gotten cycles to it yet so the data tracker has been running through cloudflare for several weeks reasonably successfully a few notable exceptions the model which is what really web models service models ought to follow um are more strictly enforced by places like cloudflare the notion that you would have a transaction complete quickly and not pinned for a long period of time is broken by our draft submission views and several nudges that we get from the rsc editor and from iana when state changes where we go out and do a whole lot of processing before we return and we are looking at changing these workflows to be asynchronous this is going to impact the people that have automated tooling for submission to the um draft submission api martin thompson's repositories in particular and i have yet to reach him but i need to coordinate with him on on the when in the how of the change that we make to that api so that these um submissions can succeed when the processing that is required to complete them is longer than the timeout that um cloudflare enforces on us"
  },
  {
    "startTime": "00:26:00",
    "text": "i have in the meantime a workaround um we tested this with john yesterday if you just hear of anybody that is having trouble um submitting a draft because it's large and this time out is hitting them um the answer will be to send them to the manual submission process and i will teach the um secretariat a way to go around the proxies to to do these submissions we still have um work to do to make it so that our num com eligibility calculations are correct this is becoming pressing as we're going to want to see the nom come before we get to july i expect we'll be collecting volunteers soon but the work to get these changes in place is likely to come in um over the rest of this month in the early part of may distracted of course by the server migration that we're going to be talking about a little bit later we are at and of course we need to insist on being ready right for namco so if we miss being ready for this next num com it just means that somebody probably the secretariat will have to look a little bit more carefully about whether or not the people that are um that the tool thinks are eligible really are and that we didn't exclude someone that should have not been excluded so we have a fullback plan basically on your procedures yes okay it's a bad one we should avoid it but of course um we're working very hard right"
  },
  {
    "startTime": "00:28:00",
    "text": "now to um get to the point where we can make a new data tracker release and our plan at the moment is for that release to be on the bootstrap to include the bootstrap 5 work so um if we follow through with what i'm hoping we can do later this week or early next we will actually shift the production data tracker to the bootstrap 5 styling and then everybody will be shocked and will get rage and violence because of the um surprise even though we've tried to warn people about this for a while um unless anybody has concerns um we're going to continue down this push i don't know if we should signal this in any way at the moment i could send another note to chairs i could send a note to ietf at that this is going to be coming um but thoughts greg you think there's anything else we should do differently another hit i think another heads up wouldn't hurt and um uh the other question i had was uh are you expecting that the colors will match the current data tracker or will they be like sandbox or the the colors will be the colors you see on the sandbox except for the top menu bar which will become a bootstrap blue okay okay yep anyway i i think another note to the folks that you mentioned would be good and i'm happy to help if i can do that but also happy to stand aside sure i'll coordinate with you um later in the day"
  },
  {
    "startTime": "00:30:04",
    "text": "because sorry um to go through your sections here the domain those release on articles this week and the big change was i had to temporarily remove go sk2 because that tool had a change after a year or so and that change break lots of things uh and i don't think that the people who's developing that tool is going to move forward with the tool they are suggesting few folks and some of those folks don't use that tool as a command rather than a module so and i i have temporarily removed that from other tools until i can find a re better way to install it uh we still have a cg or north tools for sk diagrams to convert them to svg um other thing about other tools is we are planning to have a release every fortnight so every two weeks so we are trying to make a orthodox release any questions about autotunes you got cabo's um note in the chat it's next time when we're going to pull something we should um give people a heads up yeah just a simple note because i i can of course work around things like that um as svg and gold are functional equivalent and i probably made the"
  },
  {
    "startTime": "00:32:01",
    "text": "mistake of distinguishing it in the input language so now what i should do is map goat in the input language to actually calling aasvg if i had known this earlier i could have done this so so nobody notices the whole thing i think the damage is very limited so this is just a heads up for for future changes like this just send a one liner and then we can add it to that the the other thing i would uh like to point out is that the the cram down rc tool changes much more often than every fortnight and not people not being able to move to the new version of the tool becomes an actual problem when when there are fixes that have been initiated by users of the tool and then they have co-authors that that can't use the new version until we have this two-week period elapsed so i would hope that we get a more more regular um update of that specific tool if necessary um so that people don't have to wait for two weeks for for fixes to become available i think we can hear like eventually looking look at ways to automate that because both both are github now um we don't have automatic way of deploying autotools yet but when you have that under the hood on itf servers we can look into automated so because it's not going to be a big change to update cram down on autotune side so"
  },
  {
    "startTime": "00:34:01",
    "text": "but i i'll keep right now i'm i am doing manual uh releases so i will keep an eye on cram down for changes and yeah try to match that face at least with the version updates great thank you and maybe i should also mention that i do have a plan for how to make the tool coverage much wider uh than we have at the moment but maybe we should wait for the current wave of uh activity to be over before we start that activity okay sorry go ahead and move forward to xml rsc so on xml rfc there are no real releases in previous period but there will be a one coming soon i guess this week or the next week with few fixes uh on the xml rxc side the [Music] the development right now i'm i'm looking at any bug fixes that really needs to happen and i'm putting my time on the tools replacement so that that's it from me any questions on xml currency all right um"
  },
  {
    "startTime": "00:36:01",
    "text": "i don't see ryan on the call um if anybody has questions about the things that he's commented on in the notes send it to ours to to to tools discussed please um at the web analytics line i'll note that we have um adding matomo to the data tracker as a a work item that will likely happen after we get through this the big set of disruptions that we're working through right now um we have an initial set of recommendations health contractors they are fairly simple on the planning to implement them after transitioning to the new server nothing particularly error shattering in there it's basically the distilled set of recommendations from the normal scripts that you would use to tune mysql um zdx security is standing by waiting for us to complete the transition to the new server before they start in on testing the remaining set of the web services and re-testing the data tracker they're currently on we're currently on their schedule for mid-may to mid-june and they're willing to adjust should our transition to the new server need to take longer than it's currently taking um i'm going to skip forward to the rc model rc editor model transition based"
  },
  {
    "startTime": "00:38:01",
    "text": "on conversations i've had with various stakeholders um the need to have support for the new model and the data tracker is expected to come in sometime around mid-may so we are waiting to make those changes until we are post new server deployment and on the bootstrap 5 branch as well if that's not true if somebody sees a need earlier than that please let me know um that leaves us outside of taking the other things that we haven't talked about besides the server transaction transition is read to the server transition itself and that's going to take all of our available remaining time if anybody else has any other business that they really want to get to on this call let's go ahead and cover it now not hearing anything glenn has a replacement for itfa up and running um he's got a process that is keeping it um in sync we are iterating with him right now on improving that process so that we have as um we have a correct replication and we have as little downtime as possible but the thumbnail estimates um at the moment based on watching what the sync the synchronization is taking is that we'll end up with a two to four hour downtime and the downtime will be across many many services most notably male and that means this mail won't be delivered for that four hour up to four"
  },
  {
    "startTime": "00:40:00",
    "text": "hour period it won't be lost it will be queued mostly on the senders systems because we will just stop accepting for a little while and we'll catch the re-transmissions we need to verify that anything that happens internally we're trying to work i'm working with glenn right now on a plan that will let us have um close to zero downtime of the data tracker itself even though mail coming out of the data tracker would go into a queue and and be delayed um wes go ahead and anybody there there are a few there are so few people here just um turn on your audio yeah i didn't want to get in the way uh so the real question was right now you guys have only one mx record for ietf.org and i've always wondered why but you know you might consider uh just as a fallback adding an mx for a short period of time through an outsourced you know agency there's certainly plenty that could do it i'm sure there's probably uh you know companies within the ietf that would be willing to do it temporarily just to cue it all guaranteedly somewhere else to not depend on all of the sending parties doing the queuing i'll carry that suggestion to glenn yeah not to mention that some sender already sent a warning email after one hour i was unable to deliver this email i will retry later which is four hours by default on postfix and send mail if i'm not mistaken that's correct so my opening volley for when we do this assuming that our testing between now and then doesn't show a reason for us not to is during"
  },
  {
    "startTime": "00:42:00",
    "text": "the work week on monday morning pacific time april 25th so that we can have the right hands available to assist with any baubles if there are bubbles along the way lars had a last minute conflict with this call i confirmed with him that as with his chair hat on he doesn't object to us um taking a chunk out of out of a normal work day like this um any thoughts from anybody else on the call are you comfortable with this i'm hoping that as we go through the next steps after this one that this will be our last multi-service outage we have one more data tracker outage that is going to be big but this would be the last time that we have you know the issue with everything going away because we're we're messing with the infrastructure under it can we rely on cloudflare to present something the server is an unexpected maintenance or something because they check right if the origin is not there there's something there so they the cloud player can be configured for um things that don't change things that aren't forms right so website the website itself we're not expecting a we're not expecting it to to actually go down as we're moving um the uh but even if it had to we could configure cloudflare to serve the pages that it has this is not as easy to do with the data tracker"
  },
  {
    "startTime": "00:44:00",
    "text": "most of the data tracker users are logged in and we have cloud for necessarily configured to not cache most of the data tracker pages for anybody that is logged in right so they wouldn't have anything to serve to those people if they logged out we could probably configure it to to have something available for people that are just attempting to browse um but it's a it's it's a hit or miss kind of experience and all in all it's only for two or three hours right so as long as it's announced it's okay so if people are on board with this i did and i will get together with uh um greg oh there you are sorry i lost your image um this is the kind of message that maybe we should send out super visibly with the a you know things going to be disrupted and we should probably send it out today because that is less than two weeks from now today or tomorrow if today is too crazy but all right i'm not hearing major pushback i gotta admit this is not my favorite way to to do things i'm i'm going to be losing a lot of sleep over the next couple of weeks over this so i hope very much that we don't ever have to do it quite this way again so um i'd really hope to have glenn here to have some conversations some detailed conversations about um the actual mechanics but we don't so [Music] and by the way if we cannot get a backup"
  },
  {
    "startTime": "00:46:01",
    "text": "server does it mean that we don't have one i don't understand your question so we are doing migration right so typically when you do migration you pull down we have two servers one active one standby or whatever you put the standby you migrate this and then you switch the role and that's if you have good synchronization that's very easy it should be really easy so do we have um yes we we essentially had that i just realized that i had intended to have a link for the plan that glenn had put together available for you to to see i will send that on to um tools development shortly um so that you can skim through what what we're planning to do but yeah we have ietfa and itfn they're separate machines and the basic plan is as you described there will be a point where some services on atfa are stopped there is a final replication step from itfa to iatfn then certain services like making the mysql database master on ietfn will happen and services will come up on ietf in um if the services were architected a little bit differently um we could do this where the cut over time was instantaneous but some of them are not and the dependencies as we understand them on the male processing chain in particular are such that we need to have the male um q quiescent while there are some rather large um re-synchronization efforts that go by i'll be talking with ryan and glenn um"
  },
  {
    "startTime": "00:48:00",
    "text": "between now and then and if we discover that those are not in fact something that needs to happen um before the mail service is resumed then we can cut that down time um to something much smaller so right now just the file system rsync is that that final file system rsync is taking two hours just for the sheer number of files that it's checking to see if they have changed or not right so um and that's something that i'm working with glenn on attempting to tune so that um that we can we can shrink that window as well thank you for the information um unless anybody has anything else i think that we've done what we need to do on the call today and thank everyone again for their participation in in these calls um and we will see you online as we go through this transition and um i'm sure we'll have a lot to talk about when we get to um the next version of this meeting thank you you"
  }
]
