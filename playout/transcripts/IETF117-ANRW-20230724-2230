[
  {
    "startTime": "00:00:30",
    "text": "Yeah. on very"
  },
  {
    "startTime": "00:02:17",
    "text": "believe. Hello, everyone. please be seated. We're starting the last session of today in 2 minutes. let it work. That's true. but"
  },
  {
    "startTime": "00:05:33",
    "text": "Okay. Welcome to another session. It's now time for Alfred Arora, Arona. to speak. And The title of the talk is lowering the virus to working with public career level data. Yep."
  },
  {
    "startTime": "00:06:04",
    "text": "Good morning. So my name is Alfreda. know. So today, I will present our work on lowering the barrier of work it with public reliable data. This is done with my -- Uh-huh. Alfred, if you could maybe, like, speak closer to your microphone or, like, speak a little bit louder, that would be super helpful. So -- Thank you. Okay. So, yeah, I was saying that I will be presenting our work on lowering the barrier for working with public variable data. And this has been done with supervisor you are now from similar metropolitan and Matthias from New Jersey of 20. So the goal, actually, of this work is basically to of this paper is basically to introduce our consolidated data to the community so that you can avoid going through all the challenges while working with level data. So Next slide. don't know how to control Next slide, please. Yep. So, yeah, you know, of today, I will start with a little background about AI system. then I will introduce the data the original data that we have collected from the area and talk a little bit about some inconsistencies that we have seen on the data. and I will finish my presentation with our proposal, our consolidate data. So Internet resources, like, such as as a number or prefix is managed by several organization. such as the regional the network history,"
  },
  {
    "startTime": "00:08:02",
    "text": "on behalf of Ayana and Ikan. So we have 5 antenna registries, and they have Clarish. they have, sorry, original coverage, but they share basically the same core function But for the purpose of this one, we'll focus on we decided to focus basically on 2 main function. The first one is related to maintaining a territory service including Hoists, and and each And I have Actually, extract part of the directory into publicly available hobbies and also delegation file also called statistics file. each area also provide reverse DNS for the delegation to the customer. And those files also provided they also provide, sorry, the reverse DNS for this one. So we decided to use those data for a project. So we'll assume that this should be easy. So let's look of the data So here we have Two example, jelly of reverse tennis, on the rear level. So on the real level, we are expecting delegation to customers. So we are mostly expecting an s record. you can see on the example of the first example on the top, that we can also we also have ipv4 and ipvcs of say, record that is not expected to see on the AI level, The example on the bottom is classlessdelegation. which is the the best way to delegate allocation, the word, and slash 24 using Centimeters. And in this example, we have 2 adjacent prefixes. 169 21539120 East West 26,"
  },
  {
    "startTime": "00:10:00",
    "text": "and 192 slash 26, which are just some prefixes. that had been delegated to 2 different name server. So why this desktop was important for us at the beginning of the project. because we were looking basically to try for example, limb delegation, they are reliable map it prefix to a NIM server So once we have the prefixes we need to collect additional information about the prefix. This is where we go to who is data set. So the who is provide basically general information about the resources and the way that the public available who is data is presented is which object is supported by an empty line. So here on the left, you have one example from adding who is database where they use the routes attribute which is not used by the the other bridges did. Other which I use the inlet number attributes for IPV 4 address. On the right hand side, you have 2 Object from Lightning and they used the inlet norm, which is common across other history. But you can see that you use custom notation for the inlet norm object. So if you, for example, have your script running and you are expecting to see well from, for example, prefixes. you will encounter a lot of problem trying to address all those inconsistency on the in it norm how the in it top attribute is used in this region. how however to who is still used by need for a researcher and operator, as I told you, I just show sorry. is come with some limitation. So we while we are working with this data, we decided to first."
  },
  {
    "startTime": "00:12:00",
    "text": "some of the challenges related to this data, So In addition, we've seen that the data that is publicly available on the Huawei's is not you cannot have access to historical data. This is just one of data. Different area. I use different URL. where they public they produce the public available who is dataset, and I'll show you there is an inconsistent term of object and also key. So on the table, you can see where I just show you on the previous slide. I didn't using the the root attributes instead ofinitnam for a p 4 of prefix. and and there is no net name. Instead, I will use description attribute in Lucknow region, there is no maintainer. There is no mail. For example, So we'll try to fill those missing, obviously, attribute relying on other datasets, On the request in this part, is similar to the who is data, is not possible to have access to historical data. We've seen an expected resource record, and In RRC 1035, is a Zoom file should at least have an SOA record in addition to NS record. But seen on the data that is probably part of the area that most of them did not provide the SOA got according to this diversification. So We try to addresses are needed with our consolidated data So how do we proceed. we propose actually our consolidated data in the common format which is enteroperable and optimize"
  },
  {
    "startTime": "00:14:01",
    "text": "So we organize the data in a year of the year, actually, so it's possible to have access in marginal is possible to have longitudinal analysis And the data is also designed to support large scale analysis tool. So we base our work on longest for this matching. and we create what you call in a in a trailer. So we have a start in address that you use as a key for each record that we have from the who is re rely on the delegation flight to complement missing information from the who is, the reverse zone, we also convert the domain to prefix for both class list and the class tools. The regulation also apply the same idea of identifier, we have a start and end address that you use to easily identify each Object. So Yep. So here we have One example of both who is in reverse team 10s of the consolidated data. So we use some color code to show What we did Help. the black color basically show what was on the original data. they Aresh color so shows the the key that we introduced in the data. So we have the start address and the end of it. to introduce all the data. The Gring, shows the data that we that was missing, for example, in this case, we have the status and we have the country that we were able to complement from the delegation file, And for the reverse DNS, we add flat to show whether the the data was from classless or classless delegation. So this kind of easier analysis when we want to compare classless versus classes the location on the"
  },
  {
    "startTime": "00:16:00",
    "text": "level. Yeah. So to summarize a little bit, what we basically did is use the public key available data from the who is Rupas Dennis. data and we try to address some of the challenges So we add what you call identify. So we have the start address and the end address that we that we can easier our analysis using these 2, Alimy. limitation. We provide the data in a logical manner we stopped collecting it as since November last year, so data is publicly available. The data is compatible, we'll data engineering tool. On the website, we provide more information on the data dictionary and also propose a a basic Python notebook that you can use to that you can customize actually for your own needs. So yeah. things for your addition. think there is all there is already one question. Tobias? Wias Ebig, Max Plant Institute for informatics. I'm a little bit confused by the slide you had on the IR format, and maybe I just misunderstood something. Mhmm. but there, it was said that a picnic, for example, doesn't have MNT objects, which I do actually see, for example, in the apenic n34, quad 1. And I actually did a who is on the before prefix there. and that -- Mhmm. -- shouldn't have a route entry, but net range and the side of range."
  },
  {
    "startTime": "00:18:00",
    "text": "Okay. This This you are talking about this table. Right? Correct? Okay. Yep. Yep. Yep. So these information come from the public available data, so we download the the raw who is the dataset that are produced by each area and we we Yeah. This is the baseline of analysis. We use the public available data. So maybe when we run the who is on your client, you are go in, to another I don't know. which which more enriched data from the from the registry. But this table is basically based on the public available. The one that is extracted and publicly available on the on the registry website. So, yeah, maybe this is where the missing information comes up. Yeah. Mark Oster is Aaron, so I'm one of the regional registries here. So one of the things that just to clarify, you're using IRR data to do or who is work as opposed to actually looking at who is on port 43. So it's it's slightly different. I understand the infusion between the two. It is what it is. It's been years that this been this way. So that, hopefully, that helps clarify things like your question that you had. here earlier. Thank you. Alright. Since we don't have any other questions, let's thank the speaker again. What's"
  },
  {
    "startTime": "00:20:45",
    "text": "So while we are trying to display your slides, we have soft node here that will be talking to us. Like, you'll basically have a call for collaboration for DNS Integration. and and his slides are amazing. This is why we were just building. interest here. That holds clicker. Yes. Right. Hello. I'm Andrew Kaeser from VeriSign. today, I will be briefly detailing our lightning paper a call for collaboration, DNS Integrations. One of the ways the deployment of the global DNS has become more diversified is through through the integration of DNS domain names to new application environments. Telnet, FTP, email services, and then, of course, later web browsing. In the past few years, we have also observed blockchain and decentralized application have emerged as a new use case for DNest Domainnames can lead to new application integrations beyond the traditional use cases such as email and web."
  },
  {
    "startTime": "00:22:01",
    "text": "one such use case, is to use a user friendly DNS domain name to be associated with a blockchain address or a piece of decentralized content, which can make it easier for users to interact with such applications. The way these interactions work is via a DNS integration. ADNS integration is a method. that makes an association between a DNS domain name and a resource in an application environment. Today's integrations can be categorized into 2 broad types on how the association is created, utilize and maintain. DNS based and server based. ADNS based on integration primarily uses DNS records while the server based integration primarily manages the integration, via a server. We will touch upon examples of both of these to show how they are using both pre existing and novel applications today. Finally, we will discuss some challenges that these integrations face, such as accounting for the domain name life cycle, and why these challenges should be addressed? We will also suggest principles for a responsible integration. between the global DNS and new application environments, and the hopes of starting a conversation now that can continue at a future IETfoth and culminate in a set of best practices for different types of DNS integrations So current and future applications, while a clearer path towards safely and securely integrating with the global DNS namespace. Now on this slide, we should see a graphical example some of these relations that I just mentioned. First, you register a DNS domain name in the global DNS, and then you relate it to an application. And one of the questions we always ask is, could this pattern, repeat itself for new use cases. Now before describing some of these use cases, the integrations they use. want to highlight that many of the new applications not just from the blockchain and decentralized application community. there are, in fact, many, many discussions happening throughout a much broader set of community"
  },
  {
    "startTime": "00:24:00",
    "text": "The slide here shows a very partial list that includes IETF participants, IRTF participants, ICAN w3ccavforum, blockchain and even private sector entities all engaged in discussions about DNS integrations. given this wide range of integration, from a number of organizations and given that each integration makes their own trade offs, it would be useful to bring these and other communities that we are aware of together at a future IETF. to start the process of establishing best practices around different types of DNS Integration. To begin with, a DNS based integration primarily makes this association between a DNS domain name and another resource using DNS records. This is the type of integration that most of us in the room are probably familiar with. because it includes the most common DNS use cases. such as using a record to relate a DNS domain name to a web host, or using MX records for email services. These are the kind of integrations that you use on a daily basis whenever you open a web browser or user a client, newer examples are coming from the decentralizationapplication community. including through the use of w3cdecentralized identifiers. For example, what Blue Sky is doing, to link a DNS domain name, to a w 3 c i d DID. through a TXT record, for their platform. Another example is the proposed w 3 cdidmethod DID DNS, which stores a DID directly in the DNS as a URI record If we dig a little deeper, There are also DNS based integrations that can be used to initially proof control of a DNS domain name. the rest of the integration occurs somewhere else. And let's look at a couple examples to see what we mean by this. So the classic example is using your DNIS domain zone, prove control of a domain name to be granted a web certificate. as if you were to use the FME protocol's DNS challenge."
  },
  {
    "startTime": "00:26:02",
    "text": "Now once the user receives their certificate, they will install it on their web server, the integration itself will take place outside of the DNS zone itself. sertificance installed on that server. A newer example of this comes from the blockchain namespace Communities. such as the theory of name service in case those domains. which are using DNSechdata and TXT records stored in DNS to prove that a given DNS domain name should be imported and integrated into their given name spaces. Now DNS is, of course, used to prove this initial integration once an integration is made, subsequent interactions will occur in that namespace's ecosystem instead of in the DNS. And then the other type of integration that we mentioned in our Lightning Paper is server based integrations. And they make the association by managing content on a SERP. Now compared to a DNS based integration, might wonder how is this gonna differ. the primary reason it differs is that the knowledge of a server based integration may not be gleaned from DNS zone data alone. For example, you may need to interact with an application tells you that a given DNS domain name supports their application in some capacity, and you have to go to their Server or some other endpoint to fetch data. Now this can provide flexibility. especially in cases where store and such data in the DNS may not be feasible or desirable. A classic example of this use case is also the Acme protocol, which you might recall that we mentioned in the DNS based integration. And in the accurate protocol, you can use an HTTP challenge, agree in the search that they get that can then be sold on your web server. Now this tells us something interesting about pre existing integrations. is that they have methods to use both DNS based and server based approaches. such as the certificate being granted using either a DNS challenge an HTTP challenge. And this kind of flexibility indicates that as we consider this topic moving forward,"
  },
  {
    "startTime": "00:28:00",
    "text": "We will also need to consider multiple types of integration to support different types of applications. A newer example comes from the proposed DID web DID method, which stores a DID document on well known endpoint of a web server. And, again, you'll notice a similar pattern here. there's a DNS based approach for DID methods, DID DNS that we also mentioned. So this flexibility appears to be important. I would also like to note that these are broad categories and that Not all integrations are gonna fit neatly into a DNS based or a server based bin. What's important here for our conversation today is observing that there are many different approaches used by both pre existing and newer applications today, to integrate with DNS domain names. So it is likely that we will need to develop best practices for different flavors of integrations moving forward, ensure that different applications target different use cases, choose an integration that best fits their operational profile and objectives. Now with all these integrations in mind, we did want to discuss some concerns such as interoperability and support, But today, I want to highlight the synchronization aspect of the concern you can check our lightning paper for a discussion of the other topics. Now synchronization, between the DNIS domain name other namespaces and applications are not guaranteed once the integration is performed. for example, the DNS domain name may be important. but there may be no clear process or mechanism or guidance update the integration when the DNS domain name expires, is transferred. the zone changes, or the content on the server changes. Now to grand why this is concerning, consider the following example scenario. First, a registrant will use a DNS domain name, and a DNS integration to integrate that name into some application. 2nd, the DNIS domain name will expire. But because the DNS integration is no longer synchronized, the now x registrant will be perceived as controlling the DNIS domain name, this integrated application. then if the DNIS domain name is reregistered,"
  },
  {
    "startTime": "00:30:03",
    "text": "2 separate parties will be perceived as controlling the same DNS domain name dependent on the application context. users of the integrated application will see the information and data set by the previous front. all users, the traditional DNS applications will see data set by the new registrar. And, of course, this will lead to confusion. amongst users, amongst the DNIS register on it themselves and amongst the integrating application. So with that in mind, we would like to set forth certain criteria that we believe are important any responsible integration method. The first circle on the slide is control. And this can be summarized as can a DNS registrant be confident. that only day, or their authorized representatives are able to use their DNIS domain name In the DNS integration in question, without being concerned that someone else may be able to claim or use the DNIS domain name without their consent or knowledge. The second circle is domain life cycle. Does the DNIS integration account for the DNIS domain cycle to avoid such synchronization concerns as we just mentioned. Additionally, is an integration aligned? with the best practices and policies of the DNS community. For example, if you support DNS sec based methods in your integration, Do you support the required recommended algorithms from the DNS SEC RSCs. of course, does the integration expand utility without impacting the ability of the DNIS domain name to be used for other purposes. including the preexisting uses, it was possibly being used for. So with all of this background, we would like to extend an invitation a collaborative community level discussion. will be needed to address the issues in this space. to come up with responsible DNS integrations diversifying the utility of DNS domain names into new application environments. So please feel free to reach out to us. If you are interested, in a future bot on this topic, and please spread the word to other communities you're aware of."
  },
  {
    "startTime": "00:32:02",
    "text": "Thanks for your time today, and I look forward to interacting with many of you in the future. the Questions? Peter? Hello, Peter Thomason. You gave an example about on slide 8, perhaps, of where the problem lies, and I think you Yeah. And the example you gave is essentially when I let my domain expire. I have a problem. Now is that the main issue we're solving? Because seems to me that that's maybe not best off with integration concepts, but rather would not let having the domain fire. Right? So so I wondered, like, what's the problem we're solving? because that doesn't seem to Right. It it depends on if you're looking at the synchronization issue from which perspective. Let's say, for example, that you might have let the domain name expire, and you still continue to use the domain name in that integration because it just happens to work. you don't realize there's been any problem. any issue Or if we look at a blockchain example, if the name expires and you still our perceived as control on that name in the blockchain namespace and someone tries to send you cryptocurrency to that address, for example. but someone else now controls it in the DNS space Demi. think they were sending it to the DNS party when they accidentally ended up sending it to whoever the previous party was. So that's part of the spirit there is to try and solve both sides of maybe you let it expire accidentally maybe you did it maliciously. But that way, we can try and account for this or understand if it should be. can't afford. Thanks question. Jim Reid. for your interesting ideas here, but I think the problem I've got is trying to figure out where this kind of discussion and collaboration could"
  },
  {
    "startTime": "00:34:03",
    "text": "take place. You've given a whole shopping list of things that could be looked after features, some look interesting, some maybe not so interesting. But there's a whole bunch of organizations and institutions that could be involved in this. We've got the idea. I've got I can. We've got various other industry for them and so for what going on. So where would you see this kind of collaboration and cooperation, the question taking discussions taking place, how would you think that could be achieved? Yeah. An excellent question. Our first step really is to try and have a bot to try and get more insight from the various communities involved to see who would be interested in tackling this question, because you're right, that some of these topics seem to be benefits for the IETF. some seem to be better if it's for more eye can level discussions or w 3 c it really sort of depends on who we can get into the room to discuss these topics and decide what direction at that bot, for example, that we can take Okay. Well, I've got two points to make first about that. Just to be a little bit picky here. If we're talking about above, that's a specific meaning and idea of context. And I think you probably don't want to have one of those kind of bots because those bosses are supposed to be talking about being formed. But certainly having some place where these people could come together for a group hug would be a good idea. I think one of the challenge you'd have trying to fight to to make it happen is finding a forum or a venue for it, And I think some of these organizations are like to be very protective about you doing this part of the problem space here. Don't bother with things that are going elsewhere. I think that would be a challenge to get these people to think that could come together and and work in a collective manner to look at look at these problems. Thank you. You have point well taken. Hi. Daniel Con Gilmore. So thanks for bringing this up here. I think you've you've outlined a really a broad class of problems. And I think it can be challenging to get people to collaborate when you know, my use case might be something completely different from someone else's use case, and the only thing that we share is that we have some kind of integration with the DNS."
  },
  {
    "startTime": "00:36:02",
    "text": "Right? I mean, I see this with the encrypted client. Hello. fronting server up DNS updates, for example. how do you imagine getting people who work across such widely different scopes, scopes, to actively collaborate on this. And secondly, due to this with the synchronization problem, One of the things that I think we see happening with the DNS is that The DNS is used as a leverage point to create things that then have a different actual time scale than the DNS records themselves. So, like, If I use Acme to get a certificate, the validity window of that certificate is not bound to the validity period in the DNS. So how how do like, How can we think about aligning those timescales? Or or is that hopeless? Like, what's do you how do you see that? So to take that last question, first, Hopefully, it's not hopeless. Part of the motivation here to try and broad in this collaboration is to bring sort of diverse communities together, especially ones such as the African community, has a much longer history of operational, understanding to help us influence and understand what maybe some of these new integrators to do. And it might be the case that the scope is, I think, as Jim was also into, it might be too much for any one sort of venue. So it might be the case that we figure out as we interact and associate and have conversations and dialogues with people that we have to fine tune the conversation to target more narrow areas. Okay. One one place that I you might wanna look for inspiration is the UTA working using TLS and applications working group. it's a little bit more focused than the possible places you can integrate DNS, but take a look at that and see how they've dealt with TLS and a a range of different options. Excellent. Thanks for the suggestion."
  },
  {
    "startTime": "00:38:01",
    "text": "Next speaker is Johannes Knab, And she will be talking about the title of his talk has got a query, I'm all again. repeatable name resolution with full dependence of prominence. Hi. I'm going to talk about name resolution. stuff that DNS is all about. or maybe about. Let's start with an example to get all back to speed. Yes. So for example, if you're resolving www.trm.de, We start with the root end. So we're going to to talk about authoritative name servers. We'll start with the root ends. We have some name servers, names. We have some blue records where we're going to start. And for simplicity reason, after this figure because we're going to fill it in later, We're going to omit all the IP addresses. We simply assume that they are somewhere in the zone, in the root dot. And for the authoritative server, QNs, meaning DNS record names. We are shortening them and simply pointing to whichever zone they are going to be answered, they are going to be authoritative. And So Honestly, we query the one of the roots server. So we're going to get an delegation back. for the domain and have the name servers, And in the delegation, we also are dealing with bubbles. We also get the glue records that we then continue on. So on the next step, we can simply ask one of the servers where we get the the glue record as well. for tm.de. we get an additional delegation back with 3 names server us, And for one of those named servers, because it's in sibling domain in DDE zone, we luckily also get a a flu record back, and then we can simply"
  },
  {
    "startTime": "00:40:00",
    "text": "Ask this one and get our answer back. So during that resolution, we more or less rely on new records. We have an happy resolution pass. but we found a lot of stuff in the DNS, all those zones Oh. in in gray where we don't know the name servers. We don't know how we get there or what they could influence. So you're going to resolve them all, then we're going to end up with some figure that's going with a bit more crowded and a bit more complicated. So how how do we get to that figure for every name that we get in the in in the delegation and the referral going to resolve it, and they're going to Continue until we have no more names to discover, no more names. to himself And then we have multiple resolution passes on how a path how we can get to our answer. and how how we can also what can influence the the answer. So what's the motivation. I mean, we want to find and and resolve all because of dependencies. We want to build the entire dependency tree. that we can figure out what can influence the name resolution. We want to identify broken. delegations. For some definition of blame, we called them previously lame. So, like, author of the name servers that do not access, so I do not seem to access. So if I try to resolve name, I might get an annexed domain back in the DNS It could be in the root zone. If it's simply root crap, it could be in some DOD. So where I might be able to at my own records, CP errors. It could be performance problems,"
  },
  {
    "startTime": "00:42:01",
    "text": "and also it performs problems, authoritative servers that don't answer or don't give any useful answers back, meaning various DNS error indicates non authoritative answers. re recurs us that are entered into DNS errors. And in addition to simply getting rid of the dependency tree. We also want to query them all, and we want to also query the the multiple data or query and compare the data copies. what are multiple data copies or what what do we have in the DNS We have endless records in the Republic South, but we also have NS records in the origin. do they match what can be learned additionally here? We have blue records, and we also have the blue records, hopefully, authoritative data. And last but not least, we also have for each domain, hopefully, multiple authoritative servers So do they are are they all synchronized? Do they all serve the same data? Or do we have some configuration with? Why is it important? Or why do we want to investigate that? And one is the security aspect. If there are some hidden dependencies there, some broken dependencies that could influence the the resolution and also the performance impact if we have records or name servers that do not work, and We're going to maybe spend some time there, but not speed up the resolution for the user. Coats. for our goals. So our research questions are we want to study the DNS dependency graph. we want to find potential inconsistencies in these configurations. Try to evaluate with the impact. but the left was a problem. If you want to do that, regular Resolve us, come Resolve us do not expose that data. and they do not even internally necessarily get all that data because they might rely on few records. The the primary task, the primary goal is get the an answer to the user"
  },
  {
    "startTime": "00:44:00",
    "text": "as as soon as possible. That's more or less the benchmark there. So we we did the the only thing that we thought to do. We build our own reservoir foolishly in in the attempt because how hard could that egging be? And for our implementation goals, more or less guided based on what we want to achieve. want to discover all reasonable resolution passes. if the s in hidden primary server we are not going to brute force the entire IP space to find that server. We want to query all data copies. as reasonable. We want to capture all those where we save them that we can later on provide provenance on Why did we get that answered? Why do we have an additional answer? Why do we didn't get an answer? We want to be deterministic repeatable. we also want it to be fair and efficient. So we don't want to overburden authoritative name servers, especially 6:30. all data, copies. So we want to Yep. Good. Good. netcitizen So implementation of such and resolver on with very rough overview here because more details in the paper, because that's gets a bit tricky on what we need to consider. So we structured the resolution model as we try to build our zone tree. and how it's observable in and divided, how it's observable in the Internet, we find our alternative server candidates, meaning blue records, routems, resolve the name server, name ID DNS names within the resolver, and also have to consider, like, if we can have a name server that's authoritative for a parent, and you try it as well. do not get any referral back, but we simply get if we ask for delegation or try to figure out the delegation simply from the authority to answer back in in this or cut. And for all the service candidates, we are querying SOA record and the MS record."
  },
  {
    "startTime": "00:46:00",
    "text": "the MS records simply because that. Could that. lead to additional information that we cannot cover the SRA record, should exist. might not exist for for interesting configurations. might get some hints on whether or not they are properly synchronized even if we don't see deferging data. And we consider the name server, and we are going to use the name server as any those 2 queries is going to provide us an alternative answer. If we get conflicting answers, then we are going to use the super set of them because that's what all if you want to figure out all resolution parts, that's what we need. And we end up with a problem if we have names in the Nasdaq cards that we're going to resolve within the Resolve We're going to add new zones. We're going to discover new stuff in the name of they are also going to have name records So a resolution all the way down? Let's get back to our figure. If we squint hard enough, I mean, that's already oriented that way, we're going to find some zones that seem to and to depend on each other. Like, for example, lorzet.eu, lorzet dotdellesets.iron. are all zones that point to each other where the name server records point to interling or interdependent to the each other, so including soft If you go a bit back to graph theory, we are going to find out or that looks a lot like us from the connected component. meaning from each node. I can re or in in this group. I can reach I I can walk. each other node and come back to the origin, and I can influence myself for the DNS impact. That means that if I have an if I have name server in such a group"
  },
  {
    "startTime": "00:48:01",
    "text": "that's going to to write an answer. it adds additional information, it might be even able to impact its own token If you fill that in for everything, we are going to end up with our model structure dependency tree and have also the the additional completed graph. we're going to complete the the resolution process or it's based on postponing our queries until we figure out these strongly connected components. by an online graph search, and we are going to need to that's details in the paper. a bit tense on how do we figure out what are the name servers, We get queries. Do I need to resolve that query in order to complete this strongly connected component? if it's not needed, then I can postpone it. So we, in order to to figure out those strong component, we need our zones, we need to detect on what might be a zone what what can be externally observed as a stone. So we need to for all all dots within the zone, we need to figure out Is that the zone? Is this the the is there a delegation there? Or is that something in in subdomain in in another zone? So q and a minimization provides a framework for that. So we simply query for each delegation. They're going to query, Is there something that's a medication for for these specific name and not of the complete name. And compared to the the ROCs or the the suggestions, we're going to use SOIC queries. Since for 8 queries, we have the problem example that if the pound is also authoritative the child, we're not going to discover this delegation even if it exists. For endoscaries, that's what the original proposal was I tried that once, and I found some interesting name servers that"
  },
  {
    "startTime": "00:50:01",
    "text": "answered and delegations in the answer sections. I don't wanna know. And as an additional optimization, because even the the last label could be separate zone If you have top to top, It's most likely not going to be on a separate zone. So, initially, for for all those labels within our single labels within the 2nd level effective second level domain. we're going to ignore that for now. and only going to do a photo zone cut discovery and those records, the world records. if we get an answer back that indicates some delegation. So if we query all name servers, that might be not completely viable. For example, the root for the d.comzone has 26 authoritative servers based field addresses. It might be more servers. It might be less servers. I'm interesting on information there. And this is very large stone So if we resolve a lot of names, we are going to hit a lot of various end names of us And if you employ rate limits on our own, that's going to be the bottleneck for our resolution. And I would assume that Verysine is an operator would probably prefer to not answer the same questions 26 times. if you can avoid it. What's the the solution here? We can simply we we extend our assume that TLD servers are somewhat synchronized, consistent, properly managed, And simply query consists of our deterministic set of of the name servers. so that we don't have to probably 26, but pick 3 based on the name that we are currently asking based on the appearances of of the candidates that we can ask. And you find any discrepancy, so if our assumption They're consistent. We we observe that it does not match. We're going to worry all."
  },
  {
    "startTime": "00:52:02",
    "text": "Additional optimizations are if we have is on file. For example, from the standardized zone data service, we can use those delegations that we find in those zones directly And skipcar in the dot com name servers completely. For testing, I mean, if you implement a resolver that going to be a lot of bugs, trial and errors, and a lot of gray hair. rerunning against the Internet is not a viable option because at the number 1, Burncy, authoritative sales service, We don't want to be a bad net citizen. And even if we would do it, the results are not 1 to 1 comparable. if there are changes in the DNS, we don't know if it's code changes spots in our code. or if it's simply DNS changes. So we have procedure on where we capture the previous recorded data. and one that simulate the ID name servers that we've seen. and the Linux Network name space. and can then record queries that we've known queries, meaning queries that we've opted in the original data, for right now, I've in in the simulation, And we have also query stat We know in the original data, but we skipped in the simulation right now. So indicating bugs. due to time authenticating unresponsive name servers that comparing the results. If you run it multiple times, if you run it against yourself, it's a are complicated with more details in the pickup. So let's conclude. we have unresolved that. That can just cover the entire dependency tree. provides a repeatable and deterministic resolution process independent of cash in ordering, etcetera. We're saving all reasonable resolution passes. including all the alternative servers that we can ask for later analysis and have a process to to test that."
  },
  {
    "startTime": "00:54:04",
    "text": "With an sample data set on TCPresolve bucketub.i0. with an update to the Alexa list, which no longer are up to date and the Mallet Majestic William list for a reasonable record set including w.subtubsupp domains, and a few name servers, a few domains that might be interesting. only a simple dataset if we have I'm I'm that access more data. but that needs to be analyzed. Those impacts and misconfigurations need to be evaluated And I'm especially here are interested in are there new interesting questions? could be answered by such datasets Do you have information, or do you do you have input on what needs to be considered additionally and open for questions. Daniel Con Gilmore. Thank you for this work. It's giant disaster. I'm impressed that you've persisted at it. Were you doing the so so I'm trying to understand the relationship between this and 2 name and the position. because one of the problems that we found with human minimization, if I remember correctly, is that you could get different answers if you were sending the full query as opposed to the the suffix, to the higher level, measures. Yes. I mean, if the that that access the possibility that the Current name server is a hidden primary for a child. Right. That that simply could be an old configuration still on on the server, but it's now delegated. Right. We have a malicious a malicious response? A malicious. And it's especially if if there are multiple levels, like, sub child of a child."
  },
  {
    "startTime": "00:56:02",
    "text": "Because then if I ask for the DHL directly, get an delegation. But if I ask for the child's child name, grandchild name, The answer or the server still has its own configured, and it's going to answer directly. Right. So that there are some differences that could happen there. Not sure if that's the question exactly. So I guess my question is, did Were you able to analyze this to identify scenarios like that where that happened. Because, I mean, one one of the concerns that I have is that is that these things are possible, and we wouldn't notice them today. Yes. I did not analyze that specifically. hope the data at least could give some indication where that could happen. And since you know the all the name servers that we could ask to verify if that the case or not. I hope that would be possible to to figure out Okay. The other thing that I think would be very useful, and I don't know if you've produced this or not. be something that a domain administrator, someone responsible for a given DNS record. could run that would do all of these queries and map everything and give you the kind of diagram that you gave to show you know, here's the range of answers that I got, and here's the past that I got to those answers. Do you have such a thing? No. Unfortunately, not yet. Okay. would be really nice to have. Yes. Thanks for that. I agree. we will take one more question, and that would be from Victor. Lead vector. Yes. Okay. Find me later. I think I'm gonna read this. k. I was gonna ask, bro, I mean, what users have you put it to? looks like your making the data so we're available for others to poke at a little bit. But what have you done with it? Coming out. What what we've already done with the data, If I understand the question correctly, yes, no. What's the what are the implications so far? So the the application so far more or less the the engineering challenge I looked a bit into the data,"
  },
  {
    "startTime": "00:58:04",
    "text": "a few scary things. Like, if you find an x domains for financial records, that point into top level domains. where I did not yet figure out if they are completely registerable or not. That's the the the scary part. And other than that, that there there is data, but I did not completely evaluate it yet. initially, which was the the engineering challenge. That was the interesting part. Great. Let's thank your hands again. Next, we will hear about enabling multi hub p, Hypergiant Collaboration, from Christian Montana. Hello? Am I audible? Yes. Alright. Good. Hello, everyone. My name is Christian, and I would like to present today my work on enabling multi hub ISB Hypergiant Collaboration. So so Let's start. looking at the Internet nowadays, we see that more than 80% of all the traffic is coming from Hypergiant, namely Google, at least Meta and or others of them. Now who they are sending this traffic, well, Usually, those are the ISPs like we see here, AT and T airtel, of ink, of ink, In order to do so, the Hypergiant's tend to interconnect as much as possible So so they tend to connect to as many networks as they field sea feet. Now nowadays, large Hypergiant you more than 10,000 different networks. And example, we see here, like, Cloudshare already reached the 10,000 and Google has more than 12,000 networks. However, just connecting to the ISPs is not enough. So in order But the the second thing that HyperGen has to do once that client wants to access some resource, the hyperjat needs to select the optimal server."
  },
  {
    "startTime": "01:00:02",
    "text": "Right? And this problem is not trivial because there are a lot of things that are changing on the Internet all the time. How that previous work by prioritized design, a system that actually help the Hypergiant to select the the the best server, But just for those ISPs that are directly connected to the Hypergiant. Right? And couple of questions, how about the networks that do not actually peer with this Hypergiant since the largest one goes up to 15,000 networks. It means that there are more than 40,000 small network guys out there that really don't built to a hypergiant So during our collaboration with a large European transit provider, we actually saw that a really large number of these small ISPs didn't didn't didn't didn't do not deal with the majority of the Hypergiant they actually rely on their transit provider. Let's have a quick look. We have here on the left side, actual Hypergiant that would like to send some traffic to to a small European ISP. And the first thing that it does, it will send the traffic to the transit as we can see, the traffic is split into different locations. In this case, It's 2 different countries. And what the transit is we'll do is but we will just simply pass over if it can. all this traffic to the small European ISP. Right? So in this case, it'll just move forward to some other routers, but keep to the same location. Now what will happen in the small European IP is that Some of the traffic actually needs to be rerouted from one location to another in order to reach the end clients. Right? And in this situation, we have this small different amount of percentage that actually went to to one location but needed to be in another location. For the investigation, we we wait for them litigation here. We looked at all the router at their capacity. There are no congestions."
  },
  {
    "startTime": "01:02:00",
    "text": "there are actually no problem anywhere. The only issue that this is happening is actually the improper choosing of the server on the hypergiant Right? And Now if we try to look at the entire week, we want to see what's happening over the entire week with with the traffic coming from the high giant and what we see from the total traffic coming from this Hypergiant see a typical dual node pattern. It's very typical for residential ISP with high peaks in the evening. And when we look at the non optimized traffic, we see that kind of follows the same pattern. But what's most important for us is that there is a large amount. So the the nonop my traffic is very high during the peak times. Right? So it's almost 30% here. No. No. No. we see this behavior in more than 20 European ISPs during our collaboration with the log transit provider. And we asked ourselves is is there the possibility to help the Hypergiantes to improve the server selection for non directly connected ISPs. Right? So can we actually reduce this 18% or maybe completely remove it if it's possible. And in fact, the answer is yes. We can do that. we can do that, but by enabling i the ISP to Hypergiant Collaboration. The idea is that the ISP is to send some additional information to the HyperJET in order to improve the server selection. This sort of collaboration can go multiple ways for example, you're gonna have a multi hub collaboration with the ISP collaborate directly to hypergiant and no other in between transit IS multiple. Only one of them is involved. Another sort of collaborate can be 1 plus hop collaboration where there is a chain of collaboration between all the neighbors starting from the ISB ending up."
  },
  {
    "startTime": "01:04:01",
    "text": "in the Hypergiant. Multiple other collaboration are possible in cast them in our paper. So if you're interested, please go ahead and read. But for our presentation, we'll focus on the multi hub collaboration. The idea here is that the ISP would like to send a set of key value pairs, the Hypergiant where the the key is an IP prefix. of the ISP, and the value is a list of this similar IP prefixes. Give you an example. You can interpret it as a letter. in sort of the Hypergiant, can you please send data to prefix a as you do for prefix b, board for prefix c and this order of preferences. Right? At the Now once this is set, the idea the the the question comes up is how does the ISP select these prefixes? What what what prefixes should choose? And where these are the 3 different possibilities. you can go either with BGPNL's prefixes, which is actually not efficiencies, the the the the Hyperjet already know them. and take them into account. The second one is the I be DNS resolver working prefixes. And the idea here is that inside the ISB actually uses a a small fine graded the prefixes that it's working with. especially for the DNS resolvers of the clients. And we we we call this in our fuge in in the in the future slides, we'll we'll called called called called called called called this specific work prefix is the DNS default. A third option, it's complete this application. Like, slash 22desegregation where where we can de aggregate disaggregate all the prefixes of that speak. to slash 24. The thing here is that both the DNS resolver and the and the DNS server should have the ECS enablement No. No. No."
  },
  {
    "startTime": "01:06:00",
    "text": "going back to our traffic and unoptimized part, we ran a retrospective simulation on this real traffic. And as we can see with the GNS default simulation, we managed to reduce from an average of 18% down to one point 3% of the amount of non optimized traffic. Now if we use the slash 24 prefixes that we end up with fully optimal traffic. in here. This was the case of only 1 Hypergiant And we went forward, and we look at all the traffic that's coming to that ISP. And we identified 11 different Hypergiant there and also some other networks that sending traffic. As you can see, 3 of them here are marked with a star. The point is that for these 3 Hypergiant, they only connect in one location with the transit AS. Therefore, it doesn't really matter whatever separate, they will choose the there is only one possible way to to to send the the data to the to the ISP. So the further optimization can be done only by the transit layers itself if you wish is to do something some changes inside of it internal routing. The next column shows you the the amount of traffic coming from this Hypergiant's and also the following up, the the amount of non optimized traffic that is there. As you can see on overall, there's about 14% of the of the traffic that's incoming. coming to the ISP, and this this 14% are potential have the potential to be improved. On the last column that we have here, we see the amount of non optimized traffic per own traffic share, and we see a large discrepancies in between hyperjack For example, for 1 Hypergiant, the traffic is actually pretty well optimized in the server. Our very well selected, and there's only one 0.8% of"
  },
  {
    "startTime": "01:08:02",
    "text": "non optimized traffic. While for the other hypo giants, we see almost a half 46% in this case. So looking over the time, on all the all the traffic that the incoming into the ISB, we see the average of 40 percent, and we see, again, the the same tune of pattern of the non op optimized traffic, once we ran with respectively the our simulation with the DNS default, We managed to reduce the the the amount of unoptimized traffic down to 4% What's more important is during peak times where the resources are really scars for for small ISPs. We managed to reduce it even more. So in this case, as you can team. from 30% down to 10%. If we run the simulation with the slash 24, we again ended up with optimal. traffic, Right. Right. going to the conclusion, during our research, we show that it's possible to improve server selection even if there is no direct connection in between the hypergiant and the ISPs. We also showed using real ISP data that our system can actually improve the non optimized traffic up to 10% without any additional implementation or in improvement to the DNS. I'm trying to say about implementing and adding ECS to the DNS. Also, our results show that there is discrepancies in between the the the traffic coming from different Hypergiant. And for some of them, there's up to 46% of the traffic that's coming the non optimum interaction. And we are good that the out there are more than 40,000 different networks that can potentially benefit of this of this system. Thank you."
  },
  {
    "startTime": "01:10:02",
    "text": "can ask something since nobody is waiting. I Right? Okay. So Thanks for the presentation. can you comment a little bit more on how likely is it that a Hyperzyme will have alternative paths that BGP can expose or, like, how do you really change the password, change the password, He's being used Oh, actually, we we we don't want to change any path at all. Right? So the idea that what BGP did and how to pass are selected is all fine. Our point is that unfortunately, the hypogne has not enough information when the request comes in in order to select the proper server. And by accident, some distance server coming from a different place, may be selected for that client. and then a different route will be used. So it will be the best route in between the server and the client, but it's just the wrong server or the not the best server was chosen. And the the servers that you are allowed to use, can you can you comment a little bit on the set of those servers? Like, which servers are are they on the same a yes or, like, what do they have in common? So, Donald, they are not in the same AS. So during our collaboration with this transit transit AS we know for sure that the there is no presence of the hypergiant side of the transit So therefore, once the hypoGen want to send traffic from its servers, it's from inside, actual hydrogen, it has to send it via the transistors and end up in the ISP. Right? -- Yeah. My my my question is more the servers. So that you actually selecting among Right? So if you're saying that the Hyperzyme does not select the optimal server. Correct? Yes. So what do this set of servers that could be selected have in common? Oh, but the the the"
  },
  {
    "startTime": "01:12:01",
    "text": "They're basically similar. They are duplicates, but they are in different locations. And the idea is that which of them will respond to your client. Right? Gotcha. Okay. Thank you. Alright. Since we don't have more questions, let move to the next and last up for the day, It's from Alex Phamten. And we talk is a type daisy practical anomaly detection in large B2P MPLS of B2PS vxdpnnetworks. Yeah. So hello, everyone. I am Malik appeared the student at Pinsaleon, and we are presenting today, Daisy. Our framework to detect anomalies in large scale, BGP MPLS and BGP SRV6 VPNnetworks. So I will first present the scope of our project, what do we consider normally in our BGP MPLS and VGPS service with VPN Networks. will cause all the different basic architectures we've we we've been working on. the IETF contributions with working to have not only an open source solution, but also a standard IETF solution. and at the end of our results and the ongoing goals of this project. So A VPN allows having a connectivity for the customer between 2 OMR sites. And in this project, we define an anomaly as an event that occurs in the network and impacts the customer traffic. and therefore makes the customer unhappy. This event can be provider inflicted do to an incident inside our network. fiber cut. interface not working properly. it can be also provider self inflicted. when there is a maintenance window and the operator"
  },
  {
    "startTime": "01:14:04",
    "text": "is pushing a new upgrade. And in we in this upgrade, there is a bug but it can also be customer inflicted when the router the customer Edge router, but that are managed by the the customer themselves. They push a wrong configuration to the router, and they lead their own traffic problem. So why this is important for ISPs? But because at the end if you match your outages badly and they last you end up in the news. We all know that issues happens to all the network but what it matters for ISP is how you manage them. This service interruption, of course, they make you look bad, but cost you a lot of money. And that's that is why we in this project We are focused on how to detect these anomalies in at early stages. and how to provide the the necessary information for network operators to analyze the data. find the root cause and, of course, fix the issue at the end. So this is a project that is financed by Swisscom, and we do research but also development in open source. throughout all the chain. So from getting the data from the network, to get visibility of what is happening in the network. We do research, but also standardized the telemetry protocol at the ITS and implement different publishers and and and collectors to get this information. We propose also a new network measurement that could be interesting for anomaly detection, And at the end of final goal of this project is having a scalable solution of of course, anomaly detection."
  },
  {
    "startTime": "01:16:00",
    "text": "So first requirement of the project is that it needs to work. We are working with SWISS coming production data, we need to take into account the different operational constraints that the global operator than they have. Let's present the different components we've been working on. in Daisy, So first, we will understand that we need to know the behavior of the customers to make it work I will go through the different standards we are using to get the die different dimensions we are getting from the network. how do we post process them, And at the end, based on that data, how do we detect anomalies? And, of course, once we have detected the anomaly how do we report them to the knock so that they can fix the issue? We this research, as I said, is based on production data from the Swisscom VPN network. and we quickly realized that customers differ a lot in their behavior. And, therefore, there is no one features algorithm to detect anomalies. For example, they are customer data super predictable with half with flat curves of traffic. or repeated day night cycles, but there are also some other customers that, for example, regular drops of packets, is is for them normal. And therefore, we cannot use this drop metrics to detect the anomalies for them. On the other hand, they are managing around 10 to 11,000 VPN customers, we cannot do one recipe for customer and, therefore, we need to group the customers into into customer profiles so that we can base our anomaly detection recipes on this profile. So we are getting different dimensions from the network"
  },
  {
    "startTime": "01:18:00",
    "text": "So first, for data bank, we are using the IP fix to get traffic counters and packet drops. from from the ad work. in control pain, to get the BGP's topology and the BGP state we are using BNP. capturing BGP events, such as updates, withdraw, and Pure downs. And in management team, it's still a bit work in progress at the ITM, but we are already deployed Yamposh using UDP in order to get the interfaces state changes. And, of course, theamples is to get all device related information. Of course, once we have got all this information by the collector, we need to correlate this information to the customer so that we know to which customer we are impacting. And we are correlating IV fix to the BGBP path so that we know which counters belongs to which customers. we are doing the same for the interfaces of Yankush. to know when there is an interface which goes down to which customers we are impacting. we are doing all this with the open source solution PMC developed by Paulo Rucente. and this allow us to rely Not in the inventories, but of in what is happening in the network. So once we've got the data correlated to the customer to the customer identifier. we base our customer our anomaly detection on the customer profile. for each customer profile, we apply a set of strategies that are a way to capture the service health. These strategies are are organized by a set of pipelines that are sequence of conditionally checks. and the checks are the actual algorithms on the data. That's how the tech"
  },
  {
    "startTime": "01:20:02",
    "text": "if there is something wrong on that data. simple check, for example, would be check if for that customer at that time if the traffic is there is a big difference from the last if there is a big difference, we'll raise the alarm. of course, all all of this is configurable. we also allow BlueJeans so that the network operators can define their own rules and integrate it integrate them easily? And, of course, once we've detected And, normally, we need to report them to the NetGoPay so that they can fix the issue. For that, we are interfacing with come through our Kafka topic. that we are sending a ticket to the NOC and so that they can get the data. within this message, we are giving them also to roll data from where we have executed the algorithms but also the details and the parameters of the different checks we have executed so that they can have the full view of what we what did we raise this alarm? Since we are in a big data, we cannot save all the data. When there is an incident, we are saving this data in the in a permanent permanent storage. to play around with what if it's analogous to experiment with newest strategies and new checks so that we can continue improving anomaly detection and continue improving the accuracy of our platform. As I said at the beginning, we are also contributing a lot within the IETF to not only have an open source solution, but also a standard solution with the different RFCs. For example, we"
  },
  {
    "startTime": "01:22:01",
    "text": "have proposed a UDP based transport for young push to allow the streaming a large amount of data from the router directly from the Lycap without stressing the route processor. of course, at the IETF, we we have seen that there are new technologies, semi routing over ipv6, are starting to be deployed. We are proposing also extensions to IP fix so that we can monitor these new technologies through the same system. And we are also proposing new metrics in that in our case is the on pass delay, which is the delay between the encapsulating note and the different notes along the path, and we are sporting these delays using also iBC so that we can already have the aberration the knot. We have also other contributions I will not go into details on each of them, but piece if you are interested this ping us. But, basically, what we are doing is extending the young push header so that we can actually monitor not only the data, but also monitor the whole pipeline of And in the second instance, we are also extending IUM direct export so that we can compute in Passport mode, the on pass delay that we proposed. 30s. So what's the status right now this project. Right now, this project has been developed in Python as a proof of concept has been deployed in production for a set of customers of the Cisco VPN Network. And so far, we have detected 6 outages. 3 in real time and 3 in replay mode. And currently, we are continue studying if There could be new dimensions that could be interesting. to detect these anomalies."
  },
  {
    "startTime": "01:24:03",
    "text": "And, of course, at each IETF discussing with the different vendors if our different draft could be actually, we implementable. In the future, we also plan to study the same framework could be also be used to to detect anomalies in Internet policies. since at the end in ISPs, it's another service. but they use the same system in a way of monitoring So in a way, in our system, they could be another profile with different recipes dedicated to Internet services. And, of course, at each IETF, we are present at each hackathon, but also different working groups to continue progressing with the standardization. So that's it for me if you have any questions or you are interested actually in any network telemetry, topic. We are a bigger group than the authors of this paper. please ping us or reach out. We are currently at the whole week at the IETF and attach I at each IETF, we are present working with Swisscom, but also different vendors such as Cisco and Huawei. If there are no questions from the audience, I can chime in. So From all the data that we have seen, so far and, like, all the different incidents. Can you comment on the more common or the more disrupting one. what would you Like, what's the worst the more common one."
  },
  {
    "startTime": "01:26:01",
    "text": "From the different incidents, we have seen there is a no. common incident because it it was common, we would just have fixed the issue and not c seat newly. Right? So no. From all the incidents, we have seen so far each each incident is a new one, and we've learned new things from that incident and implementing So new checks, new strategies to see if we can improve it. Sure. Sure. I meant more, like, what what was the more influential telemetry metric that you have done that catches many anomalies, for example. Ah, okay. So, yeah, most of them, they impact a lot on the traffic amount. So on the forwarding plane, But when there is something happening on BGP, for example, you have a lot of on the BGP events. So on mainly on right now, at least, on control panel for Whiting things. Thank you. Okay. Let's check next speaker again. Okay. So it looks like we're ending the workshop right on time, and I would like to thank everyone for joining us today, and for asking those interesting questions. Please always feel free to reach out to the speakers if we had to catch our questions due to time constraint And I would like to also thank our speakers once again fertil's insightful research talks, Do you have anything to add? Yeah. I want to also add to thank our panelist that were super insightful and everyone of you for the really nice questions in participation. Yes. And it's our pleasure to be part of NRW. and then we hope it will continue to bring new ideas to the new community and to this community and will continue to be"
  },
  {
    "startTime": "01:28:01",
    "text": "more dynamic and likely. Okay. So let's call it a day, and thanks, everyone, for coming. And thank you, Maria, and Frances are putting this together. No. This is"
  }
]
