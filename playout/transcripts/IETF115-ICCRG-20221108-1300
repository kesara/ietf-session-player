[
  {
    "startTime": "00:00:04",
    "text": "foreign thank you that's the right because thank you foreign"
  },
  {
    "startTime": "00:02:13",
    "text": "foreign thank you if you join Ed I figurative yes thank you foreign"
  },
  {
    "startTime": "00:04:26",
    "text": "foreign thank you all right welcome everybody should we get started um welcome this is ICC RG if you don't know what they said that what that is you're on the wrong room you might since you could say here you might learn a thing or do so either ways welcome to iccrg um it's been a while since we met in a room it's wonderful to see everybody here um I'm um going to start off"
  },
  {
    "startTime": "00:06:06",
    "text": "um by um asking Colin to step up and say a couple of words about our new chairs it's like this opportunity to come stand at the front so um hi her I'm Colin Perkins uh I'm the irtf chair I just wanted to um as some of you may have seen we've we've had a change of the chairs for This research group uh in the last couple of months um so I wanted to uh firstly thank Jonah who has been sharing this group for I don't know how many years very many years some years some years yes and has been doing an excellent job cheering it's uh um we we figured after a decade or so we should perhaps give him some help um so I'm very pleased to welcome Simone Ferlin and Michael Shapiro um both of whom are remotes this time unfortunately they're not here in person um but they're joining generous chairs and so I just wanted to thank Janna for his service thanks Simone and Michael for for being willing to take this up and I look forward to the reinvigorated uh group and uh there's a some of some of you will have seen there's a chatter uh revision going on uh trying to come up with the ideas for the next few years of the group and I look forward to the research to come thank you Colin um I want to say uh that it's wonderful to have folks joining in and and and and picking up uh the chatting I want to ask Simone if she wanted to say a couple of"
  },
  {
    "startTime": "00:08:01",
    "text": "words foreign I just want to say hi for those in the room that I know from the past and yes please read and comment on the charter that um I circulated on the list yesterday and I think we are ready to get started um um thanks Simone uh I I want um to um I just want to say that it's wonderful to have more uh chairs more people to help support what we are trying to do and we're going to start with doing a re-charter of the research group especially in light of the new work that's happening in the ietf around the condition control working group and so on we thought it was a good opportunity and time to think about exactly what it is that we want to do in the research group so yeah focused on that um uh we you'll definitely hear more about that please stay engaged on the list and elsewhere as you see opportunities to chime in we would love your input uh and we look forward to it um the charter is the first place that we start but you're going to see from there us trying to build a community us trying to gather more people uh to to form a true research Community around um around condition control including Academia including practitioners people who are actually deploying stuff um so we want all of those folks to be engaged so look forward to your contributions going forward and please chime in on the charter when you see it but uh Simone what do we have today um Let me show my face again so we have um we're having a bit of a problem with"
  },
  {
    "startTime": "00:10:00",
    "text": "the first speaker um so I would suggest that we could start with Michael Wetzel um if you don't know him then you are probably wrong in this room so we can start with him and run through the agenda and I try to see what what's happened in my power first speaker so yeah Michael you're up okay cool hello everybody and sorry for the slightly silly title um but the point is that this is a bit more you know there is a bit more of a point to it than it may seem okay um next slide you know first of all uh the reason I do this the reason I present this is that in the world I am from which is the world of Academia when I try to work on performance Improvement I get no more money there is no more funding for just making the internet faster the results the other side of it that when I talked to my 13 year old daughter about that I'm improving the speed of the internet she says why and I have no good answer because it just works well enough for her it works well enough for many things so I'm trying to do you know to see more of a point of what I'm doing and uh had this idea probably if you do congestions all right there is some game to be made in terms of energy saving and it turns out that this is true that's what I'm trying to present here so first of all uh energy saving for the internet the first thing I did is try to I tried to understand how much of a point there is anyway all together because the internet helps us save energy it helps us safe you know CO2 emissions and so forth by"
  },
  {
    "startTime": "00:12:00",
    "text": "having Meetings online but it does also contribute it's a significant number the number is very hard to get because what is the internet you know there are many many studies there are some pretty serious studies uh but they're also old there's a hot Nets paper from I think 2011 or 12. but if you look through these studies and you try to get an idea turns out that the contribution to uh greenhouse gases is maybe in the range of 0.5 percent to 1.2 percent uh the aviation industry appears to be in the range of 3.5 so it's something like a third to a seventh a sixth a fifth something like that of the aviation industry depending on what you look at depending on which study you look at so it's uh you know it's a significant amount still and uh what is interesting about it I think is that if you would think of let's go out and change all the aviation industry that's something that is basically impossible to do whereas here we're doing standards for the whole internet when we make a big change to something it gets rolled out into our asses we can actually change the world right in terms of the internet so there is a big mileage here I think and this is why I think this is interesting another side note to consider something I took from looking at these studies of energy saving energy consumption for the internet and also the way that Energy Efficiency is evolving is that most probably it's probably good to consider the last mile consumer premises equipment and and hosts in terms of energy saving I can have a long long-winded explanation for why and what is but I mean let's not get into this so next slide please okay uh today often commonly energy saving is regarded as a performance trade-off right smartphones expose it to us as something that we can turn on and usually people turn it on as"
  },
  {
    "startTime": "00:14:00",
    "text": "a way to save energy when the battery runs low but not just for the sake of the planet because it's it's regarded as a trade-off right if you look I mean I found this explanation from the Microsoft page about some laptop going into power saving mode depending on whether it's on power or unplugged or not right so it is often regarded as a trade-off of my point here is that it doesn't need to be not not everything here is a trade-off next um yeah so that's my main point and the key thing here really is that if we reduce the flow completion time which is something we all want in this room I hope then it means better performance and it means less energy and that's very true very simply because we got you know more sleep time longer sleep periods that can be done with better internet congestion controls I I'm showing you a simple strongman example for this in the next slides please next um so I wrote this short paper which contains the detail of the details of of the test that I'm gonna explain briefly here if you want to know the details it's in there I looked at Wi-Fi I just looked at can we well how much energy will be saved by cutting the flow completion time again the point is to make it very very simple example I'm not proposing to increase the initial window it's based on just the initial window for the sake of a simple case right of saying here's a straw man let's look let's look how it behaves next so first of all in Wi-Fi there are quite a number of schemes that put devices to sleep it gets really complex with 802.11 ax that that offers quite a number of things but well it turns out that the most commonly implemented scheme is a pretty old power saving mode uh from the old 80211 standard which basically uh puts your device to"
  },
  {
    "startTime": "00:16:02",
    "text": "sleep after 200 milliseconds of not seeing any activity now there are phases of trying to see if there's no data there are also different states in the whole thing so it's a bit more complex but I think it's a reasonable approximation to think of it like that it's basically putting everything everything to sleep after 200 milliseconds um there's a relatively recent paper from infocommer 2021 where the authors looked at some Modern smartphones and they found the same behavior despite these smartphones actually being compatible to either 211 ad which also supports much more sophisticated ways of doing it so it seems that the most commonly implemented thing today still is basically sleeping after 200 milliseconds of inactivity now that plays a role when transfers are short when I have a very long transfer than 200 milliseconds in relation to the transfer becomes small right when transfers are short uh well actually the majority of internet traffic is like that packet loss is relatively rare transfers often terminating slow start or we have app limited periods so we have bursts and nothing very often and under these circumstances the flow completion time becomes a function of the round trip time of the length of the transfer like the amount of dye of pair of bytes and the initial congestion window pretty much next please so here's here's what I did um the point being that you know we have this way of sending data which is not saturating the window and then on the left and you have a way which saturates the window on the right which is shorter so that is the point list is is shorter it's faster and it's also better in terms of energy what I did is I ran a local test in the test bed a very simple short data transfer of 10 packets 80 packets 10 on the left 80 on the right uh it's a wired test pad so I just ran this with different initial window"
  },
  {
    "startTime": "00:18:00",
    "text": "values got the pcap choice and then there's a tool called Energy box that people I think from Leo University have developed and that has been used also by Spotify to evaluate the Energy savings of of the Energy Efficiency of the mobile app phone mobile app mobile application and this is using this old 200 milliseconds model but as I just explained it probably still applies and what you can do with that tool is that you can put the pcap file into it and it splits Out tool you know it tells you how much how much energy that was spent and what you find here is that the savings get quite significant right I mean if you look at it for instance with large rtts obviously right uh when we're talking about such short transfers the rtt plays roles with large rtts you can save up to something like a third of energy now this is just going familiarism into two to ten if you think of larger values if you think of more interesting strategies that are able to ramp up the window faster then you could have much bigger savings so I found that interesting next please okay that's already it uh my point is not to say we should increase the initial window but uh that there is going to be made in terms of Energy Efficiency when we cut the flow completion time and it can be significant there can be plenty of ways of doing this right for instance I came across a paper that uses reinforcement learning to update the initial window value over time you could be doing couple congestion control running from past history using ongoing connections using proxies which is something I'm a big fan of there are various ways of doing things better but um you know reduce the number of X that we're sending when we don't need to for instance congestion control I think really didn't happen because it didn't"
  },
  {
    "startTime": "00:20:00",
    "text": "matter so much but now maybe it matters because X actually produce energy or waste energy it's all together you know it's an important topic it's growing in importance I think it's probably it's also interesting because at least in my world I see a decline of the importance of just performance Improvement you know my academic trying to get money world uh where was energy saving is growing in importance and I think this is an opportunity to combine the two things to actually you know make gains in both on both sides but it's worth measuring as well right I'm looking at that as a metric basically and I think that's everything any questions um who's in the queue Martin's in the queue Martin Duke Google um thanks this is um interesting uh for sure and I wonder how you have this changed so I mean one way of thinking congestion control course is kind of a trade-off between people can this is maybe an accurate way of saying it but a trade-off between sort of speed and unnecessary re-transmission um in some respects at least what I'm getting like has this has has this given you a new way of thinking about minimizing unnecessary transmissions and the energy impact of that or you pretty much focus on completion time and waking up radios yeah I'm talking about photos on completion time I because I think that most congestion results today are pretty much try to optimize to avoid unnecessary Transmissions anyway you could try and above you know optimize towards that even more but there's probably more gain if you start doing let's say a congestion control well and one thing I'm like one thing I'm thinking of it is high starts plus right which is trying not to just blow away you have slow start just completely blow through the buffers in a way that that um uh would result in that okay well I I mean that's that's a lot of loss so"
  },
  {
    "startTime": "00:22:00",
    "text": "that's you know potentially so that that's a that's a good point High style probably could you know it could be interesting to look at you know with and without high style how much energy are we wasting yeah it might be interesting access for you to the sport thank you yeah Christian Europe oh yeah so I wonder how much this is sorry because since Christine's there first quite and are you usually can you get on the Queue hello yes hello do you is it working yeah okay so uh just one question to play diverse advocate you you are mentioning that processing transaction faster serves energy because you sit on the radio for short of time that's nice but don't we have a feedback loop there that processing transaction faster gets you to perform more transactions Maybe yeah in the long run yeah maybe yes because I mean we have seen the same thing with cars I mean that that shipping car faster if they stop less often Etc double unless you so you make bigger highways and and if you make bigger highways you said hey if I had the same traffic I had five years ago it will process much faster and I would save energy but what happens is that you get more cars yeah yeah can't say this is wrong okay so maybe maybe we should monitor that too hmm all right well that's a good thought thank you question uh"
  },
  {
    "startTime": "00:24:00",
    "text": "I I want to move on to Jordy yes uh Jordi Ross from Qualcomm yeah and I'm gonna also have this conversation connects with efforts like um coinergine an idea of um adding some uh competition at the core at the switch to help congestion control uh providing explicit feedback from from the from a from a network element um and then with the idea that you know if you look at what TCP congestion control is trying to do with the end-to-end design is basically trying to do sort of like a gradient descent and using this whole network to to arbitration is taking one round in time to get the feedback whereas you could actually do some calculations at the core of the network and uh instead of using the rtt you can actually use these calculations in network and then provide much faster feedback so any thoughts on uh you know if we really serious about sort of kind of producing energy consumption uh on opening up the box maybe and and adding some intelligence at the core to actually reduce conversion style of TCP which is as we know it's I have many thoughts and there is no way that I can give a concise answer we can talk very long in the break I mean there's just no way that I can keep this and just keep the swords on it it's like there's so many pros and cons and different design that's a big design space so but I have many thoughts about it yes I mean can be done in principle Maybe I won't say yeah I was like too long thank you for that Lively discussion I have I I put myself in in line and then I pulled myself out because I will ask you separately uh because we do need to move on to the next one thank you Michael yeah it was definitely a very different sort of talk um and it's going to be interesting to think about what we can do next year thanks"
  },
  {
    "startTime": "00:26:03",
    "text": "all right now moving on to the next um venkat Arun is here and he is on video so hey venget um thank you for for joining I'm gonna pull your slides up um and I'll uh I'm gonna be running your slides from here so just call out whenever you want to um let me briefly introduce venkat is a graduate student at MIT um and working on condition control he's been doing a number of things uh and uh I he's he's done other controllers in the past as well Copa is probably the most well-known of those but I I want to uh give it to you when you introduce yourself and get it going and to everybody else um uh please be kind be gentle but not too gentle um thanks man good take it away hi Janna thanks for the introduction I hope you can hear me I'm sorry for the confusion the daylight savings time threw me off uh so I've been working on congestion control and now I've started using uh trying to figure out how we can formalize the area of congestion control and other heuristics used in networks um in ways that are practically meaningful so in this talk I'm going to talk about a property that we identified that every con end-to-end delay based congestion control algorithm we are aware of shares then it turns out is really bad so next slide please um in the modern internet users have want interactive low latency applications but are loss based condition control algorithms don't bound delay and therefore people have developed all of these delay bounding condition control algorithms including the famous bbr"
  },
  {
    "startTime": "00:28:03",
    "text": "and they use a wide variety of methods uh some use queuing delay others use receive rate and yet others are learning based and despite the vast variety of techniques they used they all share this one common property next slide that they're all delay convergent and so next slide so what do we mean by this one common property so what what is this property that we identify um so suppose we run the algorithm on an Ideal link some we pick some constant link rates some constant delay and we plot the delay experience by the packets and it could look something like the cartoon diagram here now we call an algorithm delay convergent if after a certain convergence time the delay variation is small and what we found to a surprise that even though this looks perfectly innocuous and a very natural thing to do um this is bad it can lead to starvation where one flow dominates all of the band when multiple flows are sharing the same bottleneck queue one flow dominates at the cost of all the others next slide please um so this happens because an end-to-end CCA can really only measure the end-to-end delay and we can write the total end-to-end rtt as the sum of three components first is the constant propagation delay then there is delay due to queuing at the bottleneck that's the congestive delay and there are every other source of delay variation we call it non-congestive delay and the problem is from end to end measurements alone it's very hard to distinguish between these two types of delays and that is the root of the problem and that's why uh delay convergent algorithms are bad next slide so what are some sources of these non-congestive delay variations right well we have Wi-Fi which sends uh"
  },
  {
    "startTime": "00:30:01",
    "text": "because of its frame aggregation feature it sends packets in larger groups seller base stations have a notoriously complicated service process um end hosts in order to save on CPU costs again send packets in bursts and operating systems schedulers have their own quirks based on you know because they're not only for uh optimizing for package scheduling they're also optimizing for every other process running in the in the system and the problem is One path can have multiple of these so even if we manage to model one of these phenomena and account for it uh modeling their combination is much much harder next slide so what does this non-congestive delay look like so this is an example of the delays experienced by packets between uh a seller node in Stanford and AWS California this is from the pantheon project and if we zoom in next slide to you know a small section of next of this uh this picture we find that the delay is kind of complex and note that this is just 14 milliseconds on the x-axis which is like 10 times smaller than the RTD so the delay variation was not because of congestion because this is bbr and bbr does not vary iterate that quickly and what we see in that highlighted region is a Telltale sign of aggregation but it's not just that for instance if we pan to the left next slide um next slide so there are animations yeah if you pan to the left we see that there is another type of non-congestive delay variation and this is not act aggregation and honestly I don't even know what this is and if you know if we as humans can't figure out what sources of delay variation are it's really hard for an automated thing to do so and it's not just cellular networks the next slide if you look at ethernet link so this is the cleanest link we could find in the"
  },
  {
    "startTime": "00:32:00",
    "text": "pantheon data set and um so here you know some of the delay variation is because of congestion so the sudden drops in delay are because of bbr's rtt probe but if we zoom into one of the constant parts next slide um we see that the delay variation here is smaller but it's just as complex and significant so in cellular it was around 10 milliseconds in this case it's about two milliseconds but we have very complex delay variation and it's hard to distinguish uh how much of this delay is because of congestion and how much of this is without congestion now the question is does this matter so let's see how this could matter so Suppose there are two floors sharing a common bottleneck um and say one of them behind Wi-Fi and the other is using ethernet now because the non-congest some of the path of these two flows is different they're going to estimate the non-congestion they're going to experience different non-congestive delay and because all they can really measure is congestive plus non-congestive delay they're going to estimate the congestive component of the delay differently so one person thinks it's 20 milliseconds the other person thinks it's five milliseconds and so the ground truth is actually the queuing tell is five milliseconds um and this is going to mean that there's going to be some unfairness but that's not the surprising part you've accepted some amount of unfairness for a long time the surprising part is um that the unfairness can be arbitrarily large so the next question is can we just correctly estimate the congestive component of the delay um but the problem is every people have tried a lot of these estimators and every one of the estimators we tried is uh has failure modes in and realistic failure modes and the problem is the Internet is just too complicated and it's you know the the non-conditional delay component is complex next slide okay so I'm going to Define what I mean"
  },
  {
    "startTime": "00:34:00",
    "text": "by starvation and then show you that every every delay based algorithm we've designed thus far suffers from this problem so the first Criterion is that the when two of us share a bottleneck the ratio of throughputs that they obtain is arbitrarily large and we are not just interested in transient phenomenon for instance if flow a has been running for a long time and flow B just starts off then of course flow B is going to for a little while get lower throughput but we are only interested in cases where it remains that way forever because that's much worse um next slide okay so before I make the general claim let me show how this can happen in a family of schemes Vegas fast and gopa even though these three algorithms have very different Dynamics um they have the same equilibrium Behavior where the sending rate is inversely proportional to the queuing delay and if we look at some of the numbers on the x-axis uh for 20 milliseconds queuing delay it will send at one megabit per second if it sees two milliseconds roughly then it's in a 10 megabits per second and 0.2 milliseconds it will send at 100 megahertz per second and here you might see the problem with just the difference between 0.2 and 2 milliseconds of uh queuing the estimated it sends at a rate that's 10x you know different between 10 megabits to 100 megahertz per second and as we saw in the graphs before it's very easy for there to be non-congestive delay that's you know two milliseconds large and therefore just because two flows are sharing the same bottleneck does not mean that they are estimating the same congestive of the length they could be sending at arbitrarily different rates or next slide but is this just a problem with uh Vegas this family of algorithms could we do better um and this is where the original notion of delay convergence I defined came in which is all of these algorithms share the same problem so what so what the theorem improve is that if an algorithm is delay convergent then we can always and suppose it's delay"
  },
  {
    "startTime": "00:36:01",
    "text": "convergent with delay variation Delta so after the convergence time the delay variation remains within Delta then the theorem is we can always construct non-transitive delay variations that is smaller than a capital D such that starvation occurs and D is you know any number greater than twice Delta so the LA the smaller the Delta the easier it is to make it start so the more delay conversant it is the more susceptible it is to starvation and for many ccas Delta can be arbitrarily small or even zero so if you only need an extremely tiny amount of uh not congestive delay for this problem to occur next slide um so let me try to prove this here so I mean I'm not going to prove the entire theorem I'm just going to give the intuition so the little vertical lines so I run the congestion control algorithm on ideal links with different link rates and for each link rate I plot a little vertical line which shows the range of delays to which the delay conversion algorithm converges and we know from the definition of delay conversions that it's going to be smaller than some Delta um we also know that the delay is not going to go up to Infinity because you'll assume the algorithm is also delay bounding so not cubic or aimd then what we find is that the delays are going to lie within a small range because you know there's only so much finite amount of space between 0 and the maximum delay variation and eventually as we find different link rates we are going to find that there are going to be two link rays that have very similar delay ranges and we'll also be able to find them so that they're arbitrarily far apart so for any ratio of linkages that we want we can always find two link rays that are at that ratio or larger and their delay range Falls within the same range so why is that a problem well it's a problem because we have two very"
  },
  {
    "startTime": "00:38:01",
    "text": "different link rates that have the same uh very Sim that are experience very similarly so go on next slide so we can simply construct non-congester delay next slide such that um we add different delays to each uh each flow and you know the two flows think the link rates are that different next slide please next slide so the question is is this construction realistic I mean did I just make up some theoretical thing that doesn't matter in practice next slide so if you take a look at bbr uh BBN is a complex algorithm as many of you are aware but the only thing that you need to know for this talk is that if there is some Jitter in the network any an even very small amount of any type of Jitter Works bbr will maintain a queuing delay that's equal to its propagation delay that's because it should be C when cap for those of you who know how bbr works and the consequence of that is that if the propagation delays for two flows are different they're trying to maintain very different uh cues at the bottleneck and the the flow that is trying to maintain the smaller queue is going to start so this is an example of running the kernel version of dbr on an emulated link with 120 megabits per second and one flow has 40 milliseconds uh propagation delay and the other has 18 milliseconds propagation delay and we find that the one with the smaller delay has like less than 10x the throughput of the other flow next slide and you know we discussed how Vegas fast and Copa start and we did a similar experiment where it's a 60 millisecond propagation delay this time and just one packet gets a delay of 59 milliseconds and that gets it to underestimate its minimum rtt and therefore send it again um more than 10x different link rates and this is something I observe this is a problem I observed in Copa before sort of starting on this project on figuring out how to fix fairness and then finding out that we really can't next slide"
  },
  {
    "startTime": "00:40:02",
    "text": "so I said that delay convergent algorithms uh don't work so could oscillate desperately oscillating delay help well it could so the reason is the ambiguity in estimating the congestive component of delay effectively discretizes it and the delay convergent algorithm is going to measure the same sort of discretized delay every time next slide so instead if it were trying to oscillate the delay then it's getting different information each time and since it's getting different information each time it's effectively getting an infinitely large share of bits whereas a delay conversant algorithm is trying to find a map a finite number of bits of information to an infinitely large link rate next slide so in conclusion the ways to overcome this problem could be to find a different style of CCA design that deliberately oscillates the delay by large amounts or designed for finite link rates and in the paper we can we tell you uh have an idea for how to make that better than Current Designs or use in network support like ecn in fact and with that I can show my slides then I'm happy to take questions thank you venkat um we have Roland uh in in queue they are rolling bless kit thanks for the presentation quite interesting uh we we designed TCP Lola which hasn't been tested with large link or Links of varying link rates so it may not work very well with that but it has some kind of explicit fairness mechanism built in in order to try to achieve fairness and it does is by similar to to let's say water filling algorithm which does not I guess not um belong to your sending rate in various delay queuing"
  },
  {
    "startTime": "00:42:01",
    "text": "array equation so maybe it does not apply to your um to your theory I have to think about that my other observation is um maybe it's do you think it would help I mean we experimented with that having explicit uh information from the the intermediate nodes uh on on queuing delay go on so that you can differentiate between queue delay and non-congestive delay yeah so the answer to your second point is yes I think if we have access to explicit queuing delay then we can have almost perfect condition control um I mean and we've known this for a long time right both with xcp and RCP I think both are great algorithms if we if we could get them and as for your first point I think the so when you're analyzing Lola the thing you need to look at is if you run it on an Ideal link the perfect mathematically perfect link or any other link does the delay converge to some constant or you know small range of values if it does then it's going to suffer from this problem so even if it does not follow the Vegas fast Copa line like bbr convert this to a small set of delays if the network has a little bit of Jitter in it because it will just be C when cap so that is The Telltale sign to look for okay thanks has a question on chat uh I'll read out um do you differentiate between self-inflicted delay variation versus delay variation caused by a competing flow would a delay conversion CCS staff even while competing with a non-delay convergent CCA um so we only looked at the case where"
  },
  {
    "startTime": "00:44:01",
    "text": "the same CCA is competing against itself because that's the simpler case so since we found problems there we just assumed that if there are two different CCS then that problem is just going to be harder so so we have not looked at that explicitly but I imagine that's much much harder yeah that's a fair point but I think the the the the uh the question that's lingering in my mind there is how does this change when you have um uh when you have another uh CCA that's actually going to force the deliverations you're talking about um uh in the sense of yeah I'll hold off on my question I need to formulate it better but I'm going to move on to uh Christian who's next in line foreign thank you for this talk it's it's really nice it's a real problem and we need to address it that poem has been known in a lot of the lead but literature as to let come our problem and and it has led to mitigations like in lead back plus plus there is a mitigation to sometimes just stop transmitting to get a delay measurement on a liquid link and bbr does the same thing with the rtt probe mechanism do you have any information of those mechanisms and whether they are actually helpful so the late common problem in I think in both cases is if I understand your question correctly is that it estimates the Min rtt wrong because the flow that comes later gets a higher notion of minimum rtt right yeah that was how it was formulated right right so this problem occurs even"
  },
  {
    "startTime": "00:46:01",
    "text": "minimum rtt to the to all the flows so this is not so and the late power problem is usually a finite amount of unfairness what I'm talking about here is starvation so yes so to answer your question even with those mechanisms the problem persists okay so yes that means that we we absolutely need something else yeah yeah okay thank you Bob hi thank you um I I did read your paper it was now a couple of months ago you know just after sitcom so um but I've screwed all over it and I can deal with that on the list but I just have one question really to um see if I can get this right um when you say that these things could happen and then you said how realistic is it still not really Quantified how often it happens how often the starvation happens and and yes it can happen and you've just explained again how it can happen but is it yes so that's a good question what we have found is that there are phenomena that we know up here on the internet like just rtt different flows of different rtps that share the link which if they would happen if starvation would occur we have also found that this is not hard to reproduce at all on test Networks where you know pretty much anything you do you'll be able to reproduce this pretty much on any setup you want for instance you know there you need some Jitter but we didn't even explicitly have to add Jitter the operating system Jitter was enough to cause this problem operating systems that are coupled with the emulation but if you have not Quantified exactly how often this happens on the internet and I also think that's it's almost"
  },
  {
    "startTime": "00:48:00",
    "text": "impossible to do that I mean I won't say impossible but we've tried and the data is hard to look at because you don't actually know when two flows are sharing a bottleneck because even if they're sharing a link that's congested maybe one of the flows is genuinely congested somewhere else and I think that's also the reason why this phenomenon was discovered first on pen and paper rather than in production because fairness is extremely hard to measure on in the wild okay can I ask you for a question have we got time go ahead um well it's not sorry it's not it's a different question um and that is of these um possible techniques around the problem on the last slide is there also the category of um congestion controls but like um delay gradient where you're looking for correlation between your sending Behavior and the congestion so that you can you can um distinguish that from the non-congestive delay I think that's kind of exactly the type of thing that we'd have to do I don't know exactly how I'm desperately trying to figure that out but uh yeah that's probably the way to go yeah and I was thinking similarly you know with them work on chirping and all those things where you're actually changing the sending right to see whether there's a correlation yeah thank you yeah that's an interesting thought I think that uh venka you might be familiar with someone it's basically frequency analysis of traffic and and using some of that information to figure out if now there's I I I'll um I just say that this is uh I would personally love to see some thought put into the confounding factors right I mean starvation is possible but that's also happening in a particular in in a certain vacuum with a certain spherical cow right so uh that's that's what you're showing and I think that's that's absolutely reasonable the"
  },
  {
    "startTime": "00:50:00",
    "text": "question is how confounding are the confounding factors of reality I don't know how to articulate that any better but I think it's important because what you do see is that something like bbr like you said despite the fact that it gets even capped most of the time which is what we understand it to be it it does end up doing what it does and there are a lot of reasons why things get deployed and the question to me is how do you figure out um um how relevant the result is to real traffic it's it's a it's a variant of what Bob was asking for but uh just slightly differently formulated so for that I'd like to say so I I absolutely agree and what I believe has happened is that operators have kind of implicitly understood this not in in exactly the starvation and congestion control is the problem terms but we've deployed a ton of isolation on the internet you know from bue and so on to fair queuing to just the uh relate limiting people that the entry to the network and therefore in in the internet it is not very common that two La long-running flows actually compete on the same bottleneck when they do I think this problem happens but I think most of the internet basically works because we've prevented uh you know the the original end-to-end vision of the internet where all of the intelligences at the end hosts and we put a lot of intelligence in the middle yep that's fair and true um anyways I will I will uh leave it here thank you venkat for presenting and um I want to welcome you to join the mailing list where hopefully Bob will post your comments and uh hopefully you'll have a good forum to engage with so please join the mailing list let's continue this discussion there and we'd love to see you at uh at Future iccrg meetings um and welcome you to see on for the rest of this one uh and enjoy the uh"
  },
  {
    "startTime": "00:52:01",
    "text": "rest of the presentations uh okay so uh let's let's thank venkat again first time presenter into ICC and now moving on to the next one so we are sorry hang on let me pull this I'm going to be talking about the challenges and benefits of precisely specifying condition control algorithms uh Lenore Lenore Lenore I can't get the accent right I'm sorry uh heavens that's always fun you're not expected to be able to pronounce it you might want to get closer to the mic still my name is still in also because it was a second ago Ken McMillan who is my collaborator in this work is here too and uh we are next please uh we are trying to Advocate formal specifications next sorry hang on I don't know what's going on still have your slides on yes yes oh it is missed everything unstable connection Simon are you able to run the slides I seem to be completely out here uh let me see"
  },
  {
    "startTime": "00:54:02",
    "text": "um can you see it you cannot hear me oh yes we can see it yeah okay thank you someone okay uh Simon what I was asking is that whenever you see just a title in the slide just automatically do the next okay I had some transport transport problems so next piece oh great okay what we are trying to do is to obtain formal specification for congestion control algorithms we do it for new Reno for no particular rationale historical reasons that I cannot even remember those formal specification should allow us to formally prove some high-level properties at the model level of the formal specification level we're never talking about proving anything formally all the way down and I know that lots of people here don't like formal verification and we don't particularly either even though we've been doing it all our lives but there's something else that we can gain and that is we can automatically generate tests the truly stress test implementation against the specification and that actually buys us a lot next please next okay so what are formal specifications formal specification we mean and ambiguous description of the protocol and that will be used as a reference to what does a protocol do by doing that we'll end up clarifying the intent of the protocol and all the"
  },
  {
    "startTime": "00:56:00",
    "text": "hidden assumptions that are sometimes hard to find in the natural language description next please I mentioned the specification based testing so we now have the natural language documentation we have the formal specification we have implementation in the wild and we somehow needs to show how they relate to one another so the specification-based testing allows us to show that our formal model Fades what's really being done on the implementations so it allows us to expose conformance issues with implementation versus the specification itself for for example is that you know in SSL there was an attack with that the version negotiation wasn't tested and therefore was later attacked that will not that was kind of sub the conformance testing should avoid it also exposes errors in the and some ambiguities in the rfcs themselves and of course it also allows us to expose errors in the specification formal specification and to change it to um and to correct it next please next we have done it for a quick several years ago and in quick we have um the uh the quick connect the UDP with the application layer and what we could prove formally prove the property on the on the model where we take the quick client in the quick server and the udps that interact below that is that it really implements the functionality that it satisfy the guarantee that of the user level that is that the stream that is"
  },
  {
    "startTime": "00:58:02",
    "text": "sent is a stream that is received next please we also did refinement testing this slide only showed the refinement of the server it could work for the client too where we take the client specification the formal specification of the client with a real server we generate packets from the clients automatically by our test generation and we take the server packets the incoming packets and test them again specification whether they follow the requirements so we cast both ways and by that we check the refinement that every behavior that we're getting with our formal client and real server follows a behavior that is allowed by the formal specification that's what we mean by refinement next please and next and as expected we found numerous problems that is only to be expected because it never happened to me that I looked carefully at a protocol without finding some issues there but we found some things that were interesting because those we looked at four implementations those four implementations went to very thorough interrupt testing and yet found new problems that were not revealed by the interrupt testing so we found of course Corrections for formal model because it was partial and ah it was nice to find bugs and Nathan to be able to correct it next please oh I added here that the the most strengthened and weak and so what I mean by strength and that the formal protocol model was too constrained it didn't allow some behavior that should have been allowed and it could have been too weak that it was too liberal and it needed to be made"
  },
  {
    "startTime": "01:00:01",
    "text": "more conservative uh we of course found numerous errors with implementations as I said to be expected conformance issues crashes that were due to low level bugs and implementations but also things that um they did not conform to the specs or the RFC itself was ambiguous and um the the reason we could do it is because the specification-based automatic generation allows us to generate much more General stimulus than you would just with interrupt testing and I um should say that okay next please the surprising things is if we found some security vulnerabilities this is surprising because we never actually tried to intentionally to find security vulnerabilities and for example we got denial of surveys in the client migration because of some issues and implementations and then the RFC in some issues and implementation little hardly like of leaking memory of disclosing memory that shouldn't have happened but I should emphasize that we just found very strange runs and it is the community and question is out there in cyberspace in particular that really helped us understand what was going on and understand the nature of the things we saw because we on our own could just tell that this looks wrong but we needed others to tell us whether it's or at least I am not a native uh networking person next please so we want to do the same with the urano and it's really really hard one we don't really understand the protocol I guess I mean we I don't really understand the protocol the lots of little behaviors there that I don't understand there are"
  },
  {
    "startTime": "01:02:00",
    "text": "Chanel but moreover in quick it's a transferred layer it's sitting on top of UDP it's very clear what are the assumptions from the network are but here to this to look at congestion control we actually need some Quantified model of the network and we just heard a talk why it's really so hard to get but without knowing what the network is we can't understand the properties but anyway we can't understand what the properties of congestion control are and being here for a couple of days it became clear that one of the things is for different network models you may have different requirements for congestion control see I listened uh gory is here nodding and but we need those like to know what they are before we can do it next please so we are asking you guys at large in the Cyber in the rest of the meeting to help us come with reasonable motto that that you can agree on some and some quantitative properties that then we can try and to prove formally and to test for conformance next please I said that it's hard for us to understand you we know and this is because we either look at the specification and that the rfcs and some papers that are written about it and as it's like returned from exponential backup into slow start can generate some really weird Behavior at least what's written in English and some steak machines that we saw can we reverse from the Linux implementation I guess we can but would actually dealing with like the specification to be where to go and not"
  },
  {
    "startTime": "01:04:00",
    "text": "to resort to reverse engineering implementations next um then as I said the quantitative model of the network um in Quake again the the functional model was really easy here we don't need a functional model here we need a qualitative model and that is sorry quantitative model and this is something that yeah we can make it up we can make one up others have been made up in the literature we can make one up but what is this good enough I mean it's your judgment it's not ours when there's a model we're working on the quantitative model good enough so you accept it and accept the proofs and we can't Define quantitative properties without the model next please next please um as I said the properties of new Reno in Quake we have a very simple functional guarantee so the transport layer and we can build it on top of that next please but the congestion control guarantees are much harder to work and we have very ill-defined networks next please there has been prior work on this but that assumed sort of an ideal aimd and we renoid the increments are not constant the multiplicative decrements are not constant it just doesn't fall in if you define properties like some properties have been defined in the literature based on those constants one those constants aren't there is no way to interpret those properties and those studies that we have looked at ignore timeout and the timeout is where we see really weird Behavior just looking"
  },
  {
    "startTime": "01:06:01",
    "text": "at the behavior on paper or in our pseudocode or under specification next please so why are we here to ask me help and to try to convince you that what we can do with formal specification is really worth it you actually buy a lot and also formal verification but much beyond that and we need help in understanding our congestion control protocol it doesn't have to be neureno just one that is really used that is interesting to understand the properties and how they relate to the Assumption from the environment from the network and possibly the properties as a function of the network and next please so I was trying to convince you that formal specification is truly beneficial at the risk of repeating myself yes we can verify and you all hate it but proving Key Properties the very top level of if you have some properties that you know the high-level model satisfies and then you know that the implementation conforms to the model you will not be able to establish Normandy that is really that's really a rabbit hole that's just not going to happen but if you have good enough testing that will convince you that the implementation conforms to the specification and that the specification has some nice properties that should be convincing that the implementation has the same nice properties so in our humble opinion that's ioho um the formal specification is really important and useful"
  },
  {
    "startTime": "01:08:01",
    "text": "but to make it work we need to get definition of the network of the environment in which a congestion control algorithm works and then the property it is supposed to satisfy as a function of the environment it operates under thank you thank you Lenore uh we got Bob and gory in the queue and I'm going to close the queue after that Bob hi um I think it would be useful if we knew how networks what um how you would model a network and we don't um but even if we did I think I think we'd have to constrain it to certain types of network I remember talk I don't know how many years ago a couple of years ago five years ago maybe um where someone was showing that um for instance a radio network scheduler actually changes its Behavior depending on what you do and so you're not going to be able to model back because that's Secret so so the more you push it sometimes it gives you a bit more and then it stops giving you a bit more and and those algorithms are not um they're deliberately proprietary so it's not a the network environment isn't independent of the um congestion control so these are the proprietary algorithms that change the environment as you go yeah yeah okay so maybe we should stay away from them and move to something that we don't have to but that's that's all mobile networks that cut out straight away yeah Gotti so Gary Ferris speaking there's an individual that was super I really"
  },
  {
    "startTime": "01:10:00",
    "text": "enjoyed it thank you the thing I take away is we don't actually know the questions you're asking so we can't dance to them because these questions are hard maybe we don't want to model for the network maybe we want a model for what we want the network to respond to maybe we want to know where the corners are because I really go with Bob that real Networks even local DSL networks have lots of intelligence and their things very very hard to predict but we do perhaps know where the sensitivities are and how to design against them and we could test that that would be super so it'd be really nice to write them down wouldn't it sure let's talk I mean yeah yeah so I'm leading to the rest of the group really for this I think this is a discussion we should have on the list really good with the ICC IG and see if we can try and understand a bit more in this space great top thank you um yeah thank you for the comment gauri Colin do you have a comment as a chair as yes please come on in that's it hi Colin Perkins sorry uh chair over right um just a very quick advertisement there's a uh some discussion about bringing up a uh creating a new irtf research group on the topic of usable formal Methods at the side meeting on Thursday lunchtime in um I forget which room but it's on the side meeting list uh please do come along if you're interested in this topic thank you so much Corey the mental note that I forgot to read and I had in my brain was in the end of the talk to say there is this meeting and I'm sure calling those all the details thank you for that um Ian I'm gonna ask you to take your question offline but uh thank you that was a great presentation and discussion um we've got uh we are running way behind on the agenda so you may have noticed we have 19 minutes left and"
  },
  {
    "startTime": "01:12:00",
    "text": "about 35 minutes of agenda time left uh to go through which we obviously can't do so ingamar do you want to come and you want to do a presentation can you can you shorten it if you could that would be very helpful Marcelo is that good for you I don't see you um the option is to do both of your presentations or to choose one of them I'll leave that up to you all right go for it anymore okay um yeah so I'm having all kinds of trouble here uh Simon can you uh bring English presentation up here stop sharing first and then start again yeah for some reason I keep dropping Network here government okay you can go directly to the second slide right and uh what's the first on uh going to describe a bit about how the l4s implementation is done in the radio Access Network that we had on display on the at a hackathon this week and then on Monday as well and uh you probably know a lot about this and we can see we have an application sort of an application clients and you've got the traffic either in Uplink or in downlink or both and what we do actually in uh that is that we do the congestion marking regardless if it's upling go down they could actually do it and the radio Access Network in the base station and because uh the protocol stack in the radio is encrypted by below the p2cp layer we actually do the marking on the PCP layer I know but we can take information from lower layers and use that as an input for the marking and we typically compute some kind of a mocking probability that we update uh on a regular at regular internals so"
  },
  {
    "startTime": "01:14:00",
    "text": "there's nothing much about the intention with the forensic culture that you you get congestion marking just before you start to build up two or when you detected you begin to build up that you and we can take the next slide please and what we do is that we uh for ease of implementation and also when the specification in 3dp standards we set up a separate a separate barrier for the l4s traffic that means that the user plane trans user plane functions UPF here you detect that you have a flow with a given power sample that is still for escape it's just the easy ect1 code point then we steal that Twitter they dedicated better but hostile for S marking and uh and the dedicated doesn't mean that this is a higher priority or anything like that than the normal mobile broadband error it's just as a dedicated that makes it easy to congestive marketing because you uh steel Alfred traffic into that and you can keep the latency flow as long as the l4s flow complied to a congestion control that is Alpha escapable so I believe you can take the next slide here and we and uh um press one important thing is that you do uh the congestion control for 5G networks and here we have an example of the resource allocation that means that you have a resources in time and foreign that are active are data to transmit in this case I try to illustrate the other number of user first you have two users and then another one you own some wants to transmit something and yet another one and then one user perhaps drops off and you get more resources the complicated part here in terms of congestion control is when you have a a"
  },
  {
    "startTime": "01:16:02",
    "text": "new user arriving either you get your hand over from a different cell or not just start to transmit something and you have very little Grace time when you start to see that another user and saw enters so when you when you start to see that the trooper dropped the navigation need to drop the through the transmission array quite quickly in order to get the increased skill and another that is not the only thing is that the actual throughput that you get is that is uh depending on the number of resource blocks you have and then the transmission congestion conditions you cannot pause loss due to uh distance and uh in higher frequency bands you can have a trees that are in the way that reduce the the number of bits that you can transmit transmit per resource that you have and that means that you have kind of fast fading and slow fading and throughput varies sometimes to pretty often fast trading is you know in a master of milliseconds slow pain and that is more like when you go around move around the corner and and uh and depending on the movement and things like that um and on top of that you can have a difference in the actual throughput depending if you have a losses on the Mac layer that temporarily reduce the tube throughput and and uh Dr rough part of it and we can take the next slide actually and depending on how much time I have I'm not very much I believe we can take this slide on look one question that is uh up uh should we use alpharettes for everything by heart I think we should it's a it's a really good thing to yourself right but there are some uh consideration to think about is that we"
  },
  {
    "startTime": "01:18:00",
    "text": "have as we have a fast fading but in particular we have a radiation that goes up and down is throughput on a very short time scale so if you want yourself rest and avoid a cue in the life we actually need to transmit the bit rate that is on the bottom line and to avoid getting queue delay spikes if you buffer up traffic you're able to get a higher link through but what we get the larger variations in Q delay so there is a trade-off you want highly linked utilizations or small Network buffers and some somewhat lower so I know one thing is that this generally more efficient to transmit larger chunks today they've got both the data in above the transmit it's a more efficient radio wise to transmit it and referring back to Monica's presentation you can save some battery some energy and and the other side usage can increase the overall capacity in a radio National every data to transmit and also things like a multi-user Mimo that generally works better if you have the data and the buffer to transmit so there's a these are concentrate considerations and I believe if you have a really latency sensitive traffic interactive tradition then of course you should use self rest but if you have a things like video and demand but also the second buffer in the clients then it could be better to consider using classic the traffic instead and one possible Middle Ground even even though the issues that's been in Cut here this place is that uh can consider and from the rate based algorithms even though they are not perfect so there are some some human Trader things here so not sure how much time do I have water cut here or take the discussion offline later on or I'd say a good discussion offline I do want to get to"
  },
  {
    "startTime": "01:20:00",
    "text": "marcelo's presentation today yeah I don't believe it will be around here uh the entire week to have our questions around this time please do yeah all right well reach me thank you Mark um please yeah please reach out to England or of course I want to engage folks to use the list as well and continue discussion there yeah I hope I can manage to get the main points at least thanks again Marcelo Europe which one do you want to do speaker's choice um Simone can you get the leg button DVR one up okay from uc3m I'll try to do my best with the very few minutes that I have left um so I have been doing some experiments with leadbat and leadbat plus plus and bbr as you know both let that box plus and DVR are congestion control algorithms are being specified in this in this research group so let let that plus plus is uh let's be invested for transport right which is targeting to get uh 60 milliseconds Q okay Q delay DVR on the other hand is a best day for transport that is aiming to work as close as possible to the optimal operation point which is essentially without queuing delay right so you can see that some things here may not go as well as planned next slide so the the experimental setup that we did is essentially this so we have two servers one window servers running live.plus the R1 and Linux that is running cubic or DVR your version depending on the experiment and we have a client which will configure with a large received window so flow control would not affect the results next slide"
  },
  {
    "startTime": "01:22:01",
    "text": "so this is the first experiment so what we have done here is we have one level plus plus flow one one flow and sorry one PVR flow a b vrb one flow right and we have done a series of experiments using different base rtds right what you would expect in this case is that since laptop plus plus is a less than F best less than best for flow and bbr is the best effort flow is that let that plus plus yields in front of PBR uh but we don't observe that for all the possible rtps right we observe that that's true for rtts that are larger than 60 milliseconds however for rtps that are smaller than 60 milliseconds we can see that if they share the flow the the capacity even fell if you want so essentially what what seems to be happening here is that bbr has a flight size cup that is essentially one bdp so when you have smaller rtps right is the is the bbr a flight size Gap that limits the amount of extra data that the DVR flow can put in flight and hence the rest is seized by lead but up to reaching the 60 milliseconds Target queue right uh we have done a few other experiments confirming that this that this is essentially the case so so in the next three slides and so move to the next slide we have a varai the the rtt for the for the bbr flow but keep the same fixed rtt for the netball flow we observe the same behavior this means that it only depends on the bbr r rtt next slide um the result do not vary on the capacity so we test for different capacities and we we always obtain the same same results for the different rtps next slide please and if you have several flows of each what we observe is that no matter how many BBA how many lead blood flows we"
  },
  {
    "startTime": "01:24:00",
    "text": "have to a mix they always the the aggregate of all the uh laptop plus flows capacity for all the level plus plus flows is still more or less the same however when you start adding bbr flows each of them eats one bdp more so essentially they push out the leadbat plus plus flow next slide so all these previous experiments were performed using a large buffer able to sustain a 60 milliseconds Q if you reduce the the 60 the buffer so you cannot reach the 60 millisecond queue right what we observe is that the point at which a level plus yield is essentially the size of a buffer right the the sorry the rtt which fits in the size of the buffer right so for rtps that are smaller than the size of the buffer we have this this equal split and for rtts that are larger than the size of offer we find that leadbot plus plus yields next slide so with these similar experiments with bbr V2 and interesting enough the situation is worse with bbrv2 right um in this case with DVR V2 the DVR flow for small activities actually completely yields in front of lead by plus plus okay so next slide so what are possible solutions if we want to actually achieve that level plus yields in front of DVR so what what we did is because we have a lab plus plus implementation working what we did is simply change the level plus implementation to define the target to be the minimum of the current Target 60 milliseconds and the base rtt this basically will imply that level plus can only generate a q of one a rtt or one additional rtt if you do that that actually works right and and you obtain the the results that we can find here that that essentially a level plus"
  },
  {
    "startTime": "01:26:01",
    "text": "with this modification yields in all cases and that that's that's one part so the second part is we also find some interesting results when we were looking into how these two uh congestion controllers measure the base rtt so both these congestion control algorithms perform periodic slowdowns to measure to accurately measure the base RTD right the thing is that they do not do use the same approach to do the Slowdown right so bvr performed periodic slowdowns with a fixed a period right of a 210 seconds right however lead but plus plus performs periodic slowdowns but the time of the next Lowdown will depend on the time that it takes to ramp up back round back up again right so depending on the size of the of the of the of the window that you want to restore you will it will take more or less to to perform the leg slowdown next slide please so this is one experiment one Trace from one experiment and basically what we're showing here is that these uh slowdowns for the different bbr and let that plus plus are not synchronized they are they do not synchronize they do not slow down at the same time and this is essentially the results that they miss estimate the basicity and that implies that they consider that and that implies that they are aiming to a different queue and this implies that uh they will they will they will try to send more aggressively than they should okay so next slide so of course suppose possible solution is for to make both of them to use the same slowdown mechanism right actually we believe that that we they should be using the the if they need to choose one they should be using the bbr1 because we think that letter the one that left users doesn't actually work in all cases I have some work out some examples on"
  },
  {
    "startTime": "01:28:02",
    "text": "how this could be the case but one possible solution for would be for both of them to use the bbr one right and next slide and this brings us to to my next thought that is basically whether we it wouldn't be necessary to refine some congestion control algorithm invariants that basically means that all congestion control should Implement some things in the same way in order not to fight against each other right if we want to be able to properly assess the base rtt then they probably they all need to do something that they slow down at the same time because if they don't they won't be able to measure the basicity accurately right so I guess this transcends a single congestion control algorithm and I mean I'm I'm bringing up whether it's we shouldn't be thinking about some properties that should be implemented in in oil congestion control algorithms in the same way next slide and with this I'm done so if you want the details you have you have the link there these are all the measurements you can find all the results there and that's it thank you Marcelo we have a minute for questions if anybody has any questions or comments okay it's gone when you say that they that all congression controllers should implement the same algorithm to to do the Slowdown do you mean that they all need to synchronize in time to to do the Slowdown or can they do that at different points in time what I'm saying is that they should try to do synchronize and slow down at the same time if the if the point is to enter your Q they need to always slow down at the same time one way of achieving this is by all of them implementing the same mechanisms there can be over more uh a relaxed way of achieving that right finding some some form of conditions that will imply that all of them slow"
  },
  {
    "startTime": "01:30:01",
    "text": "down at the same time even though if they don't do the same thing right but if we want to for instance measure the base rtt we shoot all of them try to enter your Q at the same time because if they don't you won't be able to enter your Cube and maybe even do it at the same period right yeah that too so there's a question in chat uh from Ian uh which implementation of bb or V1 and let by plus plus are these and so uh the bbr we want the one that it's in in the Linux implementation and the the laptop plus plus the one that's in the window server the Windows Server one okay um all right well thank you for that oh gauri you have a question last question you get to to the last one okay I have two questions the first question is to the chairs we decide to see more chairs at the front can we have more chairs in the room next time and to the I'd like to have better connectivity to my laptop in Guru next time too good on the uh on the table um when you say do it at the same time do you mean at the same time where they have vastly different rtts how does that work um or we can eat we can talk and eat right uh I would think that if the period is similar they should they should they should uh it should work out I mean and what bbr does is it says it's at least two rtps and at least 200 milliseconds The Slowdown so that they overlap that that that's that that would be my my take but we do have some tasks which are much much longer than that and they are more problematic right well but that would imply that that even two bdrs flows wouldn't actually synchronize that's what that's what you mean I haven't done that experiment but yeah I do want to say very quickly Colin yes we should look for a bigger room next time um and and we will do that and that's what I wanted to follow up on if you"
  },
  {
    "startTime": "01:32:01",
    "text": "haven't scanned that QR code and follow the URL with your phone that's the the people scanning those QR codes are how we count how bigger rooms we need scan multiple times um thank you and thank you for being a bit flexible as always this was the packed agenda and um we are really pleased to have had wonderful discussions today thank you everybody I want to thank uh Simone again for helping out all over and uh enjoy the rest of the IDF and look forward to discussion online thank you folks foreign did everybody"
  }
]
