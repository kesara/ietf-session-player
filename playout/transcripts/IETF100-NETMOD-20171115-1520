[
  {
    "startTime": "00:00:52",
    "text": "study yeah is Kent in the room by chance so we should get started again we\u0027re actually gonna give back five minutes when we start because all we\u0027re gonna say is the note well because that\u0027s the minimum intro we\u0027re required to do so here is our note well and maybe we can get people to start coming back in one comment about blue sheets we\u0027re gonna use the same blue sheets from last session so if you already signed it you know you don\u0027t have to sign up again but we\u0027re gonna pass them around anyway just to make sure we do it kid everybody okay so we\u0027ll leave this up for one minute and then really get started in one minute since we have so few people back from the break "
  },
  {
    "startTime": "00:03:55",
    "text": "okay now that we\u0027ve done the note well [Music] so unfortunately our sound problems are back can we still have the sound person in the room okay well that\u0027s cute I managed to lock the screen and you don\u0027t make it possible nope okay okay okay so come on hear me well this is quite close boots okay can you hear oh we\u0027re back right okay so I\u0027m going to give an update on the nmda datastore architecture architecture draft that\u0027s been through last call we fed to resolve most of the comments there\u0027s a couple of outstanding issues I am going to go over the the major changes we\u0027ve made and just people know what they are hopefully these all been discussed on the list and tracked anyway so yes working working call working with law school summary summary of the changes and then I would spend most the time these things at the end if possible and get some time back if necessary so we had 18 issues raised they will been tracked on github and that\u0027s the URL to where they are 16 of those issues that effectively regarded has been closed so we\u0027ve sent the final text or a post text back to the Ã¦lis and in some cases people have agreed to it in other cases just silence I think that\u0027s fairly normal but we\u0027ll ask people to check the latest version the draft the sort of two issues still being tracked as I open one under them is moving TRC to one one nine language and that\u0027s open just so the people who are requesting that once they\u0027ve read the latest of the draft and check that they\u0027re happy with that and then the other issue is is regarding actions and our pcs again that\u0027s been discussed on the alias there\u0027s a proposed solution here which is the same one that\u0027s been "
  },
  {
    "startTime": "00:06:55",
    "text": "around but I will cover that as well so in terms of the summary of the changes looking back and the differences the six major changes that I can track a new objective section has been added this is really like an introduction text so feel free to read that and have a look I\u0027m not gonna cover anymore we updated the document to use our c2 on 1/9 language to our best efforts so again please can that be reviewed so people are happy with it but otherwise we weren\u0027t covered any further and then the next four I\u0027ll go into more detail on one is about already metadata so we\u0027re restricting that to the confi true subset of operational one is we\u0027ve now defined configuration transformation so and that\u0027s to sort of get around the issues we had regarding inactive config and templating we\u0027ve defined a datastore data source schema I\u0027ll cover that a bit more detail and clarify the relationships between the data stores in a bit more detail as well and then finally there\u0027s this actions and our PCs issue that is sort of slightly sloping so origin metadata previously that applied in the previous version the draft of our site ef-2 all the contents of operational now it only applies to the config true subset of operational the reason we did that was three reasons one is it\u0027s slightly hard to define where the source of the origin comes from for the config force data often this is system or looks like systems I didn\u0027t make much sense we thought it\u0027d be quite hard to implement or difficult for for implementations to actually track this and provide this they wouldn\u0027t want it and the last one that probably was the winner for us was that we have quite a simple encoding that we want to use for the origin metadata which is sort of hierarchical that if you don\u0027t specify the origin for a given node it inherits from its parent so it picks up the same value so in the mainline case that if your or your configuration has come from your intended config you could mark the top node in the tree is intended and that recurse down and hits you then only had to put the extra annotations into the tree where it\u0027s not from the intended config where it\u0027s come from some other place so and the aim there is that this encoding is reasonably lightweight similarly if you have if you can\u0027t support origin metadata again you can mark the top-level node as being origin and it\u0027s unknown or not so I common what it is it might be unknown and affected that recur Sall the way down the tree so again that\u0027s quite simple to do configuration transformations so this is the new text that\u0027s wrapped on this slide some reason and effectively is defined as the addition modification or removal of configuration between the running and intended data stores examples of this are in active configuration and templates expansion so the aim here was to twofold one is to take out things like templating and inactive from the nominative normative "
  },
  {
    "startTime": "00:09:55",
    "text": "text from the document we didn\u0027t want to actually be defining either of those we just wanted to use them as example so by making this change they now effectively just become examples of what config transformations might be intended is now defined we\u0027ve tweaked that to be defined as being asked these conflict transformations it always was before that\u0027s now become more explicit and we\u0027ve said also that running allows you to have configuration there that has doesn\u0027t yet have these transformations lie to it so you might still have to process those right I\u0027m just going to pull out the data source picture to remind people of what it looks like as I talk about what the meaning these data stores are she about running intended and operational there\u0027s one change to this diagram is that know when we discussed this with the ITRs folks the last ITF he turned out the dynamic data stores only needed to be configuration so again those are now classified as dynamic configuration data stores at the moment I don\u0027t think this architecture necessarily constrains that but that\u0027s the use case that we have today so datastore schemas and performance some of the discussion points that came up was to get more clarity on the exact definitions of these data stores we try to do that and so we\u0027ve pulled in a definition of what a datastore schema is so this this has come about that we\u0027re using the term schema it\u0027s not actually defined in RFC 791 schema is so our way around in this document was to define datastore schema as the combined set of schema nodes for all the modules supported by a particular datastore so we use this and refer to it in yang library and it\u0027s effectively what common sense makes you think it is so one in sort of interesting point here is that has come up again in largest document I think on schema mount is that some of these terms aren\u0027t really defined in a brilliant way and it would be quite nice potentially to define these terms like schema by datastore schemes defined here and this document might be a possible place to put that in for those definitions in as long as we could actually achieve that quite quickly and not delay this document so that\u0027s a question I think to the working group is whether we should try and try and resolve that should we try and add those and those definitions in define what a schema is define what data source schema is a continent what are the ones who are data tree might have been another one there\u0027s a few of these terms that we also to understand I think what they are but we don\u0027t actually define them anywhere anyone has any opinions on this Oh baby excuse me John easily identity I really would like to see all those terms to speak closer to the microphone please sorry I would like to see all those terms defined in the document there shouldn\u0027t be any kind of ambiguity in my opinion okay so the there\u0027s no ambiguity in this document at the moment it\u0027s more "
  },
  {
    "startTime": "00:12:56",
    "text": "actually the schema mount one that has the ambiguity so the idea is we put those references in here because we can\u0027t put them into 7950 as this has been the next best place rather than defining and the schema map document I would say as long as you can actually determine that there\u0027s that link between the two documents to find that definition that would be sufficient but to leave them undefined I think is a bad course okay thank you so in terms of this conformance it\u0027s worth pointing out that we there\u0027s two rules to the NMD nmda conformance again this came out from discussions one is that all conventional data\u0027 stores must have the same data store schema so that says that start up running candida intended all have the same schema you can\u0027t change them you can\u0027t have different features in them you can\u0027t have different deviations in them and that allows configuration to move between those data sorted in an easy way it does also limit the scope of what those configuration transformations can do and what they may be because you know that whatever their whatever modifications they are making it\u0027s still retained within the same schema when it gets to intended and then we allow operational to be the superset of all the sort of configuration data stores so all the schema other configuration data stores but we allowed node to be removed by deviation so and modules to be removed from that list so the aim here is to effectively try and try to be realistic as to what what informations what implementations might be able to do and be realistic that there\u0027s going to be some time in terms of migration that there be times that you can\u0027t the not all implementations will be able to implement all this operation of State statement straight away so we\u0027re expecting deviations to for an interim period on this operational state datastore updated datastore definitions are running so we said that that may include configuration that requires further transformations before it can be applied but it\u0027s always defined as being valid so we\u0027re not changing our c7 950 we\u0027re saying that running is always valid but if you consider that in the case of these configuration transformations you\u0027d probably effectively say that you had to take you had to expand them as part of that validation so there\u0027s a slight sort of cheat here and we\u0027re saying that whenever running is updated you automatically updates intended at the same time that\u0027s a that happens I can it is the same step and intended is always validated at the same point however so intended it happens after all the configuration transformations intended is always valid as well but it may change independently of running so we you had a configuration transformation that was part of the system for example and you upgraded the software you could find that running stayed the same but intended changes "
  },
  {
    "startTime": "00:15:57",
    "text": "because one of those configuration transformations change so we don\u0027t have many cases for this we wanted to keep that as open and possibility and then the final one that we did in terms of these of the sort of review comments was to try and more closely relate or tie the contents have intended to the config true part of operational to say that you can actually provide a diff like diff like operation between the two because the two relate to each other finally the definition of operational so we now set as I said before the schema is a superset all the configuration data store schemas except the deviations allow you to delete nodes and you may choose not to implement modules in operational we\u0027ve defined the term in use and in particular this is to try and say that you don\u0027t have to return data that\u0027s not in use and and hence this is to avoid the case that you have to return lots of states not relevant so if you haven\u0027t configured a routing protocol then you don\u0027t have to return any state for it you might reduce to return the flag that says the routing protocol is not enabled but the rest the state that isn\u0027t in effect isn\u0027t returned you don\u0027t have to return lots of noise we say that semantic constraints may be violated and that\u0027s the same as what it was before except that we\u0027ve now included lists keys we came up with an example of where lists keys may become violated and hence we say that\u0027s required and but we\u0027re still keeping that they\u0027re constrained this is that syntactic constraints must not be violated you have to be able to encode the data you\u0027re returning so on to the issue there\u0027s outstanding is on actions and our pcs there\u0027s been quite a lot of discussion on this between the author\u0027s and on the alias and so this is coming up from Martin\u0027s email that summarized this issue the problem is is which data store is used to evaluate three choices one one is action ancestor nodes the second one is action input output parameter leaf ref instance ref must and win statements and the third one is assembly for our pcs the question is effectively when you get these actions or our PCs where do you evaluate the parameters to those operations and this is not related to what effect those actions are our pcs are doing they can have whatever effect they like so even if you say that these actions and our PCs are limited to operational you could still have an action our piece our PC that says I want to reboot the server or I want to modify different data store so this isn\u0027t about what effects they have it\u0027s only about where you evaluate the parameters for them so on the email that were for solutions that were proposed I think in terms of the discussion that has happened on that the the preferred solution was number one on that email thread which is effectively for the moment always used this draft and MDA to define that you always use operational for this is one and two should be one "
  },
  {
    "startTime": "00:18:57",
    "text": "two and three so all three of these are always evaluated in the context of operational so that\u0027s what we wanted to find now but in future we could extend the protocols and possibly the angle anguish as well to allow these to be evaluated another target data source if that was required so in the case of actions and RPC it might be the case that you would add a datastore parameter to those RPC operations to like to specify different datastore if we were to do that I think you probably need a yang statement to also denote which data stores you can actually allow that action or RPC scent to apply to as well so but isn\u0027t that it sorry isn\u0027t there also an option just to make that part of the RPC definition the rich bit which datastore it operates on so if you but if the current definition is always operational perhaps you let\u0027s say want to operate on a different datastore you can have you could end up having one that says fig Seaways operate on config and and so you can still do that now that\u0027s allowed because it\u0027s that\u0027s not so she allowed to do that now because this isn\u0027t about where that C operates is only about where you evaluate the arguments and so the new edit data RPC acts on whichever datastore you specify acts on but it has no parameters that you actually have to evaluate so it doesn\u0027t matter where you value 8 it and it\u0027s saying you evaluate against operational that\u0027s fine and you\u0027re okay with at the moment yes cuz I don\u0027t think there\u0027s as many examples of our pcs or action statements that this actually matters for okay there\u0027s one case that we thought that might might be useful but I don\u0027t think it was worth delaying this document now to do that I think that work is not important enough to delay this I think you can do that as a subset update it it\u0027s not clear that you need an update right correct so it\u0027d just be good to make sure that that\u0027s adequately covered and I certainly will read that as Shepherd to make sure I think it\u0027s adequately covered well I think well the document doesn\u0027t say now that\u0027d be useful is is that this last point here the effectively is that this we expect that this could get updated in future so implementations I was I thought you\u0027re gonna say that it doesn\u0027t cover this alternate use and the evaluation in the context of operational while the operation oh yeah as long as that that\u0027s in there I think it were covered okay I think it\u0027s nice in there now I think you should that be that could be clearer yeah okay clarification question of course the rib eye to us dynamic data store as our pcs in it this because it\u0027s in the ephemeral datastore does it mean it\u0027s it\u0027s gonna be workable under this "
  },
  {
    "startTime": "00:21:58",
    "text": "solution probably so the issue is not about where the RPC is acting the issues about any parameters that\u0027s right so if you\u0027ve got parameters that have to be evaluated against a different datastore then that would require future work that require new work but if the parameter was and so an example be if you had a leaf ref to a given node if you had an action RPC that says I want to check the existence of this node in the dynamic data store then you can\u0027t do that you can only check the existence of that in the operational datastore at the moment with this draft so if the our piece let me get let\u0027s go one layer down on that so the RPC adds a route the route adds a next top the next top and does a leaf ref into a different data store but the top RPC only does the next top so you can add you can add the data where if you like that\u0027s not a problem that\u0027s its side effects the side effects could be anywhere it\u0027s only the input parameters okay just looking at those if they had a leaf ref so if you had an action statement that said I operate on this node then that node would had to exist in the operational state datastore so just the top level load if if it\u0027s a route with the next top but if it\u0027s a route with the next stop and they interface then we\u0027re in problems because the interface is in the configuration data store if if he\u0027s checking that as a parameter so if part of the RPC actions things input parameters have a leaf rare for instance ref then you\u0027d have an issue yes that\u0027s all we yes we should look at that carefully words yes we have something from Martin which i 2r s drafts has those are pcs su okay so su says it\u0027s the rib draft Martin so su says hi to Jeff by the way so I think that\u0027s otherwise they\u0027re all these shoes on this draft and results other than that last one so effectively I think that our aims to complete the working people last call as soon as possible I think potentially worth checking this rib case so Tim Kari Nokia I just wanted to be sure of something because as I was thinking as you\u0027re speaking I was I was thinking through the that that problem that you\u0027re talking mainly in the little please yes so if you if you have an action or an RPC right that has an an argument that you would want to target a a module a plug-in that doesn\u0027t exist it\u0027s not going to be in the operational piece but you expected it to be in in "
  },
  {
    "startTime": "00:25:00",
    "text": "the intended or take advantage of the intended or one of the config data stores are you saying that that\u0027s not possible it if if it was an action statement it\u0027s under a particular node then that node has to exist in operational for the action writing but if I have a pluggable element that\u0027s not going to exist in operational write because it doesn\u0027t exist correct and then you would not be able to xq the action okay or or it goes down to this bottom step here you say well we we need to specify this work we need to define you\u0027ll be able to specify or detonate the data store yeah you do action to is that is the proposal is you define a new action that has a datastore as a parameter and that datastore would would dictate where you process the argument so we we know how to do it we just not sure we need isn\u0027t going to be care when we do these right you know where you\u0027re saying if it\u0027s yeah disappears the operational you\u0027re gonna have to put it into the actual action yeah hello close Christian ops et earlier there was a little confusion I think when Lou said something about the RPC on acting on something so I went back and checked my math and everything I\u0027m not sure if we\u0027re really precise and it\u0027s in the draft right now but we could use function terminology right in which case an RPC on would be the input right okay that\u0027s the domain yeah so when you say the RPC is on it would actually be the inputs the inputs the matter right so so I mean it but it is confusing because when you hear the RPC on acting on yes you know then you were thinking he meant yeah you know was it acting on configure this or that and they could be acting on anything but no in fact it\u0027s acting on the operational yeah right because that\u0027s the input to the RPC yeah I don\u0027t know all right comment from jabber Martin says these are pcs I think referring back to Sue\u0027s seemed to work today since they use strings not leaf rafts and Martin are you talking about in general or the ones in the draft I think you\u0027re talking about the ones in the draft so I just your firms it\u0027s in the draft okay so I just checked the draft we\u0027re actually not looking at the interface we\u0027ve done something intelligent maybe for once and it\u0027s just an ID so I think we\u0027re all we we took away the interface reference so I think we\u0027re just in the datastore that we\u0027re a part of so that\u0027ll be good so tell Martin to look in the RPC section there\u0027s the next top ID that\u0027s what he wants to look at okay I think you actually just told him so does that mean "
  },
  {
    "startTime": "00:28:00",
    "text": "effectively we\u0027re happy with this conclusion in which case I think that we can make the updates that do to the draft to cover this which minor and then we post it out to the working group just to check yeah I think you can make the changes unless you have some question on the language if you have question on the language send the language to the list and let\u0027s close on it once we have the next version the updated version that captures this change I\u0027d like to I\u0027ll do a read through a shepherd but expecting I don\u0027t think I\u0027ll find anything would then do another last call okay because we\u0027ve had a bunch of changes that were result of the last call I think this week we\u0027ve hit the the threshold for doing another one and then we\u0027ll publish it or submit excuse me we\u0027ll submit to the is she they get to decide what it\u0027s actually published okay thank you very much I just want to request people to try to get on either pad and help us take notes hi I\u0027m presenting a draft that Martin and I have been working on Martin is remote so I\u0027m sure he\u0027ll chime in if I say anything he doesn\u0027t quite agree with which hopefully won\u0027t occur so we\u0027re talking about the tree diagrams document and as we\u0027ve talked about in the last couple of meetings we have tree our note annotations for trees show up at the description that show up in a bunch of different documents different RFC\u0027s and they don\u0027t know they\u0027re not always aligned and the trees are an important part of our documentation process to help make these long modules more accessible and it\u0027s important to keep in mind that even though we have tools that produce our trees the product of trees and the the target of trees our us people reading the documents and more importantly the next folks who are going to read the documents and implement them but it is a documentation tool it\u0027s not a tool that gets that that generates code or generates structure that\u0027s going to be executed by code what we\u0027ve done since the last meeting is we\u0027ve tried very hard to close off all the issues so that we might reach last call be ready for last call and we thought we had done so until we had a little discussion on the list we\u0027ll talk about that in a moment the other things that we\u0027ve done is is we\u0027ve completed all the open sections we clarified some of the open issues as well as some of the imprecision that was in the previous version notably related to mount points and parent references "
  },
  {
    "startTime": "00:31:02",
    "text": "and and we also made some changes from the the previous version in terms of how we do annotations and I most of the changes that were potentially controversial have been discussed on the list so the current representation is what you see on the right hand side and if you\u0027re tracking it carefully you lis with the discussion if you\u0027re not the the representation should look pretty familiar the biggest change is related to how we\u0027re dealing with mounted modules and parent reference modules one of the things we did is and this was as a result of looking at some documents some modules that are being produced in other working groups we gave some guidance on long trees and that ended up opening to a general discussion on lists of whether we\u0027re guidance blocks we have a document that is describing a format that was this primary purpose but then we ended up going on to giving guidelines but keep in mind we\u0027re talking about guidelines to authors for documents not for modules it\u0027s very important to remember that we\u0027re talking about this is a purely a documentation focused effort not anything that ends up in code or around the wire so so we now have some guidelines in this draft we also have 60 87 bits which is guidelines to authors and it does a couple of things which sort of interesting one of them is has a section 251 that points to this document then it has another section 3 4 with very minimal guidelines on what to do about trees and so we have our document that I\u0027m discussing that has a format definition and some guidelines then we have 60 87 bits that has a whole bunch of guidelines the authors plus a little bit on trees but still depends on this one and it seems like this isn\u0027t the optimal arrangement and that\u0027s really but in the discussion on the list so one option that we have is we remove any guidelines from this document put it into 60 87 bits well that may be nice it means that we have to reopen 60 87 bits which we really would like to close and and push out it also goes through the approach of whether or not we want to bundle everything into big documents or distribute them into focus documents that are that cover a topic completely so one option strip out guidelines move it into 60 87 bits another guideline I think this has been another proposal that I think was circulated on list or maybe it was just privately is let\u0027s just get rid of this old guidelines thing this is just turning into a never-ending process and since it "
  },
  {
    "startTime": "00:34:02",
    "text": "doesn\u0027t it doesn\u0027t end up on the wire who cares let\u0027s just put it in a wiki and then we can update it on the fly as it goes it\u0027s an interesting idea but one of the issues with that is other people who don\u0027t aren\u0027t here or aren\u0027t sort of in the know the way they know about our what we do here is rfcs we publish so how will someone who\u0027s not in the middle of the IETF use our work and know that they\u0027re supposed to follow a guideline they won\u0027t so the last option is let\u0027s put all the guidelines into the into one document in the same document that we are defining the trees change do you make a change eighty-seven bits but one that\u0027s very minor that says just four trees go read our draft so those are sort of the three options I\u0027ll state a personal preference I\u0027m not going to speak for Martin my personal preference is I think we should be doing three it sort of seems like the obvious thing to me quite honestly but there are lots of different perspectives and with that questions Rob Wilson Cisco I I actually quite like if there\u0027s guidelines to go in any documents I\u0027ve put in 68 he said you know I can\u0027t hear you I\u0027m sorry if this okay is that better just if there\u0027s any guidelines that need to go in a document I put them in sixty eighty seven bits but actually personally I think a wiki is a better place for this sort of stuff because it\u0027s constantly churning it\u0027s constantly changing and I think the guidelines will continue to be revised and then in terms of your question as to how tight how did nan ITF has learned about it you have an informational RFC that points you to the current guidelines on a wiki okay so what you would say is take the guidelines out of this document and in our document the tree document point to the wiki or guidelines six to eighty seven base microphone oh I\u0027ll put the sixty eight seven bits draft into github and point to that or something and then that can just be updated it wouldn\u0027t be an RFC and then every now and again you you you stamp it Robert Stein through the process and say okay now it\u0027s for current RFC Jim Carey Nokia so you know in other places we\u0027re doing the same thing right you know and so we use the wiki\u0027s for you know guidelines type of things because you can you can rub the stuff a little bit faster and so you know I would recommend with same thing that what Rob is saying you know move all your guidelines over into a better place that you can more quickly maintain and be more responsive to to to the work right you know as things changing get added I keep hearing through here which says well geez we\u0027ve got a busy busy busy the 60 you know the the guidelines you know that stuff\u0027s just kind of changes you guys will figure out how to how to work things that don\u0027t deal with the actual modules "
  },
  {
    "startTime": "00:37:09",
    "text": "maha Shaitan and Danny my buzzing preference actually I would agree with him blow and I\u0027ll personally think that we should go with number three but what only we choose I think we need to decide soon because we have authors that we have told that need to reference this document whatever form it takes as a reference rather than trying to cut and paste which is the original problem we began with so whatever we decide we better have a clear guideline because we have people waiting on their documents wanting to publish but since this document is not making progress their document is elder and a related question would this if it becomes an RFC would this be a normative or infinitive reference that other people have to make so I\u0027m gonna state my personal opinion I think this is an informative why because it doesn\u0027t go on the wire and it\u0027s not a best common practice for a protocol so my personal opinion is is it\u0027s an informational document I would of course the FIR to my co-chair and the ad and the isg if they want to change that and at leat whatever they come up with is fine with me but I think it\u0027s informational it\u0027s not a protocol bit on the wire loo what is the current in terms of status in the document I don\u0027t remember but I\u0027m saying Ericsson I am happy with any solution although I prefer probably three what I want to avoid this wiki that is changing day by day and I have to check constantly check keep checking what what is the current status yeah I\u0027m actually involved in like an open source effort where the standard the coding standard keeps changing and I\u0027m now going crazy and I\u0027m spending more time dealing with coding standard updates than I am doing the code and that is really annoying on distributes the daily changes to the 300 work as I am responsible for Rob they think about yank so uh Benoit so on the informational yes informational because 60 87 is informational right so same category in terms of options I have a preference for one as a contributor you know I keep telling is its industry-wide problem we need people we need to have a single point where we provide guidelines we are heard at multiple times and on the guidelines how do I create yang models etc a single place and yes it evolves the guidelines in two years from now will be different they will have some ver right so reference for one row built in again so one comment about like I Triple E my understand is that when they were looking at guidelines they\u0027re looking at RC 60 87 and not sixty eighty seven base because that one hasn\u0027t actually been published yet so they\u0027re looking at the older state potentially rather than the "
  },
  {
    "startTime": "00:40:09",
    "text": "one that the current draft that hasn\u0027t yet been published because that\u0027s what they want to reference and what to look at so again the same issue about taking a long time to update the guidelines is causing a problem I don\u0027t actually I support number two and I agree with Rob that it should be sufficient and to wash comment I think it\u0027s not really rules about life and death so it\u0027s not necessary to check these rules every day I think that just if I end at for example indicate that some rules are not followed and it might be possible to change it but I think a vacay would be a good solution in this case so I personally really really don\u0027t like the notion of big massive documents that try to do everything at once I mean I think that that\u0027s just a very slow way of working that\u0027s a personal statement as off yes co-author excuse me as well as someone who has something to do with process in here I just want an answer and so that we can push this thing out so I hope we can reach consensus on that well and I don\u0027t look at my chair I\u0027m going to for a couple comments from it let me just throw out there I know mentioned this option yet the option is we published a document with a pointer to a wiki that has ongoing work and in revision right so so you sort of can bridge the gap where you have a constant standard document you can point people at like I Triple E and yet you can also track new work quickly because it develops you know you instead of developing it in drafts or whatever you always have it on a wiki and that wiki is pointed to by the published RFC for you know the work beyond yeah I think that was the modification and someone else earlier said was take two and add that a reference to the RIC the wiki in the guidelines documents okay this is Kent channeling Jabbar Martin says that he favors option number three he says wiki\u0027s have a tendency to not be alive after some time just look at the various yang wiki\u0027s we have and Jeff Haas says wiki\u0027s also are problematic for anything that is intended to be canonical access control to them would to such an item would be very important okay so with that I would yeah how do you want to result how do we go forward as authors yes I would like to ask the room to raise our hands for the various options and when I read them out one two three please raise your hands for if you support number one raise your hand now just to be clear to is modified to that we point to the wiki in in the tree document yes with with them okay so I thought so if you support putting all the guidelines into 1687 bids and removing them from this strap please raise your hand "
  },
  {
    "startTime": "00:43:11",
    "text": "microphone please I mean there are Mohammed I mean there are other options possible so asking for ham for these options is probably not optimal so maybe we will add another how about adding a number four I mean they other something else let me state first my opinion and and the option I think should be interesting I mean I support option two but because I think having everything in one document is practically very difficult to achieve because we cannot update one document again and again where another document like this one here would be interesting to publish as soon as possible and the wiki would be actually listing references to RFC\u0027s which have been defined as guideline it can be first 60 78 abyss and then other documents like this one here and the wiki is not to write down the rules or suggestions the wiki is actually only a collection of references to RFC\u0027s okay there\u0027s a few more comments from jabber Juergen says who maintains the wiki I\u0027m not sure if we have to insert that right now but something to consider I mean okay Jeff Haase says additional option there\u0027s some sort of Ayana maintenance moves faster than a full RFC and Martin says links to the RFC\u0027s also have problems try to follow the link to security considerations and 60 87 hint it\u0027s a 404 sorry we have a couple more at the microphone but we do need to do a pulse okay make a quick I\u0027m hoping to I try to so um is there a need to actually progress a document to RFC so if you have a document and you want to change it just don\u0027t make RFC and you have changed control the authors have changed control over is and process applies so you just keep it a draft and then you can can alter it so your your option number four or five depending on how are counting is just keep something you draft forever I think that would be for the guidelines part not for the base definition so I think we have one two three four which is meant for as other is that right other sorry poll yeah so yeah four four will be other in case you don\u0027t like any of the current options so again if you prefer option sorry do we have a come into the mic okay if you prefer option one can you please raise your hand and if your own job yes I\u0027ve got a comment "
  },
  {
    "startTime": "00:46:11",
    "text": "it\u0027s almost my last a meeting and I want to remind people of what we do in the ITF in term of tradition we hum we don\u0027t vote at the idea well some of us don\u0027t have great ears and if we hum it\u0027s it means that I might hear a few people in the front I\u0027m not gonna hear people in the back so I we\u0027re not required to hum and it doesn\u0027t work for my ears full stop it me neither I can\u0027t I can\u0027t handle hums I I will not be able to judge the results as a chair if we hum I\u0027m sorry I ears don\u0027t work that way it\u0027s not a required part of our process we should take this outside we should take it outside the room that\u0027s why we always say a reasonable number a good number you\u0027ll never hear us give you the account we don\u0027t we\u0027re not actually gonna count wait we don\u0027t we don\u0027t record account we record a good number you know a few people okay try number three if you prefer option one please raise their hand so a few people if you prefer option number two please raise your hand it\u0027s a little good more or a good number if you prefer option number three please raise your hand about the same as option number two and oh and there\u0027s also two more hands on Jabbar going up for number three and then if you prefer option number four which was others and another one too it spoke of a few okay I think we have what that was the least that that was the Lea stripe so we\u0027ll just record those numbers think so well I guess I\u0027ll look to the chair to figure out how we move forward between options two and three thank you very much for the input oh they we had one last open issue I believe the plan is just to go with what Martin had suggested on email and I believe that\u0027s the last thing is his use I\u0027ll allow for the the uses to be shown and if I have that wrong we\u0027ll correct it on the list so the biggest thing is of course resolve the open issues which we\u0027ve now knows between two and three published a new version move to less call thank you okay great thank you Joe and we are running behind so anything you can do to speed up great [Music] okay so last time we met this was in Prague and we presented our proposal for "
  },
  {
    "startTime": "00:49:13",
    "text": "the yang catalogue essentially the thing that backs up yang catalogue org you may recall that we are not necessarily looking for standardization a ratification of this module what we are doing is sharing with you the work that has gone on to create a metadata repository for yang modules some of the things I\u0027m going to talk about today Benoit raised in terms of semantic version these things have been driven by use cases that we\u0027ve received by talking specifically with operators so one of the things we wanted to do is expand the use cases for the catalog as Benoit has said it\u0027s about the tool it\u0027s about the yang modules it\u0027s about the tooling and it\u0027s about the metadata and that\u0027s the metadata is what this this module defines and it\u0027s directly in support of the tooling which is directly in support of the use cases that the operators have come by so what\u0027s new in this mod in this particular revision we\u0027ve added the semantic version in the derived semantic version of which we heard about a little bit earlier today we added the ability to track a module a sub module and to whom it belongs and we\u0027ve added the ability of to track the tree type the nmda tree type in addition we\u0027ve expanded the catalogs reach into data tracker and some of you may have seen the data tracker links that are sort of the yang catalog links that have recently appeared in data tracker and we\u0027ve been able to link module explorations so dynamic ways of interacting with modules into the tooling in the yang catalog for those of you who can\u0027t wait for the movie here is the yang tree structure for both the module and the vendor sub trees just-just guidelines the new use cases that we\u0027ve come up with and we think the first one is is critical every operator we\u0027ve talked to as benoit says they are composing service modules they need to understand that given a module or given a release of code for a specific vendor tell me very easily very quickly what modules are backwards incompatible meaning which ones have backward incompatible changes and then we\u0027ve added additional tooling and I\u0027ll show you that in a minute to show you what those specific changes are both from a tree type as well as the textual so those are the first two use cases then we want to say we wanted to provide a comprehensive metadata store to drive all the tooling so as part of the latest version of the yank catalog all of the tools on there directly use this module which the which creates or or stores all of this metadata and then one of the things that Benoit has been doing and is is is trek around different stos is to be able to say here are your modules and here\u0027s how they relate to an MDA compatibility or compliance you\u0027ve seen this slide I won\u0027t spend much time on it we are algorithmically or heuristic lis "
  },
  {
    "startTime": "00:52:14",
    "text": "determining this derived semantic version that will help at least from a syntactic standpoint show users or consumers of yang modules where those modules are different so at a quick glance you can see that revision n + 1 of a given module is either backward compatible or not to revision in so let\u0027s take an example this is the ietf interfaces from this we provide a link in our API in our yang ma in the yang catalog API that will allow one to look at the tree side-by-side tree diffs as well as the side-by-side textual diffs if there is a semantic version major major semantic version difference in this case revision 10.0 of the yang interfaces module as re ietf interfaces module differs in this way from version 2.0 so you immediately know there\u0027s backward incompatible change by the major revision of major version numbers and you can visualize what those differences are the other thing that we\u0027re doing is again being able to track the full store the full metadata store in in the yang catalog directly feeding this into tools and we have an API to access it so the colorful skittles rainbow you see up there of the impact analysis is directly fed from the metadata we\u0027re collecting that metadata is available via api\u0027s so you it\u0027s linked to offer the catalog page you can consume those api\u0027s for your own tooling and again it\u0027s about that tooling and it\u0027s about that metadata that helped make these yang modules consumable as I mentioned we\u0027re also tracking an MDA compliance so given a module or a set of modules you can really doesn\u0027t like me you can understand whether or not this is a split tree or an NMDA compatible tree or something else like for example open config style I mentioned that the links from yang catalog are now being put in the data tracker so that it\u0027s easier for maybe not within net mod or Netcom for in the ops area it\u0027s easier for model developers to understand what tools are available and help them write better yang modules that take advantage of the the set of tooling that we are coming up with by consensus in this area and in these working groups and finally we are integrating newer tools into the yang catalog that will allow users and modules as well as the authors of modules better make use of that learn from the the best practices in terms of the the data that the constructs that are going into modules and be able to say given a module how can I consume it how can I create a script and test it against my devices or how can I write or take a script and test it against some automated test suite so as part of the hackathon this time around we had one of "
  },
  {
    "startTime": "00:55:14",
    "text": "one of the contributors building an automated test harness based on coffee based on neck comedy that we could feed scripts from our yang development Kent integration into to be able to test for example the examples in yang modules that they are consistent with the yang module definitions our next steps are primarily first and foremost we want to make sure that all then and all stos are contributing their modules in a publicly consumable fashion such as github and that they publish their metadata to us we have links off of the contribute page on how they can do that and we are looking at potential new use cases for example how can we do things like bundling that that Andy mentioned last time and has come up as part of the discussion around the version how can we track module expiration drafts expire but we don\u0027t have any semantics and modules to understand when those modules may expire and also we want to be able to look at the yang tree on a per module basis and advertise that via metadata and then via the API questions if we can be quick just because we\u0027re I think it would be useful to be able to specify this meta metadata in in the yang module itself so perhaps it it would be nice to have an extension so that authors of new modules can put the metadata directly there so that it can be used with other tools and it will be clear so maybe sometimes your automatic tools cannot determine everything perfectly so this would be I think useful and a nice application for an extension that\u0027s a good point so we\u0027ve defined metadata in both extractable which comes we can pull from the module and non extractable having some of these as more extractable things would be very useful yes not only a good point but the topic of the next presentation which Kristin come up we\u0027ll just get right into it and very much so thank you Joe also at the end of this presentation we\u0027re going to pull for interests if the workgroup would like to adopt this document so keep that in mind as you\u0027re watching or listen to this presentation Wow thanks for that leading so I\u0027m going to talk about module tags which is the ability to add metadata to modules so we\u0027ve had two revisions since it was last presented in IDF 98 and the changes during those two revisions were "
  },
  {
    "startTime": "00:58:14",
    "text": "to remove a per module tag list so originally we had designed it so that the tags we sort of had the tags in multiple places we had them in an augmenting library we had them in a global list and we also had them per module list the probably the per module list is it would only go in there if the if the author included it and I I liked it everyone else was like we don\u0027t really need it and so they convinced me to pull it out the reason I liked it was because I thought you could do cool XPath stuff if the tags were right in the with module but um and we also originally had add and delete RPC our PCs and that was just ill ill conceived I think because we can just use edit config and and Martineau uragan pointed out that we were doing it under on there so we remove the add and delete we do still have an RPC that is config it does operate on config that\u0027s reset so here\u0027s what it looks like now well one of the modules we have a module tags list where each entry is a module identified as it is in the yang library with name and revision and then a list of tags the RPC reset tags the reason that\u0027s there is because we allow the user to delete tags if you recall the tags can be added in three places by the module author by the vendor the implementer and then finally by the user and then the user is a in case the ultimate arbiter of the truth so the user can for example say I you might have tagged that as supporting this feature but I found out your buggy and so I\u0027m gonna remove that tag because I don\u0027t think that you support that feature that\u0027s this is an example of what it looks like in XML and the other module we have currently is an augment on the yang library because it\u0027s sort of nice to have the tag list there alongside the module this sort of it\u0027s sort of annoyed me that I couldn\u0027t just put this here period and the reason we can\u0027t is because the yang library is very like read-only there\u0027s all kinds of text floating around saying you know they think they\u0027re being discussed about when the you client can read the Aang library and count on it not changing until certain things happen and anyway that\u0027s why we can\u0027t really have it there I I mean we even thought about keeping the read-only list there and then using the RPC and run scheme so we could modify it but anyway that would have been messy so the open issues right now are should we have Oh this was raised by "
  },
  {
    "startTime": "01:01:17",
    "text": "Rob Weldon he raises to me just this IETF about should we separate the tag config from the tag list right now they sort of are the same right so you would imagine the config being there if there was predefined tags they would already be there in the list and that you could then go in and write a new list that removed them that\u0027s how you would delete tags and he pointed out that they they\u0027re trying to move away from that with nmda to have it empty right if you didn\u0027t put the configure than the config is empty in which case the way that would look is that you would have a config area and then your operational you would have operational area where this the tags that were predefined would be and your config would then add or delete on that list this would get rid of the need for the reset RPC I kind of like when they\u0027re mapped together because it\u0027s just clean looking in my head but I do see this so this is something I think we should talk about in the list and also the I think glue might have mentioned that maybe we should just remove the yang library augment - I mean why have two lists we have the the case there is again I think there\u0027s a case may be for XPath we you know because you can do nicer XPath things for selecting modules when they\u0027re right the the data is right next to each other as opposed to going down to the root and back up to into a list so and then we have we do have a list of initial standard tags so I think we should you know get some maybe more a little bit more action on people looking at that just see if you\u0027re looking it over and seeing if if we\u0027ve got what people think is a good base set so so far we\u0027ve had mostly almost entirely positive feedback on this I think I\u0027d like to ask for a working group adoption and you know we\u0027ll continue to work on the open issues but okay any comments Rob Wilson Cisco so yes I mean I like this work I think I think it\u0027s useful obviously if you stop - I prefer the solution in terms of the configuration but we\u0027ve been discussed I think that\u0027s fair discussion there but generally yes I\u0027m positive on this so Jabbar from Jabbar Martin says support and then but it would be good with the solution without the reset RPC but this is a good discussion for okay so as promised we wanted to pull the room for interest in this draft so first interest and then we\u0027ll ask for adoption okay so if you are interested in this work we "
  },
  {
    "startTime": "01:04:18",
    "text": "think it\u0027s a problem that needs to be solved please raise your hand I\u0027m gonna say a few okay all right and likewise if youth who if you would like to see this draft adopted please raise your hand the same number I think okay thank you Thanks hi Kent Watson Shepard networks presenting gang data extensions okay the problem is yang extension statement yang data in the module ITF restaurant which is in RFC 80/40 can be used to define a data structure that is not intended to be part of a data store the original use case was for restaurant resources and errors but since then it\u0027s been used to define file formats and also protocol messages however this solution has well actually three limitations not just a couple the structure cannot be augmented which you\u0027ll see the reason for wanting to do so an upcoming slide the structure must define exactly one container which is an issue that the zero touch draft is having concerned with and also creates a dependency to RSC 80/40 of the rest count draft which is kind of strange you know for instance the zero touch draft needing to have a normative reference to this restaurant traffic just unnecessary for this purpose the solution is to have a separate document that defines two new extension statements the first being yang data which is yes it\u0027s the same string but it would be a different prefix a different namespace so it\u0027s not actually we\u0027re not actually removing it from the restaurant drafter and which would be intended to replace IETF restaurant yang data so I mean there\u0027d be no need to update 80/40 immediately it\u0027s that definition could stay there you know indefinitely but if ever there were an update to 80/40 it could at that time potentially move we moved over to use this other statement the semantics would be the same but it remove the limitations that the top-level node must be a single container and the other top-level extension statement would be augment yang data which could be used to augment a data structure defined with yang data so the samokhin issues first should we also define a new statement like uses yang data that can be used just like normal uses in all places users can be used the drawback is that "
  },
  {
    "startTime": "01:07:19",
    "text": "we get two ways of defining reusable structures grouping and yang data any comments on this I don\u0027t really understand why you need this augmenting later we cannot augment it I think this came from the animal working group with the voucher draft and then separately there\u0027s a brewski draft and the brewski draft us the voucher draft defines a uses gang data to define a data structure for the voucher the brewski draft basically wants to it is a voucher but it adds in some additional fields and so what they wanted to do was to use the voucher data structure and then and then you know augment it and refine it to the needs that they had but of course what we had to do since there was no option was to instead use groupings and and and then you know have the brewski draft use the grouping from the voucher draft to create its own yank data it was only possible because we had the foresight to do so if we hadn\u0027t done that then it would have been impossible in general I would say it would be really useful to to have young so that it you can do things like this this out of the book so that yet can be we discussed it already several times before so just to be able to use yank for defining some structures that need not necessarily be in some network management protocol they just are and things like that but that\u0027s of course a long shot ok a comment from Martin augment doesn\u0027t know anything about extensions ok the second open issue currently there\u0027s no way to define special error info content for RPC or actions a gang data could be used to define the structure but should we also define a new extension statement that can be used to use in RPC and actions to refer to such a structure and the example on the screen there you can see where we have the map error info which is then referring to a yang data structure and I think this would be necessary for the SUBSCRIBE notifications or notification messages draft right yeah Eric Voigt in the neck comp working group we have some described notifications and notification messages were definitely a user of the yang data and all the elements that are that are up here so we\u0027re very much supporting this and this would help so that is the "
  },
  {
    "startTime": "01:10:21",
    "text": "last slide again there\u0027s actually two dependencies on this graph from the net comp working group both the zero touch draft and also the notification messages drafts are looking for this those are chartered working group items within the neck health working group and now this is essentially a dependency for them if so we would like to see this draft adopted by the working group and maybe the chair can ask about that supplement so given that this is extension work this is the logical place for the work so I\u0027m not going to ask the normal question of how many people think we should be solving this problem because we already just established that there\u0027s another working group that really requires it and the other option would be to kick this back to the other working group and that\u0027s not really the right place for the further work so I\u0027m going to jump right into how many people have read this draft I\u0027m sorry again how many people okay that\u0027s a very few can we can jump to adoption but well why not how many think we should be adopting this this work into the working group the it\u0027s also very few the interesting thing it was more than read the document but not by much but you know that\u0027s sort of an interesting data point obviously we\u0027ll take that information back and we\u0027ll talk about it and go from there it while we don\u0027t it\u0027s Madonna have AVI support for immediately going to last sorry adoption given that we have another working group that depends on it my inclination would be to really try to bring this into the working group because it seems like it\u0027s it\u0027s pretty important so Rob Wilton Cisco so I\u0027ve not read this draft I do think this problem needs to be solved and hence I support the adoption of brokers because this is the right sort of solution and any details can be worked out in the working group process but I think we need to move this along quickly it\u0027s right it\u0027s to slow it down it\u0027s my opinion yeah actually I normally phrase it as how many think that this is a good starting point for work in this area and I didn\u0027t I failed to do that and that would have been a better phrasing of too short Kristian hops the you could also ask how many people think this should not be that\u0027s always a good okay let\u0027s go for it how many think we should not be working on this zero so that was useful Thanks thanks Chris help from in another season chair so with that we\u0027ll move on to the next one "
  },
  {
    "startTime": "01:13:29",
    "text": "okay so unfortunately because I was doing the poll I missed a couple of comments on the last one we have from from lotto who\u0027s in the room I\u0027m not gonna read it we have from your again the scope of their work remains unclear to me and there\u0027s a little bit of a side discussion that goes on there but I think it doesn\u0027t substantively change anything so we have yeah okay great thanks thanks right so in this draft we we talked about finite state machine young model for finished state machine we considered three use cases the first one on two plain strat data playing device devices in optical networks in particular transponders in case of optical physical layer degradation so they can reconfigure their itself in case of these degradation the second use case is related to custom data probing for telemetry application and the third use cases use case is related to monitoring of packet loss and Danai and delay through a network lasting approach so let\u0027s go to the first use case at the state of the art so the left part we can consider an optical connection assume a degradation so a degradation can be revealed with a bit error rate increase above a threshold an alarm is sent to the on or on demand layer there is some transmission parameter computation so consider the change of modulation format or forward error correction yes the under control the SDN controller reconfigures the transponders and the recovery takes place so operation is can be time-consuming especially if several optical connections are interested by the degradation while we can do something different with a younger considering a young model for finite state machine while the connection is still active yes the end controller instructs the transponder on how to reconfigure itself the transponder in case of a degradation by installing a finished state machine so if the degradation happens the transponder all already knows what to do and the recovery can be faster the second use case is related to tell em to "
  },
  {
    "startTime": "01:16:31",
    "text": "telemetry so we consider dynamically customized the network data collection instead of raw data extraction in order with the name of reducing the telemetry data bandwidth taking advantage of the in in network data processing so in this scenario we can we can consider that several network probes can be described can be described as a finished state machine an example is the congestion control so let us let assume to two thresholds on the buffer death with these two thresholds we can see three regions one the first one below the first threshold then a second region between the two thresholds and the third region above the the second threshold so each region can be associated with a state of the finished state machine the third use case is related to the monitoring of performance measurements in multi-point to multi-point large networks so we considered the multi-point to multi-point as a generator case bust but this can be applicated also to point to move to point to point here the SDN controller can orchestrate the performance the performance measurements and the and it can calibrate how going too deep into into the monitoring data we can we can assume the metered of alternate marking for for the performance measurement and in with this clustering approach assume to divide the network in in clusters so if we have no particular problems of the delay we can consider large clusters but we can go when we have problems we can go in more in deeper so with smaller clusters up to going to to the flows and again here we can model we can assume a finished state machine where each state represents a composition of clusters so let\u0027s go to the model we have a list of up States for example considering the first use case of reconfiguration of transponders we can have state 0 which is the normal operation of the network then we have the list of transitions one transition "
  },
  {
    "startTime": "01:19:31",
    "text": "can be triggered by the bit error rate increase of the optical connection a leaf of this list transition list is is the filter to further express the the transition so in this case we can put a threshold on the bit error rate so that when the bit error rate is above this threshold the transition is triggered to the transition is associated an action a list of actions for example the change of the modulation format to provide more robustness to the transmission or the change of the forward error correction and then we have the the attribute execute which actually calls an RPC that performs the task of modulation format change so in summary we presented the young model for finite state machine please consider that this draft was presented in Prague in another working group it was obvious WG it receives comments to to get it more generic model and including more more use cases so we included two more use cases and we submitted here in that mod we have an implementation and we also demonstrated in an integrated test bit of optical data plane and control plane the first use case and the next step we\u0027d like to ask for for the adoption comments questions leaned in really close Dalembert DeFrancisco would be possible to add also the output of the state machine related to the state instead of only to the transition but we make it more general yes this this can be done is something that we we we were thinking about we can we can clarify this point in in a new version of the thanks thanks [Music] are these describing FSMs that exist already somewhere or are these describing FSMs that you\u0027re also sort of proposing like vendors implement or sorry Kenny can you repeat where where where are these finite state machines are they are are you trying to describe ones that you feel are already there in "
  },
  {
    "startTime": "01:22:32",
    "text": "the system or are you describing new ones no well we try to describe a generic finished finished state machine trying to to include the several use cases so we have we have one use case brought by by away and another use case by a Telecom Italia but the the model aims to be generic yeah and I mean because it there\u0027s an interesting use case of you could actually define a generic finite state machine that worked off of telemetry and then yeah I mean that would be cool thanks flew just to follow up on that but it seems that the way it\u0027s defined right now you have to have the client has to have an understanding of what the server implements and so like for instance a name has to be something that\u0027s well known by both right because you\u0027re triggering specific functions that you\u0027re trying to have the client trigger specific functions in the server right yes these these functions can be defined in for example in the list actions they these functions can be defined in some way there as well as an event can be defined in the transition the name the type the description if I\u0027m from yeah I think so I just I\u0027m a little lost as to what the client is doing that what flexibility is it really triggering you know it seems that it would have to have a representation of this the finite state machine that\u0027s up that\u0027s on the on the server and all the degrees that are supported and it\u0027s I don\u0027t know I\u0027m having a hard time understanding what what this is getting at but let\u0027s get to the line and we have to wrap up in like three minutes so I\u0027m sorry - one minute so well the part in our in our opinion I don\u0027t know if well mmm there will be the right way we can we try to make a genetic model and then inside the attributes we can define the the things that the people wants to do with this with this model okay Rob Wilson Cisco so I\u0027ve no idea whether is enough support in the room to adopt this or not it\u0027d be interesting to see but I think it\u0027s generally useful to try and standardize these models so that if you got similar functionality that multi we want to use it\u0027s nice to have one model and one way of doing it so my question is if there\u0027s not enough support in this room to adopt this what happens then I "
  },
  {
    "startTime": "01:25:32",
    "text": "mean is there another way that this could go in some of the process like maybe goes and github and but goes into the yang catalog so other people can use it well just because I\u0027ll shortcut occurs that at the time I don\u0027t think we have enough understanding in this room to do an adoption call and you know that most we can say how many people are interested in in talking about this more and learning more I don\u0027t think we we can do more than that right now I know I don\u0027t personally understand this enough to figure out whether it\u0027s a useful thing or completely you know something else so my question is if there\u0027s not enough support what\u0027s the next action not what I\u0027m saying we\u0027re not going to ask that question today in general and the IETF if we decide not to work on something it goes wherever it goes it\u0027s not for us to decide Eric just to put an offer out there I know that they Igor brisk and he was looking at something called I\u0027ve been conditioned action as a probable statement and the net kampf added to the subscription and the telemetry work there I\u0027m sure he\u0027d like to chat with you about a problem statement if you\u0027re looking to help define what\u0027s needed for this group a joint problem statement would be pretty good yeah that\u0027s a good suggestion Igor\u0027s not here this time but maybe reach out to him and see if you\u0027d be interested in collaborating with you okay egor brisket okay thank you and if you don\u0027t can\u0027t find it just send me mail okay thanks come I have busy the same questions or the same comment because I think actually that the automation framework is related to what you\u0027re trying to do here as well so I think okay I cook in the next presenter shall Jim thank you RPM model shout in MI mispronouncing I\u0027m sorry is there someone here to present the ARP gang model we have your slides so this this is something that I heard is actually being presented also in grouting working group maybe they decided that that\u0027s a better place for it I don\u0027t really know oh please go over this just now you okay a Burris lay my colleague not here and I\u0027m Michael Wong from Holly and the how to you to go through this right and if you have any comments please email choose ur out sorry okay sorry "
  },
  {
    "startTime": "01:28:33",
    "text": "okay alright go ahead captain can\u0027t hear you just yet hello okay I think we can hear you can you hear me I certain yeah yeah yes please oh here yeah it\u0027s that yeah go ahead okay go ahead little slides yeah okay post instead a problem so for the every protocol and several vendors have of their own provide young modules to implement to the ARP functions suggest cisco juniper I know are we such provider model can we applied unto their own devices however due to the different functional device providing your module is hard to use a cost thirty bodies and you cannot miss the demand old common requirements of user so in this case imagine a scenario that a service provider manicure the thousands of devices and these devices are from many vendors is must be a disaster gives the earth there is no way to control this device in the stand way so so a solution can be effects uncommon for properties within or devices the implementing the app she this master has two meanings customized provide a unique or I define for IP configuration function for example they age you so the edging in some problem your model because they time out or they expire time and away summarize a common name the aging and then the similar is the interval it is sometimes called a detector in the wall of the code is a half interval so a we we can give a Ãºnica name so the user can cannot be confused by this similar similar names and the second is the way providers the common core properties for example the static 8fg entry the dynamic a few interest and the prog proxy option ok this functions is where to find the in every protocol the next surface hello yeah so here in this graph the wisdom summarize the perfecter of epic configuring functions the statical epic entry the dynamic yearning of every "
  },
  {
    "startTime": "01:31:33",
    "text": "entry the prompt cocks they actually under the credit us eruption so we we can see the tree WETA graham the docking he know a prosthetic table mister static applicant the configuration so the leaf expat time describes the edging time of the dynamic IP entry the lift a phenom describes the weather that dynamic immunity is disabled so it is enabled by default another applicant prague proxy describes the weather of the probe got Jeremy is deserve order so it is also enabled by default and at the EPI profit grouping that describes a common configuration of our happy pop so the leaf a problem interval means the intervals of the texting the dynamic a RP entries under the leaf probably x is a number of ng an improper attempts for a dynamic dynamic happy entry the unit custom means where the use of a unique custom mode to send a happy aging proper messenger for dynamic happy entry so it is also enabled by default and finally the happy graduates a grouping describes a configurating graduate to create action so the container happy if Nimitz it is described the maximum number of time ik happy entry as Dutch interface Kenda go next surface so the next steps foster the language other models here we are measuring the comments of Alex from the demo my owner list so Alex s the publish the model IDF actually it is also called as a fc7 2007 it is Oh overlaps the data being provided in this model and then it gives some suggestion to avoid the duplication of data where perform so we are we are saying the annex for this one of our questions and we are reversed they we have we applied the corresponding the answer in the posterity on the list so and the way we already revised that chapter in the next version and here we also want to the more storage comments yeah thank you okay thank you very much our understanding is you\u0027ve also presented this work or going to present "
  },
  {
    "startTime": "01:34:33",
    "text": "this work in the routing working group do you have a feeling as to which place you think is more appropriate and I understand that that\u0027s as much a discussion for chairs and 80s but I\u0027d like to to hear your opinion hmm chef Jen would only hear you that was a question for you sorry no voice is not very good kind of let me try again are you presenting this also in the routing working group yes I oh I uh present a new day I think ewu working core and tomorrow you think it belongs that work should be done there or here in this working yeah maybe I\u0027ll talk with the chair after you present there thank you thank you very much we appreciate you giving us this presentation okay with that we are finished thank you for lasting through these two sessions and giving us some good discussions that we\u0027ll see in London [Music] [Music] "
  }
]