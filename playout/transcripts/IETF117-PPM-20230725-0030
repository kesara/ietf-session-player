[
  {
    "startTime": "00:01:11",
    "text": "do we have a minute taker. Mark Reese? I'm not aware that we have a minute taker. who's volunteered. As you know, we can't start the meeting with that one. with. No. So if you could If you could take some minutes for us, that would be really helpful. minute taking has gotten even easier than ever before, thanks to our automatic transcription, there's really no need to try to write down everything everybody says. just just the the minute takers' responsibility is only to note important points or decisions that aren't otherwise captured. I'm gonna proceed with the chair slides and hope that somebody"
  },
  {
    "startTime": "00:02:01",
    "text": "volunteers to take some minutes for us. for privacy preserving measurement. So As a reminder, participating in any IETF working group meeting like this one, causes you to incur certain legal obligations. You should really understand what those obligations are This slide documents some of them. if you're not familiar with all of them, please take the time to read it, and make sure that you are able to participate here. In addition to the the legal obligations that you incur there are also some other obligations that a apply when you participate in an IETF meeting related to your your behavior and our standards of behavior. So please make sure to maintain civil and professional discourse, in IETF Spaces. Just a technical reminder, somewhere in this room, there should be a clipboard. It's right here. So I'm going to walk this around the room, just to make sure that everybody remembers or Sam will do it. Thank you. So please pass this clipboard around and use it if you haven't already connected to the on-site tool, Otherwise, they'll squeeze us all into a tiny room next time because they won't think that anybody can. believe that's the same QR code. So any way that you as long as you connect to the on-site tool, 4. However you do it, that's sufficient to make sure you're registered in what used to be called the blue sheets."
  },
  {
    "startTime": "00:04:12",
    "text": "Here is our announced agenda for this meeting. If anybody has concerns or ideas for how we're gonna improve this agenda This is your chance to tell us about them. Otherwise, we'll charge ahead with this agenda. Would anybody like to revise this agenda? Remember, we only have 60 minutes, so we will be trying to keep everybody on time for these time slots. Okay. Hearing no revisions. We'll be able to move ahead except that we still don't, have a minute taker volunteer. So we're Well, Okay. Sam Wiler has volunteered as minute taker. This is a little bit unusual, but I think we're going to charge ahead give him a type timeline. So Tim, maybe you could maybe you could tell me how to pronounce your name so that I so that I will know going forward. It's Gaegan. You just ignore the first syllable and then go for it. Okay. Thank you. Tim Gagan. Would you like to share any slides with us. I believe they're already there. Could I ask you please to drive them for me? short Thank you. as our chairs reminded us, we do have a tight session today. So for this deck in particular, I'm gonna try to zip through it so we can get to the more interesting new stuff. So What I wanna cover here is just updates on draft Graft changes since last time and some implementation use."
  },
  {
    "startTime": "00:06:00",
    "text": "Next slide, please. Okay. So we have some new draft versions here the PPM working group, is to say we have revision 5 of DAP And that goes hand in hand with VDAF, verifiable distributed aggregation functions. draft 6 of that was published in CFRG. shortly before this meeting. the headline change in DAPO 5 is what we're calling the pingpong topology. So What this means is that DAP is now specialized to always exactly 2 aggregators. one leader and one helper. Previously, there was sort of an attempt at supporting arbitrarily many helpers. In practice like all the implementations, handled exactly one, and we were really struggling to figure out, like, wide deployments would want more given that there's a sort of conceivable privacy improvement by adding helpers, but there's a huge increase in the cost of running the call and availability risks. More to the point specializing to exactly 2 aggregators brings some meaningful simplifications to both the protocol and implementations. And the really nice thing is that it enables performance improvement. This is the ping pong stuff where now instead of, like, the leader broadcasting out, prepare messages, and then gathering back the prepare mess excuse me, the prep shares from all the helpers, The leader in helper now take turns combining prep shares into prep messages. evaluating proofs and then, you know, telling each other where whether inputs are valid. So concretely, in a case of the Prio 3 family of VDAS, allows us to cut the number of round trips between HTTP round trips that is between leader and helper in half. So there was much discussion of this back at IETF 116. The slides of that deck Arling Tier. There's also a ton of discussion in the poll the the poll request this change was made up on GitHub. If you're interested in more of the context and the details, as well as some thorough characterization of the performance win. So Although GAAP now is exactly 2 aggregators,"
  },
  {
    "startTime": "00:08:01",
    "text": "Vdaf still supports arbitrarily many aggregators, which leaves the door open for, like, other protocols, built on top of VDAF to, you know, use different sorts of technologies. Also, we are open to better names in pingpong for this. because we think it's, like, a a little silly, and that's why we keep putting it in air quotes everywhere. Okay. And then on the VITAS side, the big change there again there's some extra interfaces like wrappers around the existing feed app methods that support the pingpong stuff. And there's an interesting change to 303 histogram. which is now prior parameterized by the number of buckets in the histogram rather than the boundaries of the buckets. This is way more efficient representation given that, like, some applications of the 303 histogram our vectors which can have 100 and even sometimes 1000 of dimensions. So now so now the representation of the histogram is, single integer rather than this huge vector Okay. Next slide, please. That's for implementations. So we saw the same set of implementations is previously. The pre ORS is our rest implementation of the VDAF layer. VDAF06 is fully implemented there in the 0.13 series of crates. And as we said the last couple of times, we still would really love to see more implementations of VITATH. have a couple of DAP implementations. But more diversity for VITAS would be great. It also, like, languages besides Rust would be love to see a go implementation Further, there's Daphne, which is a helper implementation, runs on their workers, It's it's dApp 5 implementation is in flight. Yannis implements all of the roles of the DAP protocol and we have a prototype implementation of the ping pong stuff. we use to prove out, like, protocol changes. We have to do a bunch of cleanup on that to, like, line it up with what ultimately landed in DAPO 5, and we're hopeful that we'll merge that the coming weeks. DBFTS is a TypeScript implementation of a DAP client."
  },
  {
    "startTime": "00:10:03",
    "text": "have a complete deck of 4 implementation, though only for 303. And we should only need minor changes, namely update like domain separation strings, to get to dap05andbdap06. And, finally, Firefox is working on integrating a DAPO 5 clients. I think they have a DAPO 41. They, again, just need to make tweaks like excuse me. domain separation strengths. Next slide, please. Okay. And as for stuff that's coming up in the future, future drafts, So one thing we wanna look at is server differential excuse me. Server differential privacy. Although as it turns out possibly this will also include local or client differential privacy. There's a you're just gonna hear more about like, right after this We also wanna invest some time in overhauling the security considerations, which a bit sprawling and out of date at this point. And I still am interested in tripping the error codes, the list of error codes and error handling in general down to the bare essentials. And, eventually, as we accumulate more operational experience, we expect that there will be some changes to make popular one more scalable. And I think that's it. Thank you. Are there any brief questions? If not, and even if there are, Chris Penn. I think you are Looks good to me. Oh, sweet. That's so cool. Technology is really advancing. Alright, everybody. So this is a topic that has been on the tip of tongues, like, think we even talked about this at the second PPM meeting back in the day. So I we started a a few of us have been, like, actually working on what is differential privacy looked like for DAP. So I wanted to kind of show you where we are."
  },
  {
    "startTime": "00:12:01",
    "text": "we have a proposal and a have some questions about scope and where should it go. So I'm I'm I'm eager for your all's feedback. So just a quick overview differential privacy, what is this thing? A lot of people have heard of it. How does it apply to DAP? So What DAP is doing is taking some measurements and aggregating them. we have this thing, the output we call the aggregate results. This is like the mean or the standard deviation, some summary statistic over your over your set of measurements. and Protecting the measurements themselves, that's all dApp is really good for. It doesn't act it's not necessarily sufficient because the aggregate get result itself might leak sensitive information about an individual measurement. Kind of a simple dumb example is let's say you're taking the average of of a bunch of people's heights, If you include in your sample population a particularly tall person, the average might end up leaking bits of that person's height. that information we consider sensitive. So differential privacy would be it's this property of the aggregate result or more generally the view of the adversary who is watching this protocol execute and maybe even taking purchase taking part in the execution. DP of the aggregate the of the adversary's view means that there's the this view should not change significantly if one of the measurements is replaced by another. So simple enough, basically, no single measurement should influence the output too much for some definition of too much. That's the hard by the way. And basically, the way we solve this is by adding noise. So clients can add noise to their measurements, Aggigators can add noise to the aggregate shares,"
  },
  {
    "startTime": "00:14:01",
    "text": "The collector could just add noise to the aggregate results. There's And, basically, the combination of of things you do here There's many combinations. We'll determine kind of the the security you get in in in what threat model. it's simple enough in principle, just add noise. So but there's a lot of complicated things that go into this. The first one is The mechanisms by which we provide differential privacy basically, you know, we have to be able to sample at sample values from some concrete distribution that gives us a particular property. These turn out to be very hard to implement correctly. there's lots of different bugs that can come up if you if you do things the wrong way. And there's also side channel considerations. Basically, the time it takes to generate noise might the value of that noise. Another consideration is that choosing a the optimal DPE mechanism or policy is it kinda depends on on the vdash. and also your application. So there might be multiple choices that make sense. like, So, yeah, And then then there are also, like, higher level protocol details that impact what level of differential privacy you can actually prove for your protocol. So in dApp, we have, like, some metadata that comes up And that limits kind of the the the the notion of of privacy we can achieve. I wrote intersection attacks here because that comes up in general. doesn't really come up in that in particular because we have a mechanism for preventing this. So and and then lastly, the last point is that the differential privacy literature doesn't provide a lot of wealth. I mean, there's a ton of work on tuning the private parameters for a particular application. But this process is is As far as I can tell,"
  },
  {
    "startTime": "00:16:01",
    "text": "requires a lot of domain expertise, and this is, like, kind of the trickiest thing. basically, you the more at noise you add, the more privacy you get, But by adding noise, you're reducing utility of the system because you're not getting the result, you're getting some the result plus some noise. So the proposal here is we're we started working on a draft. The plan will be to you know, write this draft, and then recommend changes for the dApp and vdash drafts as we need to. The statuses that were in very early stages. We missed the draft deadline. We wanted to share it with you all today, but didn't make it in time. But, you know, it's it's there's there's a lot to shape here. We're we're looking for contributors. So if anyone is interested or or has expertise in this area, please let us know. And the questions I wanna try to tackle are Basically, We don't know where this goes yet. Which working group is it PPM or is it someone else? And then I'd like to discuss the scope of the actual document itself. So let's start there. What we're proposing is kind of, like, 4 basic components. 1 would be, like, an overview of the Martin, can I go through this slide, or do you wanna -- I I was just gonna tell you. I think you I think you probably wanna the question on the last slide first. before you start diving into the details. Okay. I would say that the if if we scope it to DASH and Vdev, then it's a a slightly different well, Okay. Sure. Right. It true, but I think think it is gonna be scoped -- Okay. -- app and PDF. I and necessarily so, because of the nature of Yeah. Previous points that you made. And I think we don't deliver on the first p in our charter if we don't have something along these lines available to us at least."
  },
  {
    "startTime": "00:18:00",
    "text": "Yeah. I agree with that. there there may be some cases where you can rely on on some combination of local or shuffle DP in order to get sort of guarantees that you're looking for But this is probably one of the most important pieces to my mind of the of the protocol that we'd be building building here. So I'd like to see us do the work even if not everyone's using it. So, yeah, just to just to be very clear, Among these options, you would say PPM could adopt it. Okay. Martin has thumbsed up in the affirmative for the record. So I'm not sure where we are now in terms of, like, what the discussion's about. because it seems like Martin, actually, when we went directly to, like, what work should happen. hand, I'm happy to go back to where I was. And can I can tell you what we're proposing. Sure. So so I think what we want to initially is a brief overview of the DP literature. I I have noticed that there's lots of different variants there's lots of ways of refining the notion of differential privacy and not all of them are gonna apply to DAP or VDS. And I'd like to I don't you know, I I I'd like to see something like a literature review. I don't know how that fits into a draft. but I think we can you know, there's I think we need more information. So there are some limiting factors. So, for example, There's this notion of remove or substitution DP technically slightly different definitions. and only one applies to DAP because we leak the number of reports in the batch to everyone involved. There's also metadata. So the the time stamp the report time stamp is a limiting factor. Also, the client IP can be a limiting factor."
  },
  {
    "startTime": "00:20:02",
    "text": "So, you know, if you had no HTTP proxy in front of DAP, you might be able to prove something stronger. The other thing the the main meat of the draft is gonna be the mechanism. So the the the the the tools that we need to actually sample from a given distribution. So folks might have heard of discrete Gaussian or discrete Laplace. These algorithms are complicated, and easy to get wrong. So I think there should be a standard for these algorithms. And then lastly, this what we're calling upDP policy right now, This is basically compose a VDATH with a concrete scheme for differential privacy and and so that everyone could, you know, reuse the same scheme. So there's there's kind of, like, standard approaches to applying DP mechanisms to v f's. And then finally, we would describe the concrete DAP integration depending on how far we wanna go can be would might just be very straightforward. I would say their their there are things, you know, I maybe you should be out of like, off the off the bat, I'm not sure that we can say much concretely about privacy parameter to tuning. So things like apps How do you pick the right Epsilon and Delta for your distribution? I'm not sure anyone is is There's just it there seems to be so much domain expertise required answer this question. I could be wrong. But if it's if it ends up ends up being very application specific, I'm not sure if it would how useful it would be to get into details in the draft. And then there are also more sophisticated in DP mechanisms than the ones we're looking at right now, these can be considered And then I'm not sure we want to be more general than Vdev and DAC. So as far as DAP integration goes, You basically have some some simple API calls at various stages client will add noise."
  },
  {
    "startTime": "00:22:01",
    "text": "the aggregator might annoy add noise And then for local DP mechanisms where the clients add noise, there's this deep like, this you have to kind of remove the bias from the aggregate result. So, yeah, I that's that's it. Go ahead. Yeah. So a few points. I don't wanna make the best fit enemy the good. Mhmm. And, you know, this work group is not started to do this work. So I think it's fine to have it happen, but the way you were just talking about scared of the hell of So I so, like, certainly, this should not be a critical path item for publishing that. that's the first thing I would say. Second thing is, I guess, I don't really understand the framework in which client provided DP fits into for the system. and namely, like, the reason you declined private DP, because you don't trust that in your ears. and we already have existing experience. with client provided GP in systems like Apples. performance is invisible. Rob 4, so I guess I don't really understand like, the Epsilon tuning you need, have a situation with interrogators is It's quite un un un good for, like, actually getting results that are good. so I guess I don't understand how you would do how you would think about how you're reading about that problem. Yeah. I think I agree with you. So the way I think about local DP is It's this property that you get if everything falls apart It's like it's kind of like hedging So if you if you lose trust in both aggregators, then you have some you have at least local DP, which is not It's not nothing. It's not it's not perfect. Sure. Sure. I I understand, but my point is why bother with that Yeah. I mean -- I guess my point is, like, the the the nominal premise you would think would be that you could like use maybe a like, a less aggressive Epsilon. conservative is all I'm gonna put it because you also believed in-depth, and I don't know how to reason about that. Well, so I don't think local DP is a replacement for DAP,"
  },
  {
    "startTime": "00:24:01",
    "text": "I think that Because so, like, because central okay. So central DP is better than local DP. Is far as I understand. In terms of in terms of cost effectiveness, in terms of just privacy. Well, I mean, assuming you trust the central offer, central thing. Yes. I mean, the the premise of central VP and and what will be your different ground models. Right? Yeah. I'm looking. with with with with with five people in the queue and 2 minutes on the clock. So Please try to be brief. Ica was talking about one aspect of the threat model. I was I wanna talk about another aspect of the threat model. Assuming, of course, that we're doing central DP, there is the possibility that we could simply have each one of the aggregators add noise independently, which is one of the things that I think Dawk in 06 or something like that suggested very, very early on in in the process. that has implications for our threat model in the case that of the aggregators is this almost. Yeah. and we need to work through that. There are protocols for adding differential noise to things in MPC. It might add a few rounds to your to your VDaf, but they exist. That's the sort of thing we're looking at for IPA in particular, and would be worth considering or not that fit which one of those fit into our threat model here. we are pretty keen on the notion of not Not having a simple method. as far as I I think it goes because if you have a defecting aggregated, you were in a situation where Half the noise is known, which means the -- Yeah. -- or in our case, 2 thirds of the noise is known, which is not exactly but ideal. So somebody think Yeah. I I I definitely don't wanna I agree. We don't wanna, like, restrict ourselves one method. Yeah. I agree. Watson Live, Akamai. I think that the answer to your question of which working group is here, I think if you went to dispatch, it would kick it right back and"
  },
  {
    "startTime": "00:26:02",
    "text": "Yeah. This is something that we might have to recharter down the line. It shouldn't be bugger thing, but this is clearly the right place to discuss it. Thanks. Tim Gagan, that is RG. I guess to echo Watson there. Yeah. Like, certainly, there's concrete mechanical bits of how it the the concretely would work in a VDF and DAP that belong here does seem like there's other, you know, elements of guidance. that possibly belong elsewhere. But any case sorry. What I came up to the mic to say was more on the of local differential privacy, I've long felt that, like, Dap should say nothing about this, which is not to say that people shouldn't use it, but my opinion is that, in fact, my experience from having deployed Prio or for the explosion notification system. is that local DP can be applied orthogonally to PREO. And so from, like, a minimalism point of view and, like, on the premise that the last protocol text we write, less likely we are hurt to screw it up, I prefer to not say anything about it. which, like, we guys, Yeah. Yeah. So I I the the composition of different DP mechanisms into what we're calling right now a DP policy needs to be thought out very carefully. And there's a lot of ways to kind of to kind of do this. I definitely as far as DAP integration goes, perhaps we need to we don't have to say anything, which that would be perfect. I think I think the need of this document is probably gonna be the specs of the algorithms themselves. And then we can kind of say, like, this is how you you know, is how you compose them with the VDAC to get something that has this DP privacy guarantee, which is so it's it's the point The point is basically, We're trying to enable people to use DP with that. That's the essential problem all because, like, you know, we we we We need this feature. Yeah. Yeah. And for Christmas and for the record, like, I do think we ought to spell out explicit things for server DP. Although I'm concerned that we won't be able to say very much of use about choosing epsilons of deltas, but that's"
  },
  {
    "startTime": "00:28:00",
    "text": "just but but that's just a very, very hard problem. Hey, Sasan here from. I think so. I just wanna quickly answer the question about local DPE and why do we need I think Eric was right. LocalDP alone, not very useful. It's gonna have very poor utility. but a local DPA coupled with the aggregation amplification. I think we have a chance to have a pretty decent utility compared with the aggregate or DP where you have to add twice the noise on those of the aggregators because you can't trust both of them being honest. And for that reason, we also need to mention it in that because there is a chance if we have not enough locally added into a batch or or into an aggregation, tariff could be an opportunity for us to add a little bit of aggregator DP just to make up of the local DP that's missing from the collected about In that case, we have to see something about adding this kind of aggregator DP because the the local data has already been added. This has been written in the in the draft in in GitHub repo. I think we can talk about details over there. Thanks. Okay. Thank you, Sean. Tim Gagan, It's are microphone. I'm gonna give this phone tool It's been can I use this to drive slides that have already been Woah."
  },
  {
    "startTime": "00:30:03",
    "text": "Yeah. I'm not sure what's going on now. Like, Chris, I believe we're still seeing your slides. Okay. Mideco is haunted. But the ghosts of dead slide decks, Okay. that's try that one more time. Alright. Share. Okay. Cool. And I can even go back and forth. Alright. Sweet. Alright. So wanna talk about the query modes in DAP. So to be clear, like, I don't wanna come out of this particular conversation with a decision. because this is actually the first time that this, like, question, this topic has been broached in the working group. Right? So it's premature of late decisions. I wanna do here is stimulate discussion and particularly flush out people who, like, strongly disagree with what I am proposing to do, which is why I'm presenting here. But first, let's do a little background. So what's the query type? Right? Alright. DAP today has a notion of a query type. Like, task when it's defined among aggregators must choose 1 of these There are 2 that we have currently. The first is time interval, which is sort of intuitively the more obvious way to do things. the collector when it's getting aggregates will tell the aggregators, hey. I want you to aggregate over the reports whose time stamps are in, like, some time interval, you know, this hour, this day, what have you. The result is that the batch identifier is that interval of time. And it's definitely one of the nice properties of this is that either aggregator can independently verify"
  },
  {
    "startTime": "00:32:02",
    "text": "whether or not a report actually belongs in a batch because either aggregator can see the timestamps in the reports. The other query query type which was introduced recently, I believe in DAPO 2. is fixed size. So in this mode, When you define the task, the the clicker's subscriber, will indicates we have a desired batch size. So for instance, you might say, like, you wanna amid and aggregate over every thousand reports that get processed by the aggregators. Resultingly, the batch identifier is just like a numeric ID that gets assigned by the leader I believe they're increasing monotonically, but I forget if that's actually a strict requirement. So, you know, batch ID 1 would be 1000 reports. That 32 is the next 1000 and so on. The leader in this setting has the power to assign reports to batches however, it pleases. The protocol doesn't say what the leader should do. And for and as a consequence, the helper doesn't have any ability independently verify, whether a batch, you know, belongs excuse me, whether a report belongs in a batch or not. And further, you might also have 2 batches that cover overlapping time intervals. Right? So In either case, though, the representation of an aggregate that gets delivered to the collector by the aggregators includes the time interval that's banned by the constituent reports. So even in the fixed size case, like, the collector still gets to know that, you know, these 1000 reports, their time stamps were, like, between 11 AM 12 PM on Monday, the whatever. Okay. moved here. Oh, no. Sorry. I lost the forward back buttons. Okay. Don't let your phone lock. while you're presenting. Next thing to recap is Okay. DAP anti replay. So What we care about here is we don't want to do pathological, like, amid an aggregation over a single report because that would trivially violate"
  },
  {
    "startTime": "00:34:01",
    "text": "the privacy of, like, that contribution. So what that does about this is it has a notion of a minimum batch size and aggregators will refuse to admit aggregates if there are too few reports included. Now where this gets more complicated is there are some so in the case of Prio, right, a given batch only gets aggregated one But there's some other VDAS concretely popular one. where the same batch can be aggregated multiple times with different aggregation parameters. concretely in popular one. What this means is you're using, like, successively longer sets of string prefixes trying to drill down to, you know, the top 10 popular most popular strings in the aggregated set. So to so DAP requires in that setting that the batch membership not change across queries. because you don't wanna allow something like aggregate over a 1000 reports, then do a set second aggregation over, like, a 1001 reports because you could probably learn something about the one report that occurs in the in the second aggregation, but not the first. what this means is that the first time that aggregators service a collection request or an aggregation share require aggregate share request depending on which aggregator we're talking about they have to permanently commit 2, the set of reports that are included in a batch. Now in the time interval setting, this leads to a problem with reports getting orphaned, which is illustrated here. Excuse me. So suppose you have, like, a bunch of clients, and they're all doing uploads to the DAP You have a whole bunch of illustrated here on the right. a whole bunch of uploads going on with, say, timestamps, 1, 2, 3. But then, you know, the report of time stamp 4. For some reason, the upload doesn't make it. it gets a router fails somewhere. It just doesn't make it to a leader. Then the collector happens to issue a a collection request. over the time interval, 0 to 10. The leader, let's suppose that this satisfies them in batch size. The leader and the aggregator, excuse me, the leader and the helper, will dutifully service that collection request and admitted aggregate over the time the reports of timestamps 123. deliver an aggregate result. Then later,"
  },
  {
    "startTime": "00:36:03",
    "text": "let's say, you know, a client retries and, finally, that report time stamp 4 gets delivered. That report can never get aggregated because it 5 too late, and the relevant time interval, like, has already been collected. Further, there's no upper bound than the number of orphaned reports in this setting. Right? Like, as any report timestamp falls in the time or the collective time interval that arrives after the collection request at service can never be aggregated, and that could be a very long tail. depending on the setting. Like, you know, imagine just a spotty network connection or an application like, doesn't get to run all the time and so has to defer when it does uploads to save the next time a user happens to launch it. And there's really no good way for the collector to know what's good time to issue a collection request. Like, all it can do is rely on these sort of messy heuristics of, like, well, if I'm aggregating over, like, 6 hour window, and maybe I should wait a full day before I try to collect maybe that's enough time, but This is sort of an unsatisfying way to weigh these trade offs. in my opinion. Now on the fixed size case, it's a different scenario. Right? you know, you have a similar thing, right, where you have a whole bunch of uploads coming and one of them doesn't make it until it gets retried later. There's no harm if the collector makes its collection request sort of too soon. because the leader is free to take, like, those late arrival reports and just stick them in the next batch. as we discussed before, it's perfectly alright in the fixed size setting for multiple for, like, 2 or more batches to have overlapping time intervals or indeed the same time And as we discussed, like, the collector does still get to know what was be interval of time spanned by the constituent reports. Now there is still a possibility of reports getting orphaned because Suppose that the number of late arrivals is lower than mid batch size. But at least now, the number of orphan there's a there's a clear upper bound on the number of orphaned reports. is the minimum batch size, and that's a parameter that is like not always, but often going to be controlled by the"
  },
  {
    "startTime": "00:38:03",
    "text": "by the collector because in a lot of settings, it's a collector, but it's provisioning tasks. into the aggregators. Okay. What's next? Alright. So this is the question I was I have been laboriously making my wage war. which is is fixed size, like, kinda just plain better than time interval. Given the the unbounded data loss risk, What would lead someone to choose what would lead a deployment to choose time interval over fixed size? The other thing is We've been batting around an idea, like, on the divvy up team about sort of a kind of a variant on a fixed size queries that would still allow you to effectively simulate time interval. Right? I mean, Imagine you have, like, a population of installs some application where you know the number of installs, because, you know, if the app store tells you or you know a number of downloads, something like that, And you have an idea of what, say, the opt in rate is for, like, the telemetry system that you've deployed. you can sort of predict, like, how many uploads you expect. Right? And then you can tune your batch size when you create your task appropriately. such that you can roughly expect. Right? Like, a a given number of uploads per hour. thus with and so if you set your main batch size that way, then do fixed size, you'll still get, like, roughly, say, hourly or daily or whatever you want. Go ahead, Edgar. I have some bad news for you. listening? This is an incredibly seasonal situation. Like, this will not even remotely work. You will get, like some days, you will get an hour, sometimes you get 30 minutes, and then you get 4 hours. Like, the new intensity of systems like this is incredibly seasonal. like this will not work. Now maybe you say yeah. You may say, I don't care about time that fixed time windows, but, like, this is not a replacement for time windowing. Okay. Thanks. With that noted, I guess I'll still go through what we see. this is exactly the discussion I wanted to have as I'm happy to get that observation."
  },
  {
    "startTime": "00:40:00",
    "text": "So I I guess I wanna, like, further support the upside of potentially eliminating time interval So for one thing, I think it would allow us to clean up the semantics of the collection API. Right now, what we have is one collection API that is trying to both time interval and the fixed size case. the tension there is that at least in my under in my view of it, Time interval is kind of like the collector pulling aggregates from the aggregators. telling them, like, hey, give me an aggregation for these things and give me the result. Right? Whereas fixed size, I think it makes more sense to think of it as the aggregator is pushing batches into the collectors. And in fact, like, a pubsub type model like a or a message queue is a very natural fit for this. So the API currently is a little awkward because it's trying to do both. if we only did the 1, then we could have At least aesthetically, I think a better API. results, You can go ahead, Martin. I didn't want to interrupt you halfway through the sentence, but it seems like that's how it works Sorry about that. I like the idea of pulling. But this idea of a fixed time, and of all to have those problems that you identify previously. there not an option where you can just pull whatever exists in the aggregate right now with with no regard to with which reports were included in that batch rather than aligning on time intervals. So you can you can pulled your hourly report, and then you get whatever came in the last hour, and then on the next hour, you pull the report. and then then it would then it would fit Well, that would sound like time intervals. So, like, you wish not time intervals then. Sorry? It's not time intervals in the sense that the time stamp in the report. determines when it would be collected. variables it would be it would be essentially variable sized batching. You'd still have your minimum threshold that you don't end up with the -- Mhmm. -- problems that we've identified previously, but you would have a situation where the the the report collector would ask for"
  },
  {
    "startTime": "00:42:02",
    "text": "Whatever's happened since the last time I asked. and and that would be fine too. Right? So are you suggesting basing this leg on the time of, say, arrival of the reports at the aggregators rather than the time stamp that was like, the time of generation of the measurement in the clients? potentially yes. Although Yes. has the same property that your proposal has as well. which is that you don't necessarily know the correspondence between the window of time over which they arrived and the window of time in which they were generated. this this potentially when when there's delays in getting reports into the system, you potentially have reports from a long time ago. Yeah. But you've already identified that we have this difficult trade off already, which is delayed reports either get ignored or included in a later report. And this sort of biases more towards the including the late report side of things, which is I think a decision you're already suggesting. Yep. Yeah. A concern that comes to mind is you might end up with aggregates that end up, smeared over these sort of difficult to reason about time periods. Like, say, if the collector wants, like, hourly chunks of reports, but they end up with something that's like, oh, this here's an aggregate over a 100 reports, but it spans like a week It's it's I'm not sure if that's useful. That's that's why I was getting to the next point, which is that when you when you make query, perhaps you can say, if there are any reports that are older than x, disregard them when you go through the collection process. Now this is not something that Prio with a running count can necessarily do. but that's potentially something that you could you could consider doing. Definitely, yeah, a point in the possibility space to consider. I'm wondering how you would handle if you have simultaneous time periods that you wanna compare. So like, I have some aggregate that I wanna compare for 2 branches and experiment. Mhmm. Like, how would you sort of handle that in this Like, it seems to me like you wouldn't be able to have"
  },
  {
    "startTime": "00:44:01",
    "text": "you know, exactly the same time period compared for the same Although, I think in the first place, those would be 2 different daptasks. Right? So you'd have is, like, configure push to your client that's telling in the first place, it's telling it, like, do behavior, a or b, And then it would be reporting, like, that it did a thing or not, right, to, like, a corresponding gap task so that you could tell one or the other now. Sure. But say I wanna know exactly over a week. Right? So I wanna know, like, in this week, you know, what how what is my count for this group? And then for the 2nd week, for for the same week, I have 2 groups that I want the exact same time period. I understand. Yeah. So you need to align -- Correct. -- the time interval. Yes. Well, time interval is ambiguous here, but I think I understand. You might have 2 different tasks you wanna make sure that you're aggregating over the same span of time to both. Yes. And then also in addition to that, I think this seasonality question is also important for basically any experiment we're gonna run, we wanna know know, there's a lot of seasonal fluctuation over the day or over the week. So that's, like, definitely a use case anyways. So Yeah. Yeah. Okay. Okay. Okay. Thanks. So so Chris Patton Cloudflare. Just to remind folks kind of the intention of query types is that there might be lots of ways you wanna split up data. like, all of these questions are coming up. These are all kind of, like, different ways of slicing data. And the idea is that we would constrain them inbound in the protocol. So the idea is that the what what we're trying to do with, like, acquired query types is support different use cases where you have different relationships between the entities. So in particular, the collector has, like, control over has some control over the data. I wanna know and in an interoperable manner. Right? So Okay. My point is I support this idea because it it's it's reducing complex"
  },
  {
    "startTime": "00:46:00",
    "text": "in the protocol. Somebody likes it. what we're giving up, is the collector having control over how things are are are partitioned. But if we don't have if no one here has a reason to have the semantics in the protocol, then let's get rid of So far, like, our use cases that I've I've I've seen, we don't actually need it. My my opinion is that the seasonality argument that Ecker raised and Kate echoed, like, currently sufficient to sort of torpedo this, but my takeaway is probably, like, we need to work harder and I mean, like, the divvy up team, for instance. Think harder about whether we can do anything about that seasonality question. Sorry, Edgar. Go ahead. yeah, I wanted to make two points about your pure problem statement here. on The first is that I think that the orphaned the orphaned request problem comes in, I think, 2 flavors. Right? sorry, yes, we heard orphan request first, which is You know, many systems on Firefox included, on do not in fact trigger on their iteration on the timestamp in the in in the message, but on the arrival time. and and this works and and so there's no overlapping window problem in that case. things simply arrive when they arrive, and you deliver the fact that the time times, like, are varied. Right? And so you may think that's a bad a bad practice, but it's a common practice. and the The second thing I would say is it's sort of like This concern that you seem to have about having, like, min batch side, not being on the estimate min size and having, like, you know, having having sort of expand your contractor window to match up with the batch size. like a problem that people actually have only when like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like really, they have a very small user base. Because when you have a user base and, like, millions, then that size is quite small compared to the comparative number the message you're gonna And so and so and you simply have to set the size of your experiment or your sample, to have a very high probability that you will achieve in batch size within the time window. And that's a much easier seasonality problem because you only need to estimate the minimum number rather than trying to estimate it. And so, like, if they're moving that size to say a 1000 or 5000, All you have to do is have statistical power large enough to have everybody having 5000. So, I guess, Like, I understand that, like it seems to me that, like,"
  },
  {
    "startTime": "00:48:04",
    "text": "you sort of take those factors into account, really, you're kinda left with this complexity argument. which is not of no value, but I think, like, you know, that is already quite complicated. if we wanna do and if we don't want to be not useful, we can do nothing. So the question really is is it's useful. Thanks. So I think my takeaways here are, like, think harder about what can we do about seasonality? And how bad really is this orphan report problem in light of what Ecker just highlighted? Thanks. I very much very much appreciate all the feedback. just wanted to ask the chairs if we might have an interim on on this topic. I I I don't think we're gonna make a call on that in this moment, but thank you for the Thank you for raising the topic. it would be incumbent on, like, me and my teammates to think do more work and, like, post something interesting say to the mailing list, Right? Like, more findings, more suggestions before we go, and then Once that happens, maybe even interim makes sense. I'm gonna be quiet so we can do the next Okay. Our our last speaker is Christian Preb, of Apple I I assume you are remote. Would you like us to run slides, or would you like to request slides. Christian Preb, if you're listening, I see you in the participant list, but you haven't requested audio video. So we don't see you. Bye. Christian has left the participant list. I assume he's rebooting his connect disconnect. Can you can you hear me? Yes."
  },
  {
    "startTime": "00:50:05",
    "text": "I'm actually not supposed to present the mao should also be on the on the call Oh, then I'll He is gonna present. Okay. Thanks for the correction. then, Nava, are you are you there? Yep. Okay. Hi, everyone. Hi, everyone. My name is Lim Mow. I'm going to present a proposal for the extent foreign type extension, the coauthored Christian Pripy and myself, So the extension is about all report authentication. And portions are we trying to solve here ability, when motivation is simple attacks, the currently, the dApp protocol does not provide a mechanism to prevent a malicious party from generating a large number of reports. So what are the potential impact there could be a privacy violations and that they also could be a statistics poisoning. So the dilemma here is that to effectively account for this attack, And we often need a a weekly meeting. which very often needs cheaper authentication, But, obviously, we cannot do that on the aggregators. And how do we propose to solve this dilemma? Well, we proposed that by leveraging the so called the privacy pass protocol or protocol or protocol which is another IET draft, there is a way for the aggregators to verify the reports have been authenticated at optional real way to limit it. are learning the client's identities. So here is how the protocol the extension works. can be divided into 2 parts of the commit the communication can be divided into parts. The first part is essentially making the client allocate about privacy plus token, And as more or less as per the standard protocol So if you are familiar with the"
  },
  {
    "startTime": "00:52:00",
    "text": "privacy plus insurance. Particle, you'll see a familiar messaging flows and also entities like a test and easier. we will come back to this part later. second part of the communication is essentially making the client a attach the tokens to the dev reports and making the leader and help us to validate the tokens as part of their initialization. And Heath So here is the structure of the extension and also its location on the inside deputy report. So on the diagram on the Lower part of the east side, you'll see its location inside debt report. So the this extension sits inside each of the encrypted input strand for each aggregators And the structure of the extension is very simple. a v VCB extension type, also, the token itself and also the challenging which was used for obtaining the Tofil. So why the challenge here and and though, if you go back to the Previous slide, And in the first communication flow, you you said there is a slider deviation from the standard privacy policy insurance support in a sense that the client is the team's assizing the challenge instead of making the challenge and come in the leader and help her. and the and the y why is this deviation? Now the if we allow the leader and help us, send a message to to send a challenge to the client, they have a way to associate clients' identity token. and that defeats the purpose of the deck. So And that's why we are here, we are make making the client things aside challenge. And that's why the challenge has to be placed inside the extension itself so that there are I bring data to us. that they'll be able to rel to validate the token as per the standard. privacy pass flow, So so If a client needs wants to support his extension, what changes does he have to make Though, at first, it needs to be able to acquire token as per the standard protocol."
  },
  {
    "startTime": "00:54:00",
    "text": "and and apart from the the the the part, which which things aside is the challenge on the client side. And as we have just discussed, And, also, what's important is that the client needs to disassociate the token acquisition, from the reporter creation, That is just to avoid any potential timing attacks. So at the time of the report creation, the client simply epics and unused the token. a pre allocated aggregator enclose the token and the challenge in side, this a reported extension as we have just discussed above And all for the client. And how about the aggregators? So so aggregate has needs to be able to very the the tokens. And they first extract the token and the challenge from the upload extension. and they validate the original info field of the challenge to make sure that it contains their own host names. is essentially to prevent the client from, for example, Hub was spent in the program. across different aggregators. And the aggregators then validate the token as per standard privacy plus authentication team. And what's important here is that they need to make sure that the tokens 1. allocated with and still active is your probably a key And the reason is to prevent the client, the focus are more from accumulating tokens from half a year ago and then spend everything one go. And the the aggregators also needs to keep track of the token norms basically prevent the double spending And So inside the extension, we also have a a few recommendations and optimizations depending on the underlying security consideration and also the and the trade offs, we recommend using the REIT to replicate a variant. of the privacy pass issues and protocol. And just because it provides more control over the awaiting meeting"
  },
  {
    "startTime": "00:56:02",
    "text": "And we also have a potential optimization depending on this as the the trade offs. to to leverage the existing that the dual mechanism, to to prevent the token of spending. And another potential optimization is to for reducing, reducing, the potential traffic or load on the privacy plus architecture by conditionally allowing tokens to be reused across tasks. And these are not as as finalized yet, entities are there are still discussions on the gift hub repository. And here is the location of the at the the extension proposal, and we welcome your feedback and contribution. thank you very much. Hi. Erica Skorla. a question I think I know the answer to, but I wanna make sure we're on the same page. Why can't you just use privacy pass directly in the with the HTTP bindings? Sorry. Could could you repeat the question? can't you use these privacy pass in a normal way? as bound into HTTP rather than rather than in rather than attaching to that. So I don't I don't know if you are at talking about the this communication part, you mean that the and and I I didn't quite got quite question you. You mean the what I'm saying is What I'm saying is we already know how to use privacy pass with ordinary HTTP. Yeah. So why can't you just use it exactly like that without not changing capital? So the So I I I I guess, avid. deviate a little bit with that this challenge is synthesizing. I don't know if that's the question. And for at The reason that is, like previously mentioned, if"
  },
  {
    "startTime": "00:58:00",
    "text": "step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step, step challenge is issued by the leader and help with them, it basically associates the token with the client identity. so that's why we allow we we we deviated the phone. the standard protocol by making the client the synthesize the challenge. But I'm saying Sorry. I'm saying -- Yeah. -- I'm saying -- Yeah. Please. Go ahead. Right. I mean, I I'm I would like to answer your question, and I'm I'm having some technical issues. I can't see the queue right now. So I'm not sure if I'm allowed to speak or not, but I think it will be helpful. I think part of the reason for is that the client doesn't directly talk to the helper, for example. Right? the -- Good. -- the reports are forwarded via the leader, so we have to do it by the extension. We can't do it's to be. Oh. That's what I thought. Okay. And the thing is that that threat model does not include the leader or helper winning. And so It's already the case. The leader in help work can totally vote totally mess up the results simply by, like, adding numbers to reports. And so attempting to hyper simple attacks. like, saying you're concerned, but, like, that the helper the leader will adequately suppressed civil attack by checking privacy pass is like missing the forest for the trees. Okay. Watson I I guess I have a similar question but a different take on it, which is my understanding of sort of the PPM threat model is that the client doesn't trust The the we trust the leader and to help her as people collecting the measurements and evaluating them. What we don't necessarily trust is The client doesn't necessarily trust the leader and the helper to preserve the client's privacy. And so in that threat model, doesn't really make sense to shove the token in there versus use it in HD in ACP. And if we're changing that threat model,"
  },
  {
    "startTime": "01:00:01",
    "text": "then I think you have to change a lot of other things as well. I don't think the threat model is currently changed here. So, basically, the and and the and and At at the yes. Yes. So base basically, we at the the client, the MICE Trust, the leader, and the helper, And and the Vuitton necessarily trust the client And how how it would be The problem is this doesn't really solve for example the and the malicious client from launching it over sending a huge number of reports And so that that the reporter wants the leader and the helper, and 8 paste the light, the the then potentially can cause the privacy bridging and also statistically appoints a name. Hi. I'm Tonya. Tony Verma. from Cloudflare. I just wanted to mention that the the problem about reusing tokens across task isn't just good for optimization, we can use it across different task to track malicious behavior that would only be expressed over a period of time or across different tasks. So I'm I'm thinking of a couple different examples, but I'll I'll leave it that for now. So the reusing of the task the tokens across task is is The main motivation is so that there won't be a flat number of requests, on the on the privacy plus infrastructure."
  },
  {
    "startTime": "01:02:01",
    "text": "So for at in like like, Eddy explained the here at the foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot foot Basically, this is a extension, assumes that the client always pre fetched a huge number of tokens, And so there can be quite a lot of tokens wasted because the client not be submitting any of the reports. And so the motivation for reusing the tokens across task is so that for example, the Basically, the client don't have to allocate maybe focus on the 10,000 tokens every day. So I just wanted to, like, take a step back and the just remind people of what the proposal is. This is proposal for, like, a working group draft that we would work on. And Its goal is to mitigate civil attacks, which we wanna do. and we're not we don't wanna we don't intend to change the threat model. We would like that civil attacks are mitigated. under the the the non collusion assumptions that exist for DAP and privacy pass today. So in that spirit, I think I think this is a great starting point this this draft. I've read it. It's it might be the case that we can do the integration differently. In the past, we've looked at different extensions for for DAP and decided that an explicit extension in band doesn't isn't actually necessary. I think this is debatable for this draft. I think there is something nice about having the binding of the token to the report, but I would love to see yeah. I'd love to see more discussion on this. but I I I hope people recognize basically this is a problem that we we want to enable deployments to solve. and privacy pass makes a lot of sense. Thanks. Tim Gagan, ISRG."
  },
  {
    "startTime": "01:04:04",
    "text": "So in this scheme, the the helper will extract the token out of the report extension and then validate it. what I'd like to understand is where does the helper get, like, the public key to verify the token against. So the and this would be out of Bend that the in this case, the help help would have to app trust the issuer and it also needs to contact the Israel the public key. So the simplest is just to to go to the easiest well known property directory and obtain the to a key key country listed there, there, So, basically, you would have, like, one of the task parameters would be Okay. a URL, from which the helper should fetch, like, the current or maybe a list of public keys with which to validate these tokens. I think it would be a 2 a big piece pans It may or may not necessarily be from one of the process configurations. So for example, if the help our leader and Hasset their own maybe statistics that come started configuration, which easy as they trust, want to trust. then then those can be, like, hot coated I see. Okay. Interesting. Thank you. Thank you. Thank you, Lynelle. We have anything else before we wrap the meeting. Thank you for joining us for PPM at IITF 117. We'll see you next time."
  }
]
