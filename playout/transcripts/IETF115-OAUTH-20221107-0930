[
  {
    "startTime": "00:00:08",
    "text": "excuse me come on it's like already cats which one yeah yeah I think that should be the other one yeah okay I think we're ready to go it's 7 30 um good morning everyone welcome to the first session of oauth hey great to see you again here and um let's get going there you go uh the north well um if you're not familiar with this please take a minute to review this this governs everything that we do at the IDF so it is important that you understand what's going on there this one um these are tips to to make sure um you're able to join um and contribute here if you need to so please if you are in the meeting use that um light client um that will allow allow you also to kind of join the queue if you want and we would know that you've been in in that in the room so it's important that you kind of use that while you're here and please wear a mask during this time"
  },
  {
    "startTime": "00:02:03",
    "text": "remote but spend a please use that the full um remote client and make sure that audio and video are off until you unless you want to say something and we highly encourage that you use a headset if you're remote okay so we have um four sessions two official sessions the the first one is obviously today this one the second one is on Wednesday at one o'clock um and we have two side sessions um the first one is Tuesday at two and uh the next one is Thursday at two at the same time in the same room uh the one on Tuesday we have allocated uh an hour and a half probably we're gonna use only the first hour because we probably have a conflict with uh another Airway group but but that's that's for the side meeting uh okay so work group updates um since last time we published a jwk thumbprint URI so this is a Mike's and Christina's document so congratulations to Mike and Christina I think special congratulations for Christina I think this is your first RFC at ietf so thank you for that then thank you for contribution [Applause] a ITF last call we have a rare document at the state in the RFC editor queue we have job response for all token introspection so uh Roman what is the status of this that this has been there for some time any thoughts on this uh good morning Roman do Native responsible A.D and I guess it says something as I got up I didn't feel like"
  },
  {
    "startTime": "00:04:00",
    "text": "I'd leave could leave without grabbing my cup of coffee and I'm like Wait no that's that's not a good idea uh so in the RC Ed queue it's uh it's in misref so it's waiting for the um for the security best practices so there are two things to do we could accelerate the best practices document where we could look really hard at that document to say do we need it as a normative reference okay okay good good to know so I think we're trying to get the best practices out soon so maybe let's let's wait for that right and then just since I'm up here just to kind of comment uh the depop document uh is now in kind of revised ID so we're we're moving a little bit yeah awesome thank you yeah yeah we got the feedback from you um Roman I appreciate that so we're gonna so the depot yes exactly we're chatting about this in in the side meeting um I think tomorrow right yeah yeah yes yeah yeah 2 30. 2 30 we'll be there yeah awesome and uh waiting for security Chevron actually before the security BCP um think like you had some comments uh Daniel that you want to incorporate I think Mike used you have some other comments that you want to contribute to so let's yeah so let's if you can get that as soon as possible let's get this going okay and and hopefully after that we'll give a um harness a chance kind of to review it and push it forward as a Shepherd for this document we could absolutely yes yes yeah that works okay"
  },
  {
    "startTime": "00:06:02",
    "text": "and so our agenda for today um besides uh chairs update we'll start with Aaron talking about giving us an update on browser-based apps and auto that one uh Daniel and Christina will talk about the SD jot and Brian will give us an update on the step Authentication and a bench watch his remote I hopefully he's on the list yeah I see him there so he will talk about interactive authentication document that he submitted um just recently um at the end so that's for Monday for Wednesday uh I'll be talking about jot embedded tokens a Atul will be talking about fine-grained transactional authorization a Peter will be talking about his new draft that just submit submitted recently across device flows and Christina will talk about the client ID for anonymous say clients so that's for Wednesday side meetings uh depop Ada review there are a few comments and we're gonna discuss this directly with the with Roman um tomorrow and there's also some questions comments say from the fappy working group at open ID we want to talk about that guide the Roman yeah in the spirit of keeping things moving I think we only need to talk about algorithm agility I'm fine with the number of others It's Not My Equity you just got to document it uh so I do remember some of the reasons why we got there we just got to write it down so algorithm agility is the only thing on my list so when you say write it down just update the the shepherd's write-up or is that is that good enough yeah precisely because in ISU review that's like the first question for C when they open up the document like what you didn't I mean we saved less than five why do you got more than five and I'll defend why we got there we just need to write it down awesome thank you um we also want to talk about oauth work group GitHub I think Aaron helped us"
  },
  {
    "startTime": "00:08:01",
    "text": "kind of get this going so it will Aaron we'll talk about that um a tool we'll kind of dig deeper into the fine-grained transactional authorization um and then Aaron will have more chance to talk about a browser-based app and auth2.1 I will add a security BCP for for Daniel also to talk about those that too and I think that's all we have any questions comments foreign let's get going do you want me to hand you the control there you go all right good morning I'm Aaron praki from OCTA um two drafts to talk about today oh 2.1 and browser-based apps will start here um decided to follow Brian's tradition of including photos from the location in slide decks I think we should all do that so that is my personal challenge to everybody else um that was from uh yesterday on a little Adventure so uh oauth 2.1 the um if you're not familiar with this it's essentially an effort to consolidate the existing oauth 2.0 drafts and best practices into a document that is easier to read and more up to date on a number of fronts there's quite a lot of language and references that are now quite outdated in oauth 2.0 which has just celebrated its 10-year anniversary last month so um been a while so uh since the last time we chatted there's been a little bit of updates to the draft um the abstract has now been updated to remove the term third party since I think uh a lot of the use of oauth today"
  },
  {
    "startTime": "00:10:01",
    "text": "is actually in first party as well as third party so it now it just applies to everything um also part of the motivations of using oauth is to enable things like multi-factor auth and passwordless auth so that's now added to the introduction as well um as we talked about last time push authorization requests is an interesting kind of exception to the requirement that is described elsewhere as needing to register redirect URLs everywhere that is now mentioned as a way that registration can happen because it's effectively on the Fly registration of the redirect URL um there's a mention of if your token endpoint is going to be used by browser-based apps you'll need to support the course headers although that's something else we're going to have to talk about in a little bit and some updates to references and um little fixes here and there so overall not a ton of not a ton of uh changes there um since the spring meeting these are some of the changes as well I think we talked about all of these uh last time in Philadelphia if you were there so I'm not going to go over the one by one I do have links on the slides to the GitHub repo that talks about uh all of the issues that were closed or discussed during this round of changes and there's also the link to the diff of uh draft five to seven which was from July till now and you may notice that it's at a new GitHub org which we will also talk about in the side meeting um a couple of things that are still uh just sort of on the path to do which have uh some of these have now been on the to-do list for a long time because they're just big tasks um any help is appreciated on any of these but uh Justin Victoria's write-ups still working through the their write-ups from the original draft of this a lot of that is now dealt with and closed and revised by"
  },
  {
    "startTime": "00:12:00",
    "text": "various things but there's still just a little bit more left in those write-ups to get to um there's still the uh rather large task of identifying any normative language that may be in various sections like security considerations and moving where appropriate backup to the actual core document uh main part of the draft and then one thing that we identified at one point I don't remember which meeting it was at um adding an explicit section talking about the core differences from oauth 2 and which changes are breaking changes to who because um essentially if you are following the best practices today that it already is 2.1 but that is quite different from original oauth 2 as described in 6749 so we will make sure to document everything so people can know what to look for when they want to go and double check that their current oauth 2 systems are actually following the best practices there are a few more issues that I didn't pull out uh specifically here feel free to take a look on the GitHub repo if you are interested and um we'll be working through those on GitHub now a couple things I did want to talk about here in particular are some things that um either I felt were particularly relevant to a synchronous group discussion or things that one of them was something that needed to get pulled from the side meeting to the e-real meeting um so crossword and resource sharing um there's a one in one of the meetings we identified that the token endpoint needs a mention of supporting cross-order resource sharing so that was then added and then that's really the only endpoint in the core draft but there's a whole bunch of other endpoints that are defined in extensions to oauth 2.0 that are pretty commonly used and widely deployed that will also need cross-order resource sharing support and um I think it's worth mentioning in"
  },
  {
    "startTime": "00:14:01",
    "text": "particular in the core draft there may be extensions that require this as well and that would be things like um the the metadata endpoints or the par endpoint or what those kinds of things um and in addition the authorization endpoint itself which will be visited by the browser not used by JavaScript actually should explicitly not allow cross-origin requests uh because bad things can happen if so which means essentially there should probably be a course section in oau 2.1 that talks about this as a whole instead of just one mention of it somewhere and an open ID connect this discussion has also been happening and um I think we need to sort of have a general agreement on the language that goes into to both of these which also applies to security BCP so um essentially the um my thought is to add a section about cores mentioning explicitly not allowing cores of the authorization endpoint to support JavaScript apps then you'll need to support cores at the following endpoints that are um either defined in oauth 2.1 or in extensions that are already being referenced um in in the draft other uh these are the four that I found quickly um if anybody knows of any others that are worth pointing out specifically suggestions are welcome uh that links to the issue 133 where I'm collecting this stuff right now so feel free to go in there and add suggestions if you have any other ideas for things that will be useful to mention around General course recommendations for oauth so I think do you want to take comments here sure yes topic so anybody has any comments I I noticed some nods there but beyond knots go ahead"
  },
  {
    "startTime": "00:16:03",
    "text": "I just want to ask you bring all right can you state your name Daryl Miller from Microsoft uh you mentioned JavaScript apps but I'm just wondering in this world of wasm now whether how cores interacts with wasm and whether we should say that more broadly uh that is a great Point probably worth not using the term JavaScript in reference to cores anymore um so yeah we'll figure the right the right language that probably just browser-based apps oh I see Brian sorry oh Brian go ahead uh Brian Campbell paying identity I'm totally on board with all this I think it's a great way to go along the same lines of trying to explain the difference between used by browser-based apps and used directly by browser-based apps because like the the authorization endpoint is going to be used by these apps and it's the nature of the use that I think is both the reason for not exposing its occurs at course that's the beer from where I'm from um it's not really beer of course and and how it's used the that's sort of the heart of the issue and I think everything you're saying is right on but actually really explaining how that works um in good language that I don't have but I'm suggesting you right is is good so thanks thank you no I I totally agree and I think that actually is a a great example of why this requires like a dedicated section to talk about it so we can actually explain exactly those points so Mike Jones Microsoft as I was doing the Errata edit for Connect about this"
  },
  {
    "startTime": "00:18:01",
    "text": "um I was the one who came up with the used directly by versus used via redirect but I would love somebody to give me a short phrase which connotes using HTTP verbs such as get post what not what what is that action without being verb specific net navigate is a redirect to the mic Danielle if you want to speak up please go to the mic I'll I'll sit down but amplifies what Brian just said we need that one yeah I I agree I think uh we'll have to do a little bit of work to figure out the specific language for it but I agree that like yeah we need to be very clear about whether the browser is the one making the request or whether you're JavaScript or other languages that run in a browser making uh requests um itself directly I don't know the the right term for that so but it sounds like we're at least all in agreement on the concept so um cool yeah feel free to add suggestions into that to get that GitHub link if you have any thoughts about the language that would be useful to use in the section um cool okay moving on to new issue 54. this was uh an older discussion that we brought up during the side meeting last time we met had a brief discussion about it and then it continued to get a comment or two on GitHub um at the token endpoint there's a list of parameters required to exchange an authorization code"
  },
  {
    "startTime": "00:20:00",
    "text": "when exchanging an authorization code one of them is the redirect URI there's language in the original spec that says send the redirect URI to avoid certain types of mix-up attacks and it's um arguably not a bulletproof Solution by any means in the original spec but it actually has zero technical purpose once you are using Pixi it doesn't actually do anything else pixie solves the mix-up attack that redirect URI barely solved to begin with so my question was since we're cleaning this up can we just leave it out off because it doesn't do anything and there was a discussion uh we do want backwards compatibility of course with we shouldn't be like we shouldn't make it so that oauth clients are going to suddenly stop working if they're sending the parameter uh for example so the question is how do we support that while also um not just having it required for the sake of it because it's always been there the options are anything from just not documenting it and letting people just do it or not there's also making it required that the as would accept and also verify it if sent but not require it to be sent which feels like the right way to do the backwards compatibility I think other options or just leave it alone and make it just keep it as a required parameter because maybe that's just the simplest and anything else is adding new problems um I'm I'm on the fence about it but it feels like an opportunity to to like clean things up because every time I'm explaining this to a developer and I'm like send the redirect URI parameter because I don't know we've just always done it this way it's like not terribly compelling um so that's I would like to actually uh have a quick discussion here about about this"
  },
  {
    "startTime": "00:22:02",
    "text": "Mike Jones Microsoft um I suspect that some code is going to break because when it's not sent because it's expecting it to be there and I think not by itself yeah it's probably enough reason to uh keep it either as recommended or required I would be fine with a note being put in saying this is effectively redundant when you're using pixie but I think for um for the for the backwards compatibility discussion it's like you have to consider which end it's breaking on and I think for the as like obviously if you're not if the client's not doing pixie this is marginally useful so if the client's not doing pixie then you're not even you're not doing the oau 2.1 flow to begin with so if the as supports oauth 2.1 or requires it for a client it feels like it's okay for it to just ignore the parameter and not require it because if you're already opting into the new behavior in the spec then there is nothing to break there but yes for an oauth 2.0 authorization server obviously we're not saying don't want 2.1 clients are expected to work with 2.0 servers because that's not the backwards compatibility that's not backwards that's just forwards compatibility right and and you have plus one from George okay Aaron what type of compatibility do you want would you want uh a 2.1 client talking to an old uh 2.0 authorization server would that be would that be something that is actually okay or because that's"
  },
  {
    "startTime": "00:24:01",
    "text": "that's where the problem shows up the again because 2.1 is not supposed to be an entirely new thing it's supposed to be the collection of best practices if an as is already supporting best practices then it's not a 2.0 in that direction it should be considered a 2.1 as already right okay so naturally that the issue shouldn't shouldn't pop up is strictly speaking like if you follow best practices I I believe so so if a oauth client is um if I know about 2.0 client was talking to a 2.1 server and it does send the redirect URL we don't want the as to reject it so it should ignore it because it doesn't serve any purpose which is already what it's supposed to do with unknown parameters basically turning it into a unrecognized parameter and then if oauth client opts into the 2.1 Behavior it should work with a lot of servers that are following the best practices so if it's not sending it then we only want to let that break if everybody's agreed on the new rules right I believe that I believe that's the logic we're trying to follow here if everybody agrees on the new rules then everybody plays by the same rules but if one person doesn't then it's okay if it breaks as long as it's not rejecting it if it's sending the parameter from that makes sense I'm seeing some nods it's definitely confusing um it sounds like and then the the option two here that I have on the list of um making a note of"
  },
  {
    "startTime": "00:26:01",
    "text": "if the client uh does send it then the as should also check it because that's how it can support older clients so that's the sort of backwards compatibility note go ahead Daniel yeah then you've had yes.com um just saying so this is obviously not something we will do in a security BCP because it's not security related uh so we cannot really say there you may omit the parameter and everything will be fine um so we have to be careful not to introduce like three like another um level of compatibility so yeah security BCP compatible and then 2.1 compatible diverging when you introduce this change potentially diverging um so maybe it's better to leave it as this so if we say that if you follow security BCP so like 2.0 and security BCP you're essentially at the same like level at s 2.1 and now we're introducing something else in 2.1 which make break the security BCP compatible stuff that might complicate things further so s as much as I want to get rid of this thing as well but yeah okay thank you uh Philip uh thank you this is Phillips I would just like to Echo what Daniel said um if um if the point of 2.1 is to take 2.0 and apply all the best practices um it doesn't feel like removing it would create a new world despite me wanting to get rid of this because it makes the apis for consuming callbacks oh so ugly"
  },
  {
    "startTime": "00:28:02",
    "text": "because you always have to redirect URI and you're trying to do magic in browser-based applications by putting your current URL in there um I I feel like the ship has sailed on this and rather than create a whole new world I would just keep this as if things was also um if if like you said if I have an existing of two server and it applies all the security best practices and then a 2.0 client talks to it it should just work even 2.1 it should just work because it has all the BCPS applied but if it doesn't send the redirect URI it wouldn't yeah it is uh yeah the question is whether a 2.1 client would work with a 2.0 server that has has done all the best practices stuff right which yeah it does sound like it is creating a new class in in that case if if there was a discovery mechanism we can do this like if the client can figure out this is actually a 2.1 compatible server yeah yeah why not um so that actually brings up an interesting point which I'm pretty sure is uh I'm pretty sure that there's nothing else like this yet uh in terms of things that 2.1 defines that are not described by the by the security BCP or other drafts if there are though then it seems like um we would need to go down that road of of a discovery flag and then it would be okay to include this but I don't I don't think there are any other things like that yet but it seems like it's worth double checking Justin richer so uh if we do option two like"
  },
  {
    "startTime": "00:30:01",
    "text": "Aaron's suggesting we don't actually have to do Discovery because no matter what the client does the AIS is going to do the right thing if it's a 2.1 compliant as if the client doesn't send the URL as is going to be fine with it the client does send the URL the as is going to check it and enforce that it's the right thing so if you get a client with old Behavior or a client with new Behavior it's fine you don't have to discover anything and it all just works option two is the only one up there that actually makes sense everything else is a bad idea the only um that works great for the 2.0 and 2.1 client talking to either 2.0 or 2.1 as however that doesn't work with the 2.1 client talking to the 2.0 plus vcp as that's the one that it breaks and if we're okay with that breaking in that direction then it's fine that's the only one of the four combinations that breaks and I I see nods so are people comfortable with that and if that is the case let let's just make sure it's documented and clear right okay okay seems like you're seeing General numbers and let's lock it okay was there somebody popped up on the Queue or did that no I I don't see anybody in the queue virtual document s The Way Forward is basically number two on the slides so as for backwards compatibility compatibility should accept and verify the redirect URL if present but not require it and and make sure that that the combination that you've talked about at the end is documented and clear that there's it's a breaking"
  },
  {
    "startTime": "00:32:00",
    "text": "um that so that goes into the section I mentioned earlier of the oauth 2.0 compatibility section uh that will have to get called out explicitly right Philip do you have a comment there I I noticed you you joined and dropped um I I wasn't sure if we're okay with that break and um personally I'm not but if the consensus is you know let's just document that breaking Behavior sure but I will I'm not 100 sure that that should be the outcome unless there's discovery my gut feeling is that it's okay because um the clients don't they're frankly less likely to update in general unless they need to for some reason just because if things work then they're just going to let it work so if they want to update then if they go update to a what's labeled 2.1 compatible or a 2.1 client then they won't be expecting it to necessarily work with the 2.0 server anyway just because there probably are going to be other things that break just because 2.0 is such a broad term to begin with so it feels like it's the oak it's the okay Direction and I agree that would be less willing to have the ASP be the one that it breaks for all right um maybe maybe the other thing to note here for for the action items um once I do add the section that talks about all the changes from OSU and who they break for let's just revisit this because it may make sense it may show up as oh it's not a big deal there's other things that break in that direction or if it's maybe the only one that breaks in that direction we can change our mind about it I feel like once we get that"
  },
  {
    "startTime": "00:34:00",
    "text": "full picture we'll have more context for it yeah good good idea okay okay cool uh um okay yeah that was it for the 2.1 issues um there are a couple other things on GitHub that are open so feel free to jump in there with any thoughts um but any luck we're coming down to the down to the end of it here um okay moving on to the browser-based apps best current practice this is now draft 11 after picking it up in uh July again after some time away from it um so we had a little discussion in July and a quick recap of what this draft is supposed to do it's supposed to be recommendations for people who are building browser-based apps using oauth um which are defined as applications executing in a browser AKA single page apps um the language is not actually mentioned in the draft because it applies to any browser-based execution environment and it may include a back-end component that is part of the interaction of the application the application having its own sort of back end um since July there's been some more reworking of the document the there are these four architectural patterns that are called out in the draft in terms of uh they're like different ways that you might organize your app or architect your app the single domain architecture is just when your client runs at this at a URL that's the same domain as the as there's the uh these names are names that have changed but the patterns have been have been there for a while now um back in for front end proxy that's the uh the one where the back end basically uh has a its own session to the front end and the back end does everything it acquires the tokens it makes API calls"
  },
  {
    "startTime": "00:36:01",
    "text": "everything about the Javascript app runs through that back end where that backend is the oauth client is the thing that talks to the resource servers then there's the token mediating back end which I stole from Brian editorio's draft where the back end is only responsible for acquiring the tokens and the tokens are then passed to the front end where the front end makes API calls directly and then there's the last option which is token acquisition in the browser which is essentially the only front end option where there is no back end at all and everything happens in the browser so those are the four patterns and the token median mediating backend is the new one that was added to this based on discussion last time um the there's also now a sub section of the uh pure JavaScript or pure browser-based app which talks about using this a service worker to manage it because there are different considerations and different threat models when all of the token Management in that position happens from within a service worker versus in the Dom directly um so that's sort of like a subsection of the of the pure browser-based app and and there's also now more notes about things to worry about if you are actually storing tokens in local storage if you are either that applies to both the pure browser-based app as well as the um the one where the back end acquires the tokens but passes it to the front end that where it deals with the tokens somehow so if you have feelings about local storage and tokens which I know a lot of people do please make sure your feelings are represented in the document there are no right or wrong answers for this it is meant to capture the information that is known and um this has been a probably one of the reasons why this discussion kind of"
  },
  {
    "startTime": "00:38:00",
    "text": "stalled out last year there's a lot of differing opinions about handling tokens and browsers and again the goal is not to say that one is right or wrong the goal is to say what is wrong with all of them because they're all bad in different ways um Okay so a couple of so before before we go into changes I think Johan and I talked about uh the the issue of some people storing tokens in cookies is that I think and this is something that we as a work group we don't recommend but I don't think I've seen it documented anywhere so would that be covered in this document there is a I'm trying to think of where cookies are mentioned in here cookies are mentioned a couple of times um I don't know if there's any actual mention of using cookies as token storage though should we talk about this in this document right probably okay are there strong feelings hopefully the same feelings about about this about um a about browser-based code using cookies as a storage mechanism uh not talking about the concept of the um like the token mediating backend May set a or sorry the the pure browser the pure the proxy version where everything gets routed through the proxy in that model you need some sort of cookie between the proxy and the client which may be the token itself um I believe that case is already called out with some notes in in the document but this what we're found is mentioning is the idea of JavaScript code using the cookie API in the browser to actually store things which it can do that's not really what cookies are for but you can use it that way local storage is obviously the better solution for storing things in JavaScript"
  },
  {
    "startTime": "00:40:01",
    "text": "um is that generally the General agreement on that because I can make a note about in the local storage section of like or around there somewhere of don't treat your don't treat your cookie API as a storage mechanism a lot of nods okay cool well um can you make sure that's in the minutes so I don't forget document not using the cookie API as a storage API uh oh actually that is relevant to number two issue number two um which it uh does need a actual section calling about calling out token storage techniques okay so um there are a few different ways JavaScript apps can handle storing tokens and again it's regardless of how the token was acquired whether it's from the back end or from the browser app doing oauth itself um but there are uh you could you could keep it just in memory where it isn't actually persisted anywhere you can use local storage session storage or cookies and we'll recommend not using cookies um and then there are reasons to put it in local storage and reasons to not and just use memory instead and both of those are fine in different scenarios so we're not going to say don't do one or the other but we'll have to mention the considerations about both situations um so that's probably the last big section that's going to go in and then um one of the last two new items number six um just go back through the security BCP to make sure that this draft is is consistent with it hopefully not too much of it is"
  },
  {
    "startTime": "00:42:01",
    "text": "duplicated to begin with but I know there were some things that were at least like said as described by the security BC blah blah blah just to make sure people are uh finding it so that'll be one pass to go through that and I think that's like mostly it uh except the new thing that just came up which is a whole section about core's recommendations which um is I feel like it touches every every draft that we're working on right now so um the security BCP it either does or soon will recommend the as on the authorization endpoint not having the course headers that feels like the right spot for that one to go that doesn't really apply to the browser-based app spec um it does apply to the security BCP and 2.1 and open ID connect um the browser-based apps spec probably should make another mention of all the endpoints that do need course headers to support browser-based apps properly that feels like an appropriate place to put that um and then 2.1 of course like I mentioned and basically grab these from both of those drafts to make sure that it's mentioned in 2.1 so um nothing really new to talk about cores here because we already talked about it in the context of 2.1 but it feels like it is worth putting it into here for people who are reading this draft um and then that would be the so yeah basically two sections to add the um the token storage section and then the course section and that hopefully is the end of this draft and I think that's the end of my slides yeah um so yeah with that hopefully with those two things out of the way we are coming up on on being able to push this one through push this across the finish line awesome great any"
  },
  {
    "startTime": "00:44:00",
    "text": "less many comments questions to Aaron okay some sort I guess we not ready to kind of push this for these forward we still have lots of work a little a couple more sections and hopefully um I would very much like to have these sections added in any discussion about the specific text happen over the next couple of months on the mailing list so that by the time we meet next time there aren't any more planned changes awesome perfect thank you Aaron all right see that thank you okay um Daniel I think the slides might not be the last one because I've just approved it I just noticed that you sent it but um do you want me to I don't know how to share it otherwise maybe can you send me the slides directly I'm maybe just this plate here is there a way to pull it actually you know let me see okay let me try to where is it because this okay let me share it and tell me if this is the latest um do you know which slide that okay"
  },
  {
    "startTime": "00:46:00",
    "text": "okay send me yeah that works okay did you send it in to my email oh it got it got it got it thanks I guess let me see now not this one here oh hold on this one probably yes look at this oh this one and yeah yeah just let me do full screen oh shoot it's too much I'll have to drive it from here okay unfortunately okay okay let's go our full screen available uh I've tried I this is supposed to be supposedly you know what let me let me do this one this one no no so let's do this I'm gonna do this hold on one second come on sorry guys maybe yeah from PDF yeah maybe that would be better you on screen okay I think this is better okay let me know when to go yeah that's good"
  },
  {
    "startTime": "00:48:03",
    "text": "I'll try to adjust this a bit okay hello everybody um we're going to speak about the SD draw draft selective disclosure for jwts um this is mostly an update to um our last presentation in Philadelphia so if you're not familiar with Selective disclosure charts um Mikey worthwhile to read the draft um next slide please um keep in mind that one of the main design features of St George is to be simple um so simple to implement a simple to use um so that's um what we consider the main feature here next slide please um we did a lot of a lot of updates since last time especially last time this was still an individual draft and now this is a working group draft so thank you everybody for your support um we updated the terminology that is used in the document to be hopefully more clear than it used to be um we introduced what we called a combined format actually we didn't introduce it but we now properly name it and explain it um the combined format for transporting um SD drafts and other data um we moved away from just one hash algorithm to allow General digest derivation algorithms to um be used with a standard although sha-256 is still the default we clarified what you need to do um to verify the signature on an SD jot um and the data that is disclosed as a verifier we also"
  },
  {
    "startTime": "00:50:00",
    "text": "um hopefully improve our explanation on why we chose this specific encoding um that we choose in in this draft we get to that later um we introduced a feature that was often asked for namely a blinding claim names and we now describe a processing model that we think will be useful to most verifiers um in in processing SC draws thanks to Aaron we also now have a repository in the oauth or working group GitHub um project or organization or whatever it's called um so this is the place where you need to go if you want to see the latest like the latest latest editors draft and so on um and with that I think Christina will speak about updated terminology um thanks Daniel then you'll give a great summary but just to dive into a couple of details um next slide please server received feedbacks that the house each object slash jots being sent could be better explained better name to be more intuitive so the actual um SD job signed by the issuers that part has no changes in terminology one new part is an object that is sure sends alongside the signed jot this object that includes a mapping between plain text claim values salts and now optionally claim names that was used to be called SVC sold value container now it's called issue issue disclosures objects so we're introducing this new concept of disclosures um which is essential is a snapping between for all this plain text values so this one is issued by the issuer just"
  },
  {
    "startTime": "00:52:02",
    "text": "the original disclosures and it is not signed so it's an object and it's never signed so it's an object not a job um if you go to the next slide um and also when the end user chooses out of those issue issue disclosures which of those disclosures which of those mapping the user actually wants to disclose that object is now called holder selected disclosures jot um I think it used to be called releases um and now it's again holder selected disclosures jot and it's a jot because when holder binding is required it could be signed or it could be unsigned so those are two big Concepts so abbreviations are II disclosures HS disclosures so just so people are not confused it's same concept cryptographically but different terminology if you go to the next slide um so yeah just a summary and in addition to what I have already covered in terms of actual claim the property names SD underscore lease in IA disclosures is now is the underscore II disclosures and and they and hold or select the disclosures now it's called SD underscore H is underscore disclosures so you see this throughout examples um in the presentation and the last button bullet point is related to the topic I think I'll cover soon if you go to the next slides um oh wow it's old slides I guess give me skip it um yeah skip it so yeah from the implementers is also received the feedback that if you go to the next slide"
  },
  {
    "startTime": "00:54:00",
    "text": "um people wanted clarification of what how these objects are actually being sent transported between issuer and end user and an end user and the verifier so we introduced the concept of combined formats so combined format of issuance consists of four um dot separated parts or the first three is a DOT which is assigned SD job and the last part is base64 Euro encoded Json which is this um this issue issue disclosures object um and for the from the end user to the verifier that's where we introduce the concept of combined format for presentation which consists of six dot separated Parts when the holder binding is required um meaning the first three are always sure sign SD job but the last three um our holder signed holder select the disclosure shots and the last signature could be optional if there is no holder binding required so they started to basic units that are going to be transported between these entities um and how they're actually being transported is out of scope so that's up to transfer protocol um but just to clarify that this is a basic unit and what I'm implicitly saying is if you're sending one SD job you have to send one holder so that the disclosures per SD job so that's an implicit clarification of San Jose comments they've received um next slide please so we also received feedback that in the original individual draft yeah if you can look next slide we only supported basic hashing algorithm um but we also received feedback that some people wanted to use on hmac or do something a bit more fancier on for"
  },
  {
    "startTime": "00:56:00",
    "text": "example using really Advanced um cryptography to make but with really really small salt values but achieve the same level of security as you know really um complex um hashing algorithm for example so that's expanded so throughout the text you would see um changes in terminology from hash algorithm to a more General digest derivation algorithm um but still shot 256 is mandatory to implement hash algorithm and all the security guidance related to you know minimum bytes that has to be used on that part has not changed like the term digest derivation algorithm is a little bit unusual and then I see two types of algorithms one is an HVAC which is a key uh hash function the other one is the like unkit hash function like shot 256 so what um in which directions is going like so the intention was to accommodate both by using a terminology digest derivation algorithm which is originally I think suggested by Christian Paquin um the cryptographic researcher but if this is not intuitive enough um the purpose does not change I think it would be a terminology change to be honest and the way we clarified in the spec is so it's so much value obviously changes we're using the soul the terms sold throughout the spec for consistency but obviously if it's hmac it's the key it's not the random salt value so yeah maybe that could be clarified a bit better but just the main point here is that intention is to allow the usage of hmac too okay I think the intention is your name my name is Brian uh this is just just for the north people apologies I forget the intention"
  },
  {
    "startTime": "00:58:01",
    "text": "I think sort of does shine through but uh the realization of this is is really problematic from an implementation standpoint like you talk about interchangeability of salt and and key between hash and um hmac but the actual actually doing that is it's not specified in any kind of inoperable way and note that the salt exists underneath the string literal that will be computed for the digest so to actually make that work for an hmac you would have to parse the string that's supposed to be considered um opaque basically at that layer pull out that and then use that as the key um and then I don't know if you're supposed to leave it in there for the computation of the the digest like these are all questions that could be answered but they're totally underspecified um and then you have the name also is so I I did some Googling and the only results for digest derivation algorithm are this draft and the comment where Christian suggested it so I don't think it's an actual yeah term that that's sort of a side but the names are too long to like just from from jots we try to do things shorter so like digest derivation algorithm like it just um but the I think the real problem is is the applicability of the two different algorithms is to honest's point like it it either needs to be built in a way that it can actually accommodate the two or not and I I think actually the use of hmac here is sort of questionable the need for it based on things that uh that uh that like Neil had said he was worried about length prefix or length of pension I don't even know but the fact that it's using Json it's not really a problem referring back to the um jws ALG"
  },
  {
    "startTime": "01:00:00",
    "text": "for hmac is also like conceptually I know what you're trying to do but from a spec implementation standpoint it's it's not appropriate or interoperable at all um and then you also have basically a namespace here that's covered by two different Registries and oh you could also extend it yourself so that that doesn't really work either um I don't mean to be over overly critical but it's sort of like the idea sort of makes sense the realization of it is is um not there it needs a lot of work and I think I had some other annoying comments but Daniel's got do we have GitHub issues for that yeah no because it there's a lot to describe it I didn't know how to write it down so um I wanted to take the opportunity to Face to Face Time To Explain the rationale but I will add a GitHub issue that at least mentions um maybe not the specifics but the Brokenness of it thank you so so maybe like just for harnesses uh because he's taking notes like can you summarize some like a sentence derivation algorithm is too complicated and how H mckindies is completely under specified so either specify it or delete it I think that's a good summary it's yeah it's problematic it needs I don't know how to summarize it other than it needs to be either simplified or yeah expanded if someone can throw an issue we can discuss there like if we want to you know clarify or we want to take it out optional itself right because there are a few people asking for that but I agree that yes you have to you know take it out and then the treatment of thought as a key becomes problematic thank you John uh John Bradley um I'm going to largely agree with Brian um what you're what you're trying to do here is sort of isomorphic to"
  },
  {
    "startTime": "01:02:02",
    "text": "um uh kdfs um you're just using the key as the hash or as the digest um so I would probably try to name it similarly um because essentially for key derivation functions you have either a straight hash or hmac Etc I mean it that you're just using it for a different purpose but they're essentially the same algorithms um so I would either take it out and just say you have to use um hashing algorithm or you need a lot more specification because yeah whether you leave the the nonce in the thing that you're hashing in gets gets complicated yeah we need to look into this yeah great yeah thanks for the feedback appreciate it um should we go the next slide yep okay so the last slide from my part I think signature validation this is for holder selected disclosures jot so as I think we've said before it can be signed or can be unsigned so we updated the validation section to make clear that then verifier verifies a holder cycle disclosures because that becomes the crucial for the security of this mechanism that the verifier actually you know does all the validation Computing hashes whatnot um so it should be not passive in a sense that it's signed I'm going to verify it no hold the verifier must have a policy whether it accepts"
  },
  {
    "startTime": "01:04:01",
    "text": "it requires signing on the holders fine disclosures job or it doesn't so if it requires signing meaning it requires holder binding where it validates that the signature on the this HS disclosures joint is done by the key signed over by the issuer so the holder the user is proving control the same key both during the issuance and the verification if that feature is required and they this HS disclosure shot is not signed very far must reject like that just you know like if you wanted to be signed and it's not signed blank or Jack if for whatever reason trust framework you know policy they verify is okay to not have a holder binding and there are legitimate use cases that are okay with that if that is your use case you could accept the um jots using nav algorithm is the clarification of the edit um let's see Brian Brian there's a lot going on here it's complicated but and I don't have a specific issue for this because it's hard to express but the the inclusion of the disclosures element underneath the signature I believe complicates things sufficiently that we shouldn't do it um okay it I will shut up then thank you yeah let's get to that section so yeah is it is it a good idea or are there good justifications for not having a signature it's like yes"
  },
  {
    "startTime": "01:06:01",
    "text": "um there are use cases where you don't need holder binding so you just want to know that the document exists that was signed by an issuer for some user so for some end user and you don't care whether it's the user so the holder binding does not have to be cryptographic holder binding it could be claim based or biometric based so their use case or people trying to combines that with this disclosure feature so that's another kind of why it's not always mandated uh Tony and Edmund um question is the holder binding meant to do device binding also so I there is a little bit of a difference there so I'm just trying to understand your whether you want this to cover device binding or not yeah I guess I guess it depends on the uh sorry yes it can be it depends on the verifier um and right I just didn't see any recommendations and that's right so that would be one area that needs to be it can we document in the minutes to clarify the relationship between holder binding and device binding thanks that's a good point yep okay any more questions no okay next slide please um takes time to get there yeah um okay a word on the encoding um that we're using um I think this is actually the old slide set is it yeah anyway let's let's let's try with that um so one problem maybe next slide um this was actually oh so we have to this so yeah so there are a couple of slides that we didn't want to show and we mark them as not show but they are in the PDF anyway yeah um so we'll skip a few slides okay uh"
  },
  {
    "startTime": "01:08:02",
    "text": "but this one is good so um when an issuer creates um the SD draft um it takes the data from in this case an address claim uh with the street address locality and so on um then transfers that to a byte string obviously to then hash that and sign it um so that's very simple process um next slide please now that's a complication the stuff is sent to the verifier and the verifier needs to do the same computation um so the net verifier has some data transfers it into bytes hashes it um and obviously next slide please the verifier also gets um so so looks at the SD drop and there's some signed hash values and the verifier now checks whether the assigned hash values are the same and in this case so and there might be cases where that doesn't work especially if the data that is transferred from the issuer to the verifier is modified between the issuer and the verifier and such things can happen when you transfer Json because in Json the order of elements is undefined essentially um there are some there's some room for um expressing the same thing in different ways for example you have that with numbers floating Point numbers for example um specific things of how to encode Unicode strings for example but also white space between the elements in adjacent so the issue has sent something that is Jason um or has some data transverse it to to Json and the verifier that's a Json and then does the same computation but not necessarily with the same result so the byte string that is hashed might be different and of course when that happens there's a different hash and"
  },
  {
    "startTime": "01:10:02",
    "text": "um the the apparently the the signature um check will not work although the same data was transferred so here in this case you can see that street address and locality they changed the order and the white space is different and so on and that's a problem and that's the problem we need to address next slide please that's essentially two different approaches that you can take one is to send the exact bytes that were hashed from the issuer to the verifier um so I call this the source string because so the source of of um your hash um or you can apply a transformation both at the issuer and at the verifier that ensures that both really um hash the same byte string so both end up with the same hash at the end of the day so that would be canonicalization um where so this you need obviously to do at both the issue room and the verify in any way in any case whatever we do we need to Define what we do um in the spec to ensure interoperability we need to ensure that issue and verifier agree on on how this Computing is done um that's a question by Hannis um I obviously have some more slides I want to walk around um I think you have to do both anyway because uh if you think back about HTTP yeah also the HTTP signature work we did that's the same problem if you just compare the hashed value uh or take the hashed value to do further things then um and not really compare it to what was originally hashed so you have the same issue again then you will run into problems because someone attacker could swap out things um because you are later creating or do"
  },
  {
    "startTime": "01:12:02",
    "text": "all the actions based on the second transmitted hash value and while you actually base decisions on what's in the content of the of the original plain text that would be a problem so you I think you have to do at least the clinicalization in some form no you don't think so not necessarily okay I'm looking forward to see that just one comment so that was harness as an individual on the mic okay uh let's um see if we can get the slides back what's going on there it's not working see yeah I'm trying uh um it is connected I don't have power even like I thought still trying to connect here do we have some weighted music I'm just trying yeah yeah yeah harness will start singing in a second [Music] okay I think I'm back here this guy here um yes yeah no worries okay thank you very much next slide please ah questions is this one yes"
  },
  {
    "startTime": "01:14:00",
    "text": "um so I'd approach that we've taken and the um draft is to ensure that a byte string is transferred from the issuer to the very file that and and because we transfer the byte string we can ensure that always the same thing is Hash because you just hash this byte string if you look closely here um on the top left side you see the address and now it's not a Json object any longer instead it's a string of bytes which happens to encode Json um actually you could use any other encoding at this point um but Json is just something we use anyway so what we do is we build one string per object so that object can be an address in this case like a complex Json object can also be just a Json string it can be a floating Point number whatever but it will be turned into a string by Json encoding and that string is transferred from the issuer to the verifier and that ensures that both hash the same byte string and come to the same um hash at the end of the day um next slide please yeah it's good and next slide was one of the slides we yeah um so canonicalization gives you a clean data structure so you can just transfer things essentially as they are but the problem that we saw with that it is that it's it adds a really non-trivial dependency so you need to ensure that um issue and verifier follow exactly the same rules um that there are libraries for that that Implement that um and if you happen to implement it yourself or the library is not well implemented it can be really hard to"
  },
  {
    "startTime": "01:16:00",
    "text": "debug because the issuer just sends you something um but you will never learn what the issue actually has to get to that hash value that you get alongside um so this can be this this so obviously we can test that you can do conformance tests and so on but if you happen to have an error somewhere it's really not transparent to the verify what's happening and it can be really hard to debug next slide please um so the approach that we've chosen the sawstring encoding is really easy to implement uh with any Json Library that's also feedback that we got from the implementers of the spec um you just do a Json encode on a thing and you're good to go you don't need any new dependencies um so you just need the Json Library and it's actually something that in a similar way is being done in jws anyway so you have the whole Json thing and you're just a bit uh the downside of this is um that um it certainly looks strange so some people ask if that's an error in the in a spec no it's not um that's why we have this lengthy explanation now um and of course if you just look at the um the disclosures object where where the raw strings are in you don't so if you just have that thing you cannot apply Json schema to um in the example the address came for example um you cannot really talk about typing it's always a string so you have one layer more that you need to dig into so um might not be accessible to to such things but um but hopefully at the place we're using it in the spec it's not a place where you would do that because um you process a thing according to what's in the SD drop spec and at the"
  },
  {
    "startTime": "01:18:00",
    "text": "end of the day you get a document out that has the same data as the issuer um process in the first place so you get the same types the same object and so on Justin Richard um not exactly so Json strings um have uh the ability to have for example Unicode encoded characters and stuff like that that allows you to put a different character sequence and get the same semantic bites out the other end yes so this is something that you can do and say it's the Json string value but you have to be way way way more precise than just saying call json.encode um because uh for example uh inside Json strings you can you are fully allowed to prefix forward slashes with a backslash character and that gets sent as a as a two-character thing but it is supposed to be interpreted as a single backslash character or sorry as a single forward slash character you can also send the forward slash character without the prefix backslash because it's technically not a an escaped character but you're allowed to escape it with the same semantic meeting there's also the backslash U Unicode characters and then then there are some Json libraries that do really really weird things with uh Unicode characters without doing the backslash U prefix and coding and just chucking it in there and hoping for the best because it's using the system's underlying string libraries in other words this works until it doesn't and when it doesn't it goes really sideways really really hard so if you're going to be basing this uh off of Json raw Json strings you're going to need to be incredibly precise about how you actually pull those bites out because your normal test cases and"
  },
  {
    "startTime": "01:20:00",
    "text": "your normal use cases are probably going to work most of the time until somebody gets a weird library that is implemented completely compliantly that does something that you weren't expecting so it's not as easy as as it might seem to say Json strings and uh and on top of that I would encourage the authors to look into the uh the it's a bit of a pariah RFC the uh the JCS uh yeah I see a lot of head check exactly exactly because that's what you're doing okay let me let me just say something here I think so either I I I don't see the problem yet so I would be happy to to discuss that with you sure um but as far as I see this um we do the Json encoding so we we take back the address object right right um We call json.encode we get a string and that string Ascend as part of another Json object from A to B um of course you can add backslashes and unicode escapes and so on there um but you're actually like you call the encode on one string and you get you call the D code on the other string and you get the same data you get the same byte string um you're not we are not hashing the thing that was already encoded so um how to explain because we do do two encodings here but um as far as I see so and correct me if I'm wrong when I have any byte string and I call Jason encode and I call Json D code I get the same byte stream right it's kind of kind of sometimes right usually so like not on the wire right I know not on the right right so what I'm saying A and B see the same bite string sometimes usually right yeah so that's that's the expectation that we"
  },
  {
    "startTime": "01:22:00",
    "text": "have right be happy to validate it but it's a beautiful expectation so what I would say is that uh where you need to be precise in this is where exactly in the encoded versus decoded versus stuff stack you're expecting to be able to get those bytes because if you're throwing things through especially a Json decoder on the far end it's already gone through a Json parser most likely because it's an encoded string so it's been parsed as a Json strings and so that's been already nothing the the inner thing has been parsed as a Json string it's a Json string right right exactly so that's already been through one round of Json parser which can do all sorts of stuff to the insides of the string and uh and change the bytes that were on The Wire so what I'm saying is about the thing on the wire I I understand that I think that you I think that you need to so any anyway we could we can talk more about this too there's there's weird stuff that happens in The Wire don't reinvent JCS it was a bad idea then it's still a bad idea and uh yeah yeah that's that's why we're not doing anything so maybe Justin just quick question to you so are you advocating for canonization or are you saying just we want to be precise with this with this approach I'm saying that if you're going to do this approach you need to be very precise about where exactly you're saying get the byte stream okay and uh because there are some subtleties in implementations that are going to burn people in weird and unpredictable ways it's going to be it this is going to be the corner cases and the edge cases that really really get you here the day-to-day stuff Chuck it in Json and code and Json decode it's just going to work completely agree with that that's it like the vast majority of cases it's just gonna work and that's fine but in order for this to be a real like robust"
  },
  {
    "startTime": "01:24:00",
    "text": "security spec as we know we need to care about those corner and edge cases and this type of uh if I'm getting it I've already called json.parse and now I'm calling json.decode on something uh like maybe that's already been called or maybe that's not been called they're called No you're calling parse in order to get it out of the object in the first place yes afterwards is just to look at the value inside not yes I know yeah okay so yeah yeah anyway I I know what you're doing and I and I think that you have to just say like basically in this part of your Json stack like this is where you grab the bytes and you should be okay yeah okay so so in principle you don't have a problem with the approach you just want Precision here right I'm saying if the approach is this then it needs to be very very precise I'm not making a value judgment on this approach one way or another okay okay thank you next slide please or a steel transmute um agree with most of what he just said but I think generally you're doing canonicalization whether you want to call it that or not that's it that was my point yeah okay yeah Mike Jones Microsoft I'll actually disagree with ori's last point I think what they're trying to do is uh hardening the string for transmission I mean a lot of the Jose stuff uses base64 URL for hardening in the same way I'm not saying you want to use that choice or not but the point is to get a string that's going to survive transmission exactly"
  },
  {
    "startTime": "01:26:01",
    "text": "and I hate canonicalization just for the record thank you for not doing that yeah yeah okay you're on you're answering to it so sort of a follow-up on what Mike just said you're you're starting with the fullness of Json and all its complexity as a prerequisite you want to essentially support disclosure of anything that looks like Jason and my question is whether you can Harden it enough maybe for example by putting constraints on claim names on cardinality so can I have multiple claims of the same name stuff like that maybe yeah the the encoding of clan names to their security um doesn't depend as much on the implementation details of these libraries I think that would be kind of mechanization right if I if I say that this needs to look in a very specific way okay I'm gonna cut the mic after you're on so because we're we need to move on but yeah go ahead yeah but I don't see it as Canon canonicalization more of can I have yeah a well-defined a claim document that still uh secure securely robust are you talking about for example saying do not put backslashes in the claim actual claim values so you don't have a problem or that's not not probably not values but claim names that's what you're saying so stuff like that overhead"
  },
  {
    "startTime": "01:28:00",
    "text": "I I'm not sure I'm not sure if that Awards the problem here um but yeah okay yeah a few minutes okay I tried to do this in a few minutes yeah next slide please um so this is just a quick example um so this is what the um this is an SD dropped so what the issuer creates uh the issue of Science and then sends to the holder and the holder can send this to the verifier um this is essentially so not many changes except for the names there from the last presentation so these are the digest the hashes of the claims of the values uh so now what are the values next slide please um they are then of course um transferred and what we now call the II Disclosure document or issuer issued disclosures uh formerly called SVC um and here we have um the strings so for sup you see a string for given name you see a string and each string then encodes the Json object this Json object itself has two keys s for the salt and V for the value and this approach um is also nice because it ensures that you have a separation between the salt and the value which means that um there can be no hash lengths extension attacks or anything like that where the where part of the salt is um considered um like part of the value or vice versa so it's it's clearly separated um if you don't do it this way you need to think about how to um take the salt in the value and then hash them together you need to Define that step as well we don't need to do that we just have an object S and B salt and value and of course that whole string for birthdate for example then hashes to what's in the SD jot uh next"
  },
  {
    "startTime": "01:30:03",
    "text": "slide please next slide um this is then so the issue issue disclosures is created by the issuer sent to the holder the holder selects some of their claims to disclose to the verifier so the HS disclosures document is a subset of the issuer issued disclosures document in the current uh spec the um same document is also used for the holder binding so stuff like an on so audience or other things can be added into this document and then the whole thing can be signed and sent uh from the holder to the verifier and the verify it and then verify that this was actually signed with the holders key um now um coming back to what a point that Brian raised um this thing as we have it in respect right now actually serves two different purposes one is to just transfer the holder selected disclosures you can essentially do that unsigned there's no really no real need to sign that thing um you could just send it as a Json object and then there's the holder binding um which is just a signature over the nons and the audience wherever that nonce comes from by the way um and that is a different purpose so that is to show that the holder is able to sign something that's fresh because it has response and intended for that verifier um using its key um so two different things and we could think about separating them so just having a Json object for the HS disclosures and then optionally having a sign chart for the holder binding that's an option that we probably will consider I guess"
  },
  {
    "startTime": "01:32:02",
    "text": "um questions I think we need to move on so if like if you have questions maybe just raise them and make sure people and we can discuss it in the side meetings if you want right okay um then next slide please um we also have claim name blinding now so some of the claim names can be replaced by random strings in this case um I don't know what it was family name no family names anyway this claim was blinded so it has been replaced by a random string the Israel selects that random string um and just exchanges claim name and the random string next slide please in the disclosures we now have an entry for the claim with that random string and um we now have a third element in the disclosure which is called n for the original claim name and that element just contains the original claim name now um this might look like complicating things further for an application consuming SD drafts but we'll get to the processing model where we say okay the steel drought Library can do all the hard work on um processing this putting in the original claim name instead of the the line of claim name and the application will address that adjacent document that will will not have any traces of the planning claim named planning claim names next slide please processing model how fitting um so this is a processing model how many slides do you have yeah just loving everything okay um uh the processing model is how we think that as C drive libraries will process SD jobs um simple steps verify all the things that you get verify the the disclosures actually match uh the SD draft"
  },
  {
    "startTime": "01:34:03",
    "text": "um unblind any blinded claim names if there are any um March the selectively disclosable claim names uh claims into the non-selectively disclosable uh claims in the SD drought which means that at output time you get just one document which looks like the body of a normal dot which has been verified processed and you can just put that into your application so the application doesn't need to know anything about SD dropped next slide please okay next slide um we now have five running implementations so we have our implementation that we keep up to date to generate all the examples in the spec um and we now have a new typescript implementation second type script implementation as well next slide okay um just really quickly uh next steps um I think we need to think about how this can look like in the context of other existing credential formats um if there's a mapping between them if this is completely different um we need to think about that probably create some examples um and discuss that with the relevant um groups um we still need some security and privacy considerations and it would be great if at some point we could do an interoperability test between implementation stuff we have we could even do that offline because you can just create these things and consume these things um you don't need to to be online for that thank you very much you thank you Daniel and Christina do you need time off like on the outside meeting to continue this discussion okay sounds good okay awesome thank you Brian let's get ready"
  },
  {
    "startTime": "01:36:09",
    "text": "I'm gonna pass then uh control to you where are you here foreign there you go got it yeah there you go all right tough act to follow um I'm here to talk about the Step Up authentication challenge protocol um and yeah so including a picture as Aaron said is sort of fundamental this was actually taking ietf 89 back in uh I don't know two a while ago so without further Ado Let's uh move forward so a little bit of backstory context I have a hard time presenting without providing some context I'll try to get through it quickly basically a protected resource can technically reject like you can reject a technically valid access token for whatever reason that it wants um maybe a risk engine decision uh some local constraints I say here bad vibes like really it whatever it decides uh is a reason to reject that token it's totally within it's it's purvey to do so and really oftentimes what a resource wants then is a token obtained from a more recent user and active authentication event or a token obtained with a different authentication flow probably a stronger one um and there's no current standardized guidance on how to do this for the RS to express those requirements down to the client and the client to indicate those requirements back through to the authorization server in the flow to acquire a new token my phone locked there we go um so we uh Victoria myself uh tried to address this through a draft in the working group process so forth so forth the summary of the drafts approach is"
  },
  {
    "startTime": "01:38:00",
    "text": "extending RFC uh 66 750 with a new error code insufficient user authentication uh to for the end sorry and a new new parameters on the www authenticate header ACR values and max age and this gives then the resource the opportunity to express down to the client the the conditions that we previously talked about either and or both that uh different ACR value representative of the authentication flow or authentication context or a more recent authentication event is required associated with the access token that will be issued off of in turn then we utilize the authorization request parameters ACR values and max age to allow the client to convey to the authorization server its needs around the authentication event these are parameters already defined and registered via oidc core in the uh our oauth parameters registry and then uh Define and or reference depending on the context ACR and off time introspection response parameters and JW claims JWT claims uh to express that information about the authentication event associated associated with the access token to the protected resource so really it's just stuffing them in the job or making them available of via introspection via the same claim names they're already defined for jots we just reference them um they're more explicitly defined in this draft for uh introspection to be to be clear and um this is the kind of flow diagram I I'm hesitating whether it's really worth going over um basically you're making API requests uh up before one you get to one a tokens presented it it has token information those are the projected resource decides hey it's not good enough and in this case it's challenging in step two"
  },
  {
    "startTime": "01:40:00",
    "text": "basically saying I need a more recent authentication event associated with a token that you're presenting me as a result of to the client uh pops up a browser or directs the end user's browser to make a new authorization request and includes in this case a max age parameter saying I need a more recent event authentication event associated with the access token um then the magic happens all out of scope but the authorization server prompts the user does the authentication uh dance at per it's purvey uh and ultimately returns a new access token in step four some things submitted there but hopefully you know the drill and then uh in step five it makes the same API call with the new access token inside or referenced by that access token is a more recent authentication event represented by off time same flow could happen with ACR but using off time here and the protected resource is happy with it keep locking my phone um so just kind of quick summary of where we are uh in Vienna we first presented this draft back in itf13 it was adopted and shortly after that and followed by the pretty typical sort of standard iteration process some comments some new updates so forth we got drafts one and two and then uh talked about it in Philly a few things have happened since then published uh three uh clarified that the ACR values and the max age can occur in the same challenge when and if they're necessary sometimes you want to ask for both flush out the deployment and security considerations uh also did all the INR registry stuff that's necessary um tried to clarify because it wasn't clear to everyone that while the ACR values which is basically saying these are the accepted acrs that we would take for this can have more than one value it's a space separated listing of ACR values the actual authentication only can be qualified as meeting one though one of those so the token itself only has a"
  },
  {
    "startTime": "01:42:01",
    "text": "single ACR only one ends up in the token uh we did migrate this over and by we I mean Aaron migrated it over into the uh new GitHub org here and thank you for doing that were coming along to the the new process uh but it's it's nice tooling it's very very useful uh did working group last call was uh September 22nd October 2nd um did drafts four and five to uh mostly editorial updates and feedback addressing those and then um they were the day after each other noticed that the updates needed some updating so we fixed those um recently I updated some examples and figures to be really clear that the authorization request is sent by the client not directly but via directing the end users user agent or browser to make the call um in some ways this is related back to the the discussion of of course although it's not mentioned here um what happened here is I was I was actually sort of refurbishing vittorio's slide deck to um to do these slides and it became kind of clear that the the ideas had been intermixed and that some of the slides made it look like maybe the authorization request was coming directly from the client didn't say that but it maybe implied it and I wanted to clarify that look back at the draft and it in fact had the same kind of potential for ambiguity so I fixed those um and I say a new draft is coming soon they actually published this yesterday um there's no normative changes but I think it's an important clarification that maybe folks that are familiar with oauth would just sort of gloss over because they know how it works but if you're reading it kind of literally I think it's important clarification to fix and then uh hopefully anticipated soonish uh will be the Shepherds review here um and I know uh the shepherd you know has a lot going on including uh covet bout that slows some other things down but um no pressure but that that's"
  },
  {
    "startTime": "01:44:00",
    "text": "kind of where we're out with hopefully soon and someone recently uh kindly pointed out that iatf-16 is not in Prague I was so eager to include a picture here um and I don't have one of Yokohama and I'm going to miss Yokohama that I got confused myself but uh looking ahead hopefully uh this will be the last time seeing a presentation about this I will be probably in fog but I won't be in Yokohama to talk about it anyway so uh yeah so what question that there was taka had provided some feedback on the list and there was some discussion there do you know the summary of that discussion what's the status there uh the summary was some back and forth between him and Victoria uh ultimately he sort of came to the it would be nice if you said a little something like this but it's not a big deal um and he said he wouldn't push for it to be changed which I appreciated and I I am personally of the opinion that that what he's asking for only complicates things more than it confuses either way it's it's not protocol level it's just clarification so um okay he I think the the output is nothing's going to happen Okay but if you check the thread you'll see that that's consistent okay I'll take a look um anybody else has any comments questions uh Daryl Miller Microsoft and just one call out if you're adding a new parameter into the www authenticate header um I don't know have you looked at the structured Fields work uh there's an effort to try and standardize how structured Fields work and there's some work going on in the HP working group to retrofit uh existing HP headers to use more standardized ways and you mentioned about space delimited I need to look into it that might be the right way of doing it it might not fit into the new structured header field World um is is authenticate one of the ones that"
  },
  {
    "startTime": "01:46:00",
    "text": "retrofitting it isn't in the spec at the moment but I believe it responds to do that Mark Nottingham would be the one too I can take a look at it timeline may or may not make sense on actually referencing that the space delimited thing is actually another layer it's within the value itself of a parameter of the header and it's defined I'm not sure there is an inner list thing Institute that is space limited it might fit anyway if there's an easy way to make it fit we should just try if if there is yeah thanks okay thanks Daryl um somebody in the queue Jem and deep go ahead forensic Sciences University uh so I was just uh wondering like uh if uh we want to include uh uh the other uh parameters also which are going to the uh protected resource server uh like uh client authentication parameters like if we are also looking uh uh at that way yeah or maybe it's out of a scope for the present uh uh draft RFC okay I'm gonna cut the mic after Jim and DPA go ahead Brian generally speaking it's it's sort of beyond the scope of what we're trying to accomplish here um it's obviously a lot more to that that that's been discussed sort of ad nauseam and the thread but that for for a number of reasons that stuff isn't applicable or either isn't within the scope of the drafters and technically makes sense in in the context of where these things are communicated so okay um okay okay yeah uh next steps are um just published those six with so that's been done and basically awaiting"
  },
  {
    "startTime": "01:48:01",
    "text": "the shepherd right up at this point okay with no time pressure on that but yeah seriously but that that is that that's where we're right now okay awesome thank you Brian appreciate it thank you thank you okay um I see um Ben is on the line there let me pop your slides here let me see if I can hand you the control then hold on where are you there okay I need to find you first oh here oh no can't do it oh here from here maybe for some reason I can't okay and that's fine I'll I'll drive the slides in okay so hi everybody this is a new draft next slide that's a long title but I think the the short answer is that this is about pop-up authentication uh so this is what login looks like on the web today thanks in part to the great work of people in this working group we have this single sign-on ecosystem we have a bunch of very rich ways to authenticate users very securely through Yuba keys and pass keys and all sorts of great new Innovative stuff next slide this is what it looks like if you're not uh using the web today if you're trying to use HTTP standards outside of the web context uh so on the left we have a caldav login screen on the right we have a proxy authentication login screen these are the state of the art respectively for uh for caldap for proxies and in general"
  },
  {
    "startTime": "01:50:01",
    "text": "for any login system that doesn't benefit from a web browser next slide so non-web login is basically stuck in 1996 these standards have have in my view not significantly changed next slide so uh what about oauth uh I'm here because I'm definitely not an expert on oauth and I want to get input from this group about how to address this problem and and bring these systems into the modern era but my understanding is that oauth generally requires the client to know in advance who it's going to be talking to uh and this is really about clients that want to be able to essentially access any HTTP resource on any domain subject to potentially an authentication prompt uh with the only requirement being that that the client and this origin uh speak the both Implement a defined standard next slide so here's the user experience that I'm kind of imagining here it's it is laid out actually this way in the draft so you're you're going along your business maybe you've just changed a configuration setting or or maybe you're just going about your business on on your device and you get a notification that something on your device is requesting interactive Authentication and so you can open your browser which will open to essentially a login page you'll go through some sort of probably oauth driven single sign-on probably server to server oauth and then at some point there will be a signal that comes back to your browser that says authentication has completed at that point the browser window will close you'll see a second notification you're done and now you go back to whatever you were doing next slide"
  },
  {
    "startTime": "01:52:01",
    "text": "so this is the specific protocol that's laid out in the draft but this is really I think kind of a placeholder I would really welcome some more input on how to structure this exchange but in this case the client says Hey do you support caldav in this example and the server says who are you uh open a browser or you can see there are other www authenticate response headers here so there could also be other maybe more old-fashioned authentication options uh presented in the challenge next slide so that that www authenticate header had a parameter called location equals slash login so that tells the client uh where this login page is so the client now opens a web browser and that web browser is instructed to load this page and so here we get back again a 401 response but this time it contains HTML telling us the login instructions next slide another client does something clicks buttons enters passwords Taps you but Keys navigates maybe between different Origins and approves things at some point the client comes back and the browser fetches this login page again um this time it sends that Fetch with a cookie that's been set in some uh as some part of that flow and now it gets a 200 response which is the signal that means we're done here so the browser closes next slide and now we're back in the non-interactive uh part of this so now again the caldav client is trying to load this endpoint but it now it copies that cookie header out of the successful request from the browser into the caldav client and now it can actually speak cow Dev to that endpoint next slide"
  },
  {
    "startTime": "01:54:01",
    "text": "so this is an overview of the proposed protocol I want to focus on step five where the or step four where the browser loads the authentication path and if that authentication path ever loads successfully while this browser instance is operating the client copies all those request headers out of the request and then kills the browser and then those headers are copied into any future requests that are made by this non-interactive client trying to reach that origin so next slide there are a lot of interesting Corners that come up in this again this sort of placeholder particular instantiation of of this idea like we like the draft says cookies and authorization headers are both allowed um authorization is more natural I think in this context but it I in my understanding it would force us to use JavaScript in this browser context it would be nice to be able to support non-javascript browsers why not uh cookie seems to to allow us to do that but then uh my my personal use case here is really related to these proxy clients so we need to convert authorization headers for the request of the browser into proxy authorization requests for the proxy and that's natural enough but what do we do about cookies so the draft says find you can't use cookies for that but we could we could Define some way to copy those cookies to a proxy server um there are some interesting ux implications about um you know your your oauth type token expires at some point with some system service you want to refresh that we need to like we're interrupting the user in at some random point in their day to say oh please re-authenticate um so uh but that's enough about this let's move on"
  },
  {
    "startTime": "01:56:02",
    "text": "so this is a brand new draft it's designed to bring all of the great oauth driven single sign-on and and two-factor login stuff to the the sort of the rest of HTTP or the rest of Standards driven HTTP it definitely needs more input from folks like you um I'm especially interested for this group in what components here could be shared more with the oauth ecosystem or even you know is there a way to express this in you know us and uh by the way I'll be repeating this presentation I believe tomorrow morning for the HTTP API working group to get their input and I think uh and I'm seeking adoption here and it's I think it's an interesting question of where that adoption would live and uh if you want to see more use cases I would mention uh the access descriptions draft and mask okay that's all Aaron thanks Ben Aaron [Music] yeah hi Aaron purkey from OCTA um if I were to rephrase this in terms of oauth terms and roles that we already use it seems like there it's almost everything is already there to make this work and there's really only one thing needed so it sounds like what you're trying to do is Drive authentication event when a resource is requested so we have a we have the in one of the in I think it's 6750 talks about the error response when a client makes a request a resource server that does not contain a token or contains a uh wrong kind of token and that's also what the step of auth draft is sort of getting into as well and um"
  },
  {
    "startTime": "01:58:01",
    "text": "what we don't have in that draft or I don't think any other ones is the concept of the resource server driving the location of the telling the client where the as is and that's kind of what you're asking to do of a calendar client shows up and says I know a resource server I'm talking to try to make a request that resource says you need to log in here's where to go log in where the client doesn't necessarily know that before and I think you write them most of the oauth work is the other way around where the client knows the as ahead of time but that's the only missing piece everything else actually already fits into the oauth world so I think there's a lot of I don't think you need to define a whole lot of new things and I think that the only piece needed is defining the location of the as at the resource server which would then do all the normal low-off steps after that so normal authorization code flow you don't need to Define any new things about what to do with the extra headers and cookies and stuff because all that will just sort of piggyback off the existing flows um but we have just few minutes and we have a number of people on the Queue so I'm I'm gonna give people a chance to just to it give their comments and and maybe we can take it after that offline okay okay and so um are you done with your comment yeah that's what I wanted to say talk about that more and maybe some of us here can chat about it too okay thanks Adam um just one John just hold on a few people in the line there Armando hello can you hear me yes okay uh just one quick comment so uh basically in the Privacy past working group we are working on one solution very similar to"
  },
  {
    "startTime": "02:00:01",
    "text": "what you are proposing which is basically just uh increase a new uh uh option in the HTTP authentication header so you can do this kind of uh authorization mechanism so please take a look on the Privacy path group thank you good David I just wanted to point out that uh there was an internet draft I believe a while back where someone proposed uh might have been that Tech Middle to use uh the link header to advertise the AES that could fill the that 401 okay thanks David Philip foreign thing I guess that it depends on who the draft expects to drive this orchestration whether it's the client running inside the JavaScript inside the browser or whether it should be the user agent um it the the presentation did not clarify that for me and if uh that's the user agent uh then this is really just limiting to web experience and it cannot be extended to use well I mean to authenticate clis Etc because then there is a Handover information from the browser to a CLI um that we currently do not have yes this is definitely meant to be compatible with with a CLI experience where you you've you've run some command and your tools complies with the standard and and pops up a browser window which then closes when authentic okay so it's complete so you're expecting the client inside the browser"
  },
  {
    "startTime": "02:02:01",
    "text": "or whatever to orchestrate all of this uh that's right this would all be be driven from from inside the the web page essentially but the the CLI needs to have its fingers inside the browser to be able to pull out these uh these outputs right it needs to be able to pull out these these authorization or cookie headers for then use essentially by a different user agent there's a browser user agent then there's this we're out of time so let me give John a chance to wrap it up here John um I think this is a interesting idea that needs to be explored when we were originally doing oauth if memory serves me the error from the resource server set returned what Scopes were required but we explicitly didn't say what the authorization server endpoint was because there were a number of security concerns around essentially having the resource server being able to do fishing on the user so we have to think about what if we're going to change the model and yes Nat did have a draft around that but if we change the order that these things are happening in we have to do it deep Security review as to you know what those implicate if there are any unintended consequences of that um the other is that um you know as the one of the editors of um of uh the Fido specifications there are a bunch of things in browsers that stop um uh web views from being able to use web authen Etc so exactly how you're calling these things if you're planning on grabbing headers out Etc um that may be at odds with the security that browsers and and the os's are"
  },
  {
    "startTime": "02:04:02",
    "text": "implementing the other thing to that you should probably talk to Google about is that they're doing a bunch of work on um proof of possession for cookies uh to prevent cookies from being exfiltrated outside outside of the browser so that may also be somewhat at odds so there's a bunch of things that have to be coordinated to figure out whether or not you know this can actually be practically used end to end but it's probably worth thinking about thank you John thank you Ben just wrap it up now if you have any last minute comments because we are way past that our time no thanks for all that input uh we'll we'll discuss on the list awesome thank you Ben and thank you everyone was great great discussions great presentations see you next meeting foreign okay"
  }
]
