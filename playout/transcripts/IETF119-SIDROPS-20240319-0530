[
  {
    "startTime": "00:00:06",
    "text": "Okay. By my watch, it's time to begin. So we're gonna do that because the agenda is ridiculous for. Last meeting, you may recall that we had a pretty light agenda at this time. We were quite surprised by the amount of, requests we got, so we're gonna keep it moving. The, note well. This is the right privileges, and most importantly, responsibilities for contributing to the IETF. Please make sure You are aware of these before you do so. Please, please, please. Be respectful in all your interactions with everybody all the time. And, in person people, please make sure you've logged into the data tracker. This is how we do blue sheets now. We're not passing anything around. And, please, if you're in the room, keep your audio and video, inputs turned off. And remote participants, please keep your audio and video off except when you're sharing or presenting. These are links if you don't know where, all the things are. Most importantly, is the agenda? The Based on the number of requests for speaking slots that we got, We we're hoping we can get through all of them, but the question is, Should we be planning a virtual interim The chairs think we should if there's anybody who thinks that the wrong way, please speak now. Okay. So we will be starting a doodle for a good time Yes, Jeff."
  },
  {
    "startTime": "00:02:04",
    "text": "Jeff Houston. Might I remind the chairs, a, the world is actually a spheroid. Well, it's not quite, but close enough. And and the sun is only on one side at any time, So once we get into these virtual meeting systems, Some poor bastard has a 3 o'clock in the morning time. And according to the run rate, around the world of these kinds of virtual meetings. The subs in Australia, and and the poor guys around this area of the globe, get the short end of that stick. And, quite frankly, No one wants to hear me at 3 in the morning. Let alone the rest of this virtual working group. Think on the whole because of that and because of the wide diversity of practice patient in this working group. It's not a core of folk and, you know, this place or that place. This is going to actually favor some and really, really cut some others out. And if you start quoting back it as well, we decided in the virtual meeting, then, you know, someone who is probably not there because like I said, you don't wanna see or hear me at that time. This is not a good idea. I appreciate that slows things down. But nevertheless, trying to find a time that's acceptable for literally all around is incredibly challenging. And and, you know, someone always loses. Oh, let's rotate them. Oh god. In my diary, having rotating meetings is about as bad as 3 in the morning. So let me gently counsel the chairs. That that that This might not work for this particular widely dispersed set of participants. Thank you. We understand your concern on another group that is similarly, Broad, in the, across the community, the RSWG I have faced this. And This is why you confirm all the decisions on the mail list. But I understand your point and agree. So the agenda here is here very full."
  },
  {
    "startTime": "00:04:04",
    "text": "So the first question is, is there any agenda Bashes like somebody who doesn't want week. That I didn't think so. Okay. So the first thing we're gonna do is, is go to the first presentation. Which is on ASPA. Yes. This is. Hello, everyone. So I'm going to just give a brief, update on the, ASPA based, part of, part verification, The status is, we did a version bump up, just for keep alive. From, 16 to 17, that was in February but the real changes, will will appear in version 18. Currently, there is a the editor copy which has, the changes and it's in good shape. So we dev devoted a a reasonable time and effort into it to get in get it into a good shape. We are waiting on one author who is taking another look at it and then hoping that, soon we'll upload the version 18 to the IETF. Next slide, please. So the changes that are coming in version 18 first of all, significant I mean, number of places where we have improved the text for clarity and, for removing the redundancies the second the security consideration section, received some working rude comments, to make it more substantial are more balanced between what the procedures of a SPA accomplish for clarification and and where there may be there may be shortcoming. So so that section is"
  },
  {
    "startTime": "00:06:01",
    "text": "is substantially, is substantiated further to make to achieve a good balance there. Per per comments received. One key change is, it's it's normative. We say that, the this document updates RFC 9234. RFC 9234 is about the BGP roles. And only to the customer OTC attribute And that RFC is complimentary to this document because, this document focuses on route leaks that happened somewhere else in the path. But for route leaks, that the local AS might initiate RFC 9234, is where we need to go. So we are, recommending the the use of 9234 together, with the procedures in document. However, we have to say that there is an there's that new BGP role, which will transit which was not included in 92 before, but but that is part of this document. And we simply have to say that because it is mutual transit, these 2 ASSs, which are in that and ship they can send any routes to each other. And therefore, they really do not need to they really do not need the RFC 9234 procedures. On the mutual transit session between them. So that is a one one change. Next slide, please. So that's that's all about ASPA, uh-uh verification draft update. I just want to mention that that we have a new with with with y'all and, have a new draft on assigned the prefix list prefix list based route origin verification and operational considerations. Reason I wanted to mention it here is it does tie into ASR also in some way."
  },
  {
    "startTime": "00:08:00",
    "text": "Because it turns out that the spl or sign prefix list all, also complements ASPA in addition to 9234 RFC. This also complements in some ways, protecting against as forgery and reducing attack surface for forced outage and prefix hijacks. So both of those ASP also does, but only in the upswing direction, but the SPL is, is capable of doing when mitigating these threats in, actually, in in general in any direction. So with those with that, I close We would welcome any comments on this draft on the mailing list. Thank you. Any questions for she wrong? K. Next presentation. So I need to fix this. Sorry. No. I'm sorry to get this Testing. 123. Yeah. Okay. My name is Job Snyder's, and I'm gonna do 3 presentations in hopefully quick succession. While we wait, for the IT system Voila. First moment, signs prefix lists. Next slide, please. The purpose of assigned prefix list is to enable autonomous system holders to publish a list of prefixes, they might originates. So the authorization is done through the AS identifiers, not through, the prefix holders. This allows relying parties to conclude that, when they receive a given BGP routes, and it originates from the same ASM as for which an SBL exists"
  },
  {
    "startTime": "00:10:02",
    "text": "but that route is or isn't included in the prefix list. They can make decisions like, reject the routes because it wasn't listed something they may originate. Originate. This mechanism is intended to be complementary to existing rollout based DGP prefix origin validation, is not a replacement. It is a additional tool in the toolbox. Next slide, please. In terms of implementation status, example object that conforms to the current profile specification is available. If you download the slides, you can click the blue words and, it will bring you to to the particular, Doctor encoded objects. So there's an object for validator implementers to to play around with. Next slide, please. And I created experimental support for assigned prefix list validation in RPI clients. So this means that CA developers, can use a tool to see if they conform, with the profile the fines at this point in time. Next slide, please. Next steps that I suggest for this working group not only review, the signed prefix lifts list draft, but also, review the independent submission by Srirum, how to use SPL objects in context of PHP. Another step in the process will be to, do a draft proposal how to ship SBL information efficiently into a router And then study what it takes to implement this in a BGP implementation and make use of it. And if there are interesting gutches or good lessons learned during such an implementation phase. Next slide, please. It's the last one."
  },
  {
    "startTime": "00:12:00",
    "text": "Alright. That gives me 2 extra minutes for the rest. Uh-huh. Any questions? If you have questions or comments, send them to the mailing list because today, we have a packed agenda RPKI validation reconsidered. This to me is the exciting talk Next slide, please. Some terminology that may help us to reason about the topic at hands, there are 2 types of trust that are useful to to think about in this context, There is assumed trust. This is the trust, a relying party instills in the trust anchor. And compromise of this entity in which you instill trust is outside of the scope of the trap threat model. Then there is the right trust. The right trust is what you calculate Based on the thing you instills your assumed trust in. Derive trust is calculated following specific rules. Rules that this working group sets. Next slide, please. You hope that's very easy. So in other words, It is possible to define multiple validation algorithms for PKIs like the RPKIA. Which algorithm? Is the correct one or pure one or a valid one or right one, Is a matter of This group reasoning about what the outcomes are that we want."
  },
  {
    "startTime": "00:14:03",
    "text": "In my opinion, the current algorithm is problematic. It is defined in two places. And an implication of the current algorithm is that number of resources that appear in extensions Unrelated. To the thing we are validating at hands can negatively impact the validation outcome of of that entry. And in my mind, When this air condition occurs, the blast radius is too big. And this class is friction in interreg transfers but also in transfers between LARs, under the same so some of the, the states you can arrive in with current validation algorithm in my mind, are disproportional to the objectives of what we're doing with the RPK. So how does the current algorithm work? The lower left we have a robot payloads 10 slash 24, which must be contained in the issuer the e e surf which, for demonstration purposes, contains an additional entry, Then the EE search Both entries must be contained in that issuer, and, all entries on the yellow CA must be contained in the, the resources subordinated to the CA in the the green, colored box. We contrast this with what I call the George Jeff algorithm. In the jerks, the jerks, Jeff, in the g g algorithm. This. You you take that roa payloads entry. 10 slash 24 You check if it's contained, In the EE search, you check if 10 slash 24 is contained in in"
  },
  {
    "startTime": "00:16:00",
    "text": "the CA cert that issued that EE cert and so on. So instead of roping in more and more resources as you go up the chain towards the thrust anchor, you just look for a certification path specific to the resource you were validating the woman in the blue box. So let's step through some of the consequences of one algorithm versus the other algorithm. Here we have a layouts the green thing, has, 2 support, 1 subordinate see him, and, that support in SCA. Issued, issued 2 products, 2 e certificates, 2 roa payloads. With The current algorithm If the green CA removes the 17216 slash 24 Entering, All certification paths of all the subordinates, products, No longer are valets. And this is not ideal. If we contrast that with The g g algorithm very different outcome arrives. The row up payloads in the lower right button, that, no longer has a path towards the trust anchor because it was delisted in the green entity, And it's only delisted in the green entity in this example because in reality, the different components on this screen might be managed by different organizations, run on different timers, So this could be a very transient state that that lasts a few hours perhaps a few days. And to me, the desired outcome is that the robot payloads in the lower left in the blue box 10 slash 24 which is utterly unrelated to 172.16/24 that that role act continues to be valid despite,"
  },
  {
    "startTime": "00:18:01",
    "text": "172.16 being delisted somewhere higher up in the chain. I believe that this new algorithm is what we want. And an attempt has been made through RC8360. Which was called validation reconsidered. It it proposes that number resources unrelated to the VRP entry at hands are are not considered in the validation algorithm, and this causes the blast radius to be precise. So if somebody higher up in the chain, delists a specific resource then also subordinate products contained within the resource of us being delisted Those should be invalid everything else, should be valid. And this particular approach removes all friction around trends verse. Unfortunately, 8360 is is not deployable. It, relies on the, new OIDs being set by CAs. The the text itself lacks the required definitions of the parameters for the 82 5280, way of extending, validation algorithms. But adding more texts to fill in what those parameters do and how you map the policies in in different layers of the tree. Not solve the core issue that it requires coordination corporation of all CAS and all relying parties. So If I'm creating this as homework, I'd say it is a great idea execution plan is not feasible, c minus. The path forwards, In my mind, it's quite simple. The, RC80 360 is deprecated it's associated code points are tossed out because they serve no purpose. The draft"
  },
  {
    "startTime": "00:20:01",
    "text": "that that, is behind this presentation proposed the surgery, uh-uh, various RFCs to to insert the validation algorithm the the mother one, and all Efforts, gonna be with the relying party implementation project. Project. So, at this point in time, that would be our piece. Forts, RPKI clients, Ruthinator, RPKI Prover, and and that's A very manageable group in terms of efforts So If people want the new algorithm, I think now is the time to speak up and and, help this draft move along. So next steps for the group. Refill the draft that I would like this working group to adopt. For team open business team. It's gonna be a bit of fiddling in RPKI clients to, implement the new validation algorithm. Once we make progress on that, we'll share it with the group to allow other people to experiment with the new way of validating And also with that, I would like to ask the chairs to consider starting, a call for working group adoption for for this specific RPKA validation update draft. End of this presentation slide deck, I think we have some time for questions. We're We ran over on this one, but less on the other Keep it to a minute. This call could write NCC because you ask for, acknowledgements 1st of all, this variation of this algorithm was very simply I've seen it in our old metadata And I think that this would really remove some problems that you can face with the delegated CAS. Because she once yet is managed by the trust anchor, you can avoid many of these problems. But once you have independent entities, there's a real problem, and it's there's our attention."
  },
  {
    "startTime": "00:22:07",
    "text": "Rob Austin. I'll try to keep it short. Yeah, we've been going around in for a decade or more the semantics I completely agree. This would have been a better way to do it if we had thought of it back in Right. No question there. The problem now is the RPs. You're talking about a flag day. No. I think you are, and that's what concerns me. We we can have the argument later, but I think the part of this, the concerns, vanity went to do what that applies there. I think you're alluding to a desire for implementations to behave in a deterministic way. But I would post it. We we don't have that today anyway because all the RPs have slightly different ways of doing things and verifying things. So I would be happy to talk more about this, but Flecte is not possible. This this will just be a matter of RP implementations independently updating their implementations and releasing that the general public. We are Yep. It's Invernay's license, Cecil, started to keep short. Think is really valuable to discuss this again. So I support, Which report on this adoption call? To have a discussion. I think an issue with the original proposal is that's It would only be deployable if you follow the algorithm rollover. Which is a much longer discussion. And I'll leave it at that. But I think there's value in limiting the last radius. As far as I can tell, That isn't the key right now. A security concern s with the resources that can no longer accept all of this, with this product right now. For this I'll go ahead and let you know. Thanks. I'm Ben Madison working on. Just"
  },
  {
    "startTime": "00:24:01",
    "text": "Not quite. Real quick. Just in response to to to Rob's point. The a 88360 made it a flag day because of needing to change the policy of IDs, which precisely why we've taken a different approach here. What we're what we're suggesting that we do is interpret the existing policy differently at the at the relying party side. Which can be done independently by different relying parties. Without as far as we can tell any adverse impact for the transient window whilst they're relying their relying parties are behaving differently. That's precisely the reason for the updates approach. Oh, again, keep it quick, Jeff Houston. The reason why I wrote Sorry? this. I I am berating the chairs for just letting the mic the queue fill up No. It got locked. It's down near the end. The reason why we wrote RFC80 360 was to make solve this kind of problem of lock stepping. We were unaware of the way to do it in X509 world and all of a sudden got assigned the new OID and it's just kind of this is never gonna why. Had we known that as Rob Austin said originally, have we known that the outset, we would never have done that. Algorithm, we would have done this one. Had we known there was a way to actually slide this one in with revisions and updates, we would have tried that. But by the time we got to publishing 8360, sort of came out with a new OID. Kinda got frustrated with all of this, and it's sort of It's a better way of doing it, but we don't know how to get from here to there. You seem to be proposing away from here to there, I support its adoption by the working group because I think it's a good thing to do. Thank you. I will draw any, concerns I may have raised, on earlier cycles of this discussion and for the flex day concerns"
  },
  {
    "startTime": "00:26:01",
    "text": "I think it might be a very good idea to explicitly state in, the new ruling that CA's are advised to try to avoid the problem for a certain time, that gives relying parties and their developers time for actually switching Oh, I I am confident CAs are doing their utmost best to avoid this problem already today. There's no question about that. Yeah. Well, lucky. I I kind of kind of actually putting a little bit more, effort into thinking how to, avoid stuff there, problems there. Might actually might actually improve. Notice. Where's the thing? Huawei, We're gonna skip through and go to NF FCBGP. Yeah. And then call you back. I 2 minutes. Then call you back. Yeah. But but don't have slides for that. Yeah. I uploaded them. one. We're gonna find them. That's why we're moving to the next Alright. Thank you. Thank you all. FcbGP. I've seen GV the letter. 8 minutes go? Hello, folks. I'm Joe Howe from Qinghall University. I'm gonna have some updates for FCPGP. Give me work anymore. Should. Should. It worked yesterday. Okay. Okay. Oh, it works right now. Okay. Errors. So, So the problem scope of this thing is we have discussed FCPGP in"
  },
  {
    "startTime": "00:28:00",
    "text": "Prague last year and also yesterday in IDR group. So some of the materals has been covered I would encourage people to read that on the video if I missed today. So, let's first start with some, know, design goals to keep everybody on the same page. So fbfcbgp is a, secure routing protocol. It has to the ability to secure post the control play on data plane. We focus on Control Play today. So it has 2 goals. The first goal in full deployment, we hope that it can guarantee that any PHP pass authenticated by our protocol is a secure pass. That means it is invisible to the adversary to claim a forged pass to be secure, to be authenticated. In partial deployment, we hope that FCPGP is compatible with native BGP but it's incrementally deployable. That means it provides strict, strictly positive security benefits. Case of deployment. So FCFCP GPU is built upon a primitive called verifiable routing commitment forwarding commitment FC. So let's suppose an ASB receive a BGP update from its peer, a for prefix p. So then the ASP will use the forwarding following forwarding commitment to publicly certify its routing intent its next hop. To the SD. So the construction of the essay symbol, so the ASB basically is saying, okay, I accept the route from a s 8 for prefix p, and then I would would like to extend this path to my neighbors say, basically, it's a science, the entire stuff. As we can see from the construction, so FcPGP adopts a per per patholates validation scheme rather than a per pass validation scheme like in bgpsec So we believe this per pass related editing scheme has 2 benefits on the first days, it has the same security guarantees as bgpsec in in full deployment and second it is sort of like more compatible with the current, BGP. So let let me start with"
  },
  {
    "startTime": "00:30:02",
    "text": "that's the recap. Let's start with some new stuff here. The the first slide is how about the about the FCPGP past attributes. So basically the, we carry the FC in a new pass attribute defined here. So this this new attribute is called FCPGP pass attributes. It is the It is a optional and a transitive. And in each passlight, pass attributes includes a list of FCS. So each individual essay, the format is as false. As follows, so it has 3 a s numbers, basically, to define the path And then it has the, signing, signature to verify authenticity of this pathway. So STPGP is compatible with native BGP on escape this, slight due for the time of sick And then, we can start to discuss how the AS could process that CPGP basically. So, the first step is need to extract this new pass attribute to check it's that that, to check its authenticity and then do the 5 gb passes selection. So the verification of the FC itself is simple. It's basically you check the signature, to verify the pathway is consistent the a s pass basically. Here I won't spend more time on the BGP passes election part so FCPGP is actually very conservative in terms of the PGP pass selection. Because, the highest priority is still the lower local preference. So that means even if a a pass then it carries some axis, these assets were in invalid. But the a s may still end up using this path if its local preference likes this pest. So, basically, FCBGP comes after a local preference. So only if the local preference has a tiebreak, we move on to the secure passes selection. In this case, the I'm sorry."
  },
  {
    "startTime": "00:32:00",
    "text": "Sigma. That's called 3 that's called 30 seconds. Give a slide for you on. Let let me do this. That's it's here. So only if the local preference have that high we move on to the secure path selection. In this case, the AS should secure the select secure pass over now secure pass for the Secure Pass, we actually have 2 subcategories. The first and subcategory is the the the entire pass is fully authenticated which means that, for every single pathway on a path there is a Fc that is corresponding to certify this, this path of and the second subcategory is called partial validation partial past meditation, this is just a special case of the project deployment saying it's from our series thing that s in a past, there exist a sufficiently long sub path that is fully authenticated than the entire pass is also fully, secured even if there are some latency that are behind this subpath. Remember that, you know, only this case counts for the partial validation. Other other cases, like if the subpass is very short, or surpass does not from the origin that does not does not account. So the reason we choose a very conservative past selection policies, we believe that the partial deployment we are lasting for a long period of time. So it's it it is difficult to ask everybody to upgrade their routers. So in in that case, we want to make sure that FCPGP is a very less disruptive to a current routing. And due to a time of sync, I also do some very status updates about the deployment status. So, currently, we've built in the high level strategy is still we want to only strategically deploy, I've seen PGP capable devices to avoid universal upgrade of all routers. So currently, we'd be on an overlay"
  },
  {
    "startTime": "00:34:03",
    "text": "in China across multiple ASS to test a software functionality next step would be we're really deploying this FCPGB capable prototype or devices to the network so that they can really propagate all the, VGP update messages through the real network. So we encourage people to rate our, draft and also send an email if if we have any the discussions here. Thank you Go ahead, Rudiger. Really good folk. Actually, I think you do not actually have to state new rules for path selection. I would actually take your supposed rules as suggestions for policy that may be used by operators. Gotcha. Thank you. Mendelous. Thank you. We're gonna go to the next one. Regular. Thank you. Memphis number, Henry. Tom, Alright. Thank you. So this document is about what happens when manifest numbers, become too large. So manifests were originally defined back in 2012. And that document that defined them, set out, that they have this field called manifest number,"
  },
  {
    "startTime": "00:36:02",
    "text": "it needs to be incremented whenever a new manifest is issued. And that CA's and RP's shouldn't go past 20 octets for that field. Then in 2022, there was the manifest, biz document. It got much more prescriptive about the behavior here. In particular, it said that the line parties need to check when they retrieve a repository. That the manifest has a higher manifest number. And if it doesn't, then that counts as a failed fetch, and they fall back to the case state if they're happening. So this document came about because RPCARE client implemented or rather implemented in accordance with 9286, and then there was further discussion with the route manager developers about how they were going to handle it. And that in turn prompted this work. So a strict reading of the text in 928 6 is that once you reach that largest manifest number value because it's a bounded value. Which is called MN Max here. Then the CA is effectively frozen. You can't. Mike, a new manifest that relying parties will actually download. So the counterargument is, well, M and Max is a very large number. Are you ever going to hit this program. And that's a reasonable argument because it's a 20 octet number. It's huge. But you can set this value to arbitrary value if you're the issuer. And in particular, Aaron have CEOs that use huge manifest numbers that take up 1128th of that space. I mean, there's still a lot of space there. It's a huge value. And you could have bugs or corner cases or similar That mean that you're incrementing the manifest number by a large value or or something like that. So It's at least possible in theory. To hit this largest value. Now if you're a subordinate CEO, this isn't a big deal. You just the key rollover, get a new CA, and life goes on. But if you're a trust anchor,"
  },
  {
    "startTime": "00:38:00",
    "text": "then you need to do a trust anchor key rollover, which is a bigger deal. Because you have to get a new towel, you have to get people to use the new towel. You have to deal with the long tail of people using the old tail. Etcetera. Yeah. So the draft has a straw man proposal for dealing with this which is that if the manifest file name changes, then RP should just skip the manifest number of check. This is the current behavior in up care client. This document includes a detailed description of how RPCout client implements this And that implementation is based on a reading of 9286 as being on a per manager file name basis rather than a per CA basis. Which is, which is a reasonable interpretation of what 9286 requires The pros of this are it's simple to implement, And if everybody agrees that this is what 9286 is actually saying, then nothing else is required. It's just an implementation consideration for people who are doing the updates. The cons are that 9286 could be interpreted as though this is not required or even permitted. In particular, if this is the interpretation in 928 6, then it will be open to to just use a new file name every time it issues a manifest and evade the manifest number of checks. Which was presumably not intended. There's also rca48. I which is rights report of how, the version 2 validator works. Or works rather. And that used a per se basis for the manifest number check. There are other options here. None of them accept doing nothing is particularly good. The first one is remove the manifest number check. Of course, you get replay issues. The second one is make the largest manifest number, function of the current time. The problem with that is you have to contend with those existing Aaron Sierras with large numbers as well as potentially needing version updates to manifest, etcetera."
  },
  {
    "startTime": "00:40:03",
    "text": "The serial number of arithmetic, which is used in DNS to recycle the number space, not really going to work here because you don't have visibility. You don't have good visibility into what RP is have in this respect, And then the 4th option is do nothing. So then it becomes up to the CAs and TAs to do extra checks on their side. Make sure that they're not causing any going to cause any problems in this respect. This is what relying party support looks like for 9286 at the moment. So with manifest number reuse and regression, when it goes backwards, RPCure client rejects those manifests and other validators accept them. With MN Max, the previous slides assume that MN Max has the largest value equal to the largest signs 160 bit interjob. RPCure client implements 9286, but it interprets that, or its interpretation is that the largest manifest number value is an unsigned 160 bit number. So that's why there are extra tests here for m and max and m and max, what's called m and max 2. Is that unsung value. So for Ford, it rejects m and max and values larger than that. Octo RP car except everything. Route neither goes up to MN Max, but then rejects everything past that. And I client goes up to M and Max 2 and rejects everything past that. The reason those rows are yellow is it's not a 100% clear what the correct interpretation is on now. So, really, the point of this table is that if we hit this value today, we wouldn't have any real problems. We just use a different manifest file name. Would address the RPK client issue. None of the other validators are going to care if the manifest number goes backwards. So even if this draft doesn't go anywhere, hopefully, the existing implementations can take this into account in their support for 9286."
  },
  {
    "startTime": "00:42:00",
    "text": "And that's it. If anybody has feedback, Robas. And 2 things. One, I have a vague recollection and may be incorrect at the the manifest file name is basically GSKI The dotmft. So it's determined by the SKI. So how are you gonna change that? Secondly, manifest was modeled very, very closely after CRL. So I suspect you have the same problem as CRLs. Yeah. With with the file name, it does suggest that the GSKI be used for the file name, but you can use all the file names. It's not supposed to reject it. Yeah, I didn't think to look at how serials handle this problem. Ben, work online. CRLs, have exactly this problem. It's you don't run into quite the same bad side effects in the case of CRLs because they use differently. I think the approach is basically the right one. Think that we need to have clarity on what the the the word file name actually means in this context. I think it intended to mean in the case of our sync where you fetch the thing from and in the case of RRDP, the XML URI attribute. I think that's the right. Way of thinking about file name in this context, and then it, and it works in that sense. The fact that we're having this conversation at all means that there's enough ambiguity in the existing RFCs. I think that doing some surgery to, to the manifest. Definition, is it's gonna be necessary. I don't think we can really avoid it. But I think it's generally the right approach, and I agree that all of the other approaches that you enumerated are pretty bad and different ways. So, for this in general, Thanks again. Okay. Thank you. Excellent. Enhancing globalization."
  },
  {
    "startTime": "00:44:01",
    "text": "By integrating validate and gia. Oh, Hello, everyone. I'm Zia from Johnson Laboratory today, I'm going to introduce our new draft on enhancing road origin validation by aggregating validated hourly pillows. In this draft, we have identified issue is, registered our data and propose a preliminary solution. Recently there has been a significant progress on the and deployment of RPKI. However, still over 40,000 PGP routes are marked as invalid. It is clear that not all these, routing routing security incidents. We've noticed that certain rules that are marked as invalid or unknown. It is because that the aggregated parent prefix is a non while only the pre aggregation sub prefixes has been registered. And this mismatch can mistakenly validate these rules as invalid. And, both, routing policies or incomplete registrations mainly to this issue. Okay. Let's take a look at some examples. In this first example, it's due to root root our aggregation as 6 to 9 5. Has announced the apparent prefix, in this red route. However, it has only been authorized to origin its reset prefix a grocery always. Only those 3 hours cannot validate this red route. And worst due is upstream provider AS144 has also rise to originate a more extensive range of prefixes So this railroad has been mandated as invalid. But could it be valid"
  },
  {
    "startTime": "00:46:02",
    "text": "And in the second example, the path of the parent perfect in this road serves as a backup pass. However, AS7482 has only been authorized to originate 2 separate fixes. Although those 2 suffer fixes could be aggregated into this parent prefix. This red route was still validated as invalid because of the ROEs from this AS 17709. So in this case, could it be valid Our answer is yes. Those PGP routes should be validated as valid. At the corresponding traffic should not be discarded. So primary reason is, maybe the complex network routing policies such as aggregation or traffic engineering, are in complete and inaccurate registrations. And this situation may occur when the address bisholders are unavailable when issuing our ways. So our solution is to extend the original ARPs by generating Aggregated VRPs to allow the parents Perfects to be validated as valid. And we will take this 3 hours as an example introduce our algorithm. Firstly, all the contiguous prefix from the same AS with the same excellence will be aggregated. And all the aggregate will be at least will be aggregated into 1 single largest payment per fix, and other smaller payment per fix will be covered by this one. A quick question is, was it aggregated the VRP's equivalent to the original ones, we will address this from 3 perspectives. Firstly, for the reachability, yes, the And secondly, from the perspective of security, The answer is also yes. Announcing a patent per prefix will not enable hijacking of any other additional traffic. However, for the usage of Are we of the aggregated VIP should"
  },
  {
    "startTime": "00:48:04",
    "text": "be different, because, the presence of aggregated GRP. Is does not necessarily mean that any other AS could not this prefix so it could not validate the prefix from any other AS as invalid just like the second up update. So, therefore, the route modification is required. And, you can find our detailed solution in overdraft. And Today, we just want to present this issue. And thank you. That's all I have. And since all jobs has been just proposed, and there may have been many issues we have not yet identified. So we humbly seek for covers or, insights from the community. Thank you. Hi. Jeffrey has So, well, it is absolutely correct that, more specifics are normally hijack things, The present of an unexpected, less specific, if the more specifics that can be caused to be blocked is exactly the same problem. So, well, more specifics are the usual problem that Grow has helped mitigate if the provider decides that they don't want to do this. I agree with you. This is mostly misconfiguration. When this happens. But if the provider had intended this to be present in the global table, they should issue the ROA. Okay. Thank you. Joke Snyder's Fastley. This work reminds me of work I did in 2019 together with Paulo Lucento. The traffic analyzer calls PM ACCT. Back then. Needed to prove to my management that deployments of RPK origin validation would be safe to do a global service provider network. So we needed to calculate how much traffic"
  },
  {
    "startTime": "00:50:01",
    "text": "was actually destance for invalid prefixes. And what we came up with in the traffic analyzer engine is in addition to not found vallots, invalids, a 4th state that was invalids, it's covered by not founds, were covered by Vellas. And we could classify the traffic, based on that 4th state that is not documented in the RCCs. N n n n n in a way, we were doing the the same approach as what is proposed here, to, to see if there's aggregation opportunity and and highlight that in statistics about where traffic is flowing and how that impact the business. But, ultimately, we concluded that for for reactive traffic analyzing it is very useful to, to consider aggregation Aggregatable opportunities, sort of a separate state rather than just market invalid. Because there still is a forwarding path as you points out. But I I agree with Jeff's Jeffrey's comments that The moments somebody announces a route into the global routing table or the intonets. And it's invalid. For simplicity purposes, the, the universal reaction of neighboring network should be to, to reject the routes and ask the prefix holder to issue the appropriate role as to to make the problem go away. And I I fear that doing aggregation, in the interpretation of the BGP routers, we'll we'll be in an unwelcome complication where the the consequences are hard to oversee. But I I do appreciate the observation that's Not all in felots are equal, which I think is what your your pointing us,"
  },
  {
    "startTime": "00:52:02",
    "text": "Thank you for your time. Job, could you resend the slides? That Thank you. Thank you. Wait. No. Ben's Ben's gonna answer. So I I I can I can duck a few like the time back? Yes. Back here. Yes. Okay. Alright. Thank you, Thank you. Ben. Is kitchen. Sign 7 minutes. No side. So I want that's jobs. Oh, account. Find someone Science have that. That one. There comes. Okay. So Right. To today, I'm going to introduce our proposal for sign the soft soft net, peering information object for deploying into the main subnet. Alicia, and, I'm presenting on behalf of my co authors So, first, let me give you some background on, inter domain subnet. Attacks based on such as, spoofing such as reflected DDoS continue to present significant challenges to internet security. And mitigating this, attacks will require a false address validation. And there are existing, magnisms, but they all have problem in terms of validation accuracy and operational overhead in different scenarios. Which we have, we have pointed out in our problem statement draft in the subnetworking group. inter domain subnet proposals to inter And, in the exchange, self specific information among the AS is to solve the problems"
  },
  {
    "startTime": "00:54:02",
    "text": "and the the staff specific information, exchanging protocols, or what we we call, soft net protocols have shown to achieve validation accuracy and lower operational overhead in our large scale simulations, and the results we have, we have presented in the last IETF meeting. So, yeah, let's go into more details about how subnet protocol works. The main idea is, 2 fold. The first, the origin AS will advertisers. It preferred a s pass through the other a s is by soft net messages. And then other a s has learned the incoming directions of the origin a s will receive the 7 messages. On the right, there's an example to see how some of the protocol works in action. First, we assume that AS 1 AS4 has already set up a, subnet connection. And, yes, one will select the pass, 134 and, 1564 to go to AS4. And the s one will send the, corresponding subnet messages to tell S 4 to pass. And the S 4 will learn that AS 3 AS 6, our value incoming directions for prefix 1, which owned by a AS1 and all the all the other neighbors are the incoming ES is for prefix 1. So the problem that we are trying to tackle here is actually in the first tab which is basically how AS1 is for set up a connection in the first place. So, let's me let me go through the problem statement. So the key issue is the, inter domain subnet for operators is incremental deployment that is the thumbnail protocol speaking agents. Within the thumbnail adopting AS is need to find and establish connections with other subnet agents. And currently, the peering is assumed to be done manually by hand, by the operators. And this is, operationally and cognitively expensive and, there's a learning curve for the operators. And the most importantly, the problem is that there's no incentive for newly adopting ASs. And There's basically no turnkey benefits for enabling softness as a feature. Because the subnet appearing must be negotiated 1 by 1."
  },
  {
    "startTime": "00:56:04",
    "text": "There are 2 potential solutions for automating the subnet peering. The first one is basically, a gossiping of flooding protocol. We all know that this will be bandwidth and computationally expensive And the the, proposal that that we are trying to that we're trying to present here is a public registry. That contains all the a s's we have which have already deployed a a soft net and are willing to set up soft net hearing relationships. That a, newly adopting AS can use this registry as a reference and pick appropriate AS is, to set up soft net connections. And, we believe that, RPPI is maybe the most suitable choice for this public registry because it's already playing a central role in routing security and the source address validation is a key aspect of routing security. So let me go into the details of how, how a SSP object will look like first, we propose a content type, which we, this, this number currently is suggested and we intend to go through the IIenna registration process. And the and there are important information will be con contained in the e content of the Sysvi object. The first one is the ASID field, which contains the AS number that has deployed subnet and can perform south on the data plane. And the second, information is the IP address of the staff agent of that, of that AS so the so that, the other a s's can make, subnet connections to So in terms of validating the SSP object, the relying party must perform all the validation check as specified in RCS 6488. As well as the following additional specific validation staff of the Cisco object. In the interest of time, I will not go into details and, please refer to our Java, for the validation steps."
  },
  {
    "startTime": "00:58:01",
    "text": "And, in terms of using this, CSP object, the full routers. And well, basically, the current draft is a bit handwavy in this part, and there's specific algorithms, and we intend to improve that in the next iteration. But, anyways, it will, it will basically involve the 3 step and in in most three steps. The first one is that, we need to determine the code closest AS, to the AS that is deploying that is trying to enable the, subnet function feature. And, it will use the a s pass from the beach announcements and, as as objects to determine the closest ASs. And secondly, we'll use the, the SIPC Ultra will be used determine which of the ASF deploy soft net and the peering candidate can be selected, using multiple criteria including existing, and peering off. Agreements And, thank you. And, if there's any questions or comments, it will be our welcome Thank you. Okay. Thank you. Thank you. I can't be so worried We're gonna we when we have found the slide. So now we're gonna jump back to the previous slide. How many minutes do I have? What was the 6. Okay. Next up. A proposal for a new best current practices document to give guidance to operators to avoid Signaling RPK validation states in BGP Path Attributes. So over the last 12, 15 years, a lot of guidance,"
  },
  {
    "startTime": "01:00:00",
    "text": "quote, unquote, has been circulated where people with the best of intentions just sort of enumerated through Look at all the things you can do. With the RPK validation states. Regardless of whether it's useful or perhaps even harmful but here's an example from, cisco.com. Cisco people don't take this personal. It's just, you know, you have section deck websites. Where where, the the example is, like, if the routes is invalid, low local pref, if it's not found a different local pref. And if it's valid, then it super duper good, double the local pref. There's another example. There's a a large transit provider that that publicly documented that they associate specific beach communities to specific alination states. And they they claim that this has some some use. And there's many, many more examples. That's that's I set up a small experiments where it had, a handful of prefixes and using the Alistra's Chromecast utility every 8 hours, I would issue a roll off for one of those prefixes And 4 hours later, refoke that roll out. And alongside it is a control prefix for which no ROAS are created or removed. It's in a steady state. And what you can see in the right press, visualizer, is That merely changing the fact that a robot exists or doesn't exist, somehow generates BGP updates somewhere. And I can assure you It is not my test box, and it is not my provider. So somewhere Down the path, is reacting in some kind of way to roll us flipping in and out of existence. And this is That's good. Because"
  },
  {
    "startTime": "01:02:02",
    "text": "At this us by ROS, are covered And if there are some kind of unfortunate events like the data center that hosts your validator splits out of existence or your validator crashes or There's an echo that's being rolled out and your routers can no longer reach your validators. Or or Who knows what? Like, there's so many scenarios where you can lose few of the RPK that that means that if you associate a BGP community, the validation states, you need to regenerate HP updates for perhaps 500,000 routes. This is another example of that churn recently my employer, issued robots for all its address space time we did so with, you know, one push off the button, and we saw an incredible spike in BGP updates related to our resources and this type of, routing instability is is not welcomes, at least not where I work. We even saw with Charles somehow being created as a result of us issuing a robot. So the expectation was We're flipping all these routes from not found 2 ballots, and and we see withdrawals come out of the system. That is not what we wanted. So The recommendation in this draft BCP, is to give operators, a guidance and say, operators, should not signal validation states intransitive BGP path attributes. Specifically, Network operators should not group HP routes by their validation state in, distinct BGP communities. And I think that having this reference out there will be helpful for people that creates, training materials"
  },
  {
    "startTime": "01:04:01",
    "text": "and, you know, fly around and get workshops or even for the vendors that tell you what you can do with RP coverage and validation maps in your router. But I think for it is in the best interest of the global routing system, to clarify that putting the validation state in the BGP community It's not a productive thing to do. Next steps. Please review the draft. And chairs, I would like to call, for working group adoption. Alright. Questions? Jared March Akamai. There's a number of things be it, that will actually cause a bunch of BGP churn either in the, you know, in the local system. So for example, you know, BMP stations, re you know, being disconnected and reconnected causes you know, a bunch of BGP events as well to happen as well are you know, and I'm seeing a surprise face because the, it'll go in soft reconfig, you know, soft, you know, route refresh all the BGP session. So it can dump everything to the BMP station. So It seems like there's a lot of related things here, as well. I do think that this is a problem, and we should be looking to address it. The only question I have is whether or not it's here or maybe grow. Yeah. That's a fair question. And, hope the chairs will, advise me on that. Ben welcome, man. I absolutely agree with the observation and the solution is is right. I think the there's an important caveat to call out, which is that there are still implementations of BGP speakers floating around where if you do not carry ROV state and extended communities in your IVG. Transitive extended communities. These things do leak. And I think I think"
  },
  {
    "startTime": "01:06:04",
    "text": "calling out the fact that there are implementations where they are unfortunately required. And that they're still widely deployed on the internet and that some people are forced to use them, including ourselves. The onus is then on the operator to make sure that information doesn't leak and cause this kind of churn, but I think explicitly acknowledging that that exists and it needs to be constrained where it has to be used as a good idea. Thanks. Julio folk. Yeah. Well, okay. Kind of, I'm seeing, 2 very different things. Causes for BGP churn and, advise advisibility of, doing RPKI based signaling. Kind of return quite obviously is something for IDR or, or grow and, the signaling the Well, okay. The churn can be even cause by bad implementations on your edge. If you just do internal communities, where we have to state, yes, if that churn actually transmits out. That's a bad implementation. Now, we recognize actually avoiding the sure in that implementation takes some effort And, the advisability of actually, propagating, RPKI derived, information. In BGPO. well, okay, the BGP does not really make sure that you are off"
  },
  {
    "startTime": "01:08:00",
    "text": "that you are, that the authenticity of that signaling can be relied on. So kind of, yes, you can send out whatever you want but, you should not trust it. If you get it some from some Thank thank you. You have a really tight on time. Lansham or dima. T Yeah. But then I'm not gonna have his name. Reference implementation of how These are reference implementations. K. And we are super tight on time. So for the 4 remaining 4 presentations, we won't be taking any comments. Go for it. Well, I'm still doing was not listed in Tom Harris in the presentation. So, Tom, I said, do you still active? Don't forget me. Don't forget episode 2. Now I'm briefing what's going on with access to can you start? So Why do we bother to do this? The way all know, the RPPI, sign objects is related to went to another, Oh, sorry. Sorry. Sorry. you. Thank Jack. Lower it Yes. So so it's desirable for the RP to ascertain, what it is to to be very dented to, that, during a transaction of incremental update from the perspective of the RP. So that's to say, the validation centers are 1 RPK some sign object is subject to change. Due to another one."
  },
  {
    "startTime": "01:10:04",
    "text": "By comprehending the calculator risk fix, of different, RPK assigned objects, the RP can, user method to find what was needed in the RPK hierarchy So, therefore, some terminologies are defined to help us to do that. But please well know that, all the thing we mentioned is, is it? Has nothing to do with the RPK assigned objects. Installed in the hard drive. Just in the memory. So here comes, horizontal correlation. That is pretty, straightforward. So eyes sold with a semi cap value defined, defined as, horizontal correlation. And the unit state of this correlations in the RPQI, as then for defined as Horizontal correlation said, So the validation standards of 1 ISO leads to be verified due to the update from another one, in the same, horizontal correlation, and then the the the former one is considered to be, horizontally involved with, cls@one. Okay. Oh, Sorry, to forget to mention the, how we implement these, with a mapping container to to implement this with a API as a key value. To, management of these, uh-uh correlation set. Vertical correlation. Well, a similar consumption. And ISO and always adjacent to child ASOs is defined as, vertical correlation."
  },
  {
    "startTime": "01:12:02",
    "text": "Something like like that. And I would like to, to put answers, why we we we, we put an adjacent here, it is it sound like a redundant, but I would like to say the process, the RP, establishes the vertical correlation for every resource advocate in the applicant. So That's the reason why I put emphasis on, addresses in the child, Iso's hip. Enrollment correlation So so Of based on what I mentioned or what we defined, we can we can tell and ISO is either Horizontally involved or vertically involved with the LOA So we we call these is involvement correlation. And the union set of this involvement correlation called evolving the correlation set. We'll know that. The involvement code instead will be removed after every transaction of incremental update and the corresponding validation of the RPKi signed objects. One minute warning. Okay. So this is a procedure of the the whole, the process. See, our, upon encrypt the update, update the RP create as the, HCS and v VCS and to the, and find the, target object and the contractor, the RTAs, do the validation, removing the ICS and, there's a local cache. And waiting for the next, incremental update. So that's that collaboration. We are not to bring any change to validation of the RPK in any sense. This is just about, the some states, hot states, and soft states in the memory help the RP to do something quickly."
  },
  {
    "startTime": "01:14:00",
    "text": "That's all. Thank you. The picture. Bye. Bye. My calendar. Give you 5 minutes. Okay. Hi, everyone. I'm Langshun from Qinghua University. I'm going to introduce this new interdependence solution, I submit this draft in cyber ops because this solution uses as part and allow us to generate self filters. The primary design go, source address addition is avoiding improper block while maintaining data should notate and existing solutions such as EFP, UIPF, typically generates some alarm lists filter. By using information related to However, still using an alarm list because legitimate traffic to be blocked if this allow list fails to identify all sees run to the custom code. Here, we use, example to show the improper pro problem of solely using an allow list. In this figure, we focus our ingress cell featuring on AS4 And in this fear, s 1 has 2 provider the IS 2 and AS3 And he has 1 attach his new exports to all prefixes announced the 2 AS2 So in this case, a s 4, we are never received through for prefix P 1 and a p 2 from its customer s 2. And then in this case, EFPUIPF on AS4, we'll have improper block problems."
  },
  {
    "startTime": "01:16:04",
    "text": "It will be a blocked legitimate that data package received from a s 2 with addresses in P1andp source is And to achieve more robust self solution, exchange exists some some more recent subsolution additionally used as part of laws related the customer call to generate self afterwards. But when some as part or laws related to the customer coin are missing. In purple block, we will still exist. For example, in this figure, if a s 1 doesn't here is as far Therefore, we'll still have improper block. Problems. So how to perform more robust English style featuring when some ASEs don't have as far as icons up additionally, generate a self block list on an interface busy, a customer, or let her appear. Yes. This block list should contain prefixes belong to the provider code, The protocol of a s is defined as the set of ESAs and AS can reach by using only customer to provide links It is because prefixes belonging to the provided call should not be used as source addresses in data packets received from any customer. Lateral peer ASCs. And then we can use BGP update messages as far as that are related to the provide a call to generate that Prochleist And due to time limits, I will briefly introduce the procedure with your first use BJP update and as part to identify as many on, and then we can use PHP updates and the lowest to identify prefixes belong to those"
  },
  {
    "startTime": "01:18:01",
    "text": "And then we can use this block list to perform cell filtering it can block detailed packets received from customer or network peer ways this source addresses covered in this block list. And it can provide immediate incremental benefits even if the generated block list doesn't include all prefixes in the protocol. It can still block some spoofing traffic while avoiding in proper block. And in this example, we assume the product coin of a s 4 includes AS 67 dash 8, if ES4 can identify all prefixes belonging to the product code, can prevent the ASES in its custom code from spoofing source addresses in the custom the protocol. But if it can only identify partial prefixes, you can still block some spoofing traffic the custom call and avoid improper block. But in practice, we suggest to use both libraries and the block list filters together. It is because it's difficult for now to make sure that the allow list can car or prefix in the Hello, list. So block list is needed. And slowly using block list may still help in proper block So so use both allow list and block list together would be better. Okay. Feedback. I'll be welcome to my Email. You don't have to go fast. Funds. So thanks for understanding role or deployment. I understand. Thank you. Oh, you call. Okay. one of my research work This is about marrying ROA deployment and the marrying manners. This work has been published in NDSS 2024"
  },
  {
    "startTime": "01:20:03",
    "text": "which is a conference supported by internet society. So we hope this work can motivate more network operators deploy our way in the future. We all know PGP hijacking is one of the most important threats to today's incurred, and manners purposes for access network operators to improve BGP, routing security, and extra y requires network operators to prevent the propagation of in legitimate BGP announcement from customer ESEs So network operators must check whether the announcements of their customer assets are correct. And IR based validation and API based research decision can be used to achieve this function. And the ROA is more recommended recommended in practice. So considering the current lower deployment to reach you, which is nearly 50% So how about our way deployments? This work in aims to answer the following three questions So to answer the first question about our way deployment and network operators comply to minus xone. We measure the prevalence of API invalid prefixes that propagated through each ES download the BTP data and collect AS relationship from public, dataset, and we finally identify more than 1000 ESs that have replicated ops k I, you know, the prefix is and we further found from that more than 60% of those nonstop EFS are not compliant to minus x1. We also measure the percentage of different classes of interfaces that accept RPQA email ID prefixes. And we found different AISs may adopt different deployment strategies,"
  },
  {
    "startTime": "01:22:02",
    "text": "We also present the first notification experiment to evaluate the impact of notification on our way But, unfortunately, we found that none of the the treatments can improve the remediation rate of our way. So why our network operators are not forming minus one To answer this question, we conduct a large service a large scale survey among network operators Mhmm. 30 seconds, please summarize it. Yeah. And we convolute the noun compliance is 1,000,000 due to economic reasons technical reasons, Then we make last skill simulation to found the best deployment its its its its its its And finally, we found that I will be at provider interfaces can work better, then I will be work at customer up here interfaces. So we think, network operators are suggested to first deploy our way at provider interface and then add customer interface and then add lateral peer emphasis. We also provide some recommendations including operation, guide, backup, participant, and some future research direction. Thank you. Thank you so much. Thank you. None. Thank you. Sorry, but thank you. Difficult. Yeah. Man, Yes. Select of Man, 3 minutes. Man, 3 minutes. Thank you. I kept running. Selective synchronization. I don't see synchurization. RPA to RPK router. That one, that one, that No. Corp."
  },
  {
    "startTime": "01:24:00",
    "text": "No. Yeah. That was 3 minutes. Okay. Hello, everyone. If we're from China Unicom, I will present these slides as to on behalf of the car also. So my topic today is the whether to synchronize the RPK data that is not NASA to rebuy the network. Just to propose an example that with the rapidly development of the ipv6 So, many we think that many ipv6 only network will be appeared in the future such as the new China education and the research network named the city. Is ended the construction where we, I think the IPBC on the network. And the China Unicom as 1 of the ISPs in China is considering to deploy IP basics only network in the future. So we think that in the IPV 6 only net network, as the figure described in the slides, we do not lead to synchronize the some RPK data such as, because the reason that it may we we do not need the RPV 4, RPK data to synchronize to 2, PGP route validation because we think maybe based off the storage and routers or maybe we need to induce some unnecessary transmission ahead of maybe because the energy efficient with reason, So so we want to discussion in this working group that at present, the applicator data type include 4 types. The first one is ipv4 prefix"
  },
  {
    "startTime": "01:26:04",
    "text": "The second is the IPV 6 proof is and the key and ask aspart. So we may we think that maybe more and more applicator in the future, such as the in the FC, 7909 and also as the on sign perfect list also AS cons and the so on and so forth in the I think in the draft in the in the side OPS. So, we just want to propose this use case that, if if we we do not need the, and we don't need the and that's 3 data. Well, so we need to to require, all types of the RPCAD data and the whether to synchronize the API data that is not required by network. So, we we just think that maybe we can't distortion in the meaning is the act of the consensus of the working group, we may think that some optional solutions to help operators to select RPK data, sync right to it. Such as the solution. 1 extend the system to support the filtering data by RP Thank you. Thank you. Thank you. Thank you. The last one Yang. Yeah. At What is it? It's only the yeah. I'll be glad. Yeah. Got it. This is Taiwan in from new SCC Technology. Behavior my calls to present"
  },
  {
    "startTime": "01:28:01",
    "text": "young data models for To router protocol. This draft is about the this is Documentify the young data models for RBK louder protocol. 4 young data models certified As following, I think I looked protocol young. Is about how to configure and manage it I'm behind all the protocol. On routers, BP on the AS verification about how is PP will cater orange in the a s Based on hourly, PP's exactly young. Is about how BP very okay that AS Based on bpstack pass. Busy is about how BP will it cater as passed our BP routers based on ASP contain a four part, four tables, The first table is about it like cast server list table. The cash server listed table container connecting parameters, searching kilometers, and the session stayed and the staff takes Also record is received from a single customer list. Another global tables As the hourly tables, the key tables, ASP tables, The all the tables record that IP 4 and ipv6 I'll wait. And that allowed the key tables cut the record of a lot of keys of PDO is PA tables could place ID 4 and ID 6 ASPA"
  },
  {
    "startTime": "01:30:03",
    "text": "PP, our end is about to when Canada Orange is based on our way PC second round is about to replicate the sec pass at build as dividing up, say, 82 Fine. PPA is about how to locate the a s pass of DP motors based on a s it's thanks. Thank you thank you. K. This was a really, really full session. We'll try and get, more time to time, in, Addition to that, expect a doodle about when we will have a virtual interim. Thank you so much. Enjoy the rest of the meeting. Kinda come up with a better name and convention. I think what we should do is keep our next meeting Yes. Exactly. I don't know. Oh, I'll tell you what it is. Just the the title on the slide. Didn't match the main image. Oh my god. Sorry. Sorry, boss."
  }
]
