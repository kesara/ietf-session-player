[
  {
    "startTime": "00:00:24",
    "text": "record anything that's in the slides. You just have to record the discussion mostly because you know, questions and discussion is mostly what they want On Deb, who was that? Do you volunteer to take notes? I mean, anything you do is better than So there should be the there's a hedge doc document you can see in the right? You got it? Cool, cool So could you add and tell yourself who? a volunteer to take notes? Can you, what's your name? Ben? He's having trouble signing on Could we have a backup note taker? as well? Anybody, so it would be nice if other people would give him a hand with the note-taking That'd be awesome, leave. Thank you"
  },
  {
    "startTime": "00:02:00",
    "text": "All right, I think your square note taker was Okay, perfect. Sounds great. I think we can begin um Hey, all, welcome to Key Trance. And happy end of IETF week. I am one of your quick co-chairs, Sivan. Krati couldn't make it. She's on PTO, but she's in her regard And I am joining remotely obviously, and thanks Deb for helping your chair Let me actually all share myself Note well, you seen this at several points this week, I'm sure, by now but please do note it extremely well It covers your participation in the IETF, both on-site and remote and covers various topics, such as patents and code of conduct Yeah, please do go through it and familiarize yourself Just some meeting tips, the most important being that please do use Meetecho for joining the queue so that we can be fair to towards remote folks and also just makes it a lot easier to manage the queue if you're using it Note, if you're not using the on-site meet-exam robot, then to please record your participation using the blue sheets QR code that should be up somewhere in the room And yeah, the agenda is up here. I'll give a quick update and basically we have one adopted draft the architecture document that's been worked on for a while. Thanks to brendan moran that and everyone who has helped him At the last meeting, we discussed how we needed protocol document to think about the architecture document concretely brendan moran felix handte an only draft up now, which got some discussion on the mailing list. And so the really the agenda for today is to discuss the new protocol draft"
  },
  {
    "startTime": "00:04:00",
    "text": "and we can keep the bulk of it and so the really the agenda for today is to discuss the new protocol draft and we can keep the bulk of the time to discuss that and Isha will also be giving a presentation on contract monitoring and auditing modes in relation to the protocol draft And yeah, the overarching goal is to make progress on the protocol draft and see if folks think we're headed on the right track for eventual adoption Does anyone have opposition to that? Going once, going to twice. Then if not, we can begin Brendan, can you come up for the protocol draft? discussion? I think are you going to run the slides for him or do you want me to to? Well, I didn't really be will you be sending slides or should I run them for you? Whatever you want Either you run it or he does? Yeah, he can, well, he's running, he's opening That sounds good The trick is you'll have to say next slide this way because I think the clicker won't work. Oh, yeah. Good he can, well, he's running, he's opening. That sounds good. The trick is you'll have to say next slide this way, because I think the clicker won't work. Oh, yeah. Okay, that's fine. But yeah, I'm here today to present a draft that I wrote with felix linker And it's the protocol draft for key transparency And just to link frame the conference a little bit I'm sure that we will get into the conversation about like is this actually the draft that we want to adopt or not in the end? But that conversation is a lot less important to me than actually making sure that people are able to understand the draft because I've talked to a lot of people who are like key transparency seems super cool and really excited about key transparency but the papers and the drafts and stuff that I read make no sense to me so if we're able to make sure that people understand the draft that would be a very, very good outcome for me. So I mean more than usual, even, please feel free to come up and ask clarify questions. Next slide"
  },
  {
    "startTime": "00:06:00",
    "text": "So just to remind people of the basic model of what we're building, we are building a protocol between a client and a server, which is basic a key value database. So there are three different things that users will want to do with the server. The first thing is to search for the value of a key. They could also want to update the value of a key and also want to monitor And monitoring is essentially saying, I already know what the values of these keys are. I just want to make sure that they haven't changed since last I checked in And what's important is that I want cryptographic guarantees under all of these operations so that I have a really high degree of certainty that I'm not being lied to, you know, like if I search for a key and I get a value back I want to be very, very sure that if anybody else search for the key, they would get that same value back So that's what we're looking for as we're building Brendan, could you? sorry, just a quick note, could you move a little bit closing? to the mic? Just so that's a little bit here note, could you move a little bit closer to the mic? Just so that's a little bit. You just need to be louder. Okay, so I was going to say one quick note on terminology um i'm going to say the word key a lot in this presentation Every single time, I will mean a lookup key in a key value database I'm never going to say key and mean a cryptographic key. There are no cryptographic keys in this presentation whatsoever just so we're clear. That's been a huge point of confusion Okay, next slide So to get into the tree construction, next slide again again The tree that we're talking about is a hash tree, to kind of introduce hash tree generally with a hash tree you have some set of data that you're interested in and you want to commit to it in a compact way so what you do is you take all of your individual pieces of data and you hash them and each of those hashed pieces of data are a leaf node and the course it in a compact way. So what you do is you take all of your individual pieces of data and you hash them. And each of those hashed pieces of data are a leaf node. And they correspond to the leaf nodes of your tree"
  },
  {
    "startTime": "00:08:00",
    "text": "and then what you do is you compute the intermediate nodes and each intermediate node is the hash of a few children node below it and you kind of keep going up like that sort of computing the hashes of lower nodes up and up until you get to having a single hash left, and that hash is what's called your root node and so next slide. Hash trees are great because they let you commit to very, very large amounts of data in a very compact way. They're also great because they let you selectively reveal small subsets of that data So in a hash tray, you have something called the copath And the copath is the list of nodes that I can hash together with some specific leaf node to recone the root. So if you look at this diagram, we have leaf node 0-01, as it's labeled here The copath for zero one is all of these other notes in gray. It's 0-0 which I can hash together with 0.01 to get nodes 0.0 and then I hash together that node with 0-1 to get node 0, and then I hash together node 0 with node 1 one to get the root So that's the copath. A lot of times in the more like protocol specific parts of the conversation, I'll say that we'll give a user you know, some value and that value is the leaf of a hash tree kind of unspoken. We're giving the user that leave value, but we're also giving the user the co-path, so they can actually verify that that leaf data is actually in the tree that we're saying it's in so next slide the first type of hand tree that we have in the draft is what's called the log tree which is more specific called a left-balanced binary tree And with the left balanced binary tree, the defining property is that every left sub-tree contains the largest power of two nodes that's possible So if you look at the diagram on the left here, it's got five leaves, and so that first like"
  },
  {
    "startTime": "00:10:00",
    "text": "left subtree in there contains four leaves because that's the largest power of two that's possible to put in that subtree. And then whatever is left kind of gets stuck in the right subtree Log trees are really really nice because it's easy to append new day to them so we have the diagram going from five leaves to six leaves. When we add our six leaves, we add a single sort of intermediate parent node And then we change where the root node sort of points to as its right child to that new intermediate now and so that's how you add new data to a log tree It's basically always constant time, or at least logarithmic And this is harder to kind of talk about graphically, but also with log trees, it's really efficient to provide consistency proofs, which means if you have a log tree with a billion entries in it and then you come back later, and there are two billion entries in the log tree it's really efficient for me to prove that the luxury with two billion entries in it actually has all of the same entries as the luxury with one billion entries in it, just with new stuff added So I have a guarantee that none of the stuff that I show you before was ever removed And yeah, so the sort of abstraction here that you want to think about is this is a log. It's just a list of entries and the tree structure on top is a way to commit to that list of entries efficiently Next slide Okay, the second type of hash tree that we have in the draft is called the prefix tree Also kind of called the tri. It varies but the defining property of the prefix tree is that every path from the root to a leaf tree corresponds to the prefix of some string that stored in the tri. So what we're committing to here is some set of strings. So if you look at the diagram we've got the strings and and and"
  },
  {
    "startTime": "00:12:00",
    "text": "do, we want to commit to those And so we do is we build that kind of tree structure on top where each branch in the tree course got the strings and and that do. We want to commit to those. And so what we do is we build that kind of tree structure on top, where each branch in the tree corresponds to one character and the prefix of that string that we're interested in. And the reason this is nice is because it's really easy to provide proof that a specific string is or isn't in the tree So if I want to show that, like, if I want to cryptographically prove that the string dad is in the tree, I can just kind of go down, I can give you the leaf note for dad, and then I can give you the copath for dad, and you can hash the together and get the root hash and check that that actually is in the tree where I said it was. And you can do kind of a similar thing for proofs of non-inclusion as well where I can basically follow a path until it hits, you know, either a different leaf or like a child that doesn't exist and that would be convinced to you that this actually does not exist in the trade Next slide Okay, so these two sort of basic hash trees are combined in what is cleverly called the combined tree So the combined tree is at the top level a log tree and then each leaf in this log can contains two things it contains one a cryptographic commitment, and two, it contains the root hash of a pre- tree. So this sort of diagram here each gray box is meant to be a leaf of a leaf tree, and you can see one comes up to the other. They're sort of in order and then you've got the little triangle which is the prefix tree corresponding to each log in tree Next slide. So to put a little bit more color on what is actually stored in the combined tree, the cryptographic commitment is a commitment to a statement that some key value pair was inserted into the log. And then for the the prefix tree the prefix tree contains the total"
  },
  {
    "startTime": "00:14:00",
    "text": "set of key version pairs that have ever been inserted in the log So if you start at this first log entry that's over here, the commitment is some commitment saying that Alice's key is now assigned to X and then we insert in the prefix tree a pair of Alice and then zero because zero corresponding to this being the first version of Ellis is key then we go to the next log entry we say Bob's key is now Y we add Bob zero to the prefix tree to say this is the first version of Bob's key. Then we go to the next one Alice's key is now Z, and we actually add a second entry saying Alice 1 to correspond to the second value of Alice's key. And we just kind of keep going like this forever Next slide So given that that's how the tree is built the next question is how do we actually search the tree and make it use? Next slide So if we kind of step back and think about what is the log, what does the combined tree represent? It's basically a write-ahead log in a database where each entry in the log is saying the database was changed in some way. And normally a write-ahead log is not super useful because you would have to read the entire write-ahead log to compute the current state of the database but in this case we also have the prefix tree and in the prefix tree we can look in there and see what the greatest version of a key that exists at that specific point in the log was So if you look here, you know, Alice's key is version 5 and then we're going until we actually hit a log entry, which changes the value of Alice's key and then the counter increases And then we kind of keep going from there. So next slide The way that you search that is you can or the way that you, you know, look for a specific version of a key is that you can actually just do a pretty straightforward binary search where you start somewhere in the middle of the tree. You say, what's the most recent? version of the key that exists at the specific point?"
  },
  {
    "startTime": "00:16:00",
    "text": "You see if that's greater than or less than the version of the key that you're interested in. And then you kind of recurs from there. You go left, if it's too high, you go right, if it's too low And eventually, you'll always converge on the exact log entry where the key that you're interested in changed to the version that you're interested in um next slide So given that that's sort of in the back of our head about how searching works we can also talk about how monitoring works. So we said earlier monitoring is the process of making sure that our keys didn't change when we don't expect them to because for key transparency, it's important to be able to efficiently make sure that things don't change unexpected Like if I store my public key in a log, I want to be able to make sure that people don't change my public key without me knowing So the way that monitoring works are, well, sort of the security of this comes from the fact that search paths change infrequently. So if I look at my key and I see the search path goes to where X expected to go, that binary search path is actually always going to be the same until the log basically doubles. And you get like a new root node for the search And then again, the search path is never going to change again until the log doubles again And that's also what makes it very efficient as the efficiency comes from the fact that binary search paths are logarithmic. So the log might double 10 times. It might be a thousand times bigger than it was the last time I checked in with this log. But with that, corresponds to is only about 10 extra steps in the binary search path that users might traverse going to this specific version of my key key and if you're going to monitor the if you're going to monitor your have to do is go to the log server and say, hey, can I see these 10 log entries? which are the new steps in the binary search for my key"
  },
  {
    "startTime": "00:18:00",
    "text": "And can I check that they're constructed correctly? So can I check that? you know, there's no greater version or you know, no greater version of my key being advertised that I'm not aware of, or we're not saying that my key has a lower version than I think it should. And as long as all of those sort of extra new steps and this search path, point people to go back to the parts of the tree I've already verified then it's secure because the searches will keep converging to the same place they kind of always have Next slide okay I guess checking in quickly, do people feel like they understand this so far? okay So the basic basic Sorry, that was I've been asking in the chat and I'm not really sure I understand it that. How do you check that somebody? didn't add Alice 1 and then? add Alice 1 again? Is there like a if, let's say I go and say, I want to update my key and I, and I, and then add Alice 1 again. Is there like a, if, let's say I go and say, I want to update my key, and I insert Alice 1, is there a way for me to check? Because I can check that, Alice 0 exists. Is there a way for me to check that Alice 1? doesn't currently exist and like someone hasn't just been messing with my key? Well, okay, so if you search for Alice 1 either that key will exist in the rude note or it won't, well hold on. I think what you're asking is, how do I make sure there aren't two different log entries? with the same version of the key? Okay. So the reason that doesn't work is sort of it kind of comes naturally out of the way that the binary search works where the binary search works any binary search is always going to go to the first element in the array that"
  },
  {
    "startTime": "00:20:00",
    "text": "you're searching where the the version is now one if that makes sense. So if you do a binary search it's not going to converge to a different place than it would otherwise But it could be in a different, like, let's say, in the first tree like if you've got the log tree, there's prefix tree has X Oh, I see. Okay. Yeah. Sorry. I follow Yeah, makes sense It's Friday, Jonathan. It's Friday friday Friday I'm like, I've read the Merkel squared paper, so I get it at a highlight level. I don't get the how the binary search path stays content unless it's like a variation on binary search because I would assume like if there's like 10 items in the set and you start in the middle, it'll be like the fifth indexed element but if one if it becomes 11 items in the set and you do another binary search you're going to like start at a different half point. So is there something? that prevent, like that seems like it would lead to not the same? binary search path each time? Is like an a naive binary search, but is it like a, in what way does it stay stable until there's like a doubling of the cardinality of this? That would be just follow the steps what I call it prefect search, but you're right that we do choose the steps of the by binary search in a specific way to make sure that it's a stable as possible. So the first node that we always start out with the search is the greatest power of two less than the tree size and so if the first note in your search is always the greatest power of two less than the tree size, there's not going to be a new greatest power of two less than the tree size until the devils Yeah, yeah"
  },
  {
    "startTime": "00:22:00",
    "text": "Okay. So to describe it more explicitly, the algorithm for searching the tree is step one, you do a binary search for the specific key inversion pair that you're interested in. For each log entry, that you end up looking at in your binary search path you have to look in the prefix tree at that specific login entry to find the greatest version of the key that exists at that point in time. And then you have to use that version to determine which log entry to inspect next So essentially, whether you want to go left or right in your search And then eventually you get to your point, you'll eventually you'll eventually entry you'll eventually you will get to the the final log entry and then that log entry will have the commitment that changes the key to whatever value you're interested in But to kind of unpack it a little bit, deeper on step A, where we look in the prefix tree, to find the greatest version, that's actually a little bit more difficult than it initially seems because the prefix tree just contains a big set of key version pairs. It doesn't, you know, it initially seems because the prefix tree just contains a big set of key version pairs. It doesn't explicitly tell you what the greatest version is. So you have to find that Next slide And the way that you find that is something that in the drawing is called the binary ladder, which is actually a really terrible name, and I don't know a better name for it but so to find the most recent version of a specific key from a prefix tree, essentially what you do is you check larger and larger powers of two and tree, essentially what you do is you check larger and larger powers of two until you get to the first power of two that doesn't exist and then once you establish kind of this upper bound on what the what the most you get to the first power of two that doesn't exist. And then once you establish kind of this upper bound on what the most recent version can be, then you just do a binary search between that to find what the actual most recent version is And yeah, so it's a essentially just binary search again recursively There was an earlier version of this document that just had the prefix tree sort of directly tell you what the most recent version of a key is"
  },
  {
    "startTime": "00:24:00",
    "text": "but the consequence of that was that we would end up changing the tree differently based on whether we were inserting a new key into the log or updating an existing key because if you were updating an existing key, you would just change a value in the prefix tree and if you were inserting a new key you would insert a completely new value. And people didn't like that because it had, you know, kind of privacy implications Next slide Okay, so the final algorithm for searching this tree is you do your binary search for each log entry in the binary search path that you're interested in looking at, you look in the prefix tree to find the greatest version of the key that it contains which is secretly under the hood just another binary search. And then you use the to determine the next log entry, you recurs left to right until you get to your final log entry, and then you open the commitment, and you're done. Pretty straight right? Next slide OK, so to kind of recap the security guarantees that we're interested in here is that users can search for and monitor keys and have confidence that everybody that searches for the same key as them sees the same value as them and also if they monitor a key, that key is never actually going to change without them knowing and being able to see it as well. And the privacy guarantee that we're interested in is to an outsider, how do we make sure that every change looks like just one new entry to the log tree and one new entry to the prefix tree so we can have really tell deeply how the contents, the actual like data stored in the log is changed um we can't tell if new keys are being added. We can't tell if new keys are being updated You could also like add a bunch of fake data just to kind of pad how busy your service looks And then efficiency, every client side algorithm is roughly logarithmic Some algorithms are actually like logarithm squared, but that's"
  },
  {
    "startTime": "00:26:00",
    "text": "kind of this as well yeah so that's all I have Any more questions? Okay. Okay. Yes you're good. Thanks, Brendan Yeah, we do have a question actually, Brendan Brennan You're on, Chef, and apologies this is kind of a higher level question and just let me know if it's out of scope, but let me ask anyone So for transparency, you want to ensure that the server that's response for this complicated data structure is not cheating and not giving different answers to different people Is that, in some way, part of the protocol? I mean, for blockchains, does the whole economic incentive thing that takes care of that What do you do here? So there's kind of a two-part answer to that. The first part is that because all of the content of the log is stored in a log tree, that log tree has a root hash, and users intentionally hold on to that root hash. And whenever they send new queries to the log, rather than just kind of throw away the old root hash, they actually get a consistency proof to prove that whatever new version of the log new queries are being answered against is an extension of the previous versions of the log that they saw. So users guarantee that anything they were ever seen is never deleted. OK, so that's part one is users ensure that anything they're shown is never deleted from the log And then the part two of the answer is that users can either gossip"
  },
  {
    "startTime": "00:28:00",
    "text": "with other users or they can either poll the transparency log sort of over an anonymous channel to see that the transparency log is showing them the same version of the log over the anonymous channel that it does when they're asking it authenticated queries Does that make sense? Yes, it does Does the protocol? specify? What's the minimum work? I as a user should do if I care about? a tiny amount of state on this data structure? structure It's not something that we can really specify in the protocol, I think but what we do specify is the way that you can interact with the transparency service anonymously, or I guess what I'm saying is yes, it is part of the protocol either checking in over the anonymous channel or sort of exporting a code that you can compare with other users. Those are both part of the protocol, yes. Thank you So do you want reviews? Reviews would be wonderful yeah. So I linked your individual draft to the Datatracker page for KeyTrans so it's easier to find. Amazing, thank you. No problem And I stayed away from the yellow button that's clear the working group. Thank you so much much I think we can do issues presentation now and then have a broader discussion maybe about the protocol draft and see if yeah what folks think about it and see if you're headed in the right direction and i guess to brindon's point if folks understand it But yeah, maybe let's do Isha's presentation right now"
  },
  {
    "startTime": "00:30:09",
    "text": "And let me see the slide control yeah can you hear me okay Yes yes okay okay great yeah thanks everyone And thanks brendan moran the presentation. So I Yes. Okay. Yeah. Thanks, Shevan. And thanks brendan moran the presentation. So I'm going to talk about two different modes of key transparency, which was in the origin of the art architecture draft. And my proposal here is to sort of show how to tweak the proposal that is currently in the draft to see how to support both of these modes so it's actually going to be pretty similar to the design that Brendan presented but I'll go over slowly to give a high level overview And the mode motivation for this is, I think, some of the current deployment of key transparency at least are is in a different mode than what is being presented in the protocol draft right now which is called the client monitoring mode So I think it might be nice to support both modes in the standardized protocol so that we can cover all the bases So to give a brief recap this is mostly taken from the art architecture draft. So key transparency in that architecture draft talks about three different modes in which we can build a key transparency system. So one is a third party auditor mode. There is a third party monitoring mode which requires to understanding a lot of data being outsourced which may not be very suitable for privacy So I'm not going to talk about that. The two modes I'm going to talk about a third party auditor mode and the client monitor mode. And the differences between these two modes are in the first mode it requires to have a third-party auditor who the transport log can delegate some work to they don't have to be trusted for privacy but they are an non-colluding third party versus client monitoring"
  },
  {
    "startTime": "00:32:00",
    "text": "mode, the mode that Brendan presented, is where the clients do all the work, there is no reliance on an extra third party to do any extra work In the auditor mode, since the auditor does a bunch, of the work, the users can run more efficiently And in the client monitoring mode, the users usually have to do more work. And from the architecture draft, what it says is it's ideally suited for use cases but the users are looking up a small number of key repeatedly. And finally, i think one of the most important distribution factors for me is the client's persistent state And this is pretty important between this two different modes. So in the first mode, in the third part, auditing mode, the clients do not need to have any persistent state. And what that means is the client software do not need to remember any cryptography signing keys or any other cryptographic state between its queries for the security of the system to hold versus in the client monitoring mode they do need to remember that and the reason that third party, while finding an auditor is often hard the reason I think it is nice is because in some cases, the clients are light devices or they are not capable of remembering states or you could lose all your devices ending up losing all your devices So that I think in my opinion, is one of the big distinguishing factors And from my knowledge, the current deployments both WhatsApp and Proton are built in the third-party auditing mode. And I do not know of any implementation that's out there any deployment that's out there in the client monitoring mode If there are, I'd be happy to know more about that so that was to recap on the different modes Any question on that so far? Well, there are, yes Yeah, hi, Escher. I was wondering about your claim"
  },
  {
    "startTime": "00:34:00",
    "text": "that clients do not need persistent state. I argued to myself a bit about it but just being naive. Doesn't the client need to remember? their key history to check that the server did not make any bad updates? Okay good. Yeah, thanks for bringing this up. So I am distinguished between the client state and the user state. And by that, I mean the client, the state that the client software your app needs to remember versus what as a human user I can remember. And if you can remember app absolutely nothing at all, I think some of this might be unachshundated so when I said clients do not need to remember I really mean the software do not need to carry over any state but we do expect the users will roughly remember when they did their updates to the extent that if there was a malicious update, they'll be able to tell that, oh, that was not me. I was not online. I didn't do that update Okay. But I think this is something you could audit if you would assume client state right With clients said you can do a lot, and I think client monitoring the proposals, including the one I'm going to propose right now, assumes client state, but I think the key transparency will give you, will be the most benefit if it can support stateless clients because the real cases like if you lose everything all your devices even then if you can detect something is wrong that it will be very nice. But yes, if you assume clients to real case is like if you lose everything all your devices even then if you can detect something is wrong that it will be very nice but yes if you assume clients that you can do a lot more and if efficiently, definitely benjamin schwartz meta, but I don't work on anything related to chat or key transparency Don't the clients need persistent state to remember? their own private key in order to be able to sign? and decrypt messages? So the claim here is if they lost their key and they reinstalled the app. So they got a new key, right? to sign and decrypt messages? So the claim here is if they lost their key and they reinstalled the app. So they got a new key. So in between, if the server tried to distribute a fake key on their behalf, they should still be able to detect it"
  },
  {
    "startTime": "00:36:00",
    "text": "So does that make sense? Okay, but if in if the client lost all its if we're assuming that the client lost all its state then the client doesn't have persistent state but if the client, uh, uh, regardless, you know by assumption, but otherwise, I think the client always has persistent state I think the client always has persistence data, at least has to have a trust. Yeah, of course of course. For some time, it feels like a memoryless client and every time it's looking that's not much good. But, but the what I'm trying to say is even if it loses state, sometimes, like you lose all your devices, you reinstall the app and when you were offline, a fake keywords distributed you should be able to detect. Like this mode, the third party auditing mode, lets you still detect it. That's what I'm saying So, and that the security is not compromised So even in that world, you get security versus in the client monitoring mode, you don't get security if that is the case OK, that's interesting I have trouble understanding how that could be true because if I lost all my state then and then i am no more the owner of my account than anybody else in the world is the owner of that account. Absolutely, and that is why the best you can get here is detectability Like if you assume you have multiple devices and you can sign one with another, you get something stronger and you never lose your devices But even the absolute worst case, you lost all your devices this is the bare minimum security gate, which is detectability So in reference, so you cannot publicly prove that there was a key change on your account, you did it and not the server, or the server did it and not you. If you lost all your state, you cannot prove it to others, but you can detect that such a thing happened what's and lad akamai i i under why we're trying to have this detectability. I do question it's real world relevance because if we have a case where the device state is laws"
  },
  {
    "startTime": "00:38:00",
    "text": "we're relying on the user under understanding when the key should have been updated in their human memory and remembering that their device was lost before or after that. I don't think that's terribly realistic Yeah, I mean, this is an assumption, of course, but do you think like, assuming that the clients always have state is realistic? I'm not sure I think talking about what happens when you lose the device, we need to be clear about what we're assuming the information actually is. It's not going to be there was a change that I unequivocally know wasn't awesome authorized Yes, yes. Yeah, we can be more clear about that but that's what I meant. Like, you can detect. And like I said, the use there's an assumption that the user roughly remembers when they change their key so that they can at least detect if something looks suspicious. But it is an interesting question if you don't have it whether you can get any anything out of key transparency. Unfortunately, I don't think the answer is very much. I thought about it some But dennis jackson Mozilla. So I think that this is obviously an important discussion on some level, but I think we've kind of started at the wrong level like these are the technical distinctions between these two but in terms of the actual usability, there's much more fine grain notions of state that we do care about so you know the most the most fine grain we've got my app is open up, and it's crashed, and it's gone, and it's lost some of its non-persistent state. We've got the medium of like, I've got a backup somewhere that I can restore to from a day ago or a week ago or a month ago And we've got the total loss. And I think you really have to talk about the actual use case for a real user whether they have like a backup or not, whether it's a fresh phone or not now there is still some like notion of I identity, even if you lose all your state based on like your phone number or whatever that you can look up"
  },
  {
    "startTime": "00:40:00",
    "text": "into the street so it's not the case that like your identity is completely gone It's just that the private part of that identity is necessarily. That's true. So like I think this is a really interesting question, but like there's so much nuance here to unpack that I think it's tricky to do it at this level with this kind of very broad overview Yeah, thanks, Janice. That's a great point. And yeah totally agree. jonathan hoyland Cloudflare. I'm just struggling to think about a use case. And is the idea that it's something like I was arrested all my devices were taken off me in an unlocked stage? and then when I finally get out, I can see oh, somebody like issued new keys Like, I, yeah what's a real use case for this a real use case is I have only one phone and I lost it and there was a malicious account like I mean that's that's also a very simple simpler, but real use cases. How would that be a malicious, like? if my phone was lost as opposed to And somebody distributed a key on my behalf right so and that will be a use case. That will be, I mean, there are a lot of realistic use case and I don't necessarily want to branch off into that discussion because that's a different and a much bigger discussion but I think it is a realistic use case, right, that you lost your device or your device was compromised, somebody, uh, have added another device on your account and distributed a key and when you reinstall the app, because like Dennis said, you haven't lost your identity in the system. So you still want to be able to detect that such a thing happened Okay, thank you All right, so looks like no other questions Should I proceed? Yeah, please agree All right, so this is a very high-level overview of the asymptotic cost that the present, like the proposal I'm going to present"
  },
  {
    "startTime": "00:42:00",
    "text": "will have. So the asymptotic cost are like this in the auditor mode the cost of audit is t log n where you we are allowing t updates per at most t updates per epoch, and say the size of the entire directory is big end. In client monitoring, obviously, there is no third party, so there is no third party auditing costs The search and the monitor, cost here, so search means I'm looking up somebody else's key, and monitoring is I'm monitoring my own key history and these two costs totally unoptimized like if you store some of the states, you can do it like way better. But um here the worst case cost is if you have V version number like you have V keys on your account then this cost is V login. And versus in the client monitoring proposal based on the same data structure combined to construction, in the client monitoring case, the cost is there's a multiplicate log e factor and let me go into a little bit depth of what e here means so in this world, we are assuming that the clients are not always online. So they might be offline for certain epochs and if, so let's say they come on online between two epochs and the length of their off- period was E. So then this cost is V-log-in times log of E. So log of the interval when they were offline. And the clients in the first case do not have persistent state. We had a lot of discussion about that, but here I meant like really you don't have to carry any state between these two queries to make sure that the security holds versus in the client monitoring case that is, you, need that you need that persistent state okay So here is a quick cost analysis difference between the current protocol proposal and what I'm going to propose and we are calling it weekly consistent optics because optics is a proposal in the"
  },
  {
    "startTime": "00:44:00",
    "text": "third-party auditor mode and in the paper I linked that shows how to do it in the client monitoring mode so there is a cost difference. And again, like I'm not making any claim of which is better or worse. I'm just showing that there is a cost difference here. But again, I think the main difference in my mind is that persistent state requirement which if you look at the third-party auditing case in the current proposal, the IETF protocol the P-Trans protocol, and the optics these are the differences. So the audit cost, I would say, I would say case, in the current proposal, the IETF protocol, the key trans protocol, and optics, these are the differences. So the audit cost, I would say, are actually very similar. So in our case, it's T log and in the protocol draft is log because only one update is allowed per epoch versus here we are allowed in this proposal, T updates for epoch and the cost is t login the compare the cost of search and monitor differs like this so in the case of monitoring the search of monitoring and so searching in the protocol draft is log v, log squared n versus it's v, log n in our model because i log squared n versus it's v log n in our model because to the best of my understanding in the protocol, you could add an audit but it is not obvious how you will change the search and the monitor algorithm to take advantage of the auditing And that's why that draft will still require the clients to have persistent state versus the draft I'm going to propose won't Those are the high level differences. So any question on that? Okay Oh, there is a question, yes really. Can you go back a slide? I just realized So you say, I mean, there is a fundamental difference in how third-party auditing and client auditing usually works in the right? Because the third party all audit they play like a global police. They look at everything and whether they"
  },
  {
    "startTime": "00:46:00",
    "text": "log as a as a whole behaves correctly right Yes, I mean, with privacy, but yes. Yeah, so you have this factor V now. For client side monitoring, v makes perfect sense because the client has like V versions. But for V in the third party auditing model, it's not quite clear to me what we here means is that the average version of any user is that they need to perform this per user no no so v is not in the in the third part auditing mode the auditor cost doesn't have v need to perform this per user? No, no. So V is not in the third party auditing mode. The auditor cost doesn't have V. So that is T log end. So T means the total number of updates that happened between two EPA epochs. And we allow batch updates, which I think is different from the current. Yeah, I got the I got your role wrong. Sorry, thanks. Okay, no problem yes brandon Uh-huh thanks. OK, no problem. Yes, Brendan. Hi, I am so sorry to rehash this conversation so I'm going to make it a clarifying question instead. When you say clients don't need persistent state, what? persistence state specifically are you talking about? Are you saying? they don't need to hold on to a treehead? Are you saying that? they don't need a signature key? What do you mean? They do not need a signature key. They do not need to cache the previous states of the keys to make sure that some, that's secure distance do not need a signature key they do not need to cash the previous states of the keys to make sure that some that security still holds they do not need to maintain any of the states Yeah. More questions. Hi Watson again. Hi. I'm a little confused by why the search costs are different in the two. Isn't it a very similar data structure at the client? is directly looking things up in? in both sides? I will talk about the data structure in a bit So the prefix tree, at high level, the data structures are very similar, but they're"
  },
  {
    "startTime": "00:48:00",
    "text": "are some slight differences. And the way, so that the search and the monitor algorithms are also different even though the server's data structure is the same Okay and a quick follow up to Brendan's clarifying questions. You used to turn signing keys. By that, you mean signing keys that a client would use to sign their message, or do you mean the public keys are used to verify the same? signatures of the lock or similar? No not of the logs. The client's personal state yeah they're like the private state of that yeah yes All right, moving on Yeah, so at a high level, the data structure looks very similar. And as you can see, this slide also looks very similar to what Brendan presented. So the idea is there is an append only, maybe there's a slight different in that there's an append only prefix tree for every epoch. And this root of these prefix trees are the leaves of a log tree. So the upper triangle for me is a log tree which I think Brendan you use grey box to indicate. There is one difference though So first is between the two roots of the prefix trees we are allowing a batch of updates instead of one and the roots of this locked the leaves of the log trees which are the roots of the prefix trees they contain the roots of the prefix tree and the prefix structure like in brand trees, which are the roots of the prefix trees, they contain the roots of the prefix trees. And the prefix is structured, like in Brendan's proposal, I think it would contain Brandon and Felix the root versus here i'm saying it only contains the roots of the prefix tree and the prefix tree underneath collect T updates between two Eples And next I'm going to show you like how and the prefix tree structure is the same, but the leaves look a little different The construction look a little different So the next slide is going to clarify. But this is the high level combined structure. Is it sort of clear?"
  },
  {
    "startTime": "00:50:00",
    "text": "I could assume that's a yes. I'm going to move on Okay, so the prefix to reconstruction is again pretty similar to what Brendan showed with two significant differences. So the first difference is, in the leaves, there is the commitment to the value itself like we call it the, like the encryption key, say the public key is hashed at the leaf itself in instead of with the root And along with that, we also add over the epoch now number T, at which it is added in the clear and then hash it in the prefix tree. So the leaves of the prefix tree look a little different Note that the leaves positions are computed exactly like in the proposal using VRS which was first introduced in seamless. So the label is basically the username concatenated with the version number, which are the input to the VRF And those bits are used to compute the tree and compute the position in the tree and then the trees computed the same way that Brendan presented by hashing them upward. Isha think this is one clarifying question from jonathan hui the chat If the client doesn't have any state, how do you know that you're talking to the right endpoint? No, I think, well, Dennis's point was right here. It's, it's nuanced conversation what exactly state means. But what I mean here is that the client does not need to remember from the previous query and this query what, like, let's say I looked up Alice's key before so does my key uh state need to remember alice key version so that the next time I do an audit? I can, is it required for the security? to hold that I need to remember Alice's state? exactly so that if some misbehavior happened on their account I will detect it. Versus, I start a fresh account and I'm doing a search on Alice's key Is that enough to detect that some misbehavior happened? I think that's the difference"
  },
  {
    "startTime": "00:52:00",
    "text": "difference Great, John. jonathan hoyland Kravlar. So you are still keeping the tree head or the root because if you're not keeping the root, otherwise, the key transparency server can just be like, hey I invented this whole entire new tree and I actually own every single key and it's completely fake. And I can't tell because I don't remember anything. Yeah, I think Felix responded to that in the chat and is right we can assume that's hard coded like the certificate of where you look up the tree. The tree head is hard-coded? No, the search certificate, like, where you look, sorry, go ahead. Somebody's- Yeah, but if I made it jump in jonathan hui think you can anyways assume that you agree on the root hash with your peers, right? Be it via- gossiping, be it via occurring it through anonymous monitor You don't need the histories to. You just need to make sure you have the same as your peers. Then the, then the cannot replace the world. But you need to do this in client sites auditing anyways. So yeah so we're saying that as long as I can always trust my peers because remember if I've got no state and I want to talk to my peers and I'm going to the transparency server to get the keys, and the transparency server is just making up keys, the transparency stuff can pretend to be all my peers and give me the tree head What I wanted to say is there, of course, I mean, you point to a right problem, but I think it's orthogonal to this discussion right you you need a way to find to ensure with your peers that you're using the same root hash in any in any sort of monitoring I don't think Escher talks about that here no that's right yeah denis please uh yeah just to sort of plus one what Felix said I'm going to wait for jonathan flat finish talking So just to clarify, right, if you end up locked in a box and you never talk to anybody else and all your communication goes through"
  },
  {
    "startTime": "00:54:00",
    "text": "your phone and the attacker controls your phone at some point, it's a over. However, for many people, they get new phones from time to time and like they have the ability to move between networks and in a real some point, it's over. However, for many people, they get new phones from time to time. And like, they have the ability to move between networks and in a realistic model, at some point, you talk to somebody that's not the attacker, and then you're like, oh damn yeah Okay, thanks Okay All right, so I think the construction, was understandable, hopefully So in this construction, now we can, we will see how to use it in both the audit mode and the client monitoring mode. So if there is an auditor, then this is how the algorithms will look like. The audit algorithms will check the append only test of the prefix tree, which means it will check that the old trees are the old tree is a subset of the new tree and it will also check that the new leaves have the correct epoch number because the epoch numbers are exposed in our tree This is, there was a question of about why the complexity looks different, and this is because, the search and the monitoring algorithms are slightly different So because in this world there is an auditor, who's checking that no entry from this prefix tree ever got deleted, we can do something lighter weight on the clients. So when somebody searches or monitors we don't actually have to find the highest version number in the tree. Instead, we will let the server serve you the latest version number in the tree, but we'll we would want, let's say like Alice's Bob's current version is V3. Bob has three key versions. So the server will serve three membership proofs in the latest prefix tree, along with the non-membership proof of the fourth version So it proves that the version one, two, and three exists in the three and version four doesn't and the reason we can do this is because the because the auditors are making sure nothing ever got"
  },
  {
    "startTime": "00:56:00",
    "text": "deleted. So if the server ever tried to make anything, insert anything, it will never be able to delete it. So this, this slight bit monitoring and so search algorithm will work. I mean, there's a formal proof in the paper in optics, but to give a high-level intuition, this is the search and monitoring algorithm that different from the current proposal and this is in the auditing mode. There are some questions questions Is there any question for me? Sorry, it is A good idea. Yeah Your answer. Back to the previous question and to Dennis's answer And so if I'm in a box and now I'm out of the box and I'm talking to jonathan hoyland I totally trust him, but I'm talking to him through the central jonathan hoyland I totally trust him, but I'm talking to him. I'm talking to him through the centralized system because all of these are centralized organizations and we're, I mean, the main threat model is the centralized organization trying to men in the middle me because they don't like me so I still feel to understand how I can recover from that So I do agree with some of that for sure. Like, I think, you know, in a world where we had like really decentralized communication and it was all federated and this kind of stuff, the case is much strong I think in a world where things are more centralized, it gets harder and harder. I agree But I think in practice, we do have systems that while still set centralized approach that, like I think signal today can make a credible claim to not know who is trying to send a message and to who they're sending it to. So for the specific use case, I think there's a very strong argument for systems like this. I agree that for like"
  },
  {
    "startTime": "00:58:00",
    "text": "Gmail, maybe doesn't need key transparency but you know, it's got its own problems And also adding to that, Yaron, I think you can Yaron, I think you, I hope, I really build on anonymous routing to agree on not to agree on, but to get the sign tree hats, like the, like kind of just the short get the signed tree heads like the kind of just the the short version of the tree because if you use anonymous routing the chance that the server can lie to you successfully it's much lower. And it would be incredibly low and these queries can be very small you don't need to do them a lot and you here, you really don't need any state, too have faith in that the the tree head you got by anonymous routing is the same that everyone else got Thank you. Hi Teebo. Tibo Cloudflare. Maybe one thing I think that could help here is my understanding of Ketra transparency is it does not try to provide in integrity even though it like approaches it. It provides concerns And so in the case of even like a like centralized messaging systems like an know we could take Gmail or et cetera, if they always like rolling out key transparency, it means like that actually would like to commit to like some consistent overall and they probably won't roll out only key transparency as like a security mechanic Like QR code might still be around. They might be like all the ways to go set your public key and to ensure that like this route that you get is actually a legit and so that like you can communicate it to yours appears So I definitely agree that like a central entity could like work in like a split view and totally isolate you, but I'm not sure if it's a realistic model and like that's how key transparency is going to be deployed in the end"
  },
  {
    "startTime": "01:00:06",
    "text": "All right, all right, thanks So moving on, so this is going to be, I'm not going to go into the algorithm here, but this is to show that using this very same combined tree structure, in the absence of an auditor, we will still be able to detect a misbehavior. For example, a server gives a fake key for Alice to Bob, and some epoch when Alice was offline in this picture So these commitments are your roots of prefix trees And in this case, we would like an Alice and Bob during the search and monitor to do some additional checks to be able to still detect such a misbehavior in the absence of an auditor And of course, there is an inefficient approach of solving it, which is Alice could, when she comes back online, she could check in every missed ipah that her key was maintained correctly already of solving it, which is Alice could, when she comes back online, she could check in every missed epoch that her key was maintained correctly or it did not change. Connix roughly takes this approach, but we would be we would want to do better. We actually do not want to change all this missed epochs and that is where we had this, obviously it's not trivial because you will need to find some sort of intersection where as Alice and Bob can still check logarithmic in the length of the interval they were offline interval they were offline and still detect that this happened. I'm not going to go into the algorithmic detail here, but in this paper, we algorithm was first proposed in this paper called Versa. We simplified the algorithm a bit to show how there's a detail deterministic algorithm which you can use to pick some of the epochs from the missed interval in a way that if Alice checks those epochs, her key consistency for those epochs and Bob also checks Alice key consistency for those epochs if there was such a misbehavior it will be detectable I didn't want to go into the detail here because I don't think that's of interest we can we can do it in a future version if it's"
  },
  {
    "startTime": "01:02:00",
    "text": "of interest or look at the paper or discuss offline But the point here that I'm trying to make is the very same data structure can support both of these modes simultaneously and with that i'll summarize just to say that here is a candidate construction. There's plenty of room for algorithmic optimization and storage optimization here for this client monitoring mode But the point is, yeah, it makes clients more efficient and with very controversial, but no persistent state in the way I originally meant it And I'd be interested to see if this is some very controversial, but no persistent state in the way I originally meant it. And I'd be interested to see if this is something of interest, if you think it would be interesting to support both most specifically because some of these larger deployments are actually in the third-party auditor more instead of contact monitoring So I'm interested to see what the group thinks in terms of expanding the scope of the protocol to support both modes. And thank you Thanks, Asha. Go ahead, Felix Yeah, hi for the, hi, hi, for the, thanks for the privilege presentation, and I was one wondering to what extent does the current draft not support third party monitor? because you could have third-party monitors that just monitor the log tree for its append-only property, right? There's something that third parties can do Yes, exactly. Exactly is missing. Yeah, so I will go into that. I had a slide on that I think, did I? Yeah, yeah, I did have a slide on that. So I think, yes, that you can definitely do, but I think it doesn't take advantage of the fact that there is a third slide on that. So I think, yes, that you can definitely do, but I think it doesn't take advantage of the fact that there is a third-party monitor. Because if you take advantage of that, the client, can do a lot less work, and that's why the efficiency looks different And also the client do not need to keep state which I understand is like there's a lot of nuance in it but in the sense I originally meant that it doesn't have to remember what was alice's previous key state to do this and i think in the current proposal, it's not supported to the best of my understanding. So clearly, because,"
  },
  {
    "startTime": "01:04:00",
    "text": "there are different tree constructions we have different complexities, right? The tree constructions are not that different, right? So if we don't allow for a batch updates in our model, it's kind of similar except that we have like the leaves of the prefixries look a little different. But I think the algorithms are really taking the taking advantage of the fact that there is a third party monitor in one case. And in the other case it's not. Although like my monitoring algorithm is slightly different from yours the one we have in the paper but that's okay. Like, it could have different slightly different trade-offs. I don't think that's the moot point I think the mood point is if there is a third party monitored that that is not taken advantage of so i've fail to see the point where this is not taking advantage of because on your last light you also showed how a client Alice needs to go over the history in like logarithmically many steps and this is exactly what's happened with the binary ladder search that Brendan explained, right? Yeah, yeah, absolutely, sorry would be curious to really understand um if you could really say like in your draft, you do this which would be unnecessary if there were a third-party auditor, I would really appreciate that Absolutely. Yeah. Sorry, I think this was not clear. So this part, this algorithm, which is a client monitoring algorithm is similar to yours. Because the Alice has to keep state and check some consistency. And this is exactly the thing that Alice, the clients won't have to do if there is a third-party monitor. So if there is a third-party monitor, the only thing that they need to check is this, this checks, that version, whatever version number they're getting back from the server, up to that version exists in the tree and the next one doesn't. That's all they have to check. They don't have to keep any state and remember what was Alice's key state before Is it consistent with that? So this is the third party auditing mode that I'm proposing was the other and the slide that you're referring to is the client monitoring mode of the same"
  },
  {
    "startTime": "01:06:00",
    "text": "data structure, which is similar to your protocol. But this are exactly the checks of checkpointing that as the server clients won't have to do if there's an auditor Okay, just to quickly double check, then I'll open the floor for others, but um so you're saying key quirk could be made more efficient exactly and also you don't have to remember the state of alice's key from your teacher. Okay, okay, great. Thanks Yeah, thanks watson ladd, Akima, I'm sort of confused by what the difference in the data structures is I think what it is, is your per epoch data structure is indexed by a combination of Alice and the epoch on her keys And so you're trading off in the lookup a binary search, which adds that line n squared factor and adds a log v. You have to go do V queries. I have to go as a client query for a whole bunch of possible epochs and then do lookups for each one, figure out what the latest one is. Do I sort of have that right? I'm not sure. It's that exactly because it's still in indexed by Alice and the version number So, okay, just to be clear, so you're talking about the client monitor mode, right? Not the auditing. I'm asking about the slide with the search costs on it There are so there are two slides. So there's a slide with the component Okay, let me go to that this one Yes, that one. Okay And I think, and, and This one? Yes, that one. Okay. And I think, and maybe it's not important if I'm wrong. My and maybe it's not important if I'm wrong. But my thought was, okay, the reason the search in"
  },
  {
    "startTime": "01:08:00",
    "text": "WC Optics, which I understand is sort of the presentation here, is you look up the epochs and you login work for each of them to see whether Alice has a key in that epoch, as Bob Yes, it's not great epochs. Yeah, but it could be in the latest tree because it's an affinand-only tree, but yes, roughly Yeah, you do it, but you have to do a look-up for each one, roughly, yeah Whereas in the Macmillan Protocol, it's, you do log v. work figuring out what epoch it is in each step of figuring out what epoch it is, I'm not sure why there's a log squared end It's because you have to do search in both the prefabre tree and the log tree to find the actual like commitment. But oh, I think right it's, yeah, roughly, yeah, it's a lot It seems to me that this is almost orthogonal to the, third-party auditing in that the third orthogonal to the third-party auditing in that the third-party auditing isn't what's leading to the speed-ups here It's just what you've decided to store. No, I would disagree with that because there is a third-party audit who checks that no entry ever gets deleted you can do this because otherwise you will have to do more work, which is exactly what we do in the client monitoring mode, where you actually have to go back and check in certain epochs that something is maintained correctly So it's slightly taking advantage of the fact append only property of the prefix trees is only that's where the extra log-end work is coming from then. Yes from having to check the append-only property on look-ups Thank you. Thank you"
  },
  {
    "startTime": "01:10:03",
    "text": "Ah, is I wanted to say that there are two deployments, third-party auditing and contact monitor in both constructions, either optics or the draft protocol, when you're in contact monitoring you have to do a lot more work, you have to do the contact monitoring work When you're in third party auditing, you don't have to do that in either protocol. So the draft protocol is not going to be as efficient as optics in third-party auditing mode, and optics is not going to be as efficient as the draft protocol in contact model monitoring mode. But when you frame it that way, it becomes a very easy, like, numbers question, like this mode has this many bytes per query and I think it's probably simpler when you frame it that way i i was i'm not sure so what is a question sorry It was just a comment that in both protocols, we can be more efficient when we're in third-party auditing mode than when we're in client monitoring mode Yeah, yes, absolutely. I think the thing is in the current protocol, and correct me if I'm wrong, but in the current your protocol, I don't know how you, how your auditor takes advantage of that have an auditor. I'm convinced that it can be extended to that, but I don't think it exists right now So what I'm proposing here is a candidate construction that supports both this months and i'm opening up the floor to the discussion whether it is worth doing in the sense of whether it's worth extending the protocol to support both modes so that it can cover the current deployments and also is more change generic yeah i mean i guess given that it is an individual draft at this point, my question was going to be to the authors of the draft specifically to see if there was interest and like what the thoughts were around finding the draft to have both modes"
  },
  {
    "startTime": "01:12:00",
    "text": "with the caveat that at some point the draft has to be adopted so also interested in what the group thinks the author should do we have about 18 minutes. I'm not sure if we need all that time, but I would love comments on by folks in the topic of whether we think the draft that brendan moran Felix has that time, but I would love comments on my folks say, like in the topic of whether we think the draft that Brennan and felix handte right now, if that's headed in the right direction and if, separately, if, isha dual mode expansion is necessary for the draft to be adopted Hi, this is on Honest. I'm listening to the presentation, it seems to be that both sort of proposals are very close to each other and I was wondering what type of runtime we actually talk about in absolute now numbers. So like if I have in the third party auditing case, like I want to check the consistency like, what would that actually mean, like, in practice? purposes? Like how large is in and how large, how costly is one? operation? Like how often do these, do these? whole operations have to be performed and so on? these type of questions? Yeah, so there are a couple of papers a series of papers i can link you to because it really depends on what is the size of your user base how often the key changes happen, but the operations are pretty cheap because it's like a one PRF verification and it's, a couple of hashes, maybe like 10-20 hashes"
  },
  {
    "startTime": "01:14:00",
    "text": "Okay. Yeah, if you have some papers that would very pay helpful thanks yeah definitely take a look at them Thank you Are the authors aware of any deployments of these? protocols? Or I guess either So this, see, the WhatsApp department parakeet with the paper paragraph is a version of that is deployed in WhatsApp. Proton has a white paper route. The construction is slightly different, but it's also in the auditor mode. They have a white paper explaining that their deployment But to my knowledge, none of the design discussed today are deployed I mean actually you need to speak for yours but um No, no the tree is deployed, right? So the tree structure is very similar. Well, not maybe exactly, but it's pretty similar to what what's up deploys in the parakeet paper Okay, yeah, then I've got that wrong things So I guess, kind of putting you on the spot, brendan moran Felix, do you have thoughts on whether? the draft should be expanded to include Esha's? proposal? So I must be honest, maybe humble I've read your paper a couple of times before and I thought, that it is very close to the design that Brandon came up with and that I helped editing in the protocol description and I've failed to see the critical differences that would yeah, like the brain point where where"
  },
  {
    "startTime": "01:16:00",
    "text": "one changes, I mean, you probably a different design and your designs has many good properties and I see that and Brendan proposes a design and I think it's just different design and your designs has many good properties and I see that and Brandon proposes a design and I think his design has many good properties and I see them and they seem so close that I personally don't understand where the exact difference lie other than a lock factor no it's not a question about the, in my mind, it's not a question about the efficient It's really a question about you could expand your protocol to support the auditing mode, which right now the protocol doesn't have. And it is then it's a good thing the data structures are close to each other I'm only saying that it doesn't support the auditor mode right now and it will be i think nice to support that But as Brendan said, we can just add it third-party auditing mode, I think. I'm, you know, I need to think this through thoroughly. I don't want to make any calls here but I think we can just add a third party auditing mode. Yeah, and you have to change the tree construction for that. Maybe, Brendan, you're at the mic, maybe you want to say something too too too too the back? What? He's going to the back Because the two protocols are so close, I think the right road resolution here is to maybe decide this offline between this and Next IETF Yeah that sounds good to me. Pratchi, my co-chair, and I can talk to yourself all three authors, and see what to do next. If folks have comments and thoughts right now, we have time to do that Otherwise, the meeting, list is always open I think that was pretty productive Does anyone?"
  },
  {
    "startTime": "01:18:00",
    "text": "have any other? thoughts or comments before we wrap up? Deb, did you have anything? else? I do not I'm sorry, I forget I have a mic right here I do not. I'm good. It's Friday Right, in that case Just remind myself. Thanks all. And see you on the main list. Thank you very much Thanks to leave for taking notes"
  }
]
