[
  {
    "startTime": "00:00:14",
    "text": "good morning welcome to the IEP G meeting at IETF 105 in Montreal hopefully you\u0027re all in Montreal okay there\u0027s no note well this Jeff will get angry if I do that a note well thing yeah he would we have four presenters five presenters sorry and potentially a six so we\u0027re gonna skip along pretty quickly between the two the presentations if you have any questions I guess the presenters can take those as they see first up is Joel we have about 20 minutes per person okay if you don\u0027t need that much that\u0027s good please to the presenters from my member can\u0027t hear you and you want to stand on the pink X so that the medical people see us yes these these graphics are better than mine since I basically don\u0027t draw stuff yeah so this is actually sort of still recent news some of us have been more exposed to this and then others but basically anyone with a Linux Linux box has done a little bit of work in the last month due to this particular complex of vulnerabilities so I mean the basic gist of what\u0027s going on here is that there are various ways to panic your your your kernel using really small packets and if you\u0027re a malicious client you can actually get the server to participate in this by setting your TCP MSS rather low one of the proposed mitigations assuming that you cannot simply reboot all of your machines in a big hurry is to limit the minimum size MSS that you can actually set on my connection so in the in in in the CVE one of the proposed mitigations is actually the iptables lines that you see here for ipv4 and ipv6 and you know hopefully nobody is actually advertising a TCP MSS of 500 on ipv6 I\u0027m not sure why we would consider that legal but as it turns out people actually do so before I actually went and slammed that mitigation and several thousand Linux machines that serve millions of connections per day per box I went and actually looked at two questions what is what is a legitimate "
  },
  {
    "startTime": "00:03:15",
    "text": "TCP MSS we can go back to some fairly foundational documents with respect to ipv4 to look for advice there obviously at one point in time the maximum default maximum Datagram size was 576 really it still is for v4 and um you can of course obviously send a packet that\u0027s smaller than that down to the size of your minimal amount of data that\u0027s associated with your tcp ack so about 60 bytes but one of the things that you get at an advice on the size of ip datagrams not on the tcp MSS is that host actually must be able to receive a Datagram of 576 bytes if somebody sends you an MSS of like 48 or 64 or 200 despite that you should be able to send them a TCP packet that\u0027s bigger than that otherwise they were violating RFC 791 as far as I can tell so that portion of mutual negotiation that\u0027s supposed to occur isn\u0027t reinforced by strong rules about how things are applied in TCP RFC 2460 sets the IP MTU for ipv6 to 1280 minimum right so that means that the MSS you should be able to send a packet with that someone can receive from that would follow on to be 1240 or so so before I actually went and whacked 500 on there I actually went to see on our network what MSS is do we actually see on packets so I looked in our flow data and that was kind of entertaining but I didn\u0027t really believe it so I just captured a hundred million sins from five different locations in the network and then captured all the values that were there yeah so they do actually all fit on one slide I guess you will see that the the largest one that\u0027s there is six five four nine six and the smallest one is zero I think both of those are rather optimistic on the part of part of the sender right there are some that are there some that are pretty common I think the the the most common very small "
  },
  {
    "startTime": "00:06:16",
    "text": "one was unsurprisingly 536 so 576 minus 40 bytes there were 3,000 odd packets there since with that MSS said I don\u0027t have a hundred million system many of these are pretty rare they\u0027re both I don\u0027t really you can actually see in this one if you do some quick math what percentage of my tea the TCP sins that we see in those pops is actually v6 but this is actually the the percentages for the common MSS sizes as sampled from a hundred million sins so you know fortunately for our expectations fourteen sixty and fourteen forty are pretty highly represented in this distribution 8960 interestingly is it\u0027s actually hard for me to separate out the portions of my network infrastructure that use that because we speak nine km to use back and forth to each other from the portion of the internet that optimistically suggests that it can support a 9k MTU I would say that for anything bigger than 1,500 they\u0027re basically counting on path MTU discovery to work and that\u0027s interestingly optimistic but for the most part we clamp those on their behalf so that it doesn\u0027t have to so um these are obviously just the the emesis is that they sent us not what we sent back okay so if we look down here at the bottom because I wanted to know if I was going to block small as MSS is because malicious clients might be using them what sizes would I see commonly down at the bottom so out of a hundred million samples the only one that gave me pause was this guy at 512 and the one at 256 the suit so this seemed like interesting choices interestingly I didn\u0027t see those in all of our pops um so they many of these oddball ones that appear only regionally so there are particular providers or clusters of devices that seem to be associated with these kinds of behaviors they\u0027re not generalized ie the 256 one was most commonly seen in Northern Virginia and when I actually specifically filtered that the rate increased so I can Intuit from that that "
  },
  {
    "startTime": "00:09:17",
    "text": "those clients were not very happy when they got that filtered so in fact when I built a mitigation for this um and socked it away for good for for future use and we put the threshold below that 256 number because even though we are down here in the 10 to the minus 6 or 10 to the minus 8 in terms of frequency of things that we see there\u0027s an awful lot of those so I guess I\u0027m looking at this um one of the things I have we have to ask ourselves is what problems were exposed here there\u0027s a generalized problem with croix understood and infrequently exercised code paths so we see a number of kinds of vulnerabilities like that IP options handling ipv6 packet to big handling in some of these are some of these are apparent in more than one operating system and in separate implementations so some of these things are a product of interpretation of IETF standards much like say routing header 0 type vulnerabilities were um so there are implementers out there that are looking at the advice that we provide and producing configurations that are feasible but not necessarily compliant or maybe they\u0027re completely illegal but MSS is lower than 536 or 1240 seem to fit into that category so when you see those and they\u0027re the product of say additional encapsulations where someone to set their MTU lower deliberately because they\u0027re not doing PPP for example those are all well understood so if you see an MTU of an advertised MSS of 1388 it\u0027s pretty obvious what\u0027s going on there right um but these 512 and 256 seems like numbers that someone picked out of a hat like you didn\u0027t arrive at those because of the structure of your of your link MTU for example um so one of the things that\u0027s exposed here is that malicious clients are they are able to control the behavior of servers right we new advertise a lower MSS than the server would prefer to use the server is going to use that unless it has a knob to prevent that and now Linux actually has one of those now as a product of this particular vulnerability but that\u0027s a behavior we try and avoid in in lots of things is to avoid cases where the client can specify things that you "
  },
  {
    "startTime": "00:12:17",
    "text": "really don\u0027t want to do and you end up doing them anyway so there\u0027s also a need to mitigate certain kinds of behavior either temporarily or indifferent indefinitely without gratuitously breaking users that have made bad assumptions that\u0027s why I didn\u0027t use the 500 value in my own mitigation so is there some advice we should be offering here RFC 66 91 for example went back to went back to what we specified for Mt use and how to calculate offsets and provided better advice basically because of a lack of a lack of clarity and I think there\u0027s some argument to be made that there\u0027s more clarity that we could provide to implementers that would either prevent the sort of behavior or give me as a as a user license to ignore these things as bad behavior so the Linux kernel now actually imposes a minimum that minimum is pretty small it\u0027s 48 you can set that higher if you\u0027d like and so if somebody proposes a lower MSS than that you won\u0027t use it and so in our cases like and we plan to set that to 200 that\u0027s not actually deployed yet but there you go so a reading about our reading that the document suggested we\u0027d be well within the IP and TCP protocol specifications to enforce sending packets at minimums that are aligned with them the link the minimum link empty use or some value near them as opposed to at 0 or 60 or 48 if these devices actually have link em to use it will not accommodate a 576 or 1280 by packet they will break obviously even before the network could fragment those on your behalf in v6 it won\u0027t so too bad yeah thank you yes sir "
  },
  {
    "startTime": "00:15:31",
    "text": "good morning I\u0027m Cassandra from JP RS I will talk about attack to passivity discovery it is a portable walks haughty presentation this may attack the passivity discovery is presented by IP fragmentation attack on DNS after ripe 6006 table meeting October 2013 and domain invitation prosperous for enmity I am religion to PKI she comes exact 2018 they these papers show that some implementations accept a champey government ation needed on the DF set with small empty body this is on 576 under the code especially the body at the tessera toot body and the posse MP posterity body may be can be decreased to 552 on Linux three-point shotting or rate or order and the paper show that says that pass my body may be degrees to 296 then I would like to evaluate to sir attack this space shows information methodology attack fast generate crafted ICMP packet details in next ride on second dose send the packet to the target and the attack Isis solder party at MTV buried target Saba under another machine and number three verify that easily verify the result on the target to machine using commands only Knox IP do to get IP address command and on previously his control or net tiny to TCP host Akash disto only TST NATO\u0027s - our Schultz passing with you this page shows how to generate to consideration people on who pockets Krusty - in people\u0027s pockets contains a shame people hate ICMP header weeds and unreachable augmentation needed on the deck set with small empty bar you undo in IP version of header and in a UDP header and specialized sauce this sauce is mauritius massaging pasanda and target which target victim Saba and the new motif and try to control remote to "
  },
  {
    "startTime": "00:18:35",
    "text": "target to demote or summative are you ok and this page shows how to generate a cross needed a simpie baiance pockets i think i crafted eyes in people\u0027s pocket contains i simpiy bar on 6 header i simpiy buttons the item IP version 6 header and across the ICMP balance sheet set that contains pocket - beak with small empty body for example 280 undo in ipvanish header and in i UDP header with large data sides and the field 0 to the end o packet under these small code shows how to generate the pocket this place holds verification with a lizard only knocks two points 6 points attitude I Peru to get come on the shows that a multi facility you change to 2,180 IP version 6 under par 70 you change two degrees to 552 on IP version 4 and this page shows the recent vegetation result of previously under NATO BST previously on previously which after attack Smt you just changed changes to change the under 70 degrees to 20 172 and only to Bastille after the attack the same you t to change to 1280 this table shows aberration result of the attack domain volition prosperous whole empty I\u0027m reading to pk paper shows that some some implementations accept crafted I simply button for augmented need under the offset or UDP and ICMP you degrees to 552 or 296 and on linux 2.6 case linux 2.6 accept accept crafted I keep Acosta dashing people John Holliman Tunisia and the except for UDP and passivity degrees to 552 and "
  },
  {
    "startTime": "00:21:38",
    "text": "4.80 previously net Obst ignore crusty Tai Chi Chi B button for fragmentation needed under the except for UDP and the previously net we see case they don\u0027t have called for crafty Tadashi they don\u0027t have called for a simpie revolution needed under the except for UDP higher bar Linux and FreeBSD NetBSD all except crafty crafty dozen people in people\u0027s pocket to beak or UDP and they and passivity you decreased to 380 this page shows summary of the attack although Linux system doc left custom people in media and DF set for UDP and pasyati changed to 500 to 550 to the BSD systems and renewal in existing ignore action people are admitted needed under the offset or UDP and DST under Linux systems except a shame people who fragmentation idea set SMP battleships Paquito too big for TCP and a changeup a 74 much to TCP session tcp case they check existing TCP session table and not not vulnerable and many busy under Linux systems except crafted in version 6 pocket Oh too big for UDP and facility the crystal clear of 180 and is easy to set remotely then I\u0027d like to propose that don\u0027t change facility discovery a simple buttons expected to be Co TCP and IP version 4 permanently did not reset are necessary ho TCP because TCP stack uses a shame PP to be Rowan\u0027s needed under the accept to adjust packet size to others MSS and UDP is safe to use with packet size up to 1280 on IP version 6 i simpiy partnerships pocket too big for UDP balloon and I\u0027m proposing recommendations to avoid the orientation in DNS so in this page is DNS might be my draft proposals through services rubadoux to set it en 0 requested the UDP pedal size to 320 and also TT service under Rousseff is observable should set it en 0 responded maximum payload size 2 to 3 120 and more authoritive service may send a DNS "
  },
  {
    "startTime": "00:24:38",
    "text": "response into its IP don\u0027t have IP version 6 don\u0027t rock options and euros observers may drop augmented the UDP responsive tribe drama penis before IP reassembly it is a countermeasure against DNS cache for the new attacks using IP fragmentation and conclusion and all questions ipvanish pocket topic is important for TCP and however many ways except crafted ICP bottom-6 pocket too big for UDP and posterity body is easy to decrease to 380 limit or e I will consider about facility discovery and fragmentation omit 1280 or disabled that\u0027s all questions yeah Jolie eglee um so this is definitely something I was thinking about in the context of could I get things to go lower than that right I don\u0027t see a real high value as a malicious agent in reducing someone\u0027s MTU from say 1500 to 1280 right that\u0027s not I mean that\u0027s kind of me being an ass I suppose but like that that\u0027s not nearly as costly as if you can say reduce it to 200 right so I mean this is it this is essentially a product of us using an out-of-band signaling mechanism because the ICMP message is not embedded within the flow that that it\u0027s purporting to control that said like I I think the question here comes down to implementation advice like if we still want to path MTU discovery to work then out-of-band messages are of course necessary if you want to do it in band well yeah there\u0027s other mechanisms for doing that that are less susceptible to this kind of behavior um but unlike the case potentially in v4 particularly with really old like architectures where you really could reduce the the MSS by a lot I think this is less dangerous I mean if you\u0027re using a huge MTU like say 65 K reducing it to 1280 it\u0027s obviously quite detrimental to your performance and behavior but you know those are um not general Internet "
  },
  {
    "startTime": "00:27:41",
    "text": "cases thank you uh-uh-uh-uh-uh people today 280 MTU is enough I see Eric fine yeah 1280 I gave you six decision to do 1280 seems like a better and better idea with every passing presentation today so this has been interesting I think the original presentation as well about the fragmentation DNS tech that you mentioned in ryf was also very detailed and fascinating I think it\u0027s possible to do more sort of authentication if you will of the ICMP packet for TCP because you have some state tables for UDP you might not because it\u0027s not the socket is not necessarily connected it does seem like if a socket is connected the Linux kernel should bother it at least check the connected UDP sockets to verify that does it do that do you know yes is if Sandown knows all sent you the old UDP packet then possible yeah I think if it\u0027s if it\u0027s an unconnected socket it might not ever you might not maintain any state about that but if it\u0027s connected it would have it so it\u0027s definitely at least try to scan that table I think that would make sense um yeah this I thought was also a problem with Idina zeros MTU advertisement there\u0027s no in-band fragmentation which was like what eight zero eight fives predecessor recommended for how to write UDP applications but I I\u0027m assuming the TC bit maybe solves all problem there I\u0027m not a DNS person well actually I\u0027m not really worried about TCP UDP fragmentation or texting Dennis because we do actually know how to deal with them we\u0027ve got we can do that we can deal with them at the UDP at the D in this little if we have to you just use t seeking that generates the cryptographically secure signature over the UDP message and there\u0027s no way a reassemble packet passes that now as I suggested I suggest the win this was originally brought up that we use of online TC TC key to completely mitigate this problem there isn\u0027t it and looking at the way servers respond to unmanned t cigs I don\u0027t believe there\u0027s a real problem other than deploying that brings the that removes that complete thing I had Nick I\u0027m not worried about DNS and fragmented UDP we can solve that completely "
  },
  {
    "startTime": "00:31:03",
    "text": "this time they\u0027ll just tell us Chris I don\u0027t think anyone here is so I\u0027m Tim April I\u0027m from Akamai I\u0027m also working on this project with a few people that are not from Akamai so we\u0027re proposing a new system that will help see changes that go into the DNS so the overall goal of this project is to try and make it so that anyone who wants to see changes to the DNS in near real-time or as close to real-time as we possibly can get it can get that data without having to go and pull the data yourself from the authorities so the problem was back in late last year early this year they\u0027re a bunch of attacks on the DNS where attackers would get in make changes very quickly and then change them back resulting in some queries some changes going unnoticed by monitoring systems all around the world and the nature of these short-term changes made it so that like mobile devices and things that were online but not actively doing and not actively having users attract with them may have sent either credentials or all sorts of other stuff to those to the target machines where the data was changed so the the problem was so the registration model that is used for the NS many of you probably already know where the registrar talks the registrar to the registry and then to the name servers the problem here was the attackers that tried to get this data either modified records at the Registrar without the registrants knowledge or in a couple cases modified had registrar credentials and modify them directly at the registry without the Registrar knowing resulting in takeovers that were essentially transparent to any of the people that cared about the names and then many people that have high-value names will set up all this monitoring stuff where you\u0027ll pull the registrar to make sure your names seem accurate you may pull the registry you may also pull the nameservers directly and see what\u0027s coming back but it really matters on what that polling window is whether you\u0027re gonna see these changes or not the solution we\u0027re proposing here is to create a open system kind of like wrist live is for BGP updates where the domain the data owners push data into the system and then it replicates the data back out to any of the end users that are interested so that could be the domain owner the domain users anyone that relies on those objects or people just monitoring for security reasons so they in phase one of what we\u0027re trying to do we\u0027re gonna try and get as much data from the registries as we can to push into this project so this is essentially a pub sub system "
  },
  {
    "startTime": "00:34:03",
    "text": "that will send the data the main goal is back to the registrant so the people that own the domain should get updates about when anything changes in their name going forward we\u0027re also talking about trying to get data from either G TLD name servers and then possibly the registrar so that we can cross check if the registrar sent us this update and the registry sent us this update it that we know that they both know about this what about the registrant did they send any data to us if not that we might notify them or we may change how these notifications happen it also lets the Registrar subscribe to this that\u0027s one area I forgot to put on this graph where the registrar could receive this data so that when the registry makes the change they can notice that they actually got the update and then what or if they didn\u0027t request the change and they see the update they know something went horribly wrong so we modeled this off of this somewhat of the certificate transparency model the one major change that we had that we built into it was that we were gonna try and build some monitoring and alerting on top of it just as a way of but you\u0027ll still be able to get to the full log but we wanted to make it so that you could also you get real-time updates for whatever you\u0027re interested in so you\u0027re not having to go and crawl the entire log all the time just to see the changes that you really want to see so as I was saying the the data we want in is data from registries mostly the public zones that are out there where they\u0027ve kind of like CCDs the centralized domain repositories service from ICANN but more up-to-date and faster we have no interest in contact information we these slides are from the ICANN meeting and I knew that was going to be a hot-button topic if we were wanting Whois information because I don\u0027t want to deal with gdpr and then we\u0027re hoping to output both our raw feed so anyone who wants to go and look at the data can see it and then the filtered updates for whatever you\u0027re interested in that could go over email SMS a web hook I don\u0027t really care how you get it if you want if you\u0027re interested in this and want a different data format let me know and we\u0027ll see what we can do so the places were the things we\u0027re working on now we\u0027re trying to create and append an entity for it rather than putting it through some corporation or some for-profit company so that it\u0027s a real open project rather than tied to some company and then we\u0027re actually starting to build the POC with a few different data providers that we\u0027re hoping to get up before the end of the year and actually sending data to the end users were mostly presenting around a whole bunch of different places to see if "
  },
  {
    "startTime": "00:37:03",
    "text": "people are interested this if you\u0027re a registry and we want to provide data I\u0027m happy to talk or we can you can reach us at the contact information at the end of the slide if you\u0027re a registrant and want to subscribe to this sort of thing let us know and what your use case is so we can make sure that we try and cover that if you\u0027re representing some sort of company and want to try and either help with resources or funding or something like that we\u0027re happy to talk mostly we\u0027re deferring a bunch of that until we get the organizational bits enough you want me to jump in now generator hi shared much I\u0027m just curious this seems like it might be something that would fit in the DNS or wheelhouse I\u0027m curious if there\u0027s a reason why not to park it there versus creating a new DNS monitoring organization we had some conversations about that I\u0027m trying to remember back to why it ended up that way I\u0027ll have to I was just curious if it there is it if there is a explicit or implicit thought process you could say I\u0027ll talk to you later offline yeah it\u0027s like it\u0027s it\u0027s something that we will consider that we had considered and I\u0027ll see okay we actually ruled it out or not and then the other things that we\u0027re looking for is if you have experience with nonprofit stuff that\u0027s something we\u0027re actively trying to work on sorry good question though what\u0027s the license hopefully code developed we\u0027re trying to make it either MIT or GPL thank you to respond to job\u0027s reaction that wasn\u0027t my I don\u0027t care how it is I just wanted out there hi Johanna say again just a nice project I really like this idea this is this a problem we have um of course the problem here is not the technical artistic interest or the registration share and two people to share data with you you could also try maybe to build another system like in parallel to the one in which people can submit their own domains if you will and then you start tracking them from the parent and the child allegation to see if they match you know there\u0027s a change you can just notify the same way so tracking that sort of change where we\u0027re getting data from below the second level is also something very much on our radar and that was we had proposed going that method first but we actually have more registry interest than we thought we were going to where there are a handful of registries that are willing to hand us their full zone file in essentially the same format they send it to their till their name servers so we we\u0027re going try and push forward on that one so because that requires less and user interaction right now I mean like because I work for one TLD ccTLD and I\u0027m pretty sure we couldn\u0027t share yes it\u0027s like we\u0027re hoping to get "
  },
  {
    "startTime": "00:40:05",
    "text": "the steel these we have online first and then go to the next level down and then try and target the other cc\u0027s after that that\u0027s a nice project thanks a lot for Dan is dr. Montgomery\u0027s there in any notion of limiting the scope of what you can subscribe to monitor some sense your resources are can i subscribe to bonnet or any and all resources i we we were planning to try and make it so you can subscribe to whatever you\u0027d like to wear the primary reason for that is if you look at trying to remember someone from Verisign may be able to correct my the domain name but I think it\u0027s trans - trust Barris on labs comm where you can see all of the name servers that your delegations rely on so like if you\u0027re if you\u0027re into a org you\u0027ll see that you have the door name servers you have the all the affiliates name sets and then you have the dotnet from Verisign net and common Verisign also rely on that so you\u0027d probably want to know any update if you\u0027re really interested in no name changes you want to know all of the updates to any of those names and being able to subscribe to all of that and then like I\u0027m a if I\u0027m a customer of like if my mail is hosted on Google I probably want to know if any of their names change so that I know if my mail is impacted things like that ready bouche um the DMS is pretty big so I think the subscribers you\u0027re gonna want to deal with scoping in many ways um but could you sometime somewhere posts a URL for those of us who do run cctlds with an ICD it\u0027s page on how to push yeah there\u0027s your URL I didn\u0027t realize I only have one side left so I\u0027ll send it out to the cie PG list a little bit with the it\u0027s right now it\u0027s a very basic website it just has the quick description of what we\u0027re doing and then we have a form that you can go and say I\u0027m a registry and I\u0027d like to send data we don\u0027t have anything like we don\u0027t have any of the infrastructure deployed yet once we do I will come knocking will do a question for you actually is what\u0027s the easiest way for you to send me data I will note that and set up instructions Thank You Lucille in responding to your question about setting up nonprofits that\u0027s actually a relatively long and heavyweight process have you looked at the Linux Foundation sponsored projects "
  },
  {
    "startTime": "00:43:07",
    "text": "we have not I you might want to do that they they take a lot of the pain out of it and those things like let\u0027s encrypt so you might be a good fit for them okay keep that in mind thank you any other questions I\u0027m here all week if you want to get trained later we\u0027re down to just the last official presenter oh yeah where are your slides I know you did I sure did it wait a minute so yeah what we needed you instead of Randy where we can do Randy than you Oh Randy Europe I\u0027ll figure this out while we do that miss Lehman gallery speaking so hi I\u0027m Randy from a J researching from our kissing um how many people here remember in mid-april DNS attack miss called sea turtle please this is scary this room\u0027s half DNS people come on okay so what happened was DNS registration systems are hierarchy and hierarchy is as strong as its weakest link bad actors were discovered by I think it was Cisco talus group to be breaking links at the ccTLD and other TLD levels and forging and so on and so forth no this is not a talk about the DNS but it\u0027s a talk about the weakness of hierarchy and we have another hierarchy which I did spend some time in luckily yob who invented it is now promoting it much better than I could and I\u0027m relieved but that\u0027s the rpki and it is has the same weaknesses as the DNS in terms of a hierarchy it has the same security model for the small deployment of DNS which is object security as opposed to transport security okay so if you can break things up chain you\u0027ve got it and it has one the way it is "
  },
  {
    "startTime": "00:46:12",
    "text": "currently deployed it has one wonderful additional brilliant weakness which is all five of the registry at the root of the registry hierarchies are authoritative for the route that is as if d dot de was also authoritative for dot all five are IRS claimed authority over 0/0 this is brilliant okay in the security universe the question isn\u0027t when this will be attacked if this will be attacked it is when this will be attacked okay we\u0027re starting more and more thank you very much for the people who are deploying and for people like you who are putting a lot of effort between in the getting deployment out there we need to get the security of the system and the operational reliability cleaned up before the disasters happen not after the first disaster happens and it will happen so some of the things we can do are pretty obvious and getting the registries not to be authoritative for 0/0 it\u0027s just one small step rudiger are you ready to leak to the mine why didn\u0027t we talk about this two years ago because then it wasn\u0027t this way because I aren\u0027t usually used to you being snarky Ruettiger and I did talk about it two years ago as his point and we didn\u0027t succeed I\u0027m trying to explain we tried to talk about it from an authority angle then and it was a bad mom it\u0027s a security problem it\u0027s not just an authority problem that\u0027s it nobody except for Ruettiger okay okay Giovanni I found your slides I found them but I have that shoe Brandi\u0027s time\u0027s up "
  },
  {
    "startTime": "00:49:16",
    "text": "Yeah right so I\u0027m standing here between you and lunch but if it\u0027s already actually for death so good morning everybody so this presentation is actually a work that we did together with some colleagues at ISI and also at the UPF this topic that spits usually splits the crowd the details on DNS so what happened like we had a study last year on IMC on details and denial of service attacks and we saw we show how the longer TTL protect users when there\u0027s denial of service attacks and on the authoritative servers and I present that to the ops folks at this idea that I now and they were like all right so it\u0027s nice to know that so which details should I use I was like oops that\u0027s a different question I haven\u0027t looked into that so that\u0027s what he\u0027s trying to do here this is that has just been except last Thursday right for publication in IMC this year in Amsterdam um which is perfect timing for this meeting we have put online the submission version at this URL if you\u0027re interested we\u0027re gonna of course created a revised version with the comments that we\u0027re gonna get last week and that will follow will be online as well now kesshun is the cornerstone of DNS performance we know like a 15 millisecond query response time is real good but 1 milliseconds far better coming from a cache hit and as I said we did a study last year we\u0027ll look how caching protect users from alternative service attacks on DDoS attacks on out servers and the thing with TTL and DNS and caching says the DNS detail set at the zones they can actually control cache and duration at resolver sized site so it actually affects latency and resilience indirectly actually very directly and there hasn\u0027t been a lot of evaluation on the topic I\u0027m just mentioned here two studies there\u0027s some more on paper we mentioned and no research actually provides recommendations or in the context of that ETF I should you use the word considerations it\u0027s less controversial here on which vendors are good for TTLs because it\u0027s very it\u0027s a very challenge it\u0027s a big challenge is very challenge this topic to determine what good vendors are because there are trade-offs intrinsic trade-offs in the choices of TTLs short TTL allows for operation Ops teams to change quickly services along details with those latencies and service load so given that and but other sort of details is no surprise there\u0027s no consensus so let\u0027s try to fill this gap into this study and we break this down into three different research questions the first one we ask we need to know if resolvers in practice in in the wire they\u0027re actually parent or child centric I\u0027m "
  },
  {
    "startTime": "00:52:16",
    "text": "gonna explain this in more details but you can get some information either from parent or child depend on how your zone is configured and they may have different TTL values so we wanted to be sure who is actually in charge of the TTL s and the second question we wanted to know it\u0027s like how the different parts of the fully qualified domain name changed effective TTL life time for example let\u0027s say you have your DNS provider has a use a certain DNS provider to gives you the NS records details for a domain for an hour what happens if you are on a record is like two hours and how those things interact with each other and the third question that we address in this paper we wanted to know right we do some background work some in one in two to see how the resolvers choose these details and firmware but then we wanted to know how actually they are used in a while how folks are deploying that we know that our roots are very conservative and they have to be for resilience so they have longer details but see the ends on the other hand they usually have shorter details because they want to change things very quickly in our grow here she provides recommendations or considerations and choosing those values so let\u0027s start here the first questions resolver centricity like if you let me see let me get let\u0027s get one TLD that\u0027s al from Chile if would if you would ask for the NS record of that CL phone one other root servers I just speak a word here but it doesn\u0027t matter kiss all the same if you ask for the NS record you\u0027re gonna get a response that say is that with a detail of this value here which is like two days and they don\u0027t within that response that value would come in this alternative section authorities section part of the response now you can also ask for the same question and type instead of the roots you can ask for directly to the authoritative surveillance itself aid that Nicosia one of them and then you\u0027re gonna get the same answer but with a different detail here or one hour and that\u0027s the child value so you see there\u0027s a difference in here for two days in one hour and this one they the response actually of this one comes actually as this an authoritative answer and comes within the answer field of their response so it\u0027s quite a confusing on DNS like a DNS response may have up to three parts answer authority and additional so that\u0027s why this one here comes in the answer and it has even a flag it just says that is actually the real authoritative so we have like two different choices there\u0027s an RFC to say that they should resolve should trust this one but do you know how resolvers are in the wild there\u0027s a lot of resolvers different versions of code you cannot be really one person sure if you can obey to that so to investigate that we chose that your UI which is from Uruguay why because well at the roots as every single TLD they had a detail in today\u0027s but the time of this analysis "
  },
  {
    "startTime": "00:55:18",
    "text": "they had an NS record TTL with a child of 300 seconds in a record for 120 seconds so this vendors are very nice video sorry for my experiments are very nice but there but not for operations they changed I let me get to that later they change those values but for if you\u0027re losing ripe Atlas that which I use all the time it takes guys phone right and those again to for this platform which is amazing if you use it all the time and if you usually would measure every 10 minutes so if you run queries tools those records every 10 minutes every time as you do it you know you get ahead get against code cache because it gonna be expired by then now so we did a bunch of measurements I\u0027m gonna cover two of them here the rest is in the paper we query for NS queries for NS records for that year by the a record with one of those rating themselves for Uruguay and it just asked the probes which were like roughly 15,000 of them 16,000 to ask their local resolver to get those records we don\u0027t know and we don\u0027t know where the gonna kid could be coming from and that\u0027s exactly what we want to know so how we want to know how do you know if the answers are actually the results are getting the answers from the parents are from the child we just analyze the distribution of the details that come back so remember that at the parent the root sets the TTL their trade data for any TLD to two days but you see here the CDF we see here to let\u0027s say 60% or there\u0027s a huge spike here that most queries but for the a record they come back with a TTL of two hundred twenty seconds meaning that most resolvers are actually trusting the child value not one set by the roots and the same applies to the NS record so what we see here it\u0027s that most reserves I child sank with centric preferring the details of the authoritative answers and as should be there\u0027s an RFC for that 2181 I\u0027m not sure filters one of the outers here in the list I think is brandy is one of those maybe in section five for that one they specified this order that they should have just records so with the to order experiments I\u0027m not gonna cover because it\u0027s kind of a repetition but we need to double check that and we use we say I said well let\u0027s use another domain name that\u0027s using Google calm because those are TLD so maybe things will be different but no those results are the same and we analyzed passive data and then I know as well so most resolvers I child-centric and I think that\u0027s a good thing because that gives the power to the whoever actually all the domains enough the parent now second question how different parts of the fully qualified domain name change detail lifetime how look how long he gets in caching uh so let me explain this is a little more tricky but let\u0027s let me try to break this down so we we have this case domain cache test of math we use for testing and I create a "
  },
  {
    "startTime": "00:58:18",
    "text": "subsonic outs of that cache test net and I configure this domain in two different scenarios the first one we use as the NS record for sublet cache test that net I used this name server and the second scenario out of bailiwick we use dishonor 1 in Bailey week out of beta wicket for enough meaner in beta week means that your name servers is under the same zone in this case of the cache test of Nats so it\u0027s part of this and out of beta wickets means in this is a difference on and we set we set intentionally to set a detail of the NS record to be shorter than the a one hour versus two hours and we wanted to know what happens with the answers in the cashy once the NS record expires does the a is gonna expire as well so we wanted to know how this different components of DNS interact to give answers to the users so let\u0027s so that\u0027s pretty much how it looks like our setup and the dot Nats on that\u0027s what we have there the circuit details and the cache test that this is I also placed a time net this one\u0027s here one hour and an hour and in our zone that we actually take care of we set the NS to one hour the a record to two hours and the trick is we start the measurements every 10 minutes with ripe again 15,000 vantage points and at time equals to 9 we read direct we just remember our authoritative nameserver and in this way and we configured a new server to give you a different answer why I\u0027m doing this fame because I wanted to know later where the answer came from if it\u0027s the new or the old because that\u0027s the thing I wanted to evaluate if they went to the if they\u0027re respecting the TTL or not and I actually asked my reservoir should ask quad-a queries they don\u0027t matter but like oh they\u0027re not that not asking directly those records I me asking records at under that tree and I use those vantage points to write office so these are the two figures the first one here is for in Bailiwick experiment this one is for auto bailiwick we see let me start here every bar here it shows a time being of 10 minutes and what we do in both figures at that 10 the time equals to 0 from 0 to 10 we allow other probes to ask for this quad a query and other answers comes from the original server that\u0027s the ones configuring this honor I think is fine both cases here at time equals to 9 when you see this arrow here in both cases we\u0027re in number the authoritative nameserver and we see what happens so for this time period here this color here in both figures we see the same behavior what is that people that knew they\u0027re having cash the values of these previous records they still keep on going that because both of them are still valid both a and ns so nothing really new here people that didn\u0027t know about that they go to the new server because that\u0027s the one that\u0027s currently available now that\u0027s when it gets interesting here here only the TTL of the NS record expires because they "
  },
  {
    "startTime": "01:01:19",
    "text": "had to detail when one hour so once an expires a resolver has to refetch again that information and we see here for the in bailiwick experiment most queries are coming actually from a new server but a very different scenario here from the out of beta week and most clear answers come actually from the old servers so what is actually going on in both cases the a records still in theory is a still valid in cache so what is actually going on and after that some of them have to both expire in the second error here some of them will keep going to the original this is a stick resolvers these are could also be sticky but I have to drill down this once because this is very surprising for me by the way but it\u0027s just I don\u0027t think all of them are sticky but the difference is like and that\u0027s important thing here once your NS record expires and a resolver is gonna get that this is just an example from in bailiwick configuration if your reserve your reserve is gonna give this answer to you but they also can include the glues here if your though in this way even though you had an NS record you which it has a duration of two hours just by ask in the aim bailiwick configuration gonna get also the glues so the resume is gonna get that information is gonna be data cache or try to figure out where this answer is located at the date later but if it is out of bailiwick you don\u0027t get that you just get attractive answer answer so what matters in here is that\u0027s like most recovers would trust cached a records on servant from different zones for auto bailiwick scenario so the independence of the records actually depends on how your system is configure your zone it is out of a tweeker of in bailiwick and if he\u0027s in bed wake is if one answer or never the NS expires is in a force you\u0027re a also to expires okay so now let\u0027s move to the third research question how people use details in the wild so we just like got a bunch of hit lists the public of popular lists I know they\u0027re biased but we\u0027re not interested we\u0027re interested like in a big picture where they have and we also use entire Grinnell\u0027s on ccTLD and other field is available in the root zone and you\u0027re gonna retrieve in retrieve like TT else for NS a quad a and Max and DNS KITT KITT sorry and we focus only on child TTL values why because we found out the most resolvers i child-centric and and that\u0027s the latch there was some discussions of our operators so we see here on this first line on response to see the number of domains that responded and the first thing we fought we they can catch the first lesson from this table you can see is that most of the domains except for the root zone they are only using out of bailiwick name servers they alternative name servers so we should like you have like example.com "
  },
  {
    "startTime": "01:04:19",
    "text": "your name server would be example and as that example that net so most of them 95 nine percent in a both not on the routes not other routes their routes a lot of them have mixed set up or in bailiwick one way it\u0027s like rough some of them are only in avail week and I have here two figures of the CDF the distributions of TTL records for different zones so the way to read this is for example the root and s records here from the child you see like roughly less than 10% that within 24 hours but if you look at the other zones it\u0027s like 40% have a TTL that just is like a smaller than one day for the other zones here for the NS record and so that means that the roots is more conservative more people are having longer 2t LZ r which is what we found but if you look at that rake a records you see the figures little shifted to the left I\u0027m just gonna move back and forth for you to see that it\u0027s because the TTL typically of the a records are shorter for the same domain than the NS records themselves so that\u0027s what they\u0027re trying to see here how people actually deploy those values and umbrella it\u0027s a list and I think it is the only one in our list that not doesn\u0027t only have like second-level domain information they have like fully qualified and have a lot of system CDN names that they have like many parts and those look like CDN domains they tend to be very short-lived some of them use like only for one time signaling and that\u0027s why I see the umbrella list here having way short details and the rest like 60% a you have less than 10 minutes which is like a lot now we found we so then we look at this and now I mean if I have to choose I like a look first look in the TLDs we need found we found a 34 of them had a TTL for the NS record which was under half an hour and one or 22 one or two hours on the child delegation and we contacted ATC T of this on this matter and six responded so three of them did not consider this question before you said it was intentional that you were changing infrastructure they want to keep it low TTL values because if the world problems would it would ease it to reshuffle reroute stuff and one said it was the way they got it from the client because they are just an operator for this Tod and they keep that way and the cool figure after them certification we found that 3 T of these actually increases the TTL of their any records because they were not aware of that and that that\u0027s was like I didn\u0027t expect that to happen but there were folks having attorneys I mean I\u0027m gonna get it one case here of your why yeah I\u0027m sorry it\u0027s something oh yeah honey it\u0027s not passing nope Chris can help me on here "
  },
  {
    "startTime": "01:07:32",
    "text": "measure with I think the next no here\u0027s fine or should be fine now can move forward next one next one yeah next one 22 it\u0027s not working maybe you could open outside the browser oh all right oh yeah that one yes 22 probably has a CDF in so if we have a problem can also read from I have it here let me just pull it for a more email so there\u0027s how about that yeah perfect yeah all right no that does the trick so as I said we know three cctlds they actually change dirty tales of their records and one of them was Uruguay we contacted them and we actually asked permission to disclose that publicly they were like they had changed the value before in the past and because they were doing some infrastructure changes but then they after that didn\u0027t change it again and they went far for the NS that went for 300 seconds to one day so from I was lucky that I had measured them before they change it and one day I may I just double check if they had changed they had change it to one day so I measure again I asked fifteen thousand ripe Atlas probes to send queers and let\u0027s see the performance that they get and this is the CDF you see here and what we can see instead like the median response time for a client of right before was 28 milliseconds for all the 15,000 vantage points for a TTL of 3600 seconds but after that he went down to eight milliseconds and then 75% I went down from 180 to 80 21 milliseconds you\u0027re shaving love like 160 milliseconds just by changing the DT L I think that\u0027s a great change as men of course it requires the records to be in cash but that shows the power of TTL here you can actually improve a lot of response times next let\u0027s see if it works so yeah I yeah if you know should we probably find a certain more figures anyways what if it works oh it doesn\u0027t yeah well got that that\u0027s fine yeah so the question so just going back to research questions a resolvers parents are centric so just answered in Mostar child-centric how the different parts interact "
  },
  {
    "startTime": "01:10:32",
    "text": "bailiwick in pets a lot how caching works depends on earlier domain how its configured how are details using the wild the Vedas are all over the place longer and answers but then Aden Cuates and most of the domains we see it\u0027s like they\u0027re out of a lake max to see if it works always obviously the FBI wastes heavy to load that means it\u0027s a problem let me sit me in here so high 24 it\u0027s not loading you can go to 25 is also fine so I think the other one probably not gonna load because there are two figures but anyway we did an experiment to evaluate what is like performance impact of using longer shorter TTLs it\u0027s gonna be available later on the webpage of the IPG but it\u0027s like 24 you\u0027re gonna be able to see that longer details reduce to improve the performance and we found out that if you have two scenarios if you have a domain name configured of a TTL of 60 seconds one minute and then if you have a domain name configured with a detail of 60 sorry one day another one with 60 seconds using any cast any case is not gonna help you as much as caching because caching even even though even though caching helps a lot it helped Cowie helps much more than any cast if the domains cache just I\u0027m trying to say here and we found out that if you use if yours longer details we will run if you use longer detail to observe like a reduced query loads you nurse your server on a reduction 77% and let\u0027s hope it works this time by 24 or 25 that we do it yeah yeah so that yeah that one so you see here that one that\u0027s exactly what I said before on this figure here on the left you see the red color we show the detail of the domain of for the bright probes for towards one authoritative nameserver in unique a single location and you see here the curve and if you use a detail change that record and run in the same unicast for a server that actually has a TTL for just any TTL to one day you see the performance improves a lot so here you know you would see like 60% of the people there under 15 milliseconds if they use a detail of one day but if the user to TL of one minutes "
  },
  {
    "startTime": "01:13:33",
    "text": "is there roughly around 60 milliseconds in any case rerun this on any cast we put the same zone we put on route 53 with a TTL of one minute and my question was like alright let\u0027s see how much anycast benefits the users and compares to a longer TTL maybe I have a big anycast Network I don\u0027t care about the tails anymore but for the point of view of performance you see here that any cast does improve that much for people of we which are very close it grows longer here for people that are both 50 dead where you actually start to see the difference but caching helps that helps more if the domain is cached of course then any cast and that\u0027s a lesson here for operators next if it works yeah Jerry those figures in PDF and they tend to have a lot of data point sometimes it become heavy let\u0027s see oh yeah he\u0027s not working yay yes so reasons for longer in shorter detail so longer TT our labels longer caching which means faster responses lower DNS traffic to authoritative server more robustness towards to denying service attacks shark action on the other hand supports operational changes it\u0027s like faster for you to change stuff can help in case you use like DNS phase response to those systems mitigation it can cope better DNS base load balancing so the takeaway here is like fergan ization should like wait and this trade-offs here to find a good balance next so recommendations are considerations that we knew there\u0027s no single optimal TTL for our users but for general users longer details hours or like even a day if you will a great like even like 440 LG\u0027s as well the exception is if you\u0027re if you\u0027re running dns-based DDoS protection servers so that we would like to have like a short it also allows you to quickly configure your service but feels like only BGP Bay\u0027s you don\u0027t care about that a and quota a records an NS relationship for out of bailiwick the records are caching the family so we don\u0027t really care about the details like but if in there and made a wiki we recommend to set it CEO of the a and quad a should be shorter or equal to the NS and location of those records as well at least you should have one preferably one at least out of beta week name server in case the zone becomes unreachable next conclusions so details "
  },
  {
    "startTime": "01:16:34",
    "text": "in DNS or a complex topic we all know that we carry out a bunch of careful design experiments to to factor how to figure out how those factors interact we show that in the wild there\u0027s a little consensus on how to TL values are used and what I really liked about this paper like the discussion of the Ops teams for some sissies here this actually led to improve a user\u0027s experience and in short longer to TL so if you can\u0027t do that just do it and this is also a very good time because we have a draft now on IETF justice messenger and we\u0027re proposing recommendations of sorry considerations for Dennis ups and I\u0027m gonna represent that on Tuesday and consideration number five multi TLS it\u0027s all based on this paper now thanks any questions hi Jerry I was just curious if you were able to infer behavior from different software stacks of how the caching hierarchy interacted based upon whether or not you got that which answer you received so the way they did is measurements like I use dry pathless and it can have we kind of agnostic because I don\u0027t know which resolvers they are and what they do I haven\u0027t looked at the problem but I know someone who is looking into that his nominee Elias works with Roland Sheffield Roland\u0027s here from University of Twente in Holland it\u0027s actually look into every single resolver version and what you see what they\u0027re actually doing because that\u0027s a different contribution of course I was looking into the big picture but we need to know if the vendors are complying to the RFC yeah yeah I was more I\u0027m curious because I know many years ago people use the additional section for you know different attack you know you know different attack vectors and stuff to go and poison you know caches and stuff and so the fact that you can tell which answer because there\u0027s different answers in different places you know which answer you received yeah and whether or not you were able to infer from from that what the resolver stack or stacks or you are doing from that is it so yeah it seems like it\u0027d be interesting in addition that\u0027s a good point I don\u0027t look at that yeah sure looking today I\u0027m not sure if my I don\u0027t think my data allows me to explain that but I could carry all their experiments generalize that Thanks that\u0027s it else Thanks morning all I\u0027m Jeff Houston I\u0027m with a peening as Randy observed IEP G meetings are exclusively for DNS and BGP and nothing else and so Joel and and the MTU work merely aberrations but you know you hadn\u0027t had "
  },
  {
    "startTime": "01:19:34",
    "text": "a BGP dos this morning so you\u0027re early you\u0027re not allowed out into the room until midday so you\u0027ve got to have your dose of BGP this is actually a summary of an awful lot of data it\u0027s gonna say 2018 was last year this is oh you\u0027ll actually see why it doesn\u0027t really matter you know BGP is wonderful because of all these pieces of work bgp is the one protocol that brings the entire internet back to you your details might vary with my details depending on where you sit in the routing mesh but the whole issue about routing protocols is you get to see a complete topology of the entire internet not perfect but not bad and so in some ways it gives you some amazing insights as to what the internet did and if you take all of route views over all of its data which starts around 93 in terms of hour by hour and this is hour by hour huge amount of data you actually see the major events that happened in the internet you know the great internet boom and bust god i\u0027ll need a microscope just around here somewhere you know it happened big boom and bust what actually really happened was the rise of consumer DSL and that fueled through the early 2000s and this whole thing about consumer based internet drove inexorable growth this is v4 but the remarkable thing is about all of this that was when we started running out that\u0027s the date at which I Anna ran out then a pina can write then lactic and then we seem to be running on empty and running at an increased rate in terms of the routing size even though we\u0027re not pushing more addresses into the routing system there\u0027s something going on there and let\u0027s have a look at it this is 24 months in detail of every pair of route views and every pair of rights rests bizarrely route views sees more prefixes and oddly enough actually more addresses than rest they have much the same growth model but in Europe there\u0027s certainly almost 2/8 less of fine-grained detail in routing of address band than in route views a number of possible reasons about about this including perhaps the notion of ghosts prefixes that are sitting in the route table that withdrawn at source but just continue to hum around in the routing system because the withdrawal is not Universal so yeah an oddly divergents average growth rate much the same and we can break it down in detail and don\u0027t forget there are no more new addresses that\u0027s 2011 over there on the left and that\u0027s when we ran out of really pushing large amounts of address space out into the internet yet we\u0027re growing and at "
  },
  {
    "startTime": "01:22:34",
    "text": "fifty two thousand prefixes per year in the v4 space almost clockwork and what is even Auto in some ways is that the a s growth 34,000 prefixes per year is clockwork so at some point I don\u0027t know how you guys organize this as operators you know only ten per day and once there attend no more no more you\u0027ve got to line up tomorrow even on weekends it\u0027s phenomenally uniform in terms of the growth rate of AAS numbers BGP for traffic engineering is still BGP for traffic engineering absolutely nothing has changed over that same extraordinary long period the amount of more specifics which used to be half of the routing table is now 54 percent of the routing table ie around half so no matter what we say in the message about aggregate aggregate aggregate aggregate whoever\u0027s not listening is still not listening in whoever it is listening was always listening and so the basic message has never really changed the only thing that\u0027s really changed over those last few years is the average size for routing advertisement because there are no new addresses right address span that\u0027s being advertised the last few years is being constant so how do you get all those address new routing advertisements in you advertise smaller and smaller prefixes the average routing size now spans 4,000 slash 32 s it used to span around 7,000 so the average routing element is getting more and more finer as we progressed that\u0027s the address span and you can see from around 2016 or so that was the amount of address space that\u0027s being routed in the Internet we actually haven\u0027t unleashed the rest of the unadvertised addresses into the network that\u0027s now relatively constant sitting there to around 170 /a it\u0027s being advertised and that is know more is coming out whoever\u0027s sitting on an advertised addresses is sitting on them whoever was going to sell has possibly sold I don\u0027t know but we\u0027re not seeing more in the routing space and the other thing that\u0027s pretty constant is the average AAS path length and that\u0027s going to be relatively important but that\u0027s remain constant for ever where I sit it\u0027s a little under 6 a SS from where you see the actual value might be different but it\u0027s consistency might well be much the same unless you change your own routing so this is another one this is the amount of adjacencies now the people on this list might vary depending on where you sit where I sit I see Huracan electric with the greatest number of adjacencies is $6.99 but that\u0027s me you look at yours you might find a different one but what you will find is around 9 to 10 a s\u0027s have an extraordinary large number of adjacencies number of neighbors more than a thousand about 2,000 or so have around you know 10 or more adjacencies and everyone else is out on the edge one or two adjacencies so that it almost "
  },
  {
    "startTime": "01:25:35",
    "text": "looks like a power-law distribution it\u0027s not exactly obvious who sits in the top slot as a global constant because I don\u0027t think it is it depends on where you are but that shape is the same no matter where you are so the Internet is very heavily not even a star network it\u0027s a dense black hole kind of network we\u0027re a very small number of foe do the bulk of the transit routing and everyone else just attaches at the end and everyone wants to get as close as possible to those magic 10 right there is no long AS path vectors flying around as sort of an industry norm it\u0027s not so this is why the actual date doesn\u0027t matter nothing\u0027s change you know the growth plots it\u0027s just business as usual the number of entries is reached you know a magic three-quarters of a million and it\u0027s keeping on growing at much the same rate 52,000 entries a year 34,000 is is a year and quite frankly the way we\u0027re doing this shorter and shorter prefixes but exactly the same topology so what about address exhaustion well you know Erin ran out just really ran out everything\u0027s going to go in about May next year like Nick late this year ap Nick and right both have these last slash eight policies you know dribbling it out AP Nick late 2020 maybe if they change their policies and I think they have to a slash 23 or something it might last a bit longer but effectively all that\u0027s left is sort of dribbling little bits and pieces so what\u0027s driving growth right now in v4 is it all transfers of these last slash eights the factor or is it leasing an address recovery what if you take a snapshot of the routing system at the start of the year and a snapshot at the end of the year and eliminated everything that\u0027s in both tables so the new staff you look out and say when was that allocated so what appeared through the year and when did the registries claim they gab give it out in 2010 80 percent of all the addresses we saw in the routing table at the end of the year were allocated within 12 months before when they first appeared so in other words you got an address you routed it and that was the major source of entries into the routing system this is every year since then and what you notice is it\u0027s a very sort of similar curve but it keeps on dropping so last year only 20% of all the new addresses I saw in the routing system were actually allocated or assigned by an RA are in the last 12 months which kind of figures what isn\u0027t quite so obvious is that almost half are really really old really old more than 20 years so this is almost predates the RIR system this is getting into legacy space and what\u0027s happened in the intervening period is more and more of the legatus from that legacy space relatively is bleeding back into the internet and the amount of sort of "
  },
  {
    "startTime": "01:28:36",
    "text": "transfer and trading she hasn\u0027t changed a lot this is the major factor of what\u0027s driving new addresses into the internet and I can look at that in a bit more detail but looking at the total amount of addresses that are marked as being out there the amount that\u0027s advertised and the amount that the RI hours are saying is out there but I can\u0027t seem in the routing system so that\u0027s that unadvertised address space blown up and again this gets interesting addresses we had 50 slash eights that weren\u0027t in the routing system we now have 48 so this whole idea that transfers and trading would unleash a huge amount of address space that was otherwise dormant isn\u0027t supported by the facts and in fact the last 12 months were even more amazing that the amount of unadvertised addresses actually rose not fell and there are a number of large transfer deals where their end result was the addresses that were previously advertised to now not advertised which again is sort of anomalous behavior so the drawdown from the R\u0026R pool just under a slash eight the change in the unadvertised and advertise place though much much higher that\u0027s the text yeah you\u0027ve had a question Julie I can\u0027t speak to the details of any particular transfer but I would observe that one of the things that people tend to require when they when they do transfer them is that the previous prefix announcements go away so that actually means that some existing advertisement advertisements have gone away that I know of specifically prior to the address base being transferred this is kind of bears out with what we see too and the predominant factor we see in the movement of addresses is towards various forms of CD ends and cloud providers and they do go away we\u0027re probably going to see them again at some point but in this case the go away was more than three months so in relatively long quarantine period between previous use and NEX use so we assigned some addresses from the our our system relatively small but 2.1 / eights actually were dropped into the quarantine pool we\u0027ll probably see them again a little while later and most of that was actually heading towards the cloud providers there so far as I can see the biggest buyers not ISPs cloud providers ISPs don\u0027t seem to be doing much v6 well exponential growth for all u v6 folk you can smile it\u0027s still exponential it hasn\u0027t gone linear yet this is probably for them really good news Jared actually I had a question on the last slide yeah these jared montz "
  },
  {
    "startTime": "01:31:37",
    "text": "he\u0027s from yeah yeah the speaker identified me thanks for Andy though so I\u0027m still Jared even just in case Randy forgot yeah I\u0027m I worked for somebody if he if he\u0027s interested that I\u0027m curious if you\u0027ve extrapolated out of the legacy space when you think that will exhaust or do you have a slide on that I don\u0027t have a slide on it but right now it\u0027s impossible to create a model that says that exhausts if you look at that trend where there was a slight downward curve healed back up again signal-to-noise kind of says we\u0027re going to sit between 45 and 50 for an awfully long time and the stats aren\u0027t draining it now I don\u0027t understand the market signals out there you know I I was sitting there going at 20 bucks it\u0027s a steal wait till it hits a thousand and I\u0027m still waiting right and maybe all these folks away until it hits a thousand I don\u0027t know I don\u0027t know if it\u0027s a price based thing a logistic thing they don\u0027t even know they own it but you know there\u0027s no clear model of when that runs out I\u0027m still Randy but occasionally I try to have manners um two things one is I\u0027m aware of ISPs purchasing address space from customers legacy space from customers and putting it in their back pocket for next year\u0027s customers in non small amounts I\u0027m not talking slash eights but I\u0027m talking 16 the second thing is I just had a little trouble with your unrouted and legacy space doesn\u0027t exist it\u0027s that some of your terminology there seemed a little extreme but did the motion what\u0027s interesting is I think what we\u0027re seeing is a motion from stuff coming from the our IRS to stuff coming from existing holders and if I were in our IR I\u0027d be very happy that I chose a rental model instead of a sale model I\u0027ll go back to this graph which is I think what you\u0027re referring to and this relatively large pool of addresses that appeared in the routing system that weren\u0027t there at the start of the year whose original allocation age was a long time ago and then in some cases predating the our IRS and the observation is just that number is increasing you know the percent each year gets bigger where we\u0027re mining ancient coal right this is not recent energy it\u0027s a long time ago we\u0027re good at this yo name is your person adopts I "
  },
  {
    "startTime": "01:34:41",
    "text": "was wondering if when you present us a similar update in two thousand twenty twenty about the 2019 day if you can also incorporate our PK enveloped announcements in a similar fashion as unadvertised graphs that\u0027s a good point times have gone into looking at bogans you know and routing issues it makes what I thought was a quick pack even longer but it is useful to understand the extent to which we understand exactly what\u0027s in the routing system is being valid versus wow how did that get there yes now specifically interested we through community effort managed to reduce the number of envelops through social media pressure by the polling but there\u0027s this long till 2000 prefixes that don\u0027t seem to move in any direction are those not in use are those up for SIL like who knows good question Yankee - no it\u0027s not - the presenter part of the chair maybe first of all I cannot find the slides on the subs that\u0027s my fault okay about the second one I was surprised to see that the IPG moved back to 2018 and it\u0027s hard for me to do even a partial analysis in under an hour it takes me a lot longer to pull through this data this is an extraordinary large amount of data yeah and then I\u0027ll move on right okay quick question back to the age graph when you see the spike of really old address spaces from your analysis is this the original owners who are now putting this into play or poison of the sell-off of part oh no I assume and you might say I\u0027m out on a plank that\u0027s just broken behind me I assume that the dates recorded in the RIR registry are real and that\u0027s the date they left the shop and when Aaron changed the date on a transfer I write it back again because I want to know the date it first left the shop not the date when they fussed around with it and changed something so I\u0027m going for the allocation date when a previously unheard becomes in play but but in but in particular if someone sold off part of a transferred part of a legacy allocation you would hope that that transfer would be recorded so that would look Heron recalled that with today\u0027s date but I assign the date of the original allocation of the /a whatever it was so I record that as old space because it is old space yes if s RI has the earlier earlier one that\u0027s the one I use that\u0027s why a lot of this stuff is very old you "
  },
  {
    "startTime": "01:37:46",
    "text": "know I\u0027m curious as to the makeup of this unadvertised space there\u0027s a lot of rumors that the u.s. DoD is setting on a whole bunch of space and it\u0027s a little um I said there\u0027s a lot of rumors around about people have squatted on it so I\u0027m just curious what what is yours what are your thoughts and opinions on all of that um the data is open you can look to Jeff how\u0027s Jeff have you done any analysis for these very old allocations to see what the relationship of his path is versus those old allocations when they have been announced no I have not so I haven\u0027t looked at a as path or you know changes in the advertisements I haven\u0027t looked for where the a s and the addresses have been sold versus the addresses only no I haven\u0027t looked at that it\u0027s interesting place to look but yeah yeah so the the two things they think might be interesting out of that the first one is that if the s paths have no relationship no that potentially implies that you know the allocations just walking away and that this is an allowed thing of course the second thing I find interesting out of that based on but some of our security friends tend to say is that those spaces have been sort of prime pools where hijacks are happening and this is where cross correlation with the rpki stuff interesting that\u0027s a different talk I do look at that it\u0027s a different talk I\u0027ll take it at that we\u0027ll press on for all uv6 vote you\u0027re waiting for this this is where I\u0027d got to the exponential growth is still growing there interestingly v6 risks and route views have a very consistent view now the this is a recent routing table it\u0027s only a you know a decade or so old in in reality there are fewer ghosts and one of the issue is to what extent is the v4 table full of prefixes that are actually unadvertised but withdrawals haven\u0027t fully permeated through the entire system and no one\u0027s going to permeate a second gratuitous withdrawal and the fact that this is more concise and tighter the written route view see exactly the same picture is interesting but more than that I don\u0027t know ghost-hunting is extremely difficult the routing indicators what was at fifty two thousand a year in v4 15,000 a year in v6 3400 asns in V 4 mm in v6 so that growth curve is slightly higher than linear so v6 is certainly accelerating and growth faster than B for more specifics one third of the routing table predominantly 48 if you are filtering on 32 or 35 s you wouldn\u0027t see an awful lot of v6 so more specifics at the 48 level and also quite a lot of 64 s where I sit yeah your "
  },
  {
    "startTime": "01:40:50",
    "text": "routers are going to melt one day take up a lot of the routing table and the average routing size is now sitting at around a 32 so if you\u0027re filtering in a 32 again you\u0027re going to miss a lot out of here so average size is getting smaller much the same as before I\u0027ve had to resort to a log scale because v6 is crazy advertised address span it\u0027s linear in a log scale remember maths if it\u0027s linear in a log scale that means in a normal scale it would be exponential but I couldn\u0027t get it on a graph so that means it\u0027s growing and it\u0027s growing at an exponential rate take my word for it the interconnection in v6 is weird and I suspect this is a relatively long baseline it\u0027s the slow decline of the v6 tunnel overlay Network and the increasing use of native connections actually means that this is behaving the way we would like it to behave you\u0027re actually seeing something closer to the underlying topology the fact that it\u0027s still noisy says here still more work to do and part of the reason I think why Huracan is that there is Huracan did the whole bunch of v6 tunnel so again your view will differ depending on where you are the v4 shaped v6 shaping connectivity exactly the same small number of players right in the middle in this case only two AAS numbers from where I sit have enormous numbers of adjacencies you will probably see a different number from where you sit and you might even see different players there it\u0027s all relative so overall it\u0027s growing what to expect well some projections this is V for daily growth rates 140 routes a day 52,000 a year you should prepare within two years for up to you know a million entries or so despite address exhaustion despite everything there\u0027s no reason to suggest that\u0027s going into a logistic curve so if you\u0027re planning on fib size and if you really want to stuff all your routes into a fib if that\u0027s what you want to do you\u0027re going to just have to grow at this kind of size there\u0027s the only other way of doing it is don\u0027t put all your routes in the FIB yeah small comments on the predictions as operators we agreed that\u0027s either people need to deploy RPK original edition or we will be aggregate them 2/24 to protect ourselves so there may be some swinging later on let\u0027s hope that doesn\u0027t inspire this could get really fascinating because these are starting to get big numbers even for big iron out there the v6 daily growth rates it\u0027s not linear it\u0027s exponential so those predictions get to be scary because they\u0027re 128 bits long so at Alinea you kind of go yawn but v6 is not growing in a linear rate it\u0027s growing exponentially we can expect and within five years quarter of a million 128 bit entries in the FIB so it starts to get significant so in absolute terms v6 table is rocketing along it\u0027ll be about the same footprint in feed memory in about five years time the "
  },
  {
    "startTime": "01:43:50",
    "text": "same footprint now as long as you\u0027re prepared to do this the internet will keep on running BGP I\u0027m like there\u0027s nothing intrinsically wrong with BGP scaling profitable properties it\u0027s going just fine but if you don\u0027t want that size you\u0027ve got to think long and hard about how you want around the other part of this is the performance of BGP which is the level of updates this is where odd things happen in BGP that none of us can understand this is the number of prefixes in v4 this is the number of updates in v4 this is the number of withdrawals in v4 the number of withdrawals per day is constant riddle me that one with more people more Brownian motion more noise more random actors more there\u0027s more that more withdrawals no no withdrawals are a different thing I don\u0027t understand roughly what is that number 10,000 a year 10,000 per day constantly no matter where I look I see a very similar number but the one thing about this is that it does have a relationship to convergence performance Randy Randy um those two that many of the announcements create implicit withdrawals so the explicit withdrawals are not necessary many of the announcements Korea implicit withdrawals yes but why it\u0027s ten thousand why does it grow even the updates grow slowly whereas withdrawals the number of explicit withdrawals kind of because those updates have the implicit withdraws okay okay maybe it only ever takes an average of two updates for a prefix to converge to its new state whether its withdrawal or a new path on average two updates on average it\u0027s been the same since 2000 or even earlier it converges at the same speed and because of MRI intervals that number actually translates to just around 50 seconds so on average around 50 seconds whatever state it was going to now stays in that state for at least a minute I think was the way I\u0027d timed that so it remains stable after that point more than two MRI intervals and that\u0027s been the same for ever this is viewed no this is viewed from 1:30 1:07 to I had a different graph that did all of route views all of the time but it does take a lot of compute it\u0027s the same property it\u0027s exactly the same property because it\u0027s actually based on average AAS path length because what you\u0027re looking at is there\u0027s a disturbance in "
  },
  {
    "startTime": "01:46:50",
    "text": "the force it has to propagate a longer convergence time actually means a longer path to propagate through because you always like connecting into the core updates happen really quickly so into the core back out again you\u0027re done so this is why it\u0027s relatively constant as far as I can see Jeff has so I think your observations are correct at the little cautious about tying it in memory I type the timers most people don\u0027t necessarily really have those for real but queuing artifacts will look like that well if MRI has been as long slow decline whatever has replaced it has a very similar observed behavior or property I think what you\u0027re going to see especially with your prior slide with amount of HP update noise just simply the link that something is going to be in a queue tends to cause that to happen so when you have a fair amount of stuff churning around you get a lot of convergence no niceties as a side-effect of that where you actually get the pathological stuff is when you have short queues okay is where it\u0027s really good that is an amazing thing like grown by an order of 10 but convergence is much the same so if you\u0027re worried about v4 update performance in BGP don\u0027t bother and whether it\u0027s M right or whether it\u0027s Q just suggests the underlying observation is the system is actually quite stable from where I sit v6 is entirely not that and I\u0027m always been wondering to what extent it\u0027s me and to what extent others see this when I look further afield I still see large amounts of operational instability in routing updates bizarre the number withdrawals are still low but it\u0027s more variable the number of updates is unbounded it just seems at some times some routers is going to catatonic update announcement systems a little earlier and never stop convergence performance as a result is all over the shop there\u0027s no true average that maintains from day to day to the amount of updates that converge and nor is there any real consistency in time now whether this is the impact of tunnels close in to the call or some other impact there is something really different in BGP carrying v6 and BGP carrying v4 and the suspicion is it\u0027s something to do with topology and the suspicion is Jeff\u0027s nods if shaking his head to say no or something to do with overlays and tunnels and tunnel behavior and if there\u0027s another explanation someone should enlighten me because I\u0027m not enlightened Jeff has made a joke a partial joke and made the Doug just too much cargo in the system and I wish I was joking absolutely much Kroger in the system there\u0027s there\u0027s unfortunately some interesting artifacts some of the older trees routing software that was out about and it causes that a huge amount of noise in the system when I was "
  },
  {
    "startTime": "01:49:50",
    "text": "doing work some number of years ago for you know the four byte a s transition and stuff we found that there is significant meta stable routing updates going around and it was showing up because it was incorrect no for by transition code and a bit of chasing down since they ended up being a customer I had a little bit of visibility and at the time was much old cargo boxes so there\u0027s just literally I think a lot of this noise is from the fact that a lot of v6 is being done know via software routing and a lot of it\u0027s being done across tunnels that you\u0027re saying and a lot of the things that are plugging those things together at the moment hopefully decreasing is a bunch of older routing stacks that need work well the good news is but it\u0027s taking a long time to get there yes and well I think that partially that what you\u0027re seeing here is also your observation about these being over tunnels as we get them more and more native v6 connectivity and the tunnels go away people are shipping their routes across actual routers which tend to actually be a little more stable versus over the things that are problematic okay thank you Randy motion um many years ago some Australian nut said that v6 performance absolutely sucks due to tunnels and crazy peering agreements and so on and so forth but that as more money started real resting on v6 this will get fixed for to get fixed those people playing in the game would benefit from even more analysis so we know what to fix I think you know finally people are coming to the conclusion that tunnels are is evil as gnats but I think the other Jeff\u0027s is saying that weak software it\u0027s a problem it\u0027s worth investigating this is not acceptable as a production network we shouldn\u0027t be worth in this and we should understand why this is still the case today yeah yeah Joel yeagley um yeah I mean that certainly is historically the case like I can remember circa 1998 the reason our BGP implementation was shittier was literally because it was a different platform because the soup zero cat 65 hundredths that we had running the core did forward v6 at that point and so like literally the Cisco 7000 that was sitting in the corner was doing the v6 overlay Network so it\u0027s literally shittier I have observed in the in the process of "
  },
  {
    "startTime": "01:52:53",
    "text": "building adjacencies in exchange points over a long period of time that people apply the principles of benign neglect to their deployment of v6 peering if you take down a v4 peer they noticed pretty quickly weirdly because the traffic that they were saving a lot of money on stopped flowing if you take down the v6 one most of them don\u0027t notice for some time dual stack happy.i Bibles yeah it was missing your v6 peering with Google tends to get a lot of notice because like the youtubes flow over it and so this that that property of benign neglect has been like slowly getting pushed out of that but it\u0027s still readily apparent in a number of regions that I operated where um that that the management of those resources and the the frequency with which they\u0027re up is demonstrably lower than the v4 ones even even in the case where they\u0027re running on the same hardware well I\u0027m happy to look deeper and others who are interested in this should look deeper as well certainly the data is available yeah jhope again the blue graph on the right side it seems to stabilize the left side yeah is that around February 2018 yes that\u0027s when a number of security updates were dispatched for quaggan based you might well have observation of correlation and reason I done causality I don\u0027t know again I am curious if you could correlate those frequencies of updates and instability with actual traffic and actual users behind it had this conversation and I assume that you were going to offer me some resources to help me do the analysis and we\u0027re still in that conversation because I suspect it\u0027s still like the more users you have behind the previous and most abilities because the Eric I both said people do not like this stop of their own own complaints it\u0027s worth more investigation Jim no matter how we cut it we need to understand this better that\u0027s true um do you need to run a successive protocol to BGP I don\u0027t understand why you would need to in terms of is BGP failing no it\u0027s not is it scaling well it seems to be there\u0027s no great sort of cracks appearing and no great huge holes in performance or even size as long as you\u0027ve got the money you can create the hardware it will keep on working as a protocol there might be other reasons to go to a different protocol but you know BGP itself is still scaling fib size line speeds equipment cost sets up to you guys nothing\u0027s going to get cheaper I make the stuff is still going to grow and as v6 grows it\u0027s going to place more pressure on that what you put in your "
  },
  {
    "startTime": "01:55:54",
    "text": "cache what you are floats up to you but the routing space itself is going to keep on growing as far as I can see in both protocols so you know if you\u0027re running this stuff understand what you\u0027re putting in your high-speed fib cards and manage that very carefully you can\u0027t ignore it it\u0027s going to get more and more critical and another thing is gets ignored is v4 v6 partitioning in your fehb that allocation which was normally static in a lot of router con things Oh 10% v6 90% v4 will not work even today you need to look at that balance carefully and quite frankly you don\u0027t need to carry every router all the time you can\u0027t just default this stuff outwards end of rant Randi again um one place I\u0027d quibble is the cost of the hardware falls radically and and it\u0027s held up artificially by market forces etc etc but it really my entire you know I\u0027ve been in this field for 50 something years those damn hardware people are just killer they drive the cost down and down scaling up and up and up bet on them don\u0027t bet on the software Meredith oh you can bet on the software bugs good and good margin net so I\u0027m not that worried about router scaling I\u0027m worried about the market forces driving what I pay to purchase it but maybe some people now they\u0027re coming to questions that\u0027s me thanks a lot thank you we\u0027re essentially out of time see you in three months what and we\u0027re done "
  }
]