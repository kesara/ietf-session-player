[
  {
    "startTime": "00:00:10",
    "text": "hello everyone um this is cindy and i working uh thanks for coming um looks like people are still trickling in but we'll go ahead and get started um welcome back to ietf um if you are not here for the cdni working group you might be in the wrong room otherwise we will move ahead this is the note well everyone should be familiar with it uh all of your contributions and participations are governed by the rules set full in these bcps if you haven't read them you should um but otherwise you should be aware that uh of the rules of the idea and you agree to them by being here sanjay and i are here to conduct um our cd9 meeting for iatf112 um if we could have a volunteer to just monitor the jabber room that would be awesome if someone wants to volunteer for that i will be taking minutes if someone else also wants to take minutes that would be great as well uh blue sheets are taken care of for us by meat echo so um don't you just step up and say yes they'll just take a peek in jabber otherwise sanji will have to do i don't think there's going to be a lot going on in in the jabra room but someone should just take a peek ah thanks chris awesome uh all right then we will move on um these are the existing milestones for the working group um we're going to be talking about uri signing which is in iusb is with the isb"
  },
  {
    "startTime": "00:02:00",
    "text": "um and still has an update for us we have two existing milestone items ups delegation which frederick's going to give an update on um i know there's been some discussion on the list i'm happy to see that you know we're making some progress there and then here is going to talk about the triggers interface um here is our agenda um we got two hours this time because we did run out of time last time uh so hopefully everyone will have plenty of opportunity to ask questions and discuss but we do have a fact agenda so if anyone has any changes they'd like to make or voice please do so now otherwise we will move on no okay then the first thing on the agenda is we had a call for adoption on two draft i've been out after the last night yeah um we just wanted to go over that um here they are the first one is the stevian eye triggers extension draft just as a recap there was originally a draft done by ori that we adopted back in ipf 103 we then made some decisions earlier this year to change it to be a full update of the existing graph just because it seems like it was easier to understand rather than as just a separate set of extensions um sorry um and so there were no objections on the list there were some people who expressed you know their their approval of this and so we are we are ready to move forward with adoption um we're just giving folks the last uh chance here to object otherwise we will move forward with the wrath uh i don't see anyone stepping up to object"
  },
  {
    "startTime": "00:04:01",
    "text": "so i i think this one's pretty straightforward um the existing milestone is set for december of this year uh i don't know if that's still a reasonable deadline here if you think that we should extend that and i think maybe updating that milestone to you know what is a reasonable time frame to get that finished up do you think we can get it done by ief 113 or or do you think it's pretty close as is i think that we should be able to get done with the marriage but by the itf 113 but i also suggest further work on this on this draft so this might take longer okay i'd like to set a deadline just because deadlines are good for us it's good for us to have deadlines um i i don't want us to rush to get it in by obviously next month but i think you know if we feel good about trying to shoot for march of 2022 i would be good all right and then the other draft was the footprint extension again this is just additions to the metadata footprint registry um by design we built a registry so that we could add stuff as we move forward and found things that we needed and i think this is a great example of that um the changes are fairly straightforward there were no objections on the list um and there were you know there was support board on the list um if anyone has any objections to it this is your last chance before we call the adoption um i'd like to go ahead and just add a milestone for this and i think this one's pretty straightforward i think we should be able to get it done by the next ietf do you see any issues with"
  },
  {
    "startTime": "00:06:01",
    "text": "that setting a deadline of march for this as well for let's call or um for for trying to finish it up yeah and trying to get to a last call by my next idea okay um i don't think there's a lot more to add to this one this one's pretty straightforward um yeah i think so we should be able to have that you know wrapped up by end of march i mean by by the idea of 113. or any less discussion topics we can talk about in march and then you know push that out if we need to but i encourage everyone to go please read these drafts add any comments you have to the list and we will try and wrap these up quickly all right so then i am going to load up near slides so that he can go ahead and talk about these two drafts and here please take it away thank you thank you so uh as they came in already explained in the preview in itf 101 uh i will propose to have assist for adoption and the first one is is heading can you please yeah okay so i'll cover both of them and give a quick recap and and open it for questions so the first uh draft is extending the foot in the copy footprint capabilities capability interfaced with additional footprint types um next place so a quick recap of of the current state currently rfc 806 defines the footwind types which can be ipv4 ipv6 asn and country"
  },
  {
    "startTime": "00:08:03",
    "text": "code counter code specifically is an iso 3166 code to alphanumeric to alphanumeric characters and it's further to discard the footwork object as you can see in the example a footwork object is built of a footman type and between values okay so in this example the footing type is ipv4 and the values uh a client matching this footprint object as one of the is belongs to one of the subnets in the list next please uh ifc808 which is the foot foot the fdi into the sci put in capability interface uses this footprint objects and in order to specify which capability the availability of capabilities for specific clients so in this example the client they capability specified capability is available for clients within the asn 64496 that resides in the u.s and so the first footprint type we would like to add is a practically a next step for the country code taking it one level down geographically level uh so geographically wise uh so if israel 316 if the country code relates to the united states or to country the subdivision codes relates to states in the u.s or province in canada and so each the"
  },
  {
    "startTime": "00:10:00",
    "text": "iso 31662 defines a three alpha a alpha alpha characters subdivision codes and we would like to use them in order to get a better granularity of the food with objects in the below example you can see a footed object and matching the state of new york within the u.s as well as the state of new jersey in the u.s okay any questions so far okay the next footman type we would like to add is that the future footwind union and i discussed it in the previous idea the current the current semantic of the a photon capability api says that the list of footprint objects is a as a narrowing semantics and we would like to have some additive semantics for exam if we want in the a part of the footprint definition marked in red we define we try to define a clients residing in the us or residing in a canada province of nova scotia okay so we it allows us to take two different footprint types and merging them into a large footprint object in addition additive way not knowing way um i think this is all we added in the in the uh in this draft there was only one change"
  },
  {
    "startTime": "00:12:00",
    "text": "since the previous itf summit meeting which changes the footage name from 3116 to code to subdivision code following a single frozen alfonso a remark um any questions concern or something like that regardless those suggestions okay so let's move on a the second draft was dealing with the the cdni control interface uh second edition um so as a reminder the rfc8007 defines the cdn control interface which allows an obsidian to manipulate manage the content and metadata held by the dance obsidian for example preposition invalidation or purge and as kevin already mentioned oe and sanjay worked on a draft um that that extends this rfc and the current and currently we want the then um and we decided to that it would make sense to merge uh all the the extension into the original rfc and create a new rfc with a new korean rfc and which will allow us to also obsolete rfco 807 and there is in this in currently in the draft there is no new content beyond the content already approved by this walking group so what what is there what we like would like to add and"
  },
  {
    "startTime": "00:14:00",
    "text": "explicit and the first added functionality is the ability to extend it to this extent stability extensibility okay currently the rfc807 the trigger of the trigger object it has a closed set of properties and there is no way to define a fine control on that over the trigger execution such as a a time for the trigger to be executed or a specific location for a through execution and the draft suggests the new generic extensions mechanism for this trigger for that video and the trigger should the trigger object the version 2 of the trigger object will have a can have a list of generic extensions similar to the generic metadata from the rfc806 and for example for an example for such a generic extension is the time extensions extension allowing the absence at the end to indicate that a concentric position should happen at 3 a.m buenos aires okay i'm up i'm not watching it so if there any if there is any question regarding it's good please stop me whenever you want okay let's proceed and the second other functionality is their propagation and in the case that they we have multiple levels of of cdn so that the trigger from the opposite end propagates through the down sedan to a further down since then"
  },
  {
    "startTime": "00:16:00",
    "text": "and we the asked the situation that the failure happens uh i think the trigger the failure in the trigger execution happens uh within the further downs and down sedan and the um draft allows a mechanism so there's a mechanism that allows the dams incident to indicate that the failure happened within the funded the food adults would see the end if it if it desire to it just had add the cdn id to the a l object okay next and the last and actually most simple uh addition to the rfc to the draft is the uh additional content selection methods okay so the original rfc has a set of properties that allows the selection of metadata or content urls or patterns and the draft suggests to the selection of content contact via regixes as well as playlists for example hls or dash okay and in order to support that we need to extend both the trigger object and so we have a version two of it and the object um okay so that that's all for the adoption stage so it would be a good time to point to stop okay and have a questions or any so all of that is what was already in there that was available in the existing room"
  },
  {
    "startTime": "00:18:00",
    "text": "yeah everything there was everything i i mentioned so far it was already a discussed in the original duff that was adapted adopted by this working group and we just want to [Music] we just merge it into the original rfc okay yeah so folks should go and if you haven't read it read it if you have comments obviously comment i think near though you wanted to propose some additional stuff to add to it um so to following the adoption of the rfc uh i would like to make an india i would like to suggest an additional change as i already said when a then as you can as you can see in the left side of the slide a trigger object has a list of a closed list of properties allowing the content or metadata selection and we now add it to a new content selection method which is the content reg access and coded playlists and and in order to do that we must define we define the trigger object and create a version two of it or as well as the error object and it i i think that it would be better to go to a a generic mechanism that a new method for selection for selecting a new content for for prepositioning for example would be done when we would like to define a new method for content selection we just do it by registration as we do now with the footprint object that we just registered so i would like to suggest that the"
  },
  {
    "startTime": "00:20:01",
    "text": "trigger v2 object will have a actually at least an objects list of generic trigger objects that can be any one of those uh of this of this option metadata urls or content urls etc and we will maintain a registry for this content selection methods we had it was um there was an email on the list about this if i recall and you and i had a discussion about this um i think it makes sense to me but as an author of the metadata draft and uh data makes sense and the use of a generic trigger x trigger object makes sense to me i think it's cleaner than having to keep enumerating things as properties inside the trigger object so i i like the idea personally i don't know if anyone else wants to step up to the mic and put out an opinion i think it does make for a cleaner object and um do we have a draft with proposed changes for that out or not yet i didn't want to make too noise with that before we have the adoption okay um i think it's an interesting idea i think i like the idea if anyone else has thoughts on it um you know please speak up or we can i think move forward with the adoption piece so go ahead and push the the updated drafts as working group drafts and then we can push out an additional update"
  },
  {
    "startTime": "00:22:02",
    "text": "with proposals and start discussing this additional change on top of that is that what you were thinking laura [Music] okay i think that's a good plan uh i'll stop here again and pause to see if anybody else wants to voice an opinion i think that means everyone agrees okay i didn't i didn't hear any objections either there are no objections um i think it's easier it's easier to discuss once we have some text in front of us so i'm happy to let's let's put out the the working group draft and then propose an update and and discuss it on the list oh i see there's a comment okay chris agrees excellent all right thanks nir um anything else are you good i'm good thank you all right um then next up we have phil phil's going to give us his update on uri signing how's it going everybody um all right so a quick rehash of some things that happened that we went over uh in the summer ietf session it was in last call in february i did post a new revision right before the the draft dead period kevin already reviewed it gave some feedback as did chris appreciate that i've already merged those changes except for one we'll talk about that a little bit so"
  },
  {
    "startTime": "00:24:01",
    "text": "back to the the feedback that we got in the last call in february i addressed all the easy things but there were a couple things that i don't think were so easy so um client ip there were usefulness and privacy concerns honestly i think they were mostly usefulness um like how does this operate when you're dealing with dual stacked how does this operate when you're dealing with mptcp how does this operate when you're dealing with switching from wi-fi to mobile all the all the cases that that were aware of it wasn't like any shockers were in there but um it was just they felt that either it wasn't um talked about enough or maybe its usefulness is so small that we should just remove it um shared keys that one was a really big one we didn't i added some more text to say hey this is a really bad idea you really shouldn't do this but if you want to here's how you do it and it's supported there were some things that i changed from uh should to must because i had no good reason why it it shouldn't be must um and then there's there's always been an agging question of more advice for designated experts i i've put some stuff in there but i really don't know what more to put and if anybody has good examples of this i'd love to have that i am going to email all the ads especially the ones that talked about that and ask for specific examples because i'm i'm kind of out of ideas for that one so i have a couple questions that i want to put forth um and i i was thinking about this as as the meeting was starting up that i probably should have done this on the mailing list first too but i will follow up with"
  },
  {
    "startTime": "00:26:01",
    "text": "that um should we remove client ip um the reason why it was put in there was because um it was traditionally in there in the proprietary implementations um so people wanted it people used it people like to be able to say uh hey here let me give you a quick preview link what's your ip and you know people would know that it needed to be their cable modem ip and if they changed anything it would fail that sort of thing like that was the use case it wasn't really meant to be a robust option that would be used for the general case um but maybe it's just not useful and we should remove it rather than trying to write a bunch of text around why people shouldn't use it um second question is removing shared keys uh uh should do we want to stop there's people in the queue i can answer if people want to comment about that first question yeah let's let's go ahead i think why don't you go ahead andrew to your point about the client ips like yeah there are a lot of caveats a lot of risks i mean i've been personally bitten between clients switching on dual stack between v4 v6 with certain vendors but unless there's a large overhead in keeping it in i feel like keeping it in is probably the best bet it's going to add flexibility and use cases where like you mentioned there could be very niche uri signing that folks are going to want to do maybe even internally within organizations you know so on and so forth um i it unless there's a risk in keeping it i think keeping it is probably the better way to go with the understanding that you mentioned there's there's caveats and it's i think just a general disclaimer that you know there are caveats with how this works and it's up to you to accept all those caveats if you implement this okay that's that's very helpful uh i so um real quick before kevin speaks um"
  },
  {
    "startTime": "00:28:02",
    "text": "i i so i'm going to send these to the mailing list it would be awesome uh andrew if you could kind of reiterate that on the mailing list because i want to use that to point the the ads to about their concerns and i'll i'll make sure that i uh make the the text robust enough if it's not already i already added some stuff to it but i might want to just um polish it up a little bit and really kind of drive home that point that that these are like you said niche use cases that it's there to support yeah i'll throw in my two cents as an individual i think that you know we had always talked about the internal use private network kind of i own i'm in isp i own it i want to use an ip address for a specific purpose and that's the only place it's really good for i mean uri signing has a lot of questionable security things in there right we all know that we could certainly beef up the disclaimers about it um i saw that you added one i mean because we can make it even more you know obvious to folks how bad it is but i think if there are people who need it and that was the original rationale right there is a vendor who wanted this and so uh i don't necessarily see a reason to pull it unless there's a lot of pushback i don't know how bad the pushback was but um we could probably put bigger folder things in there about why you shouldn't use it or and also where the one or two good use cases are for using this um you know which helps the the reader understand why we're keeping it in okay chris chris i i won't uh rehash all of that i agree with the previous commenters there's there's lots of desire for this uh the one thing i wanted to add was"
  },
  {
    "startTime": "00:30:00",
    "text": "there are um there are some situations where you know the set of ip addresses that it could be coming in from and the client ip field could actually be more useful if we made it into an array i hesitate to suggest enhancements at this point but it is a fairly simple one yeah that so i was actually thinking about that when i was going through going through this again um like we we could do a lot of things we could probably add like a v4 and a v6 client ip and do some other stuff um i also agree with doing changes this late in the game um unless people really felt like hey i have a use case for that i would kind of want to say maybe we should leave that to somebody who has a use case for it to write an extension in the future but if if if there's a use case for it like where you're like hey i would really like to do this specific thing and i and i want to do it it's not theoretical i i would consider changing it but i i again emphasize i really don't want to if we don't have to there there is actually a real use case around wanting to put in both an ipv4 and an ipv6 um and there are some cases where both addresses will be known at the time the u the token is generated um and and that can be reasonably reliably produced um uh likewise there are a couple of use cases around a token that could be used by a small collection of systems and right now you could put in a cider which is kind of what we what we picture for that i i just think the simplest solution is"
  },
  {
    "startTime": "00:32:02",
    "text": "to not overthink it and to turn it into an array but again it can go either way i think there's a marginal feature to start with okay i think that we have been in we have been an ie tf last call for a very long time um if we were going to make those types of changes this is something we would have to go and pull it and do a new round on um i'm all for it if somebody really needs this right if people are going to say this draft is not useful to me because i need the feature then we should be in the future agreed okay glenn had a comment as well in the chat disclaimer and then my question about common access token i'm not familiar with that project okay so yeah comment on the client ip should stay with with a big warning that's fair um coordinate your eyes signing specification common access token project i'm i'm not familiar with that so de facto no but we could talk about it i don't know i don't i don't under i don't really have a sense of what that looks like and what that might mean for delaying uri signing more so i don't want to promise anything but i was not aware of that before yeah i don't know if it's if it's close if it's close it's something we could think about if it's not close then um glenn i know you had uh aws signature"
  },
  {
    "startTime": "00:34:01",
    "text": "four in your draft and i and i've been thinking about that as well it's not close enough for us to you know merge or do anything with i don't think but [Music] okay okay you have another question yes so so very similar to removing client ip is removing shared keys and i kind of have the same feelings on it that it was created to do niche tests and stuff like that i did add some text about hey really don't do this this is not a good architecture there's a lot of problems with it especially when you're dealing with tiered cdns and stuff but i kind of feel like we should keep it but maybe it's just something where you know there there was a line drawn in the sand at the ietf saying hey we're not going to do things like this anymore at all ever no matter how valuable or useful they are because they are bad so before i went and argued it i wanted to again ask should we remove the shared key support so kevin i remember why we put in the shared keys because at the time we started writing this all of the implementation with um public cdns had shared keys and so right it's a thing that we wanted to support because that's what everybody was doing um i don't know if that's still the case i imagine it probably is still supported and there are probably people who would like to have it because you know their legacy systems use it but it's it's a yeah it's not a good idea we could put you know big disclaimer text again but put even bigger disclaimers text saying that it's even a more marginal value than the client ip please really don't use this unless you have no other option again the yeah yeah um but there they're going to be people"
  },
  {
    "startTime": "00:36:00",
    "text": "who argue i have my own i own the network and this is easy and we already do it today so why should i have to change it then i can't use respect i don't want to leave those people out but maybe we should force them before i i'm okay with leaving it but there's an argument there yeah chris uh i think that if you own the network and you don't have to interoperate with anybody you don't need a standard to tell you how to interoperate with people fair point so so is what you're saying that we could remove explicit shared key support and then just say something like if an operator internally wanted to to do that themselves that would be on them but we're not giving examples or ways to do it in this document that kind of thing like something in security concerns i mean you could like you you could reasonably just remove it um entirely uh we don't have a police force we can't stop people from using shared keys it's pretty obvious how you would use shared keys um if you wanted to yeah that's that's a good point i think maybe we could we could do something like that where we just remove it i kind of i kind of still want to have like a sentence or two about it maybe in the security concerns like historically people have used shared keys this draft does not address using shared keys please consider not using shared keys if you if you were thinking about it type of thing because i think i think it still needs to be addressed but then we just remove all the examples and discussion about it yeah i i think that that's probably wise okay i agree we should we should state"
  },
  {
    "startTime": "00:38:00",
    "text": "explicitly that we're not doing it right yeah because it was in previous versions uh okay and the last question um this was only brought up by 180 but he he brought it up multiple times and he said he really doesn't understand how this draft works if uh the cdni uri container is not mandatory um and now my understanding of why we chose to do that is essentially um and this is gonna probably be even more scary than share keys is is if we wanted to give somebody a skeleton key like here this token works for anything or works for anything for a certain amount of time like it doesn't matter what the uri is but he makes a good point like that that seems he made it in a way of um i don't understand how this would even work but when i was thinking about the argument to it and all the discussion on client ip and shared keys i was like yeah skeleton key is probably kind of a sketchy argument but is that something that people need does somebody need to just say look i don't want to keep getting a new updated token for my internal use case much like client ip let's just leave it in there but put something like you really really should put something in here if you're giving it up to anybody outside of whatever because you know me or maybe something like you probably want to put client ip in here if you're not using uri container i i really don't want to get into like a matrix of so you should have this if you don't this or this you know that kind of combination of claims in there but maybe we should i i kind of feel maybe we should keep it with some more text saying if you're not using this you're leaving yourself open so maybe you should have client id or a very short expiry time or something else to compensate i'm having some light issues is anybody also hearing some of the the noise on the there's some static um"
  },
  {
    "startTime": "00:40:02",
    "text": "i don't know if it's coming from from their film or i can switch headphones i've been i've been you came prepared can you hear us does that make it does that make it any better yes a lot better oh sorry apologize to everybody then that's mine so uh chris do you want how to comment yeah so there is a use case with no cd and i you um if your cdn is accepting keys from um multiple uh issuers you are likely to pro you are likely to in fact you are going to have to have a limitation where you only it's where you have an association between what uh uh what systems you accept that key for um or that key if you're willing to accept that an issuer can authorize a given uri in general then you have to be willing to allow an issuer to authorize any uri so there's no reason to they can just put dot splat in the cd and i you see right there's no reason to force them to put dot splat in there um if they can just eliminate it um it's just a waste of a regex um compilation and execution okay um yeah i i'm i'm fine with that i so"
  },
  {
    "startTime": "00:42:00",
    "text": "like i said before i'm gonna i'm gonna send these all out in question as questions to the male english too and if you can reply because it'll be much easier to to send them links to a mailing list um then links to a video somewhere and again apologize for not doing that before this call but yeah okay and if there's no more comments i'll move on um so the only the thing i don't have on this um because it just happened earlier this morning is chris updated um sent a pr with his text for the normative reference on the path parameters and form parameters i didn't merge it already just because i it was very quick and i wanted to kind of give people a chance to to think about it and comment on it and and you know see if there's any anything else people any other feelings people have on it i guess my my first thought on is it seems okay so um also thanks to kevin and chris for the other um review stuff i i've already merged that into the into the repo so whenever we're after i'm done talking to the ads about their concerns um and make the updates for the shared keys i'll do another draft so does anybody have any um comments on chris's pr had a chance to look at it i think he submitted it like 1am my time so i i'm doubting it but figured i'd throw it out there before i finish up i haven't read it yet but i will okay all right so uh yeah i guess i'm all done then i'll make sure to send those emails out to the mailing list so people can reply and thank you very much i think um the other thing is we have i don't know um if francesca is there we we put um review of"
  },
  {
    "startTime": "00:44:02",
    "text": "chat if we have to make these updates um is that something that we need to do ahead of that for francesca did you have a comment yes so i as soon as you posted the update i added it to the next telescope which is uh december the 2nd so that's why i put it up directly um because otherwise he would get filled up and he would have to wait [Applause] six weeks instead of four weeks um so if if the working group and you feel can can make any any um additional modification let's say at the latest like three four days before the telechat um but yeah as soon as possible but but let's say one week before the telechat that that would that would give the 80's enough time to to review like the stable version let's say and yeah it might it might not be needed if they remove their uh their discuss before the telechat we don't actually need to go through but i just wanted to put it up there so that the ads have a deadline and um yeah like by that by that time they will have to have reviewed the this version okay so we have some open questions phil's going to put them out on the list everybody's going to respond and we'll have that discussion and if we can close those out over the next couple of weeks then we can we can leave it on the on the agenda for the next for the december telechat um if it starts dragging out then you may need to push it out yeah and that's fine as well just let me know and i will move it to the um to to another one i don't know two weeks later or or more time if needed but"
  },
  {
    "startTime": "00:46:02",
    "text": "ideally it would be good to have one week or two to know if if we need to move it one week before before the telechat so november 25th or something like that because that's when the agenda is set okay so we have two weeks to figure this out bill okay i i'm my goal is to do work on this uh say you said the 25th which is thanksgiving i have that week off and so i was planning to do work on it during that week so hopefully um i think that aligns well with what my intention was okay great and again if if not just let me know and i can move it no like no no no big pressure just um yeah just try to make it this the most optimized way but thank you for the update phil thanks bye thanks phil all right so next we have up uh frederick to talk about the https delegation all right fred you're on can you hear me yes okay good um so uh today i will give you a quick update and draft we are on version 7 now next slide please so in the mailing list we had quite many comments on the draft first we split the document into two drafts as required by the chair in the last last cd9 working group session"
  },
  {
    "startTime": "00:48:01",
    "text": "so the first draft we are talking today is strictly about the acme star delegation methods and the other part in the other document deals with subsets only for that we will have the help of guillaume bishop that is not here today i think um i see him he's there okay okay so yum feel free to uh to comment if necessary uh so then we fixed a bunch of things in the draft we changed the generic metadata name because we used wrong wrong names wrong names in the metadata in the draft so we correct this we collected this we removed the periodicity because actually we do not need this anymore [Music] yeah we have also registered a new fci pedal type actually we need to do that once the draft is being rfc and we added a privacy section which needs to be guess completed a bit next slide please so we have still a bunch of things to do [Music] especially on the security and privacy parts so kevin you"
  },
  {
    "startTime": "00:50:00",
    "text": "ask quite many uh questions and we still maybe have to add some hints about about those so i don't know yet uh yeah maybe we we will discuss about that later um anyway so regarding security and privacy we need to be to study a bit what how to answer that [Music] we also need to fix some reference also the need to refer to the rfc 9115 instead of rfc8739 we need to remove also the star delegate some star delegation method properties as it is mentioned today we have the acme star acne server and credentials location rai and csr template so this might not be necessary to carry between cdns uh where however maybe the csr template could be in some cases [Music] needed so this will need to be discussed still and finally we need also to sync maybe with sva working group that is working on on the cdna interfaces uh notably if we need to if we need to add some more uh properties in the in the cdni in the metadata interface so that's still to be to be done"
  },
  {
    "startTime": "00:52:02",
    "text": "for the next version i guess and i think it's next like this yeah so uh yeah if we have done all this maybe we can ask for a last call but i think it's done from my side so i i sent some comments to the list this weekend i read the updated draft i think there's still some more work we need to do um we do need to keep up the privacy and security sections and i still have a couple of questions about the metadata structure and whether we need the fci um type but um that's um you can take a look at the comments that i posted okay and then i think we're a little premature for a working group last call um we can look at the next version of the draft um hopefully we can iterate over it and then you know between now and next march if we can get to a a good place uh we could probably think about that before the next iepf but um but we'll see where we can get through that okay okay good um as an individual um i will i i concur with kevin on that we need to wait on the working group last call on this draft um because of you know the changes that need to be made specifically um the rfc 9115 does a lot of work uh that aligns with what this draft is trying to do so i think there needs to be a pretty good alignment between how the interface how the downstream cdn will establish"
  },
  {
    "startTime": "00:54:02",
    "text": "um requests with the upstream cdn to exchange information for um identifying itself as the acme client and and talking to the upstream cdn as the acme server so i think all of that interaction has to be really captured well uh so i think once we have that in the next draft it should require it should be it should have a you know good review and and based on that we should decide you know how the draft moves forward thanks anyone else have any comment on on the acp delegation uh okay um i'll look forward to seeing the updated draft it sounds like the the rfc 9v115 changes could be significant so um once that update is out everyone should go and reread and make sure it looks good to everyone and then we can take that one to the list all right thanks fred up next is when to talk about metadata this is a big one so glenn take it away is"
  },
  {
    "startTime": "00:56:06",
    "text": "glenn are you muted no it looks like he's having some bandwidth issues there may be technical difficulties glenn are you there should we go to andrew first i may if no one else objects yeah um why don't we do that um andrew let me pull andrew's slides and we'll let him go first all right andrew are you ready sorry yes uh hi um my name's andrew ryan i'm here today to discuss um some potential extensions mainly to uh the fci to allow signaling of capacity capabilities and limits next slide please so this is specifically related to the draft that is mentioned here the draft ryan's uni capacity insights extensions 0-1 next slide please and so this we discussed this topic briefly at the last ietf 111 um but the the general highlights of what we're trying to"
  },
  {
    "startTime": "00:58:00",
    "text": "accomplish here and the reason we're going to be who we've proposed to this draft is we wanted to be able to accommodate the ability to allow us cdns upstream cdn's downstream gyms or content providers the ability to make informed decisions about how much traffic should be delegated we wanted to provide a vehicle um using uh cdni and particularly the fci interface uh to to handle this signaling and the goal was to be able to make s to signal capacity limits that are specific to the delegation relationship between an upstream and downstream make sure that the signaling that's being provided is unambiguous and very clearly and mutually understood and that there's the ability for uh a communication direction between uh the upstream and the downstream uh and just a quick call out that the actual transport mechanism itself is somewhat out of scope of this we're merely just talking about and making the proposal about the vehicle in which the data is going to be encapsulated with i know there's been some discussion about using alto um the specific use case that we hear uh in the street and the streaming video alliance for the open caching initiative was we there there's a another api interface that we'd be leveraging but the data model is going to rely heavily upon cdni next slide please so how are we going to accomplish everything we've laid out there um what we've kind of come up with is that we've during discussion we wanted to uh leverage fci as much as possible we felt this was very appropriate uh to act and um be you uh be conducted as a capability uh how much capacity a downstream is advertising to an upstream so as such you know we proposed okay let's come up with a a payload that um a"
  },
  {
    "startTime": "01:00:01",
    "text": "downstream could use to advertise capacity limit capabilities uh the the limits would be in units such as you know bits per second um requests per second and the way we were going to tackle the um the the compo the goal that i had mentioned about um unambiguous and mutually understood uh limits is by coupling each one of these capacity limits with a corresponding telemetry source which gave near real-time aggregated metrics about the usage of an upstream against a downstream um and so in that way if the downstream says this is a limit you have you can only send me 100 gigabits per second there's no ambiguity in what that means because the upstream can pull its lump resource provided by the downstream to see the the current uh delegation utilization and then adjust accordingly the second component here the fci telemetry this is kind of a result of the what we just mentioned how we want to couple a telemetry source with a a capacity limit this mechanism here this fci telemetry is the ability for a downstream to advertise support for specific telemetry sources or tend or types [Music] eventually uh we would like to propose and work towards putting together a draft to define a formal telemetry interface in which there's a well-known transport well-defined transport and well-defined format of the data that would be uh available for near-time aggregated metrics such as things like bits for pet requests sorry bits per second request per second etc but in lieu of that what we're we're basically going to be doing in the near term since a formal telemetry interface would be a very large effort to scope and define we"
  },
  {
    "startTime": "01:02:01",
    "text": "wanted to leverage in the near term just a stub of a generic telemetry source and the goal of that is twofold one the level of effort in defining a formal climate interface and the integration but two it's in most cases right now folks who are actively working on delegating requests between different entities there already are existing telemetry sources uh so being able to leverage already existing telemetry sources and all of the work that's done there it this will help um ease of adoption if folks start moving towards this model so instead of adding additional work to integrate in that telemetry let's take advantage of work that's already been done so those two are tightly coupled together in how the upstream the downstream can talk about the the specific limits the downstream is allowing the upstream or this this next one the semi capacity requested capacity limits element it's a generic metadata element that we're kind of using to allow a vehicle for the upstream to ask the downstream to reconsider and publish new capacity limits so the other in the in the previous models these fci capabilities the upstream would be polling the downstream um in this model the the upstream would be able to use this metadata object to send a signal to the downstream uh which would then trigger an asynchronous process in the back end which is left up to the downstream to implement and work out all the details on this is mainly just a vehicle once again to uh facilitate the bi-directional communication goal that we've been looking for next slide please um just one question i think kevin do you have a question here sure yeah you mentioned um existing"
  },
  {
    "startTime": "01:04:00",
    "text": "sources are the majority of these sources um proprietary in nature or are there standard protocols right that people are using to currently distribute the data or emit the data um that's a very good question it's it's i guess a little bit of column a little bit of column b right uh typically it's going to be a you know some kind of an http https api endpoint in which you know um content providers or other cdns are pulling a bespoke interface provided by a downstream cdn which has you know provides metrics and the the payload is in a bespoke form you know typically it's going to be json or something of that nature but there there is no standardization at all there and that's actually going back it's to your point we do eventually want to go and make the proposal to define a telemetry interface because of that exact reason there's no standardization there's no communication channel well defined it's all kind of just ad hoc based on which provider is involved so yes so we believe that the data itself is describable in a generic way because though the interfaces are proprietary the type of information that's being provided is fairly consistent across different vendors or yes that was the that was kind of the decision point we were looking at and to i think one of your other uh feedback points um was the concept of a are we going to be defining a registry and the answer to that point was yes for specific types of telemetry such as bits per second request per second uh i feel you know there's a well i guess a generally well understood concept of what those generally mean but even still um they're kind of getting back to the point of"
  },
  {
    "startTime": "01:06:01",
    "text": "could still be some ambiguity in that right if the upstream is calculating bits per second in a different manner particularly if we look at the use case of a content provider who may be collecting telemetry from clients directly via rum they may get a certain calculation of usage from that versus tracking uh internally from another system uh server side style metric there was there was no clear way at that point to have the upstream calculate on its own its usage against the downstream uh without incurring some form of ambiguity as a result therefore we felt that it was more appropriate for the downstream to provide a telemetry source of the usage that it is seeing uh based on the delegation and in that manner when the upstream and the downstream are using the same telemetry source to you to calculate or look at utilization the ambiguity is then gone and therefore the both parties involved are using a unified telemetry source to understand the current utilization got it right thank you um okay uh any other questions at that point all right um so right here what we're looking at is the what we're proposing for the format of the fci capacity limits payload type um kevin you had given some good feedback too particularly about the first two bullet points the total limits versus the host limits the current intention up here with the way it's currently structured was that the total limits object would represent all traffic delegated between an upstream and a downstream um so if the downstream says upstream you can delegate or you can uh send me 100 gigabits per second that's ubiquitous across all cdn domains"
  },
  {
    "startTime": "01:08:01",
    "text": "um this second section though was meant to allow the downstream to have some way shape or form to tell the upstream that certain type of traffic is different from other types of traffic low latency you know streaming is going to have a much different request profile than general game download or other style of bulk traffic and may have different utilization impacts on their infrastructure so being able to tie both of these elements together to say here's a general upstream you can delegate all of you know all of the total traffic you can delegate is governed by this along with if you're going to send me traffic on this host name which we know is low latency or some other high rps or whatever this is how we would be able to do make the differentiation such that all traffic is not created equal it's really going to become very relevant once you get delegation relationships where an upstream could be delegating many many many different you know content providers or cdn domains and you're going to have a very heterogeneous blend of traffic and being able to specifically pick out a particular subset of that traffic and identify it differently than others was once again the intention behind the host limits but at the same time having this total limits really cuts down on the bulk of the request such that if we just assume the upstream has a certain profile will advertise a total traffic delegation now allotment except for the stuff that's the outlier so that's really the intention and the interaction points between these total limits and host limits payloads now kevin like you mentioned earlier there may be room for improvement here on simplifying the object structure by perhaps pulling the host out of the the host limits and specifying it within an object type of"
  },
  {
    "startTime": "01:10:00",
    "text": "the total limits and just making the assumption that a lack of a host declaration assumes a total uh a total uh limit we will certainly take that back for consideration and update the draft accordingly based on that feedback um next slide please uh so this once again drew some very good chatter or very good feedback on the uh the list uh what is the fci telemetry here uh this object here is once again made to represent capabilities of supported telemetry so you know this is what it what is the downstream capable of supporting in terms of telemetry sources as we can see here um there's a type generic and this is once again a call out what we wanted to highlight in that this is really meant to be more of a accommodation for existing telemetry sources and also for the fact that there is no formal telemetry interface specified yet but we wanted to future-proof it a little bit based on the fact that eventually we hope that something will come of that and we're kind of leaving this the placeholder here but for the time being this is really the a way for the downstream to signal to the upstream that hey we support these different types of telemetry and it's also going to provide some of the linkage between what the telemetry source supports in terms of things such as timeline granularity percentile latency to near real time etc um any questions on this component here all right next side please uh so here is a very high level workflow of what we're anticipating all of this to kind of come together with so in the in this workflow here we're really describing the the upstream periodically pulling the downstream now"
  },
  {
    "startTime": "01:12:02",
    "text": "the upstream has a couple different responsibilities the upstream would be pulling the downstream periodically to get what the capability or the the capabilities of capacity limits from the downstream are so that would be the top left diagram here so the downstream will respond back with an fci payload which will contain the fci capacity limits specific payload there would be a ttl associated with that currently our thought process is that we would use uh http cache control headers to govern that ttl uh that that fits nicely into the the framework of communication that the sva open caching project is um specifying um but in lieu of that the the upstream is expected to periodically pull the downstream to get what the advertised capability limits are for capacity the upstream is then also expected to periodically pull the telemetry source that the downstream is providing to gather and understand the current utilization that is represented by the upper right hand component of this then in the bottom it the upstream is then to be is then supposed to compare its current utilization towards the advertised capacity limits that the downstream is provided and adjust traffic routing decisions accordingly to fit within the advertised limits next slide please uh and here this is a call out again to the fact that we want to be able to make the we want to allow the downstream to potentially signal to the upstream that a change has been made to uh the fci capabilities for capacity limits uh this this model here was really uh governed or really more specific towards this the open caching the sv open caching workflow such that we'd be using callback hooks"
  },
  {
    "startTime": "01:14:00",
    "text": "to um for subscriptions to uh sva or the fci capabilities updates but the the idea here is that we want the there is a mechanism for the downstream to signal back to the upstream there was a change in fci capabilities come pick up your net your your your newest version of your capacity limits um the the the main impetus for this is that we want to assume that the limits that were provided are expected to be you know valid unless you hear otherwise you know there shouldn't necessarily have to be a constant pulling from the upstream every five minutes and say hey if something changed hey something changed the intention here is that the downstream would give a kind of a long-standing order of capacity limits to the upstream which they would try to adhere to unless something changed on the downstream there was some event such as a massive loss of a data center the massive loss of capacity in a site or something of that effect which would warrant a signal up to the upstream to say come come get your updated limits uh next slide please this here is once again the the mechanism to allow the other side of the communication where the upstream would want to ask the downstream to reconsider limits up until now it's really been more of a one-sided conversation it's the downstream providing data to the upstream about this is what these are the limits we want you to hear to and the limits in this point are really considered like not to exceed style limits like please don't don't go past x amount of this telemetry source described value of bits per second requests per second etc but let's just say the upstream has some kind of a demand uh coming up that they're aware of and they'd like to ask uh their various downstream partners would you be willing to consider an update to support this"
  },
  {
    "startTime": "01:16:00",
    "text": "this is the the vehicle that we came up with to allow uh to facilitate that conversation the the scoping here is really bound to how the metadata object model is uh set up in this particular case we're stating that this signal of mi requested capacity limits is bound specifically to this cdn domain of serviceaid.cdnexample.com that we really weren't able to come up with a clean way to allow the upstreams to just ask for generic um total delegation relationship uh advertisement updates uh right now the vehicle it's described using this metadata model is very specific to a particular host so at the moment that was were kind of somewhat constrained in how an upstream can really ask a downstream to reconsider updates based at this level of scoping are there any updates on this component or sorry requests sorry questions about this component yeah i think it's it's an interesting one it doesn't it may not fit exactly metadata but it's not really it's not a trigger it's a trigger to ask them to update this piece of metal that you can ask for an updated limit it's a weird kind of a flow um something we should we should dig into more that's the way that this is the best way to go about it uh or is i i i gather that you went this route because this was the only way you could figure to do it with the given apis with the given interfaces but it's not ideal for for your use case correct i would say that's an accurate case this"
  },
  {
    "startTime": "01:18:00",
    "text": "was kind of a middle ground we came up with trying to adhere as closely as we can within the framework without boiling the ocean to your point you know it wasn't necessarily a trigger you know it's not necessarily a true configuration but it it this was proposed by one of the co-authors ben as a a very clever solution i feel to use what we have available in order to accommodate um but like we mentioned there are some caveats we wish we didn't necessarily have i.e like the scoping to a particular host in this example but i think there would be we would welcome uh feedback on this footpart uh especially because this one was one that we feel need is the least straightforward of our proposal yep got it thanks on next slide please uh and once again this just kind of goes through how we would imagine using this mi uh requested capacity limits object you know the upstream would post that to the the downstream uh the downstream would then follow its particular it's already existing workflow on calculating uh current utilization based on a policy engine that is outside the scope of this document the downstream would then decide whether or not it wants to update its current uh capacity limits and then advertise that back up to the cdn or the upstream cdn um next slide please so that is the the proposal that we're coming forth today with um like i said we definitely welcome lots of feedback kevin i thank you very much for your feedback uh it's it's it was very good and we're certainly going"
  },
  {
    "startTime": "01:20:01",
    "text": "to incorporate that in uh we would certainly love more feedback as well so please yeah if there's any questions please if anyone wants to i know the draft just came out before the deadline and so i don't know if everyone's had chances to read it but it is a good read i encourage everyone to go and take a look and um i post comments to the list that please everyone else read and post your comments as well there's you know some open questions there um some great ideas um so if other folks have thoughts um that'd be really helpful as a side note as well we're in terms of the this was kind of a spawn off from work being done via the streaming video alliance specifically for the open caching working group there is kind of a partner document to this that's specific to the sva and um we're going to hopefully have that maybe i'm not sure when available for a viewability as well which would help also provide additional context on the usage and where this kind of came from as well but once again there's been there's such a rich wealth of i guess of opportunity that's already baked into the cdni it just made sense to leverage that as much as possible and we felt that signaling capacity fit extremely well within the fci interface already we just wanted to provide the you know a vehicle to signal capacity via fci and that's really what this stock or this proposal here is mainly trying to get around to excellent thank you andrew um thank you for the presentation thank you for the draft uh again everybody go out and read the draft please and send comments back to andrew yeah uh uh a second then i think andrew"
  },
  {
    "startTime": "01:22:02",
    "text": "you you covered a little bit of details here so i think that's helpful and hopefully um folks will have now a better sense of as they review the document so please go ahead and review it and post your comments on the mailing list all right thank you very much everybody all right glenn we're going to give this another shot can you hear me yes yay yeah okay i'm weirdly set up through the phone and there's an echo i'm going to turn down the audio are you hearing an echo no it sounds okay to me all right so i'm going to kill the audio so i want me to hear you guys so i'm just going to talk okay okay good i assume you got me now yes so i'll start with some background here on the configuration metadata project and then we kind of move into the current state of the draft um so go to the next slide yeah background next slide perfect um yes there's just some background on why the sva got into this and by the way uh my co-authors and contributors are on the line here so they can chime in as well um on this but uh you can go skip ahead you need to focus on this much go ahead perfect yeah so you know we started looking at cdni metadata for this and realized that the object model needed to be extended to handle some typical use cases for commercial cdns and uh i won't get too much into the triggering and stuff but mostly we um we were looking at adding uh extensions to define cache policies and such so go into the next slide we'll kind of get right into it so this is the cdni metadata model that you all know and love from rfc 8006 and the important thing to note here is the extensions we proposed all live within the set of generic metadata"
  },
  {
    "startTime": "01:24:01",
    "text": "objects it's a new set of generic metadata objects and we made no uh proposed no changes at all to the structure um we've had one situation where this has been a little bit troublesome and i'll just mention it because it came up in capacity as well one of the metadata objects we've introduced is a service id and this idea that um you know most commercial cdns open caching systems have some sort of a skid of sku a cp code a service id and we've set it up so that those can be metadata but it's almost backwards in this model you define host names and then apply metadata to their host name so now we can say great if the host name is www.video.example.com then assign the service id but in fact cdn configurations are almost usually inverted you define a service id and then put hosts under it so this is a thing we may want to discuss in the future i just bring it up because it has been a little bit of an issue for us uh next slide this is the set of generic metadata objects that are in the rfc draft i think there's 13 of them here in red and each section in the draft just basically details the new generic metadata object its reason for being and its properties we'll get into that next slide the enhancements fall into these general categories enhanced source origin definitions and the main motivators there were to introduce load balancing and failover some authentication methods to authenticate to origins the aws method was mentioned earlier simple header off there's a lot of work done in increased cash control policies and a lot of the motivation behind those and i'll address some of the questions that kevin had brought up yes the origin or the content provider can specify cache control in headers"
  },
  {
    "startTime": "01:26:00",
    "text": "coming out of the responses from the origin but typically we need to define caching rules not only for the end client the user agent but caching rules for the cdn and they're often different parameters you may want different internal caching rules or times in the cdn than you would want downstream so that's the motivation from any of these cash policy extensions there's a whole set of extensions on dynamic cores headers traffic type i'll get into that service ids i mentioned open caching configurations as part of defining open caching request routing rules uh we had the need for some metadata there some configuration metadata some objects went in there private features came up and there were some questions kevin had about that as well you could keep adding private features by having more and more generic metadata objects this is true but we wanted to have a structure around it so that organizations like the sva could have their a registry of their own private features that would really uh sit internal into uh into this one object without additional generic metadata objects um and lastly the processing stage model which i'll dive into deeper can you all hear me okay all right sanjay want to give me a yeah hands up um alfonso you have a question here so do you want to go uh yeah it's just to to add um about the cash control policies dynamic courthouse all that glenn has explained um that other of the motivations i think for for this is that many cdns act as um super projected origins so projectors i think this is the name it's an old draft from the atf um so they present to the users more like an origin and not like a"
  },
  {
    "startTime": "01:28:01",
    "text": "so they are acting as an origin from the content provider perspective um sorry and for the user so it's very common these use cases where the audience of the of a content provider of an absence at the end doesn't wants to make all the things that could be required for a user making that request but let the super project origin to do though that that job uh one example could be the dynamic headers where uh maybe they don't want to handle that in the origin but let the cdn to do that job and to select the adequate core header to present to the user this this is the kind of motivation on some of these uh new generic metadata objects good uh next slide oh any other comments yeah so a big piece of the work that was added was this definition of processing stages and i noted kevin's recent comment that this may be big enough to be broken out into its own draft and we can consider that for an extra version the idea here though is that metadata needs to be applied at various really these four key points in the request response processing pipeline and this structure enables us to attach metadata at these four points in the request response pipeline typically you may for example have a cached response this the client response portion of it labeled d in the diagram and you may want to adjust the response for each client as it comes out of cache this gives you an ability to do that sort of a thing most processing happens generally uh on the first on getting a uh request from the client and i'm processing response from the origin but this does give we've"
  },
  {
    "startTime": "01:30:01",
    "text": "seen use cases over the years that call for all four of these stages next slide please this is the data model that we've designed for the processing stages this diagram thanks to alfonso's ascii artwork is now in the revision the latest provision of the draft thanks alfonso for that so it defines really a new generic metadata object which is processing stages and it's got a set of rules and there's rules for each of the four stages you saw on the previous diagram each rule essentially is structured as a match expression and then metadata to be applied if that match hits this allows you to effectively conditionally do things at certain stages based on a match uh on the request side it may be a match on some pattern of the request url or on a header and on the response side you might typically be matching on some element of the response or a status code sanjay did you have something no no i'm good okay i have to keep the volume low because i got this crazy echo here um so with it beyond the express the uh expression matching once you do apply a rule at a certain stage you essentially either transforming the request transforming the response or generating a complete synthetic response those are generally kinds of things that go on um there's probably enough detail here for now there's structures here that allow you to specify lists of headers that can be added lists of headers that can be replaced etc next slide good so here's a typical full example of a processing stage a very simple one in this example for edge.example.com standard host index we introduce a processing stage at the"
  },
  {
    "startTime": "01:32:00",
    "text": "origin response so dealing with responses coming out of the origin and we're looking for a match so here's a simple example of the metadata expression language which i'll get into next basically if response status code is a 200 then go ahead and apply some metadata and in this example the metadata is a cache policy that tells the cdn to cache internally for five seconds so that's a very simple uh type of rule that one might do next slide so in the original draft we proposed that there would be this expression language and the expression language really does two things it allows to define criteria for matching you know to apply rules conditionally and it also there are certain um areas where we actually have to synthesize a response so there's an example of one here and in the new um version of the draft the metadata expression language is called out with the syntax for the various variables and expressions so the first expression here is a simple expression match this may be a typical thing you would do to apply metadata on in this example is a request coming from safari for host example.com and so you could therefore apply metadata just based on a match on a user agent or something like that the second example is using the expression language to dynamically or to synthesize a value so in this example we are transforming a response that may have coming out from an origin and we're adding a cookie and the cookie we want to add specifically is concatenation of the user agent string and the host name so that's the value and then there's other thing the value is expression is true that's just a signal to the uh engine processing this that value is not a string literal that it needs to be evaluated"
  },
  {
    "startTime": "01:34:02",
    "text": "uh next slide yeah uh no actually the sorry go back yeah um oh okay that's it that's never mind go to capacity we're doing well capabilities interface sorry i'm looking at two screens at once so as we add new metadata and all these new capabilities of course fci needs to come along for the ride so we needed to add some fci objects that are also uh called out in this document to so that a downstream cdn can declare its ability to support these new metadata constructs next really out of scope for this discussion but just to put it all in context we also within the sva have extended the uh metadata interface originally proposed in rfc8006 and there's some sva documents uh being pushed through this would really work uh led by guillaume and alfonso um and that's uh probably not no need to discuss it more now but just to for reference that it's there okay let's kind of move ahead here status of the draft next slide i have a quick question um yes okay so the ability to publish so um 39 metadata is a trigger goal or invalidate his meditator um are there specific to that um interaction that were undesirable or are there any other things are you i just are you actually bringing these to the working group yeah i mean the original spec was exactly that the upstream triggers the downstream to pull metadata and that's great but really what we're looking at"
  },
  {
    "startTime": "01:36:01",
    "text": "is use cases here where the upstream pushes metadata into the downstream effectively publishing configuration metadata that was probably one of the biggest changes we've introduced i don't know if guillaume if you want to add to that and do you think that is the eventual goal to to rev rfc 8006 or is it something that just probably stays an add-on within sba we we we can discuss that with an sva within sva we have a whole set of apis that implement the suite of open caching capabilities of this is just one of many and that api implements our metadata publishing interface which effectively is an extension of the cdni metadata interface but uh sanji i think let's talk about that within the sva and see how much of that api we want to put in yes yes look to what these changes are compatible with the trigger interface so it is more than possible to use use these extensions with the trigger interface uh even if we are defining a new api for some use cases that and we are implementing at this dvd for that api but used to be a interface yeah and we can uh start a discussion in the mailing list about uh the sva apis and whether we feel there would be interest in pushing those into the iutf right that would probably be something we'd have to return her for but it's an interesting discussion point if there's you know need for that obviously"
  },
  {
    "startTime": "01:38:01",
    "text": "there's value in being able to do pushes i i'm totally down with that but and we discussed it early on but um yeah it's an interesting thing to bring back well while we're on the subject so the api that we've currently done really the big thing it adds is a push but we're going to be looking at a next round of api work that really addresses the whole workflow around managing cdn configurations and that's where we plan on handling versions of metadata publishing from staging to production environment rollbacks lists of generic metadata objects that can be used across configurations so all of that might work the ietf is interested in as well i had one other quick comment um it's impressive that the um the processing stages stuff was all um able to be implemented in just generic metadata i was wondering if there was um if that was done just because it was a good way to get it into the metadata interface or if that was the preferred method or was there you know also considerations for changes to the metadata interface that would make that more convenient well that was creative data modeling on my part to make it fit within the generic metadata rules as i was told the best way to move this through ietf is not to change the structural amount of data but in fact i think it fit pretty well if you want to go back to that scheme a few slides back i think it worked yeah i thought and then i liked the ascii drawings too those were awesome um it is impressive and i think the whole processing stage is uh issue of you know where do you apply it in ingress going to the origin coming back or coming out"
  },
  {
    "startTime": "01:40:00",
    "text": "it's a useful piece that we never considered um but as long as i yeah i just really wanted to figure out you know is this the right way to do it or is there a better way to do it and if if in the end we like this way then that's even better i feel pretty good about it and i think it's a nice clean structure of organizing it with rules applied based on the match data and then each rule is a header or response transform that's pretty straightforward stuff for anybody who's worked with web server configurations so i think it's all right excellent okay cool all right you'll move on down to the revisions we're almost on here so draft status next yeah so this is basically a summary of what changed from revision zero version zero was a very thin shell apologies for that but now there's enough information in here to sink your teeth into there's a couple diagrams there are some examples but we have many more diagrams to put in really what's going on here is that within the sva we have a set of specifications that are being actually reviewed right now and this particular draft is derived from an sva spec and so now actually and that's going through a review process and now we're going to face the challenge of unfortunately keeping an internal sva dock in sync with this draft as they both get rev but we will rev them together and as we go through our review process within the sva we'll coordinate that with creating a revision two here and that will have a lot more examples so each proposed new generic metadata object will have an example in there with it which is what you want to see um and then the metadata expression language portion was added as well here um i think the next couple slides were just some answers i had put in line of some original questions kevin opposed on revision zero probably no need to go through those now i'll post kevin the best thing for me to do is to put post all that in the sva into the cdni mailing list in response to your"
  },
  {
    "startTime": "01:42:00",
    "text": "questions right yes please yeah and i'll do that for the same for your new set of questions as you develop them or questions from anyone else on this revision and we'll plan on revision two probably in a month or two uh like i said coordinated with um sva review work that's going on on the sva version of this document and we'll just wrap it up on the final slide um can the cdi working group accept this uh version as a working group draft i think um this draft was big uh it definitely had a lot more detail than the one we saw earlier in the summer um thank you for that uh it was dense i haven't made it all the way through completely yet i need to give it another read myself um and i know it just came out so i encourage everyone out there to please go and read the draft there's a lot of cool stuff in the draft and um we should we should you know possibly divide it up into separate drafts there's some that's more straightforward like cash control policies um those are probably less um less discussion points for those than there are for the actual um expression language right there's there's a lot to parse through there that's a pun sorry um but uh i i think that you know we could probably um first see how folks reacted on the list um i'd like to have give some give folks some time to read it because it is fairly big but um and then we can start picking and choosing deciding you know how we want to pursue each other think that's in there yeah i think it could be reasonable to to break out the expression language portion and the processing stage in portions in a separate drafts if you think that's uh more manageable i think you're right and if there are specific groupings of metadata right the cash policy ones could all go together um i don't know"
  },
  {
    "startTime": "01:44:02",
    "text": "maybe put them together in a chapter on their own yeah yeah and that may make things easier if they're if you know a they have specific use cases around them they're much easier to describe and there's less you know mental context switching as as well as it's easier to digest for folks than a 60 page draft i think you're right yeah this is a problem in the sva version as well i think 13 new generic metadata objects are just listed in alphabetical order and that's probably in retrospect not the best way yeah so i i think i think that we could definitely get more velocity there if we group them and then put the use case in and make them easier to digest individually good that's just my two cents no i think that that that makes sense uh that that makes it easier to sort of read relevant information together and then um easier to respond that yes this makes sense and uh let's let's go with this draft so i think glenn that's a good idea if we if there's an effort to break it down into you know based on metadata you know similar metadata you know types and have that draft that will stand on its own i think that may be easier yeah and and i think also just for the sva folks on the call i mentioned this rather than going through the pain of maintaining two parallel docs we may start to cut out the sva part 2 dock the one that this is derived from and just have the sva dock start to wrap and reference this one that may make life a lot easier easier but we'll figure that out for ourselves in the sva excellent um does anyone else have any questions or comments for glenn sorry for the microphone mess up glad it worked out no worries thank you for for your"
  },
  {
    "startTime": "01:46:00",
    "text": "persistence um if there's nothing else that is the end of the agenda items we had for today um it looks like we finished a little bit early which is great but we had a lot of great discussions um if no one has any other comments i think we can close the session we'll look forward to seeing everybody again at the next ietf in march um please go read all the drafts please post your comments to the list um we have follow-ups uh on a number of these graphs that we want to you know move forward on especially the uri signing one um but otherwise thank you very much for attending sanjay any less thoughts no i think you summarized it well all right everybody thank you for your attendance and we will see you at ietf 113. take care thank you thanks everybody you"
  }
]
