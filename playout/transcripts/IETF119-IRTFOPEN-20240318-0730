[
  {
    "startTime": "00:00:00",
    "text": "Yeah. Alright. So, I guess it's time to get started. This is the IITF Open Meeting at IETF 119 in Brisbane. My name is Colin Perkins. I'm the IRTF chair. To like to start with the usual reminders."
  },
  {
    "startTime": "00:02:03",
    "text": "This is an IRTF meeting and the IRTF follows the ATF intellectual property disclosure rules. So, by participating, you agreed to disclose any patents or patents applications you have on on the work, or or on your contributions, and that includes, questions or contributions from the the microphone lines. In addition, we make audio video recordings of, these meetings available, we're being streamed live on YouTube, and the recording will be on YouTube afterwards. If you, come to the microphone and speak at the microphone, you will be included in that recording. And if you have made comments in the chats as well, those will be recorded. If you're participating in person, and, do do not wish to be photographed, then please wear one red do not face graph line layouts. But, as I say, if you speak at the whether or not you're wearing those lanyards, you, consenting speed. To be recorded. If you're participating online, please make sure your camera and your microphone are turned off unless you're actively asking the questions. And please do use the common, meet echo queue both for for online and, in in person attendees. By participating in this meeting, you, agree that any personal information you've made available to the IETF or IRTF will be handled in accordance with our with the privacy policy as specified on the slide. And you also agreed to work respectfully with the other participants in the, the the meeting, and the code of conduct and the anti harassment procedures do apply. And if you have any questions or concerns about that, either contact me the IRTF chair or contact the onboards team and their contact details are on the slide there."
  },
  {
    "startTime": "00:04:04",
    "text": "If you are attending in person, please scan the QR code to have your attendance recorded. Use me to join the microphone line if you wish to ask questions. Remote participants. Make sure your audio and video are off unless you're asking questions. And you you you can use the, the meet echo, queue facility to join and we have a common queue for both local and remote participants. So this is an IRTF meeting, The IRTF is, a parallel organization to the IETF which focuses on longer term research issues, relating to the internet in its development. While the IETF focuses on shorter term issues relating to engineering and standards making. IITF is a a research organization. It's not a standards development organization. I want it can and does publish, documents in the RFC series. The primary output of the research groups is understanding and research results and research papers, rather than RFCs and certainly not, standard documents. IETF is organized as a number of research groups. The currently active research groups and proposed research groups are listed on the slide here. The crypto forum group and the measurements and analysis of protocols research group Both metts earlier, today, and just after lunch. The other groups highlighted in dark blue on the slide, the centralized internet infrastructure group, global access to the internet research group, The congestion control group information centric Networking Network Management quantum internet and research and analysis of standards, processes, on meeting later in this week. So please do, look out for those sessions. Please go along if you're interested in those topics. Speaks"
  },
  {
    "startTime": "00:06:02",
    "text": "I say, the IRTF can, publish, RFCs, this is perhaps the most RFCs we've, published between, and any pair of meetings, since I took over as, IRTFG 5 years ago. And we've actually published 7 RFCs this time. I think 4 of them, I I see dirk dirk over there looking horrified 4 of them from the ICN RG, in in the last month or so, the ICN PNing and trace route specifications. The the path steering draft, and an alternative delta time encoding draft. From the crypto forum group, we had RFCs on, oblivious pseudo random functions, using prime order group and, on the Ristretto and the CAF groups. And the privacy enhancements and assessments research group also published an RFC, providing a survey of, censorship techniques some So, towards the end of last year, addition to the RFCs, We have been working on our a, revised code of conduct for the IITF, That is currently in, Draft Perkins, IRTF code of conduct. It's addressed as you can tell. Which we we're we're still discussing in the IRS g. Please do read this. Please do comment on it, send feedback, to myself to the broader IRSG to the IITF Discussedless. We want to get this right. So please do, Have a look at this. Make sure it's saying the right things. Make sure you have no concerns with and please send feedback. We're hoping to publish that hopefully, in the summer. So so as I say, please please do please do send us feedback. In addition to the research groups, the IRTF also runs the applied networking research price."
  },
  {
    "startTime": "00:08:01",
    "text": "This is something we organize, in conjunction with the internet society I'm with sponsorship from, Conk Comcast And NBCUniversal. And the Applied Networking Research Price is awarded to, recognize some of the best recent results in applied networking research, in awarded to recognize interesting new research ideas of, potential significance and relevance to the internet standards community And it's awarded to recognize upcoming people that are likely to have, an impact on the standards community, and, internet technology is going for Over the years, we've had a a a number of these we've made a number of these awards. We've had some, fantastic papers, some very, some really good awards, some really, strong people. I'm very pleased to say that we are we are continuing that trend We have, won awards today, which will go to, Dunkeehan. For his work on, contact shift adaptation, and, norma normality shift adaptation, in anomaly detection systems, and he'll be giving his, award talk, in a few minutes. So congratulations, very much to, thank you. In addition to the price, we also, have run the applied networking research workshop, which, co locates with the ITS meeting in, July each year. This year, that will be, co locating with the F1 20 meeting, which takes place in Vancouver in July. The chairs for that meeting, Jayshree, Simeon Ferlin, and Ignacio Castro. The call for papers, is on the the website on the the URL it on the slide. Papers are due in about a month on 15th April. So please do consider submitting your applied networking, research work to that workshop."
  },
  {
    "startTime": "00:10:01",
    "text": "It's organized in conjunction with ACM sitcom, and proceedings are published in the ACM Digital Library. So Please do, consider submitting your work. I think the final thing I I want to say is that we also offer a travel grant And, we have a a significant number of people attending this meeting, Using these travel grants, we're very grateful, to our sponsors. To Akamai to Comcast to Netflix who have been, involved in this program for a number of years and, new for this meeting, the the Brisbane, economic development agency and Google have both provided significant funding. We're very grateful because we've been able to bring at a a number of people to this meeting, and I think this is a a very valuable thing. We will have travel grants available for the future meetings. So if who are interested in sending the Vancouver meeting or the meeting in Dublin in November, travel grants will become available, and the call travel grants applications for the the Vancouver meeting will be available in the couple of weeks at the, the URL on the slide. So if you're interested in in that, please do look out. And with that, that's all I have to say. The main focus of the meeting is, Dong Chi's, a word talk So, Please, welcome Doug Chi up to the microphone. Gonna move on to the, the awards. K. Okay, so The Applied Networking Research Price Award for ATF 119. I'm very pleased to announce, goes to Zhong Chihan, from Chingway University."
  },
  {
    "startTime": "00:12:02",
    "text": "She is, a PhD candidate at Chinguai. His research focuses on data driven network management AI trustworthiness in security applications. Other intersections of AI And Network Security. The paper he'll be presenting today, anomaly detection in the open world, normality shift detection explanation and adaptation was originally published in the networks and distributed systems Security Symposium in, 2023. And she Thank you. Over to you. Okay. Thanks for the introduction. And also, thanks the press committee to give me the opportunity, to introduce our work, anomaly detection in the open word. And I'm Dong Chi from Chihai University, and this is a drawing work with state grade corporation of China. And as you can see from the title, this work is about anomaly detection. So that's first speaking has some background. Nowadays, several gramers are becoming more professional and coordinate so traditional defense systems like, web gateways Fairwalls And IDS IPS are supposed to be a security enough but, now skilled attackers can bypass approximately all this defense systems. So this context anomaly detection has been widely used in, diverse networking network security applications, for the great performance of learning without knowledge of without knowledge or anomalies and, have the to detect unforeseen rights. And, with the help, with the emergence of deep learning. It has been shown that deep learning has a great potential to build, network security applications So, compare with, comparies the traditional algorithm advantage of deep learning is"
  },
  {
    "startTime": "00:14:03",
    "text": "to learn better, nonlinear and hierarchical features from complex and massive data. So the the learning mechanism of deep learning based anomaly detection, is called 0 positive learning. Here, positive means anomalies. So 0positivelearning means, the deep learning model are train with only normal data. And, technically, there are 2 kinds of, 0 positive learning So the first one is reconstruction based, learning. And this is the earlier achieved by, generated models such as, GaN, auto encoder, variation or auto encoder, So during training, only normal data is fed into the model to minimize the reconstruction error between model input and model output. After training, it can, detect anomalies by computing the reconstruction. Ira, and the model is very likely to be failed when normally, which is which are very different from the training normal data. The second one is called prediction based models. So, uh-uh, which is usually used for time series or sequential data. And achieved by, such as STM or recurrent neural network. So during training, given a normal time series, with the lungs of t, which is 6 in this example, The sequential model like, RN will increase the probability of predicting the lack simple attempt t"
  },
  {
    "startTime": "00:16:01",
    "text": "through the sequence with the last T-one samples. So similar to reconstruction based, models, such model can predict the last sample in normal time series with higher probabilities but, wrongly predict anomalies, time series. Ah, okay. This is the basic idea of deep learning based anomaly detection And so far, researchers have developed deep learning based anomaly detection in whereas, network, security applications, such as network, intrusion, log anomaly detection, lateral movement detection and host space threat detection. Unfortunately, the great success of machine learning deep learning are based on the close word assumption that is testing data must be similar to the training data. Also normalize the IID assumption. However, in open word, settings the distributions of testing data can change over time in many unforeseen ways, in machine learning community, this problem is called concept drift problem. Here concept means, the distribution of data and drift means the distribution, has changed. And, concept drift can be quite common in security. For example, the change, the statistics, the change of the physical features of, malware during, malware evolution, we'll make the performance of a malware classifier picking lower So, frankly speaking, the model aging can be one of the most"
  },
  {
    "startTime": "00:18:01",
    "text": "frustrating problem, in real world applications, build on machine learning, our deep learning, and we will see, the great performance on the validation set, but but overwhelming false positives or false negatives in real deployment. And this will easily make people lose the face off learning based methods So, actually concept drift has been well studied in the context of supervised learning in security domains. There are some of excellent work previously, investigate concept drift in malware classification like, at least here, Transcend and Kate And in machine learning communities, there are many, related topics And one remarkable direction is, the auto distribution detection. So I'm sorry. So, as shown in the left picture, if a sample doesn't belong to the distribution of existing classes, it will be detected as as an OOD sample. However, anomaly detection models are built on purely normal data. So they are immune to the Sorry. So we are immune to the drift of monetures or abnormal, distribution changes, but comes at the price of more severe impact when the distribution of normality shifts Here, normally means distribution of normal data. And the normal relationships can also be quite common for for networking for security domains because, the systems, themselves and, can differ and involve over time such as, system patches are the involvement of new devices, new users on new protocols."
  },
  {
    "startTime": "00:20:05",
    "text": "And as showing in the in the right feature, We, if you only have the knowledge of Normandy Given a simple device from the normality we cannot have whether it's a drift of memory or it's a rule anomaly. So so this can be quite big challenge for detecting normality shift, and previous the previous work for supervised learning is not suitable in exciting, So that's why we highlight the difference of, concept drift and normative shift in the title of this slide. So we summarize the, first key inside is here for anomaly detect without ground truth label, a normal tissue and real anomaly is not distinguishable. And to mitigate concept drift or normative shift problem, there are basically 2 pipelines. The first one which is often used, is to periodically retrain the model with, new and old data or train a new one, a new subclassifier to, assemble. However, we think this is not suitable for practical applications, because, continuous training is labor intensive and hard to obtain, the Reason or the timing of, shift. So the second solution is to 1st determining and zoom, investigate, and finally, adapt to the shift or the drift as this is our scope. So the second, inside here is we need to decide whether when and the whole shift occurs before adapting models to the shift. To detect the shift we need to, we need to,"
  },
  {
    "startTime": "00:22:00",
    "text": "To to figure out how to represent distribution of data. So, ideally, we want to, capture capture the, data from feature space to and detect change, however, the high dimensional distribution is intractable in mathematics. So, alternative way is to, use distribution of model output which is, something like probabilities or anomaly scores. So in this way, we are facing 1 dimensional score instead of dimensional feature. And, so we summarized key in sensory is distribution of normality can be represented by the distribution of model outputs okay. With all all the three inside in mind, we proposed OAD shot for open word anomaly detection and which is a framework of detect and, explain and adapt to normality shift for deep learning based anomaly detection. As you can see, this is a, quite big and complicated figure, and I will try to explain the motivation example, in it. So suppose before any shift, we 5 samples in blue to represent the old distribution of normality and we fed them into, into the detection model to get the model outputs, which are, anomaly scores or probabilities And suppose after some shift, we collect another file samples in green from the new environment from the new distribution. We also cast their model outputs And, However, as we See from the, picture the original model outputs are, hard to"
  },
  {
    "startTime": "00:24:02",
    "text": "distinguished between all the new, distribution because they are all concentrated, between 0.9 and 1. So the first type of OWD is to, force transformation on the original model outputs, which we call model calibration. We expect the calibrated model outputs to provide probable intake or statistical information of of of the simple So in this example, we transfer, file output of file outputs in all distribution to be the percentile of original outputs, which is a onefive's two faves, so on and so forth. So as we can see, the distribution become more distinguishable after calibration. However, in practice, there won't be only 5 So we need, to compire, the 2 distribution of calibrated outputs more strictly. So that's why we introduced the 2nd step shift detection And, here, we conduct a hypothesis test on whether two sides are from the same distribution. And use P value to determine whether there whether there is a shift. So, obviously, in this example, Normandy has shift has shifted The next step is to investigate and explain the normal t shift. Here, our goal is to find the most important samples the new distribution, to that induced the shift we formulate an optimization problem to assign weight for each of the 10 samples from, old and new distribution and the weight indicates whether the sample can represent the new distribution after shift. Like this, the intuition is to use the"
  },
  {
    "startTime": "00:26:03",
    "text": "mixture of old and new distribution, to reconstruct the new normative distribution. As shown here, and here we won't use all samples from the new distribution because new symptoms need to be labeled, and we want to lower this, labeling overhead. So as shown from the feature space in this example, we use 3 samples from the old distribution and 2 from the new distribution for the reconstruction. We can also obtain some insights from the explanation results. 1st, This three samples remains, in the new distribution, Then These two are no longer normal in the new distribution. And, these 2 are newly become normal in the new distribution. After, selecting the importance we need to label, label them to filter out anomalies because we won't let anomalies, go into the to to the to the anomaly detection model. Finally, in the last step, we update the anomaly detection model adapting to the shift We retrain the deep learning model with, the same post select from the last step as it kind of represent a new distribution, and to, avoid for guiding useful knowledge when updating model we, we we assign different weights for each sample, which is exactly same to the weight derived from the formation shift explanation. So, this the basic idea of OWD and note that, this whole procedure can work iteratively to, make sure that anomaly detection model, is always up to date."
  },
  {
    "startTime": "00:28:05",
    "text": "So, let me introduce a little bit more, detail of each of the 4 step in OWD. And for 1st step, output calibration, in in supervised learning model calibration, to transform classifiers course into class membership probabilities. For example, if we have 1100 predictions, which are always model outputs of 0.8 after calibration, then, the ideal case is that Exactly. 80% of them should be correctly classified. But for anomaly detection, we don't have, 2 classes to 2 class labels. So we cannot use the above accuracy like metrics, as the goal of calibration. So here we propose a new one which is the percentile of model outputs among all training normal data. I hear you the, example after calibration, we can see that the outputs are uniformly distributed between 0 1. So how to achieve this, basically, we can we can select some, parametric function and estimates the parameter using, the the original and expected outputs. So in this study, we use histonic regression, cost, as the basic function because, it meets the all these, three requirements And about, second type, after calibration, we perform a hypothesis test on the new and old calibrated outputs to determine"
  },
  {
    "startTime": "00:30:01",
    "text": "whether there is a normative shift between 2 distributions So here, the non the non hypothesis is that, 2 data are from the same distribution. So this means no drift typing and the alternative, hypothesis is just the, the opposite. And we choose permutation testing. This work as it is, distribution free support and a test that his stakes are suitable. And suitable for, small data set especially considering we have limited news, new samples. Due to the labeling overhead. And, we use the test statistics and to compute a P value. As for the search step, as we mentioned before, due to the labeling overhead on new distribution. Shift explanation is to fund a small set of simple from new distribution That is uh-huh. Important to induce the shift. We formulate this procedure as a optimization problem to solve the weight for each samples from all distribution Sorry. And the old one, old disrepurant in order I'm c and the weights of new, new samples are denoted with So we keep the sample if it's weight in MCOMT is close to, 1 and discard it if, Close to 0. The optimization includes, 3 loss terms So accuracy loss term indicates that the mixed samples of old and new should accurately reconstruct a new distribution and overhead loss means to choose as few samples from the new distribution."
  },
  {
    "startTime": "00:32:03",
    "text": "And determinism loss means I'm CRMT should be deterministic. Either close to 0 or close to 1. So here is an example of how the mixed distribution, reconstructs the new distribution. Here, the deep losing deep blue SIM posts are, selected from the new distribution and the light blue ones are discarded from from the old distribution. And for the last step, The last step is to make sure that the model can adapt to the new distribution. So we update the model with the samples selecting us selected from the last the last step I'm here we show the new loss function. We're updating the anomaly detection model So besides the original loss function of anomaly detect model. We also introduced another loss term to consolidate important model parameters by assigning, important ways, a important way to the model parameter. The intuition here is to keep important parameter with useful old knowledge and change the wires, that's our auto update. Okay. So, due to the time limit, If you are interested in more detail of OWD, please refer to our paper let's move on to, to evaluation. We evaluate OWA in 3 security applications, detecting network insurance, log anomaly, and there's a moment. And we use a representative, detection systems from top tier security conferences,"
  },
  {
    "startTime": "00:34:04",
    "text": "we select datasets with a long time span in months or years to reflect the normal change in long time period. Long time period. And we detect, normative shift periodically or crowding to the time span of each data site. Well, So the first question we ask here is, is is why their normal to shift occurs in this security applications And the answer, is yes. From from the Sean figures which are the distribution over time. We can observe that, normalcy shift in security domain is quite common and difference for each application. So a case by case analysis, based on our explanation method is necessary. Next, we perform an entrant performance, evaluation after adapting to the shift and observe to which extent the performance of model is improved by OWD and other baseline methods. So as showing in this figure, we simulate a scenario and split the data set into different ten points, which is T0, T1, t 2 and so on and so forth. In t 0 training set is used to train the original anomaly detection system. And in t 1 later, validation set is used to detect and adapt to the shift and past set is used for evaluation. We also conduct 2 experiments to evaluate the entry and the performance. First one is, called single adaptation is to, to to to adapt to the shift only, one time and observe the performance in subsequent time points"
  },
  {
    "startTime": "00:36:01",
    "text": "The second one The second one called multiple adaptation is to, is to detect and adapt at each time point. From the result of, single adaptation experiments, we fixed the budget of label as, 30% which means, all measure including OWD And Business can only use 30% labeled samples At the most for updating, the the model And here, y axis, in the model performance which is higher, the better, and the highlight right line, is our master OWD. And we can observe that OWD outperforms other approaches at the adaptation time, namely T1 or T2 here. And also, can mitigate the performance degradation in subsequent ten points. So in multiple adaptation experiments, we relax the budget of, labels to observe the label labeling overhead together with the model performance. After adaptation. So the lines here are are, or the, performance after after shift the repetition, which is a higher departure, and the bars here are the labeling overhead, which is, lower the better. And the conclusion is that, OWD can achieve better results with significantly less required neighbors. We also observe, the number of force positives and false negatives before and after adaptation, and to read the conclusion is that, OWD is"
  },
  {
    "startTime": "00:38:02",
    "text": "the only approach that can reduce both false positives and false negatives Next, to show the potential of OWD in real world applications, we, we conduct an initial deployment on a SCADA system. Here, we worked with State's Greater Shanghai Electric Power Company and this company provides electricity, transmission, and a unified control of a Shanghai power grid in China What we do here is to assist, their so called security monitoring system, with our master and OWD. And this security monitoring systems will continuously monitor and analyze logs or events from all the devices in the grid. And, so the the problem is that it tends to be, very effective at the very beginning, but the performance of this security monitoring systems our, our soon become, very, very low during deployment. So that's exactly what OWD want to deal with. So we deploy OWD upon their system. To detect and handle normalty shift We have collected over 10,000,000 logs from 20 devices in 5 months, beginning at, October October 2021, I guess. So, we deploy OWD, safe detection, first, and as shown in this two figure, So the the the left one is, EC,"
  },
  {
    "startTime": "00:40:02",
    "text": "is, in 3 weeks and the right one is, about 4 months From the result, we can say that the left figure is not shifted and the right one, which is 4 months is obviously shifted. So next, OWD will perform shift explanation to find the reason of this, shift. As a result after, identifying the important shift logs. We finally got 2 important blocks. The first one shows, network volume increases for some, devices. And the other one shows, let's there are new, surveys continuously being launched. So after talking to the, operator, we finally found the key reason is, there is big system updates and many devices are restarted. It's about, January 2022. So there are some FTP service errors, due to this update and restart, and, finally caused this normality shift. So finally after, performing The last step of OWD, which is shift adaptation we successfully reduced over 90% of false positives caused by the shift Okay. Here are some, takeaways The first one is a normal shift is, quite common and complicated in in network security domains. And the second one is, we found that the models in security domains are highly un calibrated So, so after using, the calibration method proposed in this work. Models can effectively represent the normal distribution."
  },
  {
    "startTime": "00:42:01",
    "text": "The third one is, if we want to handle normal shift for for for anomaly detection model, labeling is inevitable. This is key difference, from from concept drift problem in in supervised learning. But the good thing is that OWD can achieve better performance with lower labels. And the last one is OWD is shown to be able to reduce post positives and false negatives. We have released our source code on GitHub And, you can try it for your own anomaly detection. Applications. Okay. Thanks for your attention, and I'm ready to take question. Thank you Alright. Does, anyone have any questions for the speaker? Shirley, somebody does. K. So I see, yeah, Aaron, Yaron, Sheffler. You're on, Safa. Thank you for the presentation. Very interesting. Could you speak a little more about the labeling The amount of labeling, maybe in the the specific experiment that you discussed and what level of expertise is expected from the First, I'm doing the labeling Okay. That's a good question. So, Okay. Thank you."
  },
  {
    "startTime": "00:44:07",
    "text": "Okay. So the labeling is, key step between, shift explanation and shift adaptation because, shift the explanation will give us some important important samples, induce the shift and we need to filter the anomalies because, the anomaly detection model here is only trained with, normal data. So, I think that you are asking about in in practical applications what is the labeling of our cracked, Uh-uh. In in the public in the experiment of using public datasets. I think in network and touring applications, we need to label about 5000 traffic flow, half a year, half a year. And, for, log anomaly detect systems, it's a little bit complicated. We'll need some more labeling our head back to. The system logs are are are are turns out to be, there are many same or similar locks. So this will reduce the labeling on our hand. And for our, deployment in in, in Scada, and, to fund these 2 important locks, logs, It just take hours of operator. Yes. Yes. Yes. Thank you. Mhmm. Okay. Thank you. Brian, Brian Tremmel. Hi. Thanks for the talk. This was super interesting. Question is a related question, but it's over on the data collection side of things. I might have missed it, but sort of like the initial data that we sort of, like,"
  },
  {
    "startTime": "00:46:04",
    "text": "boot this system with. Right? Like, so both the before shift and after shift data, that is all data that is considered normal. Right? Like, so this is like a, a 0 positive system. Am I correct? Mhmm. Yeah. Where do you get the normal data from? This is, Good question because we need, in this research, however, we must make some assumption is We, during some, time period, we can collect. We we suppose as there are no any attacks or any anomalies. So Okay. and during period, would actually have somebody who's doing the labeling come that you And and sort of like do a sniff test, or you just sort of assume that that's the case, and then you then you let the feedback loop with the labeling step sort of, push that out of the system. Yes. Soil question. So, like, Do do you you're making the assumption on the front end that you have some data that is attack free, which is you know, talk about the validity of that assumption. Do you actually have somebody check that or are you assuming that the reinforcement loop is gonna is gonna take care of that for you. Mhmm. So, it's like a reinforcement learning. Framework. Okay. Okay. Cool. Thank you very much. Okay. Thank you. Good. I'm excited. Hi, Alexander. My question is about this explanation. Could you help me understand it better. In one of the last slides, There was an explanation that an anomaly was detected, And this was because of an FTP server update, which went wrong. Was that explanation generated by the system or by the human by the system. We formulate our optimization problem and it can automatically by by serving this, optimization problem."
  },
  {
    "startTime": "00:48:01",
    "text": "Will automatically give us some samples indicating the shift Wow. Very impressive. So so so so the system said it was FTP or the system demonstrate show the system identified traffic and the and the the the user then when oh, that's obviously FTP. FTP. So So so it is is it the system which says this is this FTP service or whatever it was causing the problem, or is it the system that says this is This is an example of the traffic flow which caused the problem and the user then says all is this application. Oh, I see. It can just read some, samples some some data. If we like, the input of the model. Afternoon. finds the exact representative So it it it finds it finds it samples of the problematic traffic. Yes. The deep reason needs some some, some some security operator to investigate Yeah. Okay? That that's still very, very, very helpful. Can can you maybe, you know, obviously, it's it's You know, the the the goal is that it finds, significant samples and uses those to shift the distribution. How sensitive is it to that process? You know, if if if that process is imperfect and it finds, perhaps look perhaps the wrong samples. Does that significantly affect the performance, or is it somewhat insensitive to this, it just takes, you know, adapts maybe a little slower you mean when when on the LED makes some mistakes? Yeah. Right. If if if it's process of, finding the significant samples is is imperfect. Does that"
  },
  {
    "startTime": "00:50:00",
    "text": "really throw it off, or is it just a is it is it really sensitive to this this process as accuracy, or is it Or, I guess, have you evaluated how sensitive is it to this to this process of accuracy? Yes. I have evaluated the robustness of the explanation. So if you add some more noise on the us original, inputs So, what it will be in the mono outputs? So the result turns out to be, it's okay, but such small Okay. Thank you. And that's Hi, honeys. Could you say a little bit more about your experience with this system, what type of traffic was that? Is that traffic that is where you did the training was that sort of always very similar, or was that some sort of had a lot of irregularities. And so on how much you need to to sort of Have a good starting point so to speak. So, what I research here, you know, this work is, It's a generic Network insurance system. So it can be, many different types of traffic And the normal traffic, I think it's, universal because, we are attack free. So we just collect some some traffic that is, Does not include some attacks, some anomalies, And, theoretically, we can detect any attacks or anomalies deviates from such normal traffic. But for example, you you mentioned in one of your applications you had this,"
  },
  {
    "startTime": "00:52:02",
    "text": "if you go back a little, or forward, I don't know where it was, This is what? Yeah. Go on the yeah. Background. Scatter, scattered system and state grid. So for some of the, at least the communication protocols I've been working with, some of them had sort of sensors and actuators, involved in. So a lot of lot of the interaction that you see on the network depend on what happens in the real world, so to speak, like the actuators, obviously, someone, does something, or the sensors, if certain events are triggered. So the traffic is, sort of It's somewhat hard to say what is the normal traffic? Like, if So so I'm I'm I'm curious on how how what your experience was with some of those, those type of systems where not always the same pattern, like like you had in your example, when you when something failed, But that may be just a sort of some sense is not, triggering some some interactions like you you have some motion detection sense to us. And sometimes There is just nobody. Is very infrequent, right, Yes. I agree with you. So, I think this is a common problem for anomaly detection. We need to clearly define what's anomaly is and what, normal he is. So and, anomaly detection algorithms can only work fighter, in some some clearly defined problem. We, we we we know where we clear about what anomalies and the normality is very stable. Mhmm. This is very important for anomaly detection. Otherwise, they were overwhelming false positives. Okay. Yeah. That's what I'm worried about the the false positives, huge number of force possible."
  },
  {
    "startTime": "00:54:04",
    "text": "Alright. Thank you. Are there any any final quest some people I I guess, I I I have one more way we're waiting there. You know, obviously, this is a an IRTF meeting and, we're we're co located with the I I ETF, and the the standards body. Is there anything that you think the the IETF or of the stand community or perhaps the operates a community could do to help facilitate this type of research could help to could help make this sort of research this type of system work better. Is is there any you know, you know, you're you're you're in a room full of people who who operate network and who's standardized network protocols. Is there anything we should be doing differently to to make this sort of problem, easier. I I mean, I guess one of the things we'll be providing, to I test data to to test against, but is there anything else we could be doing Let me see. I Actually, I don't know why it's here, but Sorry. Field. I I I guess data is probably the main thing. Maybe I can see to hear about this. Absolutely. Absolutely. Alright. Chin. Yes. Just try to answer call in pricing. Actually, in IT of the currently form the new working group focused on, you know, you know, it's an ML ops, actually. And there's some, you know, AI related topic you know, focus on nano anomaly detection and maybe, one use case that is, you know, on the SRV6 availability or this kind of cases. So they can use AI to, you know, detect some, alumni. So just before this meeting, we have, you know,"
  },
  {
    "startTime": "00:56:04",
    "text": "three jobs that, you know, did, tried to explore, you know, incident, terminology, incident, management and also, metadata, connection, for, you know, anomaly detection, I think, seems to consignize with your you know, leverages some risks or results. I think even consider, whether you in to contribute Yeah. For her office. Yeah. Thank you. Rudka. Sorry. Who did you call? For quarter of a century, operator. A ago. couple of years I think I heard you answer to harness that you're not you need to know what, is well, okay. And, normally, Right? Or did you say, you need to know what's norm what's normal well, okay, kind of, to think to, two sides of the same thing. As an operator, I view anomalies as something that is unexpected and So, Yeah. Well, the thick that the operational question was, becomes well, okay. Can kit. How sure can I be that anything that is animal? Actually, triggers an alarm. Okay. Thanks for the good insight of anomaly."
  },
  {
    "startTime": "00:58:01",
    "text": "Alright. Thank you. We are about out of time. So thank you very much, for the questions. Thank you very much for an excellent talk. Congratulations again on their work. And again. thank you Alright. So that's all we have, today. Please do, consider submitting your papers to the applied networking research workshop, which will be happening at the, IETF in July in Vancouver. We'll have a also have a number of, ANIP talks there as well. Thank you everybody. And that's everything for today. Thanks, Saul."
  }
]
