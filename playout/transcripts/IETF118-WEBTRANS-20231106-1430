[
  {
    "startTime": "00:00:33",
    "text": "Is on timezone on the slide. To remember, remember, David, maybe we should make a last minute check to make sure these are the latest slide. I think Victor added some late Let's see. Is Victor in the room speaking of? Alright. We might Alright. Anyway, perhaps Victor can check to make sure. And if if there's anything, we can I can upload the new ones and do a refresh? So Let's see. I mean, if you go to slide 29, I think that's the most recently updated one. Okay. Hold on, buddy. Wanna get there. Yeah. On the rural drive. I think we're up to date. Okay. Cool. Excellent. The number Oh, right. She was joining remotely. Thanks."
  },
  {
    "startTime": "00:02:44",
    "text": "Has someone seen the physical virtual blue she thing Yeah. Pass it around if you can. It it really helps people remember to sign in. Please definitely sign in. That way, we will be in a bigger room next time. Thanks. I'm on Bernache, we get started. I think I've folks have filtered in Yep. Alright. Hi, everyone, and welcome to WIP Transport. Meeting over here in Berlin. The name of the meeting room. We're actually in Prague. And Yeah. Next slide, please. So as usual, a reminder, This session is being recorded. And some instructions here for how to use the tool. Oh, and all the icons are different. We didn't update these. Whoops. I mean, these are self explanatory. And if you have questions, don't hesitate to ask. Next slide."
  },
  {
    "startTime": "00:04:03",
    "text": "Similarly, the buttons from the tool allow you to join exit the queue, And to mute and unmute, please keep yourself muted in video off unless you're actually actively speaking. Next slide, please. And here are some links. Next slide. And this is the ATF note well. This is Monday. Probably most of you have already seen it, but maybe not everyone. So by participating in IETF activities, you've agreed to this know well, and hopefully you've read some of it or at least skimmed it. To highlight a few points, That means that if you contribute you have to abide by our code of conduct that the chairs will be enforcing. And by our our IPR policies. If you're not aware of the ATF is you really should look at them because they have impact to an to you as a human contributor. Alright. And everything else here is also equally important. Next slide. Yeah. Yes. I was mentioning that as well. We really take our code of conduct seriously here and Hope that we won't have any concerns. Thank you. Next slide. Alright. Do we have a volunteer to take notes today, please? So as a reminder, we need to notate meetings, so I'm gonna be awkwardly staring around the room until we have one. Is that a volunteering, Eric? Okay. Thanks. Well, so Eric's gonna be doing it for most of the except when he's giving a talk, can someone be Backup note taker. This is a small job. Thanks, Lucas. Much appreciate both of you. Alright. And let's use the,"
  },
  {
    "startTime": "00:06:01",
    "text": "the notes tool. That way it's you can all look at it at the same time. Alright. Thanks. Next slide. Alright. This is our agenda. If it looks familiar, it's because it's roughly the same one as our past meetings, where we'll have an update from Will on what's going on at the B3C. Then updates from our H2 and H3 versions. And then share wrap up. Would anyone like to bash this agenda? Thank you very much. Then, Will, you're up. Bernad, next slide, please. Just yeah. I think I think the agenda's been the same for the last year and a half. Yeah. Pretty much. We almost reordered H2 and H3, and then we decided that was much work. And if it's bash proof, it's best proof saying to you. So, good afternoon to those in Prague. My name is Willow. I co chair the w three c web transport working group at along with Yoni Vibrreri who's online as well. We just give you a summary of what's happening on on the browser side of the protocol. So We published our last working draft back in July 12th. We gave you an update July 27th, so there hasn't been a working draft update since then. It seems like it was just yesterday we were presenting. Our charter still expires the end of this year, but we will get it extended. That's normal 3 c operations. We have a timetable. We don't have to go into the details but we would really like to get our recommendation out the door sometime in 2024. And it should be aligned with the IETF release as well. So That's at least a request from our side. Fee, aim for at least next year. We have a a milestone tracking our the status that for us getting to a candidate release. We're 76% complete there. And we have a number of open PRs. Thanks to Nitty for picking up some of those. Appreciate that. And anyone else who has an assigned and is in the room."
  },
  {
    "startTime": "00:08:01",
    "text": "Please get on the list. We had a annual face to face meeting I put a link to it there for Tuesday, September 12th that was our equivalent of this this call, and you can read those notes if you wish. Next slide, please. So some major updates in decisions since our last report July 20 So firstly, there've been some changes to the stats object, and they're pretty self planetary. You removed, number outgoing streams and number of incoming streams created. Because the application can figure this out for itself. We added a stat get bytes lost to give us symmetry with packets lost. And we removed time stamp. It's a bit of an attack vector and performance not now is essentially an equitable replacement for it. The bigger change semantically was the addition of Sendgroup It's basically a number of space to preserve fairness between flows when we were we were using send order, which was previous creation. If you have a stream that's up up incrementing send order with every frame and an the one that's in incrementing it with every gop, then your frame one's gonna get higher order and keep having higher priority quicker. So we needed to separate those. Now you can send group equals create send group. You, get a writable and you can see that in your constructor, you can assign the N group, and you can also update it. Afterwards, as a getter and as a setter. So we think it's a a nice addition, to the spec and allowing for more equitable and fairness between competing flows. Are they quest is there a question just coming from Luke? I'm not sure. Come Go ahead, Luke. Yeah. Go I can also wait till the end. But when you reprioritize, does it, What about any data already buffered in the queue?"
  },
  {
    "startTime": "00:10:02",
    "text": "That's up to the user agent implementer. I don't think we define it. Victor, would you have an opinion? Because this was is Well, if you buffer it, but it's not written yet, it will go according to the new priority. If it's already written and on the wire, we can do anything about it. Yeah. Different quick libraries do this differently. Is why I bring it up So it's just good to know what the website will do. Okay. Alan? And then Some, has, so is there the the send group priority scheme is different than the priority scheme, which is totally fine in the way we defined it because just an API to your local user agent. Is there an expectation? Will it create strange asymmetries when you have client server with web transport applications where The client code can use these send groups, but then Or is there an expectation that that will be an extension to HTTP priorities or quick and HTTP libraries will be expected to implement something like Send groups. So this is right now, it's purely on the same side, these are the send buffers riding it to the wire. Any prioritization over the wire is is not yet connected. To this would it be ideal if it was perhaps in the future? Think so. Go ahead, Lucas. Lukas Pardo, This Priority enthusiast. Yeah. This is a, API is fine. Like, do whatever you like. We've had a lot of discussions in the 33 seat. We should just keep making progress there."
  },
  {
    "startTime": "00:12:03",
    "text": "To Alan's point, I did write a send order extension for the HP 3 priorities or whatever, like, we could do that. Like, no one seemed not bolded, but it seem to work in my head I did the implementation of it last on, and it seemed to work 2. Implementation. So let's offline, maybe and to Luke's point, like, yeah, different libraries do different things. There's another extension I have where you might want to store or it would relate in an ability to store the like, the priority at this time or this bike range was this, and then it changed subsequently through the lifetime of a stream. That's a use case that other people have found interesting, but the simple thing is just yeah, what Victor said, whatever you got buffered can change because Like, that's the easiest thing. It's maybe not exposed to it. Because they don't necessarily know how much was flushed versus buffered, but I think at the end of the day, it's good enough. To make things work. Okay. Thank you. Next slide. So continuing with major decisions and updates, So we now we ran into the problem of stream ID exhaustion and, how should the application be noted write it. So now if you try to create a stream and you've exceeded your max stream limit, It will reject immediately and throw a quarter exceeded error. There is some debate, within the group as to adding an optional constructor would let you opt in to being blocked. So you would rather just wait for the stream to be created than than have it reject and have to reconnect. And there were 2 valid use cases there. Applications would like to know that it's taking a long time because of the max stream limit versus for some other reason. So we haven't yet added the constructor, but I think there's a issue 446 for that one Then our second issue is atomic. Right."
  },
  {
    "startTime": "00:14:00",
    "text": "So this is when you wanna send a transactional piece of data where you don't want half of it arriving. So it's an all or nothing. Send This was, Yanivar's PRN implementation. Janivar, did you wanna have a chance to speak over this very briefly. Sure. Yes. So the first part is the PR. I think discussion has finished. We're just waiting for getting the check marks on the PR, I hope. But, you know, you never know. So it's not merged yet. So that's the disclaimer. So sending a So the idea is most web applications probably won't need this, but there are some more transactional applications that asked for this. So the idea is how to do this a way that, that mixes well with with a non atomic rights. So so we have this concept, concept a new method on the writer object because this isn't really compatible with pipe 2, really. So the way if you wanna send something transactional, you will instance, in this example, shows it create the introduction stream. Get the writer, wait. The writer can be ready. Instead of writer dot write, you say a writer dot atomic write. And you have a number of bytes that you expect to be sent all or nothing. And if that rejects, it so Normally, writes only reject if the whole stream has aired as a terminal situation for that particular 3, not the entire connection. But in here, we get additional semantics that you can also get and a board error if this was blocked on flow control, basically. Which is a new state the writable remains unarried the stream. Reminds unentered in this case. Unlike regular lights, right, using this mechanism, And there's some details here. If you Normally, we advise people to always await"
  },
  {
    "startTime": "00:16:00",
    "text": "when you're doing this, it kinda makes sense if you wanna make sure something written or not, you wanna test on it. But you can still if you don't do that, you should be aware that anything that you queue after an atomic right even with the regular right operator will become an atomic right because the If things cannot be sent, we basically the application has to back up to the last known good point of what was sent. And those are the semantics. K. We have question. Alan is So I my first question is, what are the what are the estimates that you guarantee? Is it just around flow control. This this is mostly about Flow Control. Yes. So if I have this, and is it stream flow control, connection flow control, or both? It would be both. Okay. As far as I know. Like, there would potentially be a possibility that even though the data could be interleaved on the wire. I could send part of this data then send data from another stream and then send the rest of the data. Yeah. So there's there's no limitation on interleaving on this one. So the other mechanisms for re preventing inter living would be send groups and that kind of stuff. So this is orthogonal to that if you if you will. So this is more like more of a, web API contract. That if this for some reason, only got partially sent. Then, we wanna hear about it. Yes, you're right. It can still be interleaved as far as I know. Although that again is the The difficulty here both with this concept and with the previous slide on send groups is that it's an API surface swimming in a sea of implementation dot find behavior. So it's more about getting the the concepts and the contracts, right, so that the user agent knows what the application wants. Okay. Yeah. It seems, I mean, sure there's doc I've not read all the documentation, but I'm sure this is something that needs to be carefully documented application developers are gonna use it for and what problems might arise or what they know, for example, they may think, oh, it's gonna I'll go in one packet. But it's the atomic rat is bigger than one packet than"
  },
  {
    "startTime": "00:18:02",
    "text": "maybe not as atomic as they think it is. Right, in terms of how the receiver processes it, for like, when it returns, does that mean the peer has acted, or it just has gotten all sent? So, there's just there's just which is fine to have any behavior. Just make sure that it's documented. Shoot. I think there was another follow-up I had on the last thing that you had said, but I've since forgotten it. So I'll go ahead and let Luke Alright. Thanks. Yeah. I'll just say, I think for most of the time in these meetings, most users seem interested in these media sending, use cases, but this would be a different use case where you wanna implement something unsectionalal and you had some application logic that relied on server responding to stream finishing stream a before it finish before it reads string b and that kind of stuff. So which could include if you're not careful people have run into, live deadlocks. Basically with the client and server. So this is a tool in your toolbox to prevent that. K. Another question in queue. I can't see who it is. It's Luke. Yeah. So I I was just gonna second what Alan says. There's there's a is a tool, but it is also a foot gun because, when you see a word like comic, you assume that the right was flushed it's it's all arrived on the other side, you know, but networking is a lot harder than that. You could be brought by cook congest control, you know, packet loss, because atomic right finished doesn't mean that anything happened actually. It doesn't guarantee any delivery So, yep, that's my only, like, feedback. It's just maybe it's, like, bike shedding. The name or maybe even the mechanism to know if you have flow control. Do a better the PR hasn't emerged yet, so there's still time to bike shed if, Well, people wanna come up with different names. And I I think, earlier as far as guaranteeing delivery, I think we stopped short of doing that even with stats and other things. For well, I think"
  },
  {
    "startTime": "00:20:00",
    "text": "because well, ultimately write only the re definitely part of the receiver is a that can guarantee that something was received and processed like, I see application itself. Right? So, that it's an out of band problem from our perspective. Okay. I think we're done with q. Oh, Bernard. Go ahead. Yeah. I just wanted to say I I always liked the term atomic rider because it's suggests that it's radioactive Let's Thank you. Definitely not the method. You should wait for 1st step. Do you have a question, Bernard, or just from that. Just a comment. Thanks. Okay. Okay. Thanks. Next slide then. Let's go Okay. Just an update on, implementations. So Chrome was 1st and, Firefox is hot on their heels. There's a good amount of green appearing down here, 73% globally, trust that number. Safari is coming along as well. So buy your developer's beers, and we will eventually have a solid green line down there. And Yaniboa did want me to point out that Firefox for Android also ports web transport, but is showing red here. Next slide, please. So we have an issue with, like, some feedback from this group on it concerns issue 559 quality of the bandwidth estimate. And you can read it faster than I can speak it, but I'll speak it anyway. We have an estimated send rate. It's the estimator rate at which data queued will be sent by the user agent in bits per second. And Victor was saying there might be some utility in exposing other attributes to to give some indication of the quality of this bandwidth estimate. And some examples there. Has there been a slow sample that's not application limited? It out of slow start? Is it the full bandwidth or the minimum of what's being observed?"
  },
  {
    "startTime": "00:22:01",
    "text": "So we have some questions around there. The primary one being, is there utility in enhancing the information reported for Sendry. And if so, which which of these signals make sense The ones here make sense, are there more useful signals and and and if we even if you agree that these signals are useful or they expensive for the user agent to generate. Look. So immediately raising my hand, because I I I implemented this at one point using the bandwidth estimate to do service and APR one thing that came up a lot is that Cubic and Reno have very different estimates than BVR in terms of, like, just even how smooth they are. Think these are good points, but I kinda think it's almost getting the lower level, like, implementation details of these estimates that you kinda need to, account for. Like, if I were to get a bandwidth estimate for cubic, I'm gonna treat completely different than the one from BBR. And these are grid points. Application Limited also makes a huge teach deal for, live video So I don't think there's a right answer there, but other than Tighter knowledge is the underlying congest control. I doubt we would go into listing the congestion controller. So how would you actually expose that in the API. You say tighter knowledge, I mean, I had a different algorithm based on if my estimate from Cubic or VBR. I don't I couldn't for my my ABR decision making, system. BBR is a lot smoother, whereas cubic would would take 40% off of the estimated bit rate after a single packet loss. Right? So the even just the smoothing function I had to use on the bandwidth estimate. Changed a lot based on the underlying implementation. Okay?"
  },
  {
    "startTime": "00:24:01",
    "text": "There's another person there. Wow. Yes. Bernard. It's mow. Oh, never mind. Mozan had it a long time ago when we did this work back in Aramcat, to, to to try to signal to an application from a congestion controller what is the usable bandwidth or what are the usable condition control parameters that the app would care about an an average send rate over a gross period of time was rarely sufficient for media clients because that average center rate is usually for something like a long lived file transfer. And not what an instantaneous you know, burst of an iframe could be right now. Or you know, what what, what can I send to the next round trip? I think something that's more in line with what can I what media frames can I actually transmit right now would be more useful than an overall bitrate of a long lived, you know, 5 seconds worth of video gop, which is what this bits per second from BBR or or or or TCP kind of congestive control is gonna give you? The transport knows nothing about media frames that you might be sending. So it's gotta deliver but the the the Transport knows what it's getting in a window transport knows agnostic. and it knows how fast it's adjusting its window and closing its window and opening its window those are the dynamics that an application would care about. Can I burst my iframe with with 100 packets right now? Do I need to pace it over you know, 10 frame times. That's the relevant thing that a web transport application would need to know Can I send this iframe or not, or do I need to paste it? And how would that be expressed in terms of API enrichment here. The estimated Send rate would not be a a long lived gross estimate of you could do over the next ten seconds. It would be what can I do in the next round trip or subinterval kinda do the next 33 millisecond frame time that would be more relevant for a media application? Makes sense. next in queue? Who's"
  },
  {
    "startTime": "00:26:00",
    "text": "That is Victor. I the draft doesn't specify which control you use, but I do not intend to ship this API until we have BBR. So it would be a BBR estimate. And BBR estimates, belief, Is he gonna do no and BBR and then quit in general in that first, I would sayings, you for pacing and DBR is especially predictable from spacing rate. So it depends if you would get from, say, API is a reasonable estimate of how much the contrition controller would allow you to send both within next 10 milliseconds and probably within next 500. Okay. Eric? Eric, can you hear Apple? Couple of things as we've been taking nuts. One of them is, I I quite like the idea of can we offer some sort of interface that is more tuned towards what are you trying to do as in, you know, what Moe was saying of of know, can I send this much right now? Or maybe even let people configure, hey, this is my Frame interval, tell me what I can send in in that amount of time. Rather than trying to do the is this out of slow start, I think that gives us some more exposure of what's going on on the underlying connection than we maybe want to expose. And so We're already kind of struggling with the analysis of the things that we do expose, adding a whole bunch stuff like this seems like it it it opens that door even further. So I'd I'd be hesitant to expose what we've listed on the slide here. I think if there's a better way to answer the question that people are actually trying to ask. Then maybe that's also more useful because if you told a bunch of random people. Hey, here's these"
  },
  {
    "startTime": "00:28:01",
    "text": "very detailed, very specific deeply meaningful numbers that you you you may not have spent the last however many years living in, if they just wanna know, Hey, like, how big is this frame that I should be trying to send right now? I'm that might be a struggle to turn those numbers into a useful meaningful value anyway. The only other thing there is to what Victor just said, I don't think we should assume that we're signing everybody up to do BBR. Web transport as much as I think we all like BBR and wanted to move forward. So let's try to not bake in those assumptions. Randall Yeah. Yes. In fact, I will echo with Eric said the, for right now, necco, which is what my mozilla uses, does not have a BBR implementation. We have cubic and Nirino. So, you know, I don't expect that to change anytime in the near future. We really do not wanna bake in a specific congestion control thing. There's a something in one of the congesting control working group on all the different quick congestion control screens that are out there right now, and their level of conformance and and compatibility. So I and I think if there's some way to to redo this such that we are providing a way for the application to feed information about what it wants to know Like, what sort of time frame it cares about sort of So so you can get back the information it actually needs as opposed to trying to infer the information as Luke was mentioning by trying to add"
  },
  {
    "startTime": "00:30:03",
    "text": "smoothing and so forth to try to try to figure out what what what's happening behind the teams. It'd be, I think that'd be preferable. Design the API may be interesting. Thanks and Victor's next, and I've cut the queue after Harold to keep us on track time wise. I actually have 3 things to say. Juan is well. Thanks for keeping us on track time wise, Victor. Yeah. Well, you may not bake and then, yes, I'm of congestion control, you do, but that says if you try to use real time media with cubic, I think you will run into the limitations of that. The second thing I wanted to say is, I did originally proposed, I don't remember where in many, many issues it is, but the original API proposed once you enter a time frame and some frac from 0 to 1, which if I called the confidence parameter, there, and that would return you a number of how many bytes you sent, and I'm saying people did not like it because they had no idea how whether Seth was useful or not, and Seth eventually got simplified to the bandwidth number. I don't mind to bore a complex API, but that's just what happened. So searching is that All of the issues we just discussed are mostly are sunkenal to the ones that is being asked on the slide. And the quality of the estimates of flight is not about whether it's Cubic or Reno or what congestion control you use because all congestion controls fundamentally suffer from the same problem, which is you do not know how fast you can send until you built your channel to its full capacity and the question of the signals"
  },
  {
    "startTime": "00:32:01",
    "text": "and the signals because that's our being aux. Gear or the the signals that's what's indicate that that has happened. Like, Yeah. I I think the unfortunate reality is that congest control algorithms are different They, like what Victor said, they they all predict the future differently. And how well they predict the future matters how reliable you think their prediction is, like, example is, like, I can just control algorithm that works better with application limited bandwidth is gonna be better for live media, and I can trust its estimate. Whereas if I if it's if I learn it's using something that's doesn't work with application limited. Then I just can't trust it. I I have to I have to rally, you know, I have to have the the estimate because, I take cake or whatever. So I think the unfortunate reality is if you don't expose something like a congestion control algorithm, which which I don't think we should do, there's just gonna be user agents straight I'm gonna have to try and figure out a browser using and then, like, hard code that Chrome's currently using VVR, so I can, I can make better decisions about it? So I I don't think there's an easy way to do I don't think that we can expose all these stats. It's either too complicated or not good enough. I don't estimating the future for bitrates is really hard. Especially led media. Thanks, Luke. Who's next in queue? Bernard. Yeah. I wanted to follow-up on what most said, which is You you have to think about what this information is gonna be used for. I mean, typically, the application, you have a couple of decisions to make."
  },
  {
    "startTime": "00:34:01",
    "text": "One of them would be what you set the encoder rate to, which is kind of the average rate But keep in mind that that average is a is is isn't necessarily necessarily what you're sending at a given moment. Because that depends on whether you're sending p frames or or key frames. But the other thing is there are algorithms that can respond to the immediate bandwidth available, like per frame could be So that's a little bit of a different estimate. That's the number Mo was referring to. Can I send right now? So they're they're different they're different numbers in the meeting does matter. I do like the idea of knowing whether your application limited or not because that might Cause the application to send probes to to try to figure out what the real available bandwidth is. So that's useful. Thanks, Bernard. I don't love this drum. Is this all? Yes. So coincidentally, just been working on the proposal in WebRTC land, we're about seeing code to transform. To expose congestion control information. And, when analyzing this I got out for the video case. There are really two numbers are interesting. One is what should they configure my my encoder for In terms of target bit rates, Can I send the frame that I currently have on hand without causing trouble? And the first translates to estimated bandwidth. And the second translate Actually, it's a buffer depths. I'm not going to overland my outgoing efforts. So No. One thing is that I would encourage us to exchange information so that we get something that's roughly compatible with between the two specs the other one is that"
  },
  {
    "startTime": "00:36:00",
    "text": "I'd like to expose signals when we have a clear idea of what the application will use it for And if we don't know what the application will use it for, in the case of exposing the name of the of of the congestion control a written shouldn't expose it. Okay. Thank you. That was good feedback on this. Let's get to our next and last slide from w3c, I know behind time here. Second issue we would like, some feedback on, so This is an issue about allowing the application to hint at the number of concurrent streams it would like. It was a use case that came up where someone wants to create 10,000 streams. And it was taking them several seconds to do it because the user agent would renegotiate max stream limit it only do it a 100 at a time. So it had no idea that the application was trying to 10,000, and it took a while So there's a proposal here, for an API. We get a a a medal for a really long method name there, but, it is what it sit is inside the can. So the idea is should the application be able to hint to the user agent on the max number of concurrent streams that it would like So these are only hints. They don't have to do it. And the questions are should the what should the user agent use for default values? And secondly, should we clamp these? Like, is 0 to a 100000 a reasonable range? Or is that crazy? And could we go with something imprecise in the low, medium high bucket type approach. So any feedback on this question Hold on. I need to open the queue. Okay, Martin. Go for it."
  },
  {
    "startTime": "00:38:01",
    "text": "Making it configurable. Some reasonable. One question, Is there a way to to see what the service limit is because if you are at the, on on the client side trying to open that many screens, There's no current API to see a service. There's no, like, max concurrent. API, unless you're proposing that one exists. I'm I'm just asking. Okay. Yeah. Yeah. Would you like one? To exist, or or or Okay. Martin said maybe for them. But that's okay. Sorry. Me to jump in here just to try to answer. So far, what's on the slide is from the server to client direction. So it wouldn't impact how many screens you could, create client side. Although there's So, there's a slight difference between whether a stream was created by the server or a client and versus and what its direction is. So there's some you know, for bidirectional streams, for example, it gets Tony Cool. And I think this is the end of your slides. And seeing if flights Yeah. That was inconclusive feedback, but, nonetheless, appreciate the time and the opportunity. Thank you. I'm just thinking we had a lively discussion on congestion control and tomorrow morning for a session, we have a congestion control working group meeting. I see one of the chairs is in the audience. Eric, do you have thoughts? Yeah. I think, I'll also do it. Was saying about WebRTC. And it would be good to make sure that, like, we're all exposing the same things in general. So happily, we now have a venue where we could have a cool discussion like that about you know, We've got all these different congestion controls and what kind of thing would people need to know we were exposing an API that would help both media and other use cases, know what they actually need"
  },
  {
    "startTime": "00:40:00",
    "text": "so that that would probably be a good thing to to bring up there both on the mailing list and and anybody's interested out and we can try to see if there's a ton of We Awesome. Thank you, Eric, and don't sit down just yet. But, yeah, if anyone has thoughts on this, please email the CCWG list. And, You can even offer to present tomorrow. You might find some time on his agenda. We'll see. Just take it off the Alright. Are we sweet? Alright. Let's do some next slide. So we've made a couple of updates to H2. Bunch of those have all landed. We've been closing things out getting very, very low in our issue count, which is either a sign that we need more people to implement or that we're almost ready. In this case, I think we've got a at least one implementation in progress, but it would be nice to get more people to and hopefully through doing so, we discover that we are indeed almost ready. So a couple of issues to talk about today. Hopefully, these are all pretty straightforward, and don't take a ton of our time. But the first one is 95, which is drain web transport session. So H2 only has the w t stops sending. Capsule, which is not bidirectional. So I can say, hey. You stop sending things to me. But that doesn't mean that I can't keep sending things to H3 has this that we defined called drain web transport session, which says, like, Why don't you stop sending me stuff? And I'm gonna stop sending you stuff. And I'm not willing to drop this on the floor right now because I'm trying to be the fact that we were just having a very nice conversation, and I'd like to complete that conversation. It'd be nice if you didn't bring up new tops just because I'd like to go do something else Familiar from the hallway from the last minutes. So, one of the things that that came up, and I I think this came up in some of the w three c discussions was that we would kinda wanna have this compatible interface"
  },
  {
    "startTime": "00:42:03",
    "text": "with with H3 so that we can offer similar constructs across H2 and H3. And I think one of our design principles here has, long since been being consistent across the 2. So next slide, please. The proposal here is This is the capsule. Is the current h three text. Can see there's a a H3 there in the middle that we changed the 3 to a 2. But otherwise, if this looks good to people, I think the proposal is basically, lift that into H2. Conveniently, we don't really actually have to define the capsule twice because These are capsules, and we can refer to them for multiple places, which is proving some doubts to be not as applicable as we thought they might have been. Capsules are coming in handy in this case. Do we want to just say drain web transport session is a for H2. It also works. It plays well with go away. It plays well with stop sending. There are some distinctions in H2 versus three land about the way that go away and stop sending work, but the nice part about web transport is we've said it's gonna pretty much like the the quick slash h three. Does in the way that we handle the capsules. And so this is another place where we can be consistent and happy Anybody have thoughts opinions? I'm seeing a few head nods is thumbs ups are nice too. Sweet. I see at least one to all multiple funds up. Super fun. Cool. So we'll lift that and and pull that in. And I think this is a this is a big part of getting us to be actually consistent across, both h two and h three. Speaking of which next slide, please. This is not necessarily the last time that we're gonna talk about this. It's certainly not the first time that we've talked about this, next slide, please. This is a statement that I've think we've all kind of agreed on and we've certainly talked about when we wanted it to be consistent between H2 and H3, but I just wanted to, before we go and I know that that already started doing a lot of this."
  },
  {
    "startTime": "00:44:03",
    "text": "Especially as we talk about this from the W3C perspective, especially on my end, trying to ask people implement this. One of the big questions that we get asked is like, okay. Great. But we have WebSockets and we say, well, No. Not web sockets. We don't think web sockets are the thing you should be using web trans because, obviously, you get all these nice new modern features of multi streaming and and of that other stuff. And the question is, okay. So, like, are you gonna come back next year and do web transport fee 2 that is this completely different thing, and are we gonna need to do one of these every time we have a new quick or a new H Four or whatever else is going on. And I think the claim that we've been trying to say is is the one that's on the slide here, which is that web transport transport agnostic, and we think that these concepts of unidirectional and bidirectional streams We don't get we have not yet talked about a tri directional stream, so we're probably in good shape for a while. And so if we're gonna do something that is truly transport agnostic, we think that there should be a transport concept probably in the web transport overview document. That represents the things that a web transport underlying wire protocol needs to provide in order to be used under a web transport API. And concretely, the statement that we're saying is that H2 and H3 aren't the end of it. That someday you could add a a third thing And I know there have been some discussions about that third thing maybe being web transport over WebSockets or H1 or all sorts of other fun stuff. So you could do STTP. As long as you can provide those same things that are in the web overview doc, then we think we're in good shape. I think that's where we have been as a working group, but I wanted to make that a very explicit thing that we decided to commit to doing because I think it's gonna take a a non zero amount of hopefully, mostly editorial work in the overview doc, to make sure that that's in good shape. So I don't see anybody hopping into the you to be super upset about that. Which I think sounds like we're on the same page."
  },
  {
    "startTime": "00:46:03",
    "text": "Fantastic. I see multiple thumbs up, but very nice nod. Next slide, please. be clear, I'm not rating the quality. If you're not, I just appreciate it. To Alright. The, next issue is about Flow Control violations. We've got some text from the draft here, which you can tell because it's in a fun font. And it says that the server must not close the connection if the client open sessions that exceed a limit because they don't they agree on how many sessions are currently open. That's totally true for H3. But in H2, I think Martin Thompson pointed out that what we kinda do actually. No. And it kinda is pretty consistent. And, like, yes, asynchronous, sure, but we have it defined ordering for things. If we go to the next slide, please. The proposal here will will come at the end, but the the background for the of this discussion is Part of the reason that we were trying to say, Hey, be a little bit permissive here is the idea that somebody could promise you some flow control credit and then take it away. And we'd originally said, well, it's really easy if we just don't have to worry about that. So let's just say you can't take it we do that. In some cases, we do that for web transport max streams. Farm, I would encourage people to go look at the, issue associated with Max Streams and and some of the editorial, how do we refer to that, in short human terms, you are saying that you are only ever allowed to have opened 10 streams in total, and that includes both closed and currently open streams. And so a lot of people, I think, take that number 10, meaning that if I close stream, I get to open a new and the answer is no. If you close one stream, have to update it to And I'm guessing that those of you, whether or not you have read the draft, maybe slightly surprised by that. So help us please find the sentence or 2 that would have made that the most clear perhaps that example is a useful thing to put in the document."
  },
  {
    "startTime": "00:48:03",
    "text": "But if you have any great ideas on that, let's talk offline. Coming to our 2nd bullet point here, H2, you can just update your settings and max streams is a capsule where we can cleverly define it so that it's impossible to go backwards. But for the settings value that comes in the web transport max sessions, you could very easily say, I I allow you to have 5 sessions and then turn around and say, I allow you to have 4 sessions. Those are act, so everybody does in fact have a consistent view of what's going on on each side. On So if we go to the next slide, the proposal here is that if you lower the limit, if you say, hey, you could have had 5 sessions, you say, great. Thank you. I will open up five sessions. Isn't it nice that we're having five conversations at the same time? And then I turn around and I say, no, actually, I think you can only have 4 because we can agree on whether or not I know that you told me it was 4, so I'm not unintentionally violating the spec, and and breaking your rule, We can allow the person on the other side to gracefully clean up that session and get rid of it as as they go. If I wanted to aggressively kill that session, I could the stream that that session is operating on. So I as a a recipient of that session as a server can always protect myself just tell you to go away. So this is only four cases where I'm trying to be nice to you. If we write a single statement, which is that the server checks this limit only when a new session is opened. I think all of the other issues go away. So if anybody who believes that they don't, now would be the time to speak up. So you have time. I say, hey, you can only do 4 now. And I say, oh, I'm so I was really enjoying doing 5, but I'll wrap up one of them. When I close that one, I can't then go open a new But no one's penalizing me for continuing to use it. If they need me to really, really stop, there are ways that they can say you actually must stop now."
  },
  {
    "startTime": "00:50:00",
    "text": "All the way up to removing it out from under me. So, when I go to open a new session, I simply check what I think current value of the setting is. If the setting says 4 and I say, oh, I already have 4, then I say, great. Can't open a new session. That just propagates back up to the, hey, you you hit the session limit. You're not allowed to go any Thank you I'm seeing a nod. Otherwise, apathy mixed with general Acceptance, Sweet. Alright. Shrugging thumbs up. And outs is in the queue. Thank you for dispelling the apotheologics. Oh, you don't know what he's gonna say yet. Hi. Hi, Alex from house key Google. I have a question. Which is Do we want to do something special for the sessions in training state? Because It occurs to me that with the fix that you are proposing, one possible edge case that we can decide we don't care about is my limit was 5. We lowered it to 4. I've started draining that session. I would be below 4. I obviously can't open anyone now because forest of a limit. We join another session. That session is still open. I want to open a new session. The draining session still counts against me. And that could be sad for handover. You're you're trying to remove the incentive to not gracefully drain things just to have room to keep going with new things. Essentially. Well, I'm saying that I I think that there is a perverse incentive that the draining sessions count against you here. And I don't have a good answer for whether or not they should or they should not. I will double check. Have a dim memory of some text that indicated that they already didn't. But I don't remember if that was in a previous revision, and we've already taking that out or if that is still there. But but even if that is there, then you have the converse problem of What Symantic do you want there to be a if you have a fairly large limit"
  },
  {
    "startTime": "00:52:00",
    "text": "that was lowered fairly severely. They're all now all now in draining state. But you're still using them. That's also bad. And we don't want people to then up on the draining industry. We we don't want jettison. Long lived the draining lingers. And there's also, by having a max sessions as a as a flat value, that means that cycle through them really, really fast as well, which is an interesting, Yeah. And that that's a variant of the recent frame shift attack. Right? So I think that that Well, I know the room is currently all nodding in agreement. I feel like this is actually an area we might wanna think more about. Thank you. Yes. I will I will go double check, what the current the latest version of the text says for for whether or not draining counts. But we can we can that on the issue as well. A very good call. Thank you. Alright. Next slide. Victor. Or Victor's voice from the sky. Alright. Thanks, Eric. Oh, it's me. Okay. Oh, We only have roughly three issues. But all of them are very interesting. The first one is probably one of the less interesting. Is So web transport is meant to be mostly API compatible with WIP. And that means that you might want to take an application that runs over real quick. And port it to web transport And one of the problems we've encountered is that quick application. Sometimes they specs that you should be able to do LPN negotiation. But there is nothing like that in web transport currently. So the proposal is to added, using, it it it it"
  },
  {
    "startTime": "00:54:03",
    "text": "ALTN header that's already defined in RC So in district 39, then that request. The original is this came out of MLQ Working Group where we were trying to use this for version negotiation. Of Do people have opinions about this proposal? Okay. It sounds like no one would judge in the oh, I see Mike Bishop raising a non hand. That's alright. Come on over. And if folks have opinions such as thumbs up or sundown. You can also show that to us. That's how full even if you don't feel like going up. Server unreachable. Hello, server. I'm David. I think this is fun as a mechanism. It feels a little weird to be seeking things in as ALPN tokens which are not protocol if you could speak over TLS. Directly. But, I mean, you can We've already twisted it to say that ALPN defines a protocol stack. And so your protocol stack could be web socket over h whatever over over to y'all. Thanks. So I'll write down an action. I am to clarify at least in the document whether there's requirement there or not. But that's a good point. Go ahead, Luke. Yeah. I'll just, on the mark side, this word, help the handshake quite a bit. Because right now, we do"
  },
  {
    "startTime": "00:56:01",
    "text": "we're also trying to support native quick at the same time. So the native quick folks wanna do ALPN And the web transfer folks like, now we need to do an extra round trip for version negotiation. So it would be nice. It it is nice to stick stuff in the connect request. Is what I'm saying. And this is seems like it's well scoped that we could put this without arbitrary headers being a thing. Tape tape tape tape tape tape tape Magnus Pestland, So it's this formally ALPN, are you expecting people to apply for them? Because, I guess, I mean, even if the registry for LP is another next review is is people in is is the Ayanna register experts here gonna all get what you're proposing here. It references existing Philippine registry. Since the idea is that some of those be protocols be defined sort of real quick. I would expect them to register those. But but Lucas Pardo. The we kind of already abusing the AILP then registry So the more beef won't hit. Maybe it's a time to to, like, revisit whether we we do something bigger, but that's a discussion of within the IETF, not just this working group, so this is the most pragmatic option, I think, even if it's not, like, the the purist good one. It's If we were to say in the spec that, like,"
  },
  {
    "startTime": "00:58:02",
    "text": "Folks are encouraged to register them and leave it at that. Would that be kind of Alright. Because I think that's kinda conceptually similar to what quick does itself. Jothenics. I'm so are we asking the w three c to provide an API for this, and what do we think web developers will do with that once we give that Luke, you wanna take that one? I think you had an opinion from the mocks side. I'm sorry. Can you repeat? He was asking if there was gonna be an API here from the w three c, which think the answer is yes. There's an issue open in W3C. And then what would web developers do that and I think you had a use case. Yeah. We would definitely need it in W3C, so I can take that up Eric, can you hear Apple? Do find the idea of just handing people that arbitrary string to encode whatever they want in. As a web API. Mildly horrifying. But who knows? Maybe we'll find a way that's less less grows to do that. The other point there, I think, is is to what Mike was saying about, you know, hey, we want this to work for any real ALPN, in theory, the ALPNs that we're using with quick are for some purposes real. And so, yes, web transport has a shape that is more complicated than a single TLS stream. But but but but but but hopefully web transport providing, essentially, what quick is providing for the web means that anything you could run over real quick, you should be able to run over web transport. And That's kind of the point. Lucas Pardo. The so I've seen interrupt issues with AL PNs, where believe to be strings and then not the by"
  },
  {
    "startTime": "01:00:00",
    "text": "strings, So you, they get casts and then things fall over because of false expectations in them. It's a bit of a shotgun. The shore Should be and and sorry. Then to answer so that's that point. To answer David's put we could encourage people to register stuff. Dot Aiana registry is also not maybe geared up for provisional registrations versus permanent ones, etcetera. Have to remind myself, but we don't wanna Doss the designated experts for that registry. Trying to use things that they maybe don't anticipate. I can't recall for all of the different quick sorry, HB3 visions that we had for all the drafts if we tried to register any of them. Apart from the final one. The No, Mike, Mike Bishop says, no, we didn't try to register any apart from the final work. It's fair, and I just pulled it up. The registry's expert review in case That's relevant. Victor, Yes. Yeah, the the w freesheet, I originally filed that they of double the free c, and we the discussion or remember was that people were okay with that success day. I think it's an alternative here. So it's either ALPM or we just introduce ahead, or is that called NatalPM. Make it. Like, a pro sub protocol is like what WebSocket already does. It would work the same. Functional app. So is this the same, but the question is are we implying that this is, like, a talk ins that should be in a registrar or Next Victor might go ahead Mike Bishop again, this server is reachable. So"
  },
  {
    "startTime": "01:02:00",
    "text": "We're already directing I've If I'm not incorrect here, we're already directing the web transport session to a resource. Does the resource not provide sufficient granularity as to the thing you're trying to talk 2 end. Because it naively, I would have thought if you need to speak multiple sub protocols, you would just put them in related resources. Almost you can put you can handle the request part, but you don't get a response. You would need to put it in a body and now you have this problem where you have to figure out which of your data streams is the ones that has a response, etcetera. So that's the complications. So, basically, what we're saying is I can speak mock version 23and4, and I wanna find out from the server which ones it speaks. Yeah. Okay. So the other flavor of this might be a server declaration or or or or protocols are. That you could discover as part of your H3 setup. Yeah. HDP connection. So speaking as chair, what I'm seeing is they're folks exciting on having this feature, and they have a use case for it. I have folks who think, this is kinda gross. Does anyone feel really strongly here? Like, our our folks strongly objected to this, because I think as far as the IETF layer of web transport goes, all this would be would be pay. Use the LPN header from 676. 39 if you want that, and then it would be working w 3 c. So the like like So I'm inclined, you know, based on what I'm hearing to say, unless someone's strongly object maybe this goes in. So does anyone strongly objectively speak up? Magnus, go ahead."
  },
  {
    "startTime": "01:04:08",
    "text": "So from an experience with the MIME types and RTP payload formats, I would maybe suggest that you actually divorce actually a a LPN. Use the same semantics, all these things, but call it your own thing here. And have you need a registry to find a new registry, but To me, it seems like this might potentially us be something you use between application and a destination to say, okay. Are we agreeing Oh, that people of Canada and you will control both endpoints and saying, it's just effective. Have I reached the entity to actually do this? So it might be worth to not saying you're not might not even need a registry for it, but hold something else, but make it like a LPM, but I think it's fun. I ever just avoid overloading the, actually, LP registry. Okay. So the proposal for Magnus is to keep the concept, but disconnect it from the LPN and the existing registry. Does Does that sound workable? Fucks, Eric, usually stand up just to say plus 1, but in thinking about how much I act I don't actually cared versus it was just mildly annoying. Given that it's not at all hard for us to have something that is just called a slightly different thing and doesn't stop on that list. That gets us basically back to the sub protocol thing. It seems like that gives us all the things that we want and avoids the issue entirely. Awesome. Thank you. Bernard. Yeah. Just referring to what Magnus just said. I I think if you do, when just need to be real careful about the ionic considerations. Just remember, Ayanna only does what we tell them to do. They can't read our minds. Oh, Cool. That that makes sense. Yeah. We'll"
  },
  {
    "startTime": "01:06:02",
    "text": "sort of errors to get it right. Okay. Does anyone object to this plan? Of doing something like ALPN, but that is not ALPN. Alright. Well, I'll confirm this on the list as per usual, but thanks folks. Victor, you're muted. Oh, I have the service. Oh, I I was trying to forecast this. I'm sorry. Okay. Let let's talk about Floken role for all And then how do we do flow control and web transfer? So web transfer flow control problem is a bit different from, It's a quick flow control problem. So in web transport we already have is a global flow control for the entire quick connection. And I'm going to talk about Strinzel in this because they're easier to reason about, but this also applies to next. Data as that's global. The problem is roughly is that we already have a mechanism to limit total number of strings. But the per but we have multiple web transfer sessions that are independent and will attempt to open strings and we will also have a CTP request, which also need to existence of connection for everything to work. So roughly is the premise is how do we let how do we ensure that with things that existing limits that we have for Mac strains, we are able to fairly split up up the strings of next slide."
  },
  {
    "startTime": "01:08:00",
    "text": "So the roughly mental model here very simple. It's you have streams that are associated with web transport sessions and storm or notes at our HTTP requests. So if you have max concurrent streams for a given connection, it would be should be max requests plus max sessions times, max streams per session, and you can kinda use that equation to figure out what you what some of it should be, and we already have max sections in the protocol, we time to have max concurrent streams, but not really since max trains is only the current indication that's appeared might be doing the thing where ramps up exponentially or something dependent on the limits or multiple schemes. So what we could do is we could send this explicitly next slide. So the proposal I have This is for strength limit, but it extends for others. Is, the server sends you a number that is This is how many masks concurs streams I expect to let you graph get flow control here at it. And here is how many max streams per set your web transfer session, you should allocate. And now that this is not a strict limit because for the strict limits for safety, we have the web transport global thing. But This is still useful for the client because as long as the client fully is this limit, it can prevent the sessions from using up all of the strings. Next slide. So PR is a simple example. We"
  },
  {
    "startTime": "01:10:00",
    "text": "use the max stream limit of 100, which is I believe So RSU 91114 actually suggest that number. And we say that here's the hands that you should allow it most 30 streams per session, and it should allow utmost free sessions And what this means is you have 30 times free equals 90 web transfer data streams, and we still have headroom for to initiate a fee request. And as long as the browser would limit. On its own side, you will not run out ourselves. So that's the proposal. Next slide. There are some alternatives that are probably all of them are in Sierra, more complex, to implement. One of them is to to quick style flow control and makes the server ensures that everything is all cases fairly. And tell what's the limits are to disappear. And so second one is, you just remove the global limit and then just for every session, you set your own limits, but you need to change the process for the app. The south is roughly the proposals. Now, we already have people in the queue. Martin I like the idea of hints. I like the idea of sending hints, I'm a little bit concerned that you can't update these hints, over the lifetime of the session. The way that quick flow control works and that also applies to streams is that a server can say, like, I I'm building trust with this client, and I'm now granting a higher"
  },
  {
    "startTime": "01:12:02",
    "text": "our stream limit because I I see that the the client is using it for something useful. That's not possible in settings because in each, three settings can only be sent once. So we might wanna consider putting it into a capsule maybe because then we can, we can send a new hint during the connection. I think this is valid. So one thing I would notice that There are some flow control limits and quicks that are not updated like, there's no to place any show strength or control window. And when we discuss that, we agreed that it's bad that it's not updatable, but if you do find it useful to update, we can always extend that. So, if there are people who are practically interested in this. This is a possible extension, but not sure about How exactly useful it is. That's just fine. Luke, Yeah. So these examples are with max streams, which Honestly, it doesn't really matter. You could just set a big limit I'm a little worried about max data. And whatnot. Like, when you start multiplying by the max number of sessions you could have, and the max data per stream and whatever these hints you get a big number at gigabytes. I I I don't something tells me that just blindly multiplying and setting a large limit like that and evenly dividing upon all streams is dot I I don't know. It it doesn't seem like a useful hint. I I don't know. I think I think just need to rethink the max data case a little bit And even assuming that all sessions have the same behavior, Like, you could have you could have max sessions 10 one of them could only use 3 streams. Another one could use a 1000 by just saying this hint, it's kinda like setting an average. Something feels weird."
  },
  {
    "startTime": "01:14:03",
    "text": "Two thoughts. One Doing something that's a hint is a little bit like we're gonna do all the work for it, than not actually do it. So I'd be tempted to unless we're really struggling with the enforcement. And I know we struggled a little bit with the enforcement, but I'm not sure quite 55 We really wanna abandon that effort is. The other one is is very much what what Luke was just saying. When we start talking about data limits, 1 of the most important things that we do in our current implementations of H2 and H3, that we're shipping is not allow gigabytes and gigabytes and gigabytes of memory to be used. Because that's often a good way to run out of gigabytes of memory. So we end up doing things where you have a high limit on any individual stream and a low overall connection limit. And that's saying, Hey, you know, I'm willing to let any stream go up to some amount, but overall, the sum of all the stream still needs to stay low so that I don't just say, hey. I have a 100 streams open, and each one can do x amount. And now I have that times a 100. So I I think we still need to be able to do that. I don't think you can imply even multiplication like that. One of the other things worth noting is I'd our our current strategy for some of this is to allow some of the streams higher limits than others. And that kinda goes back to the comment around, you know, building trust. In this case, for us, it's It's a aren't willing to allow a hundred streams to each potentially take up a certain amount, but we don't want people who are trying to move large quantity of data, not that many streams to be screwed. So don't think you can say that they're gonna be even. We know of multiple places where today, we have deployed software that is depending on them not being the same. It's Microsoft's I'm just wondering why we need this because it's"
  },
  {
    "startTime": "01:16:00",
    "text": "just a that that clients can decide on the number. It's like dividing them. Basically. It's a hint and they're they can just to to to division, there was a problem of dividing the number. We don't know all of the number. It would if there were no HTTP requests incay questions that would have been easy. Because for instance, for server initiated by directional really, and so you can just take your float and parallel window and divided by max utterance since that's your limit. But here, we, kind of don't have Zip. We have 2 accounts as there are other things that are not web transport. Yeah. There can be some poweristics that clients decide on how to divide that because it's it's just piaristic, The server doesn't know much more than the client. In this case probably. Danton Lenox. I may maybe this is raising too many rat holes, but, It occurs to me that these same problems occur with the discussion we're having earlier congestion control windows because those are also going to be affected by, you know, disconnection sharing and invisible things that are H3 that are happening find your back. And so library those are going to be similarly hard to be applied usefully by APS."
  },
  {
    "startTime": "01:18:02",
    "text": "Thanks, Jonathan. So there's not an obvious lamb dog thing that everyone loves here, but given that this is one of the few remaining issues between us and the finish line, I think be good. Figure something out. So I'm getting a sense that are 2 probably Most direct path forwards are doing nothing in the hints that Victor proposes, does someone remember we we had a reason why doing nothing was a problem. Right? Alright, Eric. Go ahead. Yeah. Well, I mean, like, all the reasons that you want control and all the reasons that we suffered through adding those to all the other protocols that we've shipped, like, quick and H2 and everything else. We we did have another proposal that we spent a good chunk of time in SF talking about that we wrote up, which was not just hints. So I'd be hesitant to go with Hints, which seems like it has some pretty clear blockers, as our only not do nothing option. Totally fair. Thank you. Luke, Luke, And just to elaborate on why, unfortunately, I think we need flow control is, if I have a, a tab that's just deadlocked or something and it's just not extremes. I don't want that to then steal all the bandwidth from every other tab. I think it's a foot gun using pooling. If we don't have flow control, that acts like it's 2 separate quick sessions or connections. So I don't think the hint quite nails that. Anything, again, it would be a reason not to use pooling. Right? If you evenly divide your your max data across every stream session, evenly. It's actually just better to dial multiple quick connections. Thanks, Luke. Alright. I'm gonna cut the queue after Alan. Alan, go ahead. The last word No. I'll get the last word."
  },
  {
    "startTime": "01:20:01",
    "text": "Nice try. Yeah. We're gonna we're gonna use QCram and call it QPAC. Oh, whoops. Sorry. Different answer. That I'm being asked to give. Okay. I is I think the main reason we don't wanna do the like actual like, define it all is that it's just a lot of work and nobody really wants to do it, and and it it it If it's really only a problem for pooling, at least right now, I've only heard that most browsers are just not gonna implement pooling. 2. At least anytime you if that's maybe maybe a browser implement, or can jump in and say if that's actually true. But are you doing it? Oh, okay. In which case, maybe it's a it's not great for interop, but we just define the complicated thing that that that gives us, like, all the knobs you need if you're gonna do pooling. And then since the browsers aren't doing pooling right now, and they've the, like, I don't wanna argument is fine. They don't actually threat, Is that is that a way forward? I don't know. What I'm getting here as chair is that we're gonna need more discussion on this topic. So we'll circle back along stairs. Maybe like a little interim meeting focused on this one topic with presentation where every single proposal might be a better path forward here. Because I think Part of it is that a lot of us, myself included, don't have the entire state of everything in their brains, and so it's hard to reason about such things. We might, like, spending more time on it would be useful. Which won't be possible in the 9 minutes we have left today. So let's, put a pin in this one for now and move on to the next slide, Victor. Oh, yeah. My my favorite topic is, are free issues here. It's a There related and overlapping. And basically, The problem is that We negotiate web transport support, using settings,"
  },
  {
    "startTime": "01:22:00",
    "text": "and we also negotiate the exact version of web trans to use using settings. And there are 2 And some of those settings alters away in which you parse your HTTP frames. So the general rule PR one that I hope is blessed controversial is. Since the client must Wait. Until it receives settings from the server before it's at the initiate set. Extended connect or send any of the data streams. And there is another one which is the server that it should must not rate. Any data on the client bidirectional strings until it receives the client settings because that's the clients that we should know what versions using I already see people in the queue. So let's go. Yes. I fully agree with the first point client must wait until it has the settings frame. That's fine. Reasonable. You can send it in point 5 RTT data, so it doesn't cost you any any latency. I actually Just a quick point. If anyone disagrees with that point, because I think that one, everyone agrees if anyone disagrees with the first point of the client must wait until the settings. Please get in get in the mic line now. But otherwise, I think that one will get agreement on. But now, otherwise, focus on the second one. Yeah. I'm I'm I'm a bit unhappy about the the second point that the server must must sit on streams until it has received the the settings. In the in the in the in the case where we have a well behaved climb, it wouldn't have to buffer anything for long. But a malicious client can just withhold the settings for an arbitrary amount. Time, and then I have to buffer and an amount of streams that's limited by my, quick quick stream limit. Which is"
  },
  {
    "startTime": "01:24:00",
    "text": "Not nice. It's also not the end of the world. The reason I like it is not because of the memory. I have to commit to this, but because of be additional logic I have to implement on my HTTP server, implementation. My H3 implementations is is currently structured in a way that I get I get a I get a request I immediately handle Okay. Right. Process it or reject it, but I never have to buffer a request. Now, if we have to wait until we we receive the settings, I have to change logic in my implementation, which is not nice, and the thing that's even less nice is that I only have to do this because Web transport version negotiation depends on the and we only need web transport version negotiation for the draft versions. So I have to now restructure my HTTP server to to support web transport drafts, And I can then remove this once the web transport RFC ships and I I for the draft versions that doesn't seem like a reasonable same same same same same same same same same So the the the thing I said is has a few caveats. Like, at the moment, the client has to send, has to send the web transport max, max session setting, as well as the HTTP datagram setting. I think there are ways around this. I have open separate issue issues, for that. I think it should be, it should be pretty trivial. So we we we could be in a we could get ourselves in in in a position where the server doesn't have to buffer Thanks, Martin. I've cut the queue given there 5 minutes less in the session, and I'm gonna just wanna take some a couple of those to wrap up. So go for it all. I'll keep it here. I'm just gonna think plus one to everything that Martin Center. Speaking of things, I don't wanna do. I don't wanna do this. Especially not for web transport version efficiencies. Look,"
  },
  {
    "startTime": "01:26:02",
    "text": "I'll I'll make it quick. Does an h three server have to wait for the settings frame? Before it can read any requests I'm getting a lot. Did, and then I don't know why this would be any different for web transport. I don't think that's a property of H3, and I'm seeing the editors of relevant documents shake their head vigorously. Well, what makes you think we want the ladder? Mike Bishop, not not sure on that comparison, but we'll see. So the design in h three was that a setting should always be the most should default to the most conservative possible value. Their settings frame could only expand the scope of what you were willing to do. So The application here would be that the default value is I don't support anything. And when you get the settings frame, then it will tell you, oh, yeah. Now there are some other things So There are so you can you don't have to wait until you see the settings frame to send a request at all. But for example, you have to assume the q pack table size is 0. case, we're talking about the the other direction So so in this where the you get to the response, Like, Can the sir does the server need to wait for the client setting before it can parse a request and set a response. No. No. No. It cannot it has to assume the q pack size is 0. All of the capabilities that are optional it can't use until it knows whether the client supports Right. Right. Thank you. Alright. So to wrap up this issue for today. I think we're in agreement on the first bit, which is"
  },
  {
    "startTime": "01:28:02",
    "text": "the client must wait until it has the server settings to send this request. Because until then, it doesn't know that the server is not gonna barf. But we're we need to figure out a solution for the second one because some folks strongly object to Victor's proposal, so we'll keep discussing this on the issue. Alright. Thanks Victor for the presentation and everyone for the conversation. Bernard, one more slide, Okay. So we're getting close to done, and I don't wanna jinx it. But we're we're Like, the number of issues is getting quite small. As we saw, we still have a few open ones 5 like, Not that many, And so modular those. I think creditors will write some PRs for the ones where we've come to agreement. And, ideally, we can resolve the ones where we haven't in the near future. And then all of our Favorite software engineers can go write some code about this. And then kind of in parallel, the software implementation can happen with the editors doing editorial work, we have a bunch of open issues that are purely editorial and that they won't impact protocol, but we'll modify the structure of the documents, which we think are really important before publication. But aren't blocking in any way to implementation. And then once we have those, we can working group last call, this and see where we get because the next bit is really getting a lot of implementation of these in a lot of deployment because that's how we're gonna find if we have protocol bugs before we ship the spec. So All in all, we're in good shape. Please stay on top of your email and get help to get you conversations and help us by writing some code. Everyone and see you on the list. Yeah. And I guess we will schedule an interim. Debit Yep. So, let's you and I sync up on that. I think that probably the best option, but let's let's chat offline."
  },
  {
    "startTime": "01:30:13",
    "text": "Sure. Yeah. I am Hello? Yep. We make a note of that. Let me guess. So Are they, in the past at this point? Yes. One option is to remove the dates as well. Yeah. No. And I think we're getting across. I think setting, you know, sending it to for next summer. Right. Yeah. It's just that during the future, not in the past. Those that part done Yep. Cool. No. No. Yeah. I can take a look. I just forget about these. Hi."
  }
]
