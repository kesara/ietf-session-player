[
  {
    "startTime": "00:00:10",
    "text": "okay welcome everyone um to oathwork group um at iatf 114 in philadelphia so let's get going we have packed agenda so this is the not well please make sure you're you understand this this governs everything that we do here at the ietf so make sure we understand what's going on there so we have the um one official session today this this this is the one we don't have another one as typically we do but this time only one official session but we have three side meeting sessions so the first one tuesday tomorrow at 10 the second one in wednesday at 10 and the third one is thursday at two all of them philadelphia south which is just that the room across here on the other side a quick workgroup update uh um we request rfc in 9207 was a published so congratulations to daniel and and co-author um um rfc editor queue has that a jwk thumbprint uri and a jwt response for a token introspection and we requested the the publication for there are documents rich of authorization requests um roman is not here he's gonna be in and out but he's aware of it and he's gonna work on it okay we have packed agenda as you know a step-up authentication victoria and"
  },
  {
    "startTime": "00:02:01",
    "text": "brian will talk about this selective disclosure jot daniel and christina will talk about this for 20 minutes and then daniel give us a quick um update on security bcp um and we'll talk about 2.1 and browser-based apps i will be talking about multi-subject chat um hersh will be talking about talking theft kelly will be talking about token and identity chaining and the a tool will wrap it up with the rpc security standard as you can see the time allocated for each topic is really short because we have really long list of topics if we need to dig deeper into any of those topics we will dig deeper into those on the side meetings right so based on what happened right now will kind of adjust the side meeting agenda so side meetings uh the main topic one of the main topics the deepap shepard open issues we're trying to push this forward and finish it so there are a few few open issues as part of the shepard's review and then brian will walk us through this and hopefully we can adapt those this up soon um uh conformist tests and sdks daniel and joseph will have will have a discussion around this if there's we're attempting to do something about this again as i mentioned we we have we probably have some deep dive into some of those say topics that we discussed today um we want to have a few open discussions around a few topics for example do we need a new rfc to for for a job to update the jot based on our experience so far um brian did a fantastic presentation during game identiverse and i think we have lots of lessons learned from that so maybe it's time for a new rfc when i discuss post quantum"
  },
  {
    "startTime": "00:04:01",
    "text": "tool um so just open discussion and any any thoughts about this what could be done here and there is some uh issues around oauth work group perception and some some people find that welfare group to be not easy to deal with so i would like us to kind of have a discussion and see how we can be a bit more welcoming and and accommodating for for people that are not from the close tight kind of group that that typically attends the meeting so that's it for me any questions comments if not i will ask vittorio to come to the mic here it's too complicated i just like to be rate you and ask you to move slides for me will you be so kind sure thank you all right good morning um in 10 minutes i'll try to update you on the work that we've been doing with the off step up authentication challenge protocol with brian can i have a next slide please all right agenda is ultra fast i'm going to refresh for you what we mean by step up and what's the scenario we're trying to solve their proposal that we have i'll give you an update on what we changed since the last time we met in similar circumstances and then i mentioned some of the discussion items as rifat mentioned we will probably have to deal with the q a in the uh extended meetings because i"
  },
  {
    "startTime": "00:06:01",
    "text": "all right thank you so resource servers can rejective tokens which allegedly are still valid for many many reasons but typically they will have a black box like risk engines that evaluate that that particular token is not enough for the operation that is being attempted or uh there might be things in the call in itself so beyond the token that calls for different circumstances like for example uh i'm trying to buy an item which is very pricey and it would be better for a very sour server if the token would have been obtained using a higher level of authentication am i speaking too fast yes no maybe i'm just trying to stay in within minutes okay so and typically in fact what the resource server wants when they reject the token is a fresher token or a token that was obtained with a different usually higher authentication level and today there is no guidance for the resource server to tell the client of a reason for which they don't want such a token and there is also no in off guidance for the client to communicate to the authorization server uh that they want something different next slide please so our proposal is ultra simple we're just extending the collection of error codes in 1650 with the new code which is insufficient user authentication and in the corresponding header we propose to send back from the resource server to the client parameters that are already defined in openly connect acr values and max age which basically allow the resource server to ask for particular authentication levels or a certain fractions of a token"
  },
  {
    "startTime": "00:08:00",
    "text": "and um and then given that we already have these uh in exitability access tokens we want to be able to embed in the tokens the indication of the authentication levels that have been used for obtaining such a token and we we have already visited vegetability access token profile we want to add visa to introspection and here there is a ultra complicated back and forth in the past you got the animated version if you want you can get the recording from the last time here don't have time to do this but super high level basically you just have the client that he's a resource server using a token which you cannot read but it doesn't have acr um then the resource server replies with a new error code insufficient of educational user authentication and includes in this particular example the desire the acr value we could have included in a max age as well but we didn't this is just one example then the client gets these parameter and goes back in lag free to the authorization server repeats the request for an access token and includes the desired acr presumably the authorization server does whatever is necessary to meet that acr level then sends back a new token and then in lag five you actually send the new token as you can see there is a little red thingy that red thingy is the new acr and then happy ending we get to 200. so that is the substance of our proposal and of course i'd be remiss if i wouldn't point out that to the wonderful wonderful graphic comes from the art and the genius of brian campbell [Music] and uh what happened since 113. well first we got"
  },
  {
    "startTime": "00:10:01",
    "text": "voted in now we are in the island and um this is an actual work item which we are working on as a off-working group um philip and peter have been a very very faro especially peter thank you peter um in describing a sec mentioning their comments on the scenario and the proposing uh improvements which we tried to reflect in particular peter pointed out that step up doesn't always mean step up let's say that the token that you obtain after the step up might not be a replacement of the token you already have and so we clarified these in uh in the spec and philippe mentioned that it would be nice to have a mechanism to signal the fact that the authorization server supports this particular scenario and so that's what we have done we have extended the authorization server metadata with one element that comes straight from openid which is a acr value supported and we established that the presence of that parameter indicates that the authorization server supports these specifications now the tricky thing which we'll mention also at this um in the discussion points is that by using that element we imply that the authorization server supports both acr values and max age in the request parameters and it's not immediately obvious because the element acr value supported only talks about this acr so these should be a bit of a discussion as in if we believe that we need something more explicit for acr or if we are okay with the current settings also we extended the example so to also feature max age and uh we fixed titles and uh i really liked the joke in the acknowledgments but unfortunately we had to start putting serious acknowledgements there so now it's gone but there are the people that helped"
  },
  {
    "startTime": "00:12:03",
    "text": "great here we're displaying a great trust in the future we talk already about potential london discussions what's left to uh to do well um there is the ayana section which of course we need to fill up because we want to extend uh parameters also for introspection and um we have to add the security consideration like all the non-normative have a big tbd so we have to do it um and then there is a thing that i just mentioned like is it okay to just use this cr value supported or do we need something more explicit and here is the uh interesting part this pack as it exists today already does the job that it's meant to do like there are no missing parts and we are getting indication of people adopting this thing already uh like uh there was a comment in one of my linkedin posts in which someone said that the department of health in norway is already adopting this thing as is so i think that uh it's important for us to discuss this spec so that if there are things that people think should be changed or removed or added the sooner we do it the less instant legacy will have because if we don't again this is something that the people deem useful and a problem that people do have already today so they are using the spec in this form so if you think that there is if there is something in respect that you don't like please make sure to to chime in if there is something that you believe should be in scope and you want respect to address please chime in because otherwise we risk going really fast through a various stages and then be close to final and you didn't have a chance to try it so i think that's pretty much it"
  },
  {
    "startTime": "00:14:00",
    "text": "perfect thank you vittorio any quick question comment to victoria that's one of those okay perfect thank you vitor daniel i think or christina hello you want me to drive from here do you want to drive um i think you can otherwise i messed it up like i did last time so okay let's try it go yeah thank you very much uh also for scheduling this um we're presenting on new work uh that's called selective disclosure disclosure for jwts and we in this case are christina and i um if you have not seen the draft yet um that's what it's called you can scan that um if you want to see the latest version in the git repository that's you um [Music] so much for the intro think the next part is yours yeah um we'll quickly cover on why do we need select disclosure for jots um i'll give an overview of the mechanism we're introducing and then daniel will go into kind of deeper dives and features we are enabling based already on some really good implementation feedback coming in if you can go to the next slide so really quickly um on the left side you have the current flow where mainly jots are being used right now where arguably there's sort of jobs already selected were disclosed being transported and what i mean is the claims included in the jot is what user has already consented to so this is very different from this emerging new model which has been really prominent in with"
  },
  {
    "startTime": "00:16:00",
    "text": "our credentials world for example or mobile driving license world but not only limited to that where the issuer is issuing a credential to the user with maximum amount of user claims like let's say issuer issued 20 claims to the user and when user is presenting that claim to the verifier verifier is different very far as would request different subsets of those claims issued by the issue is the issuer so it's unfeasible for the end user go back every time to the issuer to get those credentials have only subset of those claims that does not that goes against the promise of decoupling issues and presentations to have all these new features such as um yeah and so this is the main idea when it flows emerging flows at the corporations and presentations we need the ability to use one jot sign by the issuer and then um use only subset of those claims as a different verifiers and cavia said by stuff disclosure you do not mean end user saying i only want to present these claims no like say it's relying party the verifier who says give me those claims and this application the holder used by the user can generate those subsets you can go to the next slide oh i have it here never mind so yeah so set disclosure i received 10 claims and i want to let's say present only two or three hours go to the next slide um yeah sd dot and one thing that we really really um daniel and i have really been valuing is having simplicity as a feature um so that it's really easily implementable for the existing um drug implementations and we explain why so there are some existing sexual disclosure themes arguably and that's kind of how we see set disclosure for dot's work um"
  },
  {
    "startTime": "00:18:02",
    "text": "we're trying to keep it as simple as possible and it is possible to set disclosure without using advanced cryptography but those a you know ecdsa dsa curves you're used to um formats um some formats and disclosure are limited to binary formats here we're focusing on json.world is value developer familiarity for security device because it's in json and jot it is much easier to audit um you know whether algorithms are implemented correctly or um whether there are any errors rather than having it in the binary format um language availability there are daniel and i have been amused they're already on four independent implementations of this work within two three months we've been working on this um and this is partially because you can leverage existing java libraries and again we're not limiting those to identity use cases going next so you should be wondering how does it work if you can go to the next slide because you know in the dots um let's find the next slide the net nine slide because in the jobs you know as we all know issuer signs over all of the claims and what we did is we go next slide um the idea is when the issuer is issuing a credential issuer is not signing over the actual claim values issue is signing over the hash of those values and if you just if you hash only the value is a it becomes guessable for attackable so the issue is actually hashing over the actual value and a random unique sold which is unique to that claim value so issuer sign credential contains only these digests next slide but then you know from a end user perspective you know another thing that the issuer needs to send is this soap value"
  },
  {
    "startTime": "00:20:00",
    "text": "container which includes the mapping of actual claim values and these unique salts which is not signed it's a plain json object and and finally when the end user is presenting this object what end user does it's a dot you can't really break issue signatures you cannot modify anything in the issue assigned sd job so select disclosure jot in the blue one is being on the left side is being sent as is where selective disclosure happens is within the container so user is sending only the source and claim values of the claims that the user has consented to release and that's how verifier can verify and receive only the claim values that the user has consented and given the verifier in the release and there's optionally holder binding um yeah i think this is a high level overview and daniel go speak to details okay let's have a closer look at the sdjwt again that's the thing that's sent from the issuer to the end user and then unmodified from the end user to the verifier next slide please this is what it looks like so it's a jwc obviously um may contain public key for over so you can do holder binding or reference there too and that's really um undefined or doesn't need to be defined here the important part is at the bottom you see two new claims the one is called sd-arc with which is obviously a hash algorithm and then sd digests that's the hash the hashes of the claim values with the salts assaulted hashes um of all the claim values and if this would be a more complex object where you have um like in the open id for identity assurance where you have nested claims and so on then would have one digest usually um per claim we'll get to that in a moment"
  },
  {
    "startTime": "00:22:01",
    "text": "um yeah so that's that's like the the superset of all that can be released to um um to the verifier next slide please um and then so that was only the hashes and then we have this construct called assault value container which is sent from the issue to the holder or the end user next slide please um and here we see a construction that looks really ugly when you see it for the first time but that works quite well actually what you see here is that for each of the claims so we have the same structure as before but the claim values so to say they are now json encoded strings and these json encoded strings encode an array of the salt value and the plain text lane value now why do we do this um this looks strange it feels strange feels a bit dirty but it works really well because what we need to do is we need to send parts of this um so whatever the holder wants to select uh so to release that we need to transfer from the holder to the verifier then the verifier needs to hash it the salt and claim value and needs to come to the same digest as the one that is signed in the sdjwt so we need to ensure that everybody has just the same stuff we could just send the salt value and the claim value separately as plain uh json values the problem with that would be um that we need to attach the salt and the claim value together to form a byte string that is then hashed that's the first challenge you need to have like a separator character and then escaping or some other form of encoding result and a claim value in this case we're just sending the"
  },
  {
    "startTime": "00:24:01",
    "text": "whole thing that is to be hashed so you already have a separation between salt value and the claim value second challenge is that no please stay on that slide second challenge is that you have claim values that have more than one representation and that actually affects almost all data types in json so if you have a string you can um use some some escapings to to to encode that string for numbers you have different encodings especially floating point numbers and then when you look at the address claim here we have even have things such as a complex claim so where the whole claim is another object and all that needs to be encoded reliably into byte strings and this is why we say okay the issuer does all the encoding and just makes the json string out of this and just sends the whole string the verifier gets that thing can hash it comes to the same hash as in the sd drawed if it was not modified in between and then can just extract from the this array the second part this is an extremely simple implementation it's super easy you can do complex claim values as you like you don't have to think about cannot canonicalization at all so it just works now the next slide please and as christina already said part of that salt value container that is sent to the holder is then sent to the verifier to actually check um where or to get the claim values next slide please and to check the hashes of course so you can see here this is only a part given name and family name in this case for the verifier the task is really easy check the signature over the sd jar that is sent in parallel to this um then check that this thing here which we call the release"
  },
  {
    "startTime": "00:26:01",
    "text": "is valid can be signed um so you have to check the signature the nonce if you do bind it to the transaction and so on um and then of course and this is like the single thing that a verify can get wrong so please don't do that don't skip that um you have to check that if you hash the string for the claims then you actually have the same digest as the one that is in the sd chart um this is the one thing that you can get wrong we thought about how to avoid that and we didn't come up with anything that works better than this um so we put some work into um communicating that you really should not skip this part and then of course after that you can extract the claim value the nice thing about this whole construction is that you can as the issuer select how you want to build your sd chart you can um for example if you have a complex claim like address here you can say okay the whole thing can be um released as a complete thing so the verifier gets the complete address or nothing or you can do a deep dive and you can like go into the individual elements here it's relatively simple when you look at address next slide please but of course in some use cases you have very complex objects like here and open id for identity assurance this is an actual example from the spec where you can see that you can do very complex things next slide please um we already as christina mentioned have for independent implementations so you can go ahead and play around with this thing in your favorite language if your favorite language is python called invest or typescript um our python implementation is the one that we um [Music] call the reference implementation um we we um build it or we we created it"
  },
  {
    "startTime": "00:28:03",
    "text": "actually before the spec and it's about 500 lines of code really small it evolves with the specs so we try to keep it up to date because we also also generated all the almost all the examples in the spec from that code um so yeah it's uh hopefully useful you can also plug your own examples into it if you like so play around with it um next slide please we also have um looked at how you can um format w3c vcs with this we have a proposal in the issue checker and actually also in spec where so the syntax is not final but it will work out in the end so this will also be covered next slide please what's the current status what are the next steps next slide please um we do have a functional com specification practically complete so there are no aspects that are uncovered yet um it's somewhat stable in the sense that we don't have big items open that we need to discuss obviously there might be other input now um so let's see where this will lead us but we're quite confident that um this is already in a good state um we do have one a new thing that's the approach to also blind claim names um so that we can um also hide what the names of the claims are not the values only um some missing parts and execute and privacy considerations that should be relatively easy and due to this and also due to the simplicity we heard from many groups and organizations already that they consider this a serious alternative to existing formats um it's easy to understand it's easy to"
  },
  {
    "startTime": "00:30:00",
    "text": "implement you don't um end up with just one java implementation that nobody understands and we heard that this will also be adopted in eu projects nothing official that we can announce yet um but hopefully soon and um also by microsoft next slide please why are we here today well jwts were developed in the oauth recon group and we see this as a tool based on jwts that is useful everywhere in all authentic technologies so not tied to mighty connect not tied to identity at all um we hope that it's universal a universal building block for new technologies um for example selectively disclosable access tokens or something like that um we don't know how that will look like in the end but um yeah so these are the things that we can there are more things that we can also imagine so hopefully um some good use cases there as well um and what i already said on the on a mailing list we hope that this can be um adopted by the working group next slide please so that we can work on this in the working group get the feedback from the working group and yeah work on this together thank you thanks daniel um rick hi um oh that's really loud um thank you for this i think this is really smart work i've only got one quick question about the string canonicalization of the jwt did you look at rfc 8785 which is the json canonicalization yeah for crypto i don't um but i think oh my impression was that if we go down that route um then we make the implementation much harder because you have to implement all that stuff from that rfc as well and now you can just use any json encoder so you can just so it really"
  },
  {
    "startTime": "00:32:02",
    "text": "doesn't matter where it puts the white space uh whether like all these things it just has to encode the json faithfully okay yeah good argument my follow on question therefore is don't you run a danger of someone saying oh i'm going to put a a kilobyte of white space into my string and then my tokens are getting really fat at that point and that seems to be an anti-pattern um yeah but you have that with any jwt i guess so as soon as you build a jwt kill you can put so would you consider putting some sort of recommendations within if this was to be adopted into into the draft to say you know if you're doing this string canonicalization look at some of the best practices from 87.85 perhaps copy them but we're leaving this get out clause you don't need to do canonicalization um so it's you just need to do faithful encoding so yes sorry yeah but i think it would make sense uh um to to say um a worldwide space if you can um to to keep the thing small you don't need nicely formatted json in there okay uh with new lines and and taps and so cool thank you thank you okay any other questions comments yes sir anybody you need to get a microphone uh i definitely ever seen this work progress i i want to say maybe that you've overstated the stability just a touch um it's in good shape for this level of maturity but i hope we don't um if we do proceed with adoption that we don't use that as an excuse not to make necessary and important changes yeah absolutely um i think i also wrote that on a mailing list um what i mean by this is there's no like big questions how do we solve x or why um in that we know of yeah um"
  },
  {
    "startTime": "00:34:01",
    "text": "we don't have as far as we know this has not been implemented in um in actual running production environments so we're open to changes um and stability in this case just means that we don't expect that if we solve this issue that we have on our list everything will change fair enough just a clarification thanks thank you no just one second there until um j jim and deep in the queue there in the online queue no german deep remotes okay next yeah i'm sorry uh i wanted to know the motivation of uh using selective disclosure jwt per se in the oauth because in the oauth flow uh we do not find uh it having much utility it would be better if we can look at certain models of jwt in which if we issue a token to a particular user or an app it should not be used by any other app so maybe if we can prevent uh some kind of token stealing or token reuse that would be a more appropriate use case uh as far as the oauth is concerned so that was uh my thought on it so maybe once again i'm gonna cut the line after lathe here so go ahead um yeah thanks for the question i didn't get the first part but i think i got the question um [Music] the i think the token theft topic is orthogonal to this so you need to prevent token theft in any case um the selective disclosure here is not meant as a way to protect data from malicious third parties that get into"
  },
  {
    "startTime": "00:36:01",
    "text": "the flow and steal your token uh it's meant to just release the data that is necessary to any relying party and if that relying party of course misbehaves or becomes corrupted or something like that less data can be stolen from there but token theft is a different topic i think um i'm not we're not precluding other measures against token theft at all um great stuff so just one minor question um in the svc why is the salt field like has the why does it have the escape double quotes i mean could we just use the standard format for the store solved um the salt is a string so it's a basically far encoded random number um and the salt and the plain plain text claim value um form an array that is then json encoded that's where you get the um escaped double quotes from so the whole thing so it's it's not a json array itself like the value could be a json array and you could have the salt as one element of the array and the next one is your escape you know actual value right right that's how we define it so it's always so in the svc you always have for each claim a string that is json encoded representing an array of two elements assault and a claim value okay thanks oh and if you want so i i wanted to just address and comment on brian's um note on on maturity i think actually might be a good idea to try to capitalize on the some of the eu projects that's going to run on this to do an implementation report after this right because we are in a position to be able to take this from sort of relative we don't know whether this will actually work too well we actually know because we tested it uh so take that opportunity as my advice"
  },
  {
    "startTime": "00:38:00",
    "text": "and and talk to i would just talk to peter oldman who is in the um the eu toolbox group and i think he is kind of has his hand on the trigger to figure out what whether this goes into the reference implementation um of the wallet and then you can have a a wide sort of um widely deployed base to point that after that yeah go ahead just one second yeah thanks so much for bringing it up um yes there is a really thorough assessment of sj job happening in yes working upon toolbox and i think that would give us a bit more confidence okay thank you all so quick show friend who read the document one two three four one two three four five six people seven okay [Music] [Laughter] hey can i get a number of volunteers also to review the documents later on brian okay okay okay so i'm gonna call for adoption and see how how much support here and obviously we will have to take it to the list anyway but"
  },
  {
    "startTime": "00:40:01",
    "text": "i want to see kind of show hand who supports adopting this document as a working group of documents okay so one two three four five six seven eight nine ten eleven twelve i guess i missed you there okay okay good hey anybody against adopting this document okay thank you okay okay perfect danielle and christina thank you very much and we'll take the adoption to the list and take it from there okay thank you um oh yeah i heard there's another yeah yeah absolutely we'll continue that discussion on the side meeting it's not this much"
  },
  {
    "startTime": "00:43:02",
    "text": "um muted me as well how about now test one two [Music] yeah test one two yeah so they can hear me and i can't hear me i think so okay good um and one bigger question arose and that is regarding the pixie support um whether to whether the pixel support should be mandatory for asses next slide please so last time we discussed pixie already unfortunately what happened in vienna state in vienna which means in this case that i didn't get around to making uh the changes to the spec um"
  },
  {
    "startTime": "00:44:00",
    "text": "quickly after the meeting i did now um but yeah should have probably done that earlier um in vienna we discussed that the routes around pixie are probably fine as we have them right now um and in short what we discussed there was that the clients are with some exceptions obliged to use pixie the exceptions is if you have a confidential client and you're using open id connect then under precautions you may use nonce instead servers are not forced to enforce pixie we said that um is up to the servers to do that or not that's not what we say there um and we said that all of this needs a bit more explanation that's the changes i did already next slide please now mike asked whether we should so currently we also have the sentence saying authorization servers must support pixi um and so the question now is do we want that in there because um as we said clients under some circumstances when they use open id connect confidential clients and take precautions then they can uh use nonsense steps so what's the value or what's the damage that we cause by saying that um authorization servers must support pixie um next slide please um currently we say that every as should offer pxe so to say to the client the reasoning behind that is that pixel provides robust defense not only against token theft or token misuse but also against csf so as a client as a modern client so to say i can say instead of state or additionally to state i can use pixie"
  },
  {
    "startTime": "00:46:00",
    "text": "um and that's more robust than than just using state the reason reasoning behind that is that the nonce may contain may be contained in the authorization response as well and if the attacker can intercept that then the attacker can forge a new response with the same nonce um and then inject that obviously this requires some sector that can intercept the authorization response what's also nice is that the check for pixi's authorization server enforced and that's really something that we see in the yes ecosystem with the implementations so speaking from the experience we have there um it's and and also experience that we discussed earlier when creating this draft from others the if you can enforce the check by an as which in this case obviously would be an additional check then it cannot be skipped by the client or if the client starts using pixy the as can say okay you also have to provide your verifier that's really robust from our experience next slide so so just one second here so um i see justin and george in the line are do you have questions about something that was mentioned here or yeah i had a question um i'm still struggling a little bit about so sorry sorry go ahead go george that's fine um when i think about the state and the way we talk about people in implementing state right we tend to use it as a mechanism to ensure the requests are coming through the same device the same browser right especially in a redirect based flow um and i don't think anything in ixy requires that so are we to basically say if we use pixi in place of state right again we're ensuring that only the entity that started the flow can obtain the tokens but"
  },
  {
    "startTime": "00:48:00",
    "text": "at the end right but we don't know if that bounce through a whole bunch of different devices or not so is that something we're okay losing because that tends to be where state gets pulled in at least the implementations i've heard of state or we need to add something to pixie that basically says when you do pixie from you know a confidential client or from a back-end web server right then stick some value in the browser so that when you get the redirect you can connect them all together which isn't technically required by pixie so anyway that was my question so i'm not sure if i've got the question correctly but um [Music] are you saying that um pixi might not be usable in like cross device flows oh no i'm saying pixi doesn't have any requirement in it to basically track the the the redirects are happening through the same device where in the use of state basically did require that so i feel like we're losing something if we say do pixie and not state um now maybe in a certain context tracking that the requests are coming from the same device is not critical um but in many cases you don't want the request to start on one device and finish on another device not sure what the running around state is or was but um pixie has this this nice automatism where the so whoever uh goes to uh the token endpoint has to provide the correct verifier so you have to so you have to take care that you don't handle and that verifier to anybody who's who should not retreat yes i guess to me the difference is they were they were handling two different aspects from a security perspective for the a.s and so i'm always a little bit concerned"
  },
  {
    "startTime": "00:50:00",
    "text": "when we say just do pixie don't worry about state is basically the the end of the thing we can we can you know take it to the list or whatever um yeah yeah let's let's continue that discussion on the list um justin um on this topic uh agree with george that they uh that pixie and state address um sort of parallel but uh orthogonal things and that the advice should really should be to use the mechanisms for what the mechanisms are each are each meant for and um probably advice to use them together in appropriate ways so sticking something in the browser so that you know that you're looking at the right pixie value instead of trying to smash everything into the pixie values probably better advice uh in terms of sort of the layering here um i actually forgot that i was on thecube because i've been on the q since the last presentation um and uh what i wanted to say there was that any call for adoption of sd jaw needs to be done in the context of jwp which is having it off i think it's later today um i know that daniel and christina are aware of that proposed work as well um i just want to make sure that the rest of the oauth working group is also aware of that um because i do see a a very real possibility of two parallel efforts solving exactly the same set of problems in very different ways if we're not careful here uh so i just want to make sure that everybody uh that's interested in that is tracking uh what are what's currently two different pieces of work uh ideally i'd like to see us turn that into one piece of work okay thanks both good points um regarding the first point um mike posted this on amazingness i think this would be the right place to discuss that so i would welcome both yours and george's emails there to to discuss this further"
  },
  {
    "startTime": "00:52:00",
    "text": "you did i think you did so time ago but yeah and i think that's the place where we should discuss it excellent um and site meetings obviously yeah next slide please i'm not sure so i'm finishing up anyway okay not much here okay um yeah um also if you have pixie everywhere offered by an as this provides consistencies uh consistency for the clients so they know what to expect next slide um yeah let's discuss that then in the site meeting and yeah that's it from my side um i hope to finish up a 20 this week um and to close the issues soon thank you and if if we need to discuss more in the same meeting happy to do that thank you thanks president look at that all right hi everybody i'm aaron parecki um i decided to steal brian's uh idea of putting photos on the first slide this is a photo i took yesterday morning from the top of that statue um so here to start with 2.1 and then we're going to also get into browser-based apps and i have only 15 minutes so this is going to be mostly just a status update and i hope that this will spur some ideas of things to talk about during the side meetings but um we probably almost certainly don't have time for actual questions about this stuff here so um last we talked about oauth 2.1 was in vienna and um the we made a lot of progress on some of the open issues that there it was really"
  },
  {
    "startTime": "00:54:01",
    "text": "great um the big the biggest change is probably this uh the whole concept of the credentialed client and the result of that discussion was to take out that term and at the same time simplify the definition of what we actually mean by confidential and public clients which previously they had a lot of additional meaning behind them that was sort of baked into a lot of the text so the definition now is just confidential client has credentials public clients do not and there is no other assumptions made about other things about the clients like how much you can trust them because it turns out you can trust both confidential and public clients differently depending on a lot of other factors uh incidentally that actually knocked out a whole bunch of the open issues that were open on the spec because there was a lot of stuff that was baked into that whole idea um number 46 was an easy one um that we discussed last time so that's now um there's just a mention of that parameter and reference out to to the new spec um there is a new section we were the editors were discussing this and we realized that actually um both core rfc and the uh bearer token spec never actually explicitly say that a resource server has to actually validate tokens there isn't that actually isn't spelled out and it's kind of just an assumption that everybody makes because it's kind of we all it would be a good idea um so there there that was in the context of this lifetime uh limited lifetime token discussion and we realized actually there's nowhere that actually says you have to validate so there's a new section it's very short that's what that says um that's the only really new thing everything else was things we discussed previously um and then yeah the last one there is the the whole discussion around the uri the redirect uri schemes of which is supported so they're in the or the better order now the more secure order and um"
  },
  {
    "startTime": "00:56:01",
    "text": "the sentence that says asses have to support all three is removed because that was really just for that was in the data about bcp for as supporting native apps um some other changes since the uh since the last meeting there's a bunch of references that have been updated so thanks everybody who's working on that um there were some unused references and a bunch of new rfcs again so that's always great um there's some more clarifications of of some of the terms and uh and other bits and pieces these are issues that were already open from a lot of the feedback that was uh from vittorio and justin so uh those are all linked to there the um url there will give you the diff of what changed in the source which sometimes easier to read than the actual spec difference and then um i made a milestone link that links to all of the issues that were closed from this as well in case you're curious about tracking all the details um [Music] so some of the i did publish last night sorry draft six which is snapshotting all that stuff um again there's not that much the only thing that's really new that uh was that sectional validating access tokens which again shouldn't be a surprise to anybody uh other than that it was all of the changes that we discussed in vienna um so future stuff uh there are still several more things uh open on the github page to discuss and then there's two big sections left from the initial feedback on the very first draft which is some of the security considerations still and then the native apps section i know vittorio is waiting very eagerly on me to review that section so that should actually probably um that'll be an interesting discussion when we finally do have that um there's still more to do's on um moving some of the normative language from the"
  },
  {
    "startTime": "00:58:01",
    "text": "security considerations that's been sort of an ongoing in progress of like pulling up the things that say you have to do this and putting it where it belongs um which is again part of the whole idea of this is now taking several documents and collapsing it into a shorter one um so that's basically it um i'm hoping that we can have some of these discussions uh in the side meeting i don't know which day but would love to have some time set aside to uh knock out some more of these and i've got a couple of uh tags on the issues i do encourage you to go like look at the issues that are open on there and then see if any of them stick out to you as things you want to make sure you get discussed sooner rather than later okay how long was that five minutes maybe great okay so second document new photo from uh also from the town hall city hall um browser-based apps this is a draft that um we started a while ago and it turns out actually hadn't discussed in any of the meetings since april 2020 so that was one of the virtual interim meetings and it is completely my fault for just not putting it on the agenda since then so some quick quick refresher of this document the idea is this document is meant to be recommendations for people who are building browser-based apps also commonly known as single-page apps using javascript frameworks in a browser it is meant to be a sort of counterpart to the native apps bcp which goes into detail about browser specifics and browser apis that really are only relevant relevant to javascript developers so the um the last time that this was discussed"
  },
  {
    "startTime": "01:00:00",
    "text": "was draft six there's been a little bit of work in between then and now uh we just haven't had any presentations on it in any of the meetings um there are the the big changes are here um the the clarification around the requirement for pixie um is now it now explicitly says it's only about the access token issuance um there's the reference to the new iss parameter there's some more notes about the the scenario of the single page app is actually on the same domain as the rest of the system which is the sort of smallest possible deployment scenario of an oauth system um and then there's in may may last year again i did an update to pull the recommendations from the security bcp at that date into the browser bcp which means there's now more changes to do from may until now any of the security bcp that's changed and there are um a bunch of just minor text uh cleanups and stuff the big thing that is still in the works um yannick has been very helpful and in in adding this pattern this was something that we've identified previously and i think actually this was brought up at the last time at the last interim meeting uh the whole idea of using a service worker as the oauth client within the context of a javascript app is a pattern that exists it i've seen some documentation about it from people's implementations um that is now there's a whole new section about that pattern is one of the main patterns basically there's a section in this document that talks about like here's a couple different options you have about how you can do oauth in a browser whether that's using a a back end that"
  },
  {
    "startTime": "01:02:02",
    "text": "if you're doing purely a single page app as the oauth client and then even within that you have the difference between just javascript code running in the browser or a service worker being the client itself which can better protect tokens so there are a lot of trade-offs between all of them so the whole point of this draft is to spell out the trade-offs not necessarily say this is the only possible right solution but this was a big big chunk it's still in progress there's a couple of notes in that pull request about some details still to work out on this but this is a great great start there um and then the things that we're going to do in the future and the two biggest items are we need to make sure that it's up to date with the latest info in the security bcp since that has been changing and then secondly um there's a lot of people want guidance on how do you store tokens in a browser like where do you put them what is what do we need to know about that and again there's no right or wrong there's just a lot of trade-offs for the different options so it's important to spell out the trade-offs because people will end up making different decisions anyway the two i put on here is like you could just store them in memory where there is no persistence at all which is technically the most secure or you store tokens in local storage because you want them permanently in the browser and again both of those have down upsides and downsides and there's a couple other versions of various things you can do as well so we just want to make sure that we get all those listed out um so those are the big things to discuss i'm sure a lot of people here have opinions i see a opinion face happening in front of me so um we will uh i'm again hoping we can have some time during the side meetings to discuss that um i think that's the last slide so yeah thank you and aaron do you have a quick comment tutorial in the queue go ahead"
  },
  {
    "startTime": "01:04:06",
    "text": "thanks for the update very very exciting during the osw there was there is a big discussion about the fact that um storing the tokens isn't as much of a problem but is the acquisition of the token which is the critical part because you can be super secure where you save stuff but if an attacker has the ability to use you to means new tokens then all that production is pointless and um so there were like long discussions about the bff pattern and the intermediaries like the tmi bff which with brian we suggested so would you contemplate the possibility of uh covering those things in uh in the browser based up spec in any capacity from mentioning that they exist all the way to actually embedding uh for example the guidance that we were putting together for tmi bff yeah definitely um definitely at the very least mentioning all of them uh are a good plan that's what i want to do with that new section um i noticed that we haven't had a lot of discussion on the tmi bff document in a while either so maybe it does make sense to combine these two and just put it in as here's how you do this pattern because it is basically one more option in the tool belt and that will be a fine place for i'm happy to do that that's if that works for you as well so wonderful thank you okay awesome thank you aaron thank you all thank you thanks i'm gonna present now 22. drive here [Music] okay"
  },
  {
    "startTime": "01:06:01",
    "text": "um we're gonna talk about nested jot or multi-subject dot or jot embedded token so the whole idea and the whole document started with the idea of a specific use case that needed a way to embed one jot into another jot and the natural way or a natural place for it was kind of to start with a nested job then this the jot is a a mechanism that allows you to have a payload of that the payload of a jot contains another jot but it has it doesn't allow you to contain doesn't allow the outer jaw to contain its own um its own claims and so after i kind of published that i started getting more requests for different ideas or different use cases that need something like this and it quickly becomes clear that there are so many use cases that need something like this and the relationship also needed to allow you to embed one or multiple tokens into the jar itself and the relationship between those tokens and the main token and the focus was mainly on that in access token in that case and then more recently it was clear that there are even more use cases and in this case for the id token to um contain a jot with multiple a tokens like this so um and for this reason you see dick and giuseppe are now going to be a co-author of this document we're going to work on updating the document and kind of provide spell out all those details and and so for this reason we're not asking for adoption at this stage just and i present the idea the concept the problems we're trying to solve and um maybe next time we'll ask for the adoption but for now just say get some feedback a next slide please okay so the goal is to define a way to allow you to embed one or more jots into"
  },
  {
    "startTime": "01:08:03",
    "text": "a one jot next slide so why is that useful the obvious one is the other trail you want to be able to know who accessed what and when and in some use cases it's very useful because it allows you to present a information from those jots in real time and we'll talk about an example later on and obviously an evaluation you get that token you evaluate this you can make decisions based on authorization basic decisions also based on the context that's that token next next slide please so i run a quickly for with many examples of use cases so a primary a secondary related subject so you have think about a child in a parent use case or a pet and an owner use case you have a parent that wants to access the resources of the child so that's um the parent will be able to log in and then when the resource gets that the token will get that token for in the the parent and the child right so next slide another example would be for a multiple primary subject so you have a think about a marriage couple uh when um that the husband or wife want to log in and perform some operation based on on the um on the ability to access the other the other side resources um next slide another third example is that a delegation of authority think about an admin for example doing some operation on the use on the user's behalf and accessing this those resources right next slide please so all the use cases that we we mentioned so far where"
  },
  {
    "startTime": "01:10:02",
    "text": "the tokens were issued by the same idp that the ones that i'm going to talk about are different in the sense that tokens could be issued by completely different idps and prime example is a stir a use case which is a telephony use case think about a use case where a calls b again we're talking about telephone here a calls b but b has a redirection to c so when when that happens uh when c receives a call they want to know that the actual the initial call was actually from a to b and not directly to c and in that case what would happen that when the redirection service received that initial call with the token they will create their own token and embed the original token into this token and then ship it to to see next slide please another example is um the nsm in a project which is um a mechanism that tried to replicate the the concept of a mesh but at the layer 2 and layer 3 layer um so in that case they have a bunch of middle middle um [Music] intermediaries and those intermediaries receive one token they change that the message and then create the token embed the original message and hand it to to the next intermediary so another example again these are different idps next slide please um another example this is talking about now id token here so for example you have a multiple issuers that issue claims for the same subject so an example you get an id token uh with that date of birth token that talks about the date of birth from one in identity a idp and"
  },
  {
    "startTime": "01:12:01",
    "text": "professional accreditation for example from a completely different idp next slide please and a last example is uh um this is a use case in in italy they have a multiple attributes attribute authorities and they need a way to embed multiple tokens in the id token to allow the client to access those attribute authorities um um directly using those embedded tokens right and we'll show a few examples like this and next slide please so what we're trying to do is just define a new a claim and allow that claim to embed a number of tokens it could be one or could be more and define the relationship between those tokens right that the main token and those embedded tokens next slide please so this is an example like you you see that the child as for example this is a childhood parent token the child as the main primary in subject here and um you have the tokens and inside that tokens you have one token in this case that talks about the parent and the relationship between the embedded token and the main token next next one please and this is um an example of um a multiple embedded tokens this is specifically from a italian government in the way they're doing it so so you have again the main subject and they have multiple tokens again one id token that contains a number of special tokens that will be used with the authorization authorities right next slide so again as i mentioned this is just to give an update and about the problem that we're trying to solve the use cases"
  },
  {
    "startTime": "01:14:00",
    "text": "that we have any thought about this would be appreciated mike jones um first a question about the example on the previous slides your tokens list i expected the embedded jots to start with double quote eyj okay and not have um unencoded claim values so you're missing the signature and the headers is that intended or is that just an artifact of what you presented i'm not sure i will have to talk to giuseppe about this and see what's uh why why is that right okay i'll assert that you want the full jot right you know typically we want the three dots separated correct bits correct um [Music] and you've done a good job demonstrating there's a lot of diverse use cases um it's not clear to me there's a lot of commonality between them although having this syntax and letting people use it may maybe the best we can do i mean otherwise applications such as stir will have to define their own claims for embedding jots for their own purposes which is not terrible i mean the question before the working group and the draft i think is is the commonality worth standardizing maybe it is right thank you yeah thanks mike any other comments questions yeah there's george on the on the george can you go ahead yep um so i guess the only thing i wanted to add is the token exchange work that brian and others did has the concept of an actor token which so i think i couldn't tell some of these use cases you know maybe purely you know claims"
  },
  {
    "startTime": "01:16:00",
    "text": "related some of them may be things that you would want in an authorization model and so i think we just need to make sure that if we pick up this work we look at it from a lot of you know like when should these things potentially be present in access tokens are these concepts be present from an authorization perspective as opposed to an identity perspective i think will be an important um you know aspect if we want to look at the whole the whole space holistically okay sounds good thanks george church can you also post this into the chat window so i have to your correct question captured in the meeting minutes please yes okay any other comments questions okay thank you yeah hirsch alrighty uh hey folks uh first time listener or no long time listener first time caller already screwing things up so uh do you appreciate any patience um here to do a bit of an informational session effectively on how github is looking at token theft um going all right um how github is looking at token theft and what we want to do to protect against it into the future um bit of a call for a conversation and perhaps progress at the end of this uh next slide and just for reference uh i do identity at github uh next slide so the reason that we're bringing this up is that we're seeing a different kind of attack than what we are typically reading about when you read the token binding specs or typical expectations of an attack is we're not looking at"
  },
  {
    "startTime": "01:18:02",
    "text": "um we're not seeing rather attacks in the wild that are based off of xss or getting malware onto somebody's desktop it's actually attackers breaking into trusted cloud providers or integrators to steal masses of tokens in order to then attack the real victims so all of us trust a certain set of providers out in the world to do ci cd for us run actions and all of them have access to our code bases so what these attackers are doing is coming in and stealing access to your code base so they can then look into your code base and find secrets to move laterally into your infrastructure and pull off the actual attack that they wanted to do um and so this isn't about preventing the actual attacks where they're getting into your infrastructure that's spire spiffy all that kind of stuff and it's not about preventing the attacks on those cloud providers in the first place we're really looking at mitigation what happens once they've gotten into your cloud provider or your integrator and um what can we do to make that less bad next slide um obviously just some kind of tokens that you can lose um this is mostly for reference next slide um but all of those tokens do share some similar weaknesses that we're kind of focusing in on none of them are center constrained right basically no tokens out in the wild right now um actually have binding a lot of them have way too long of a lifetime and most are over permissioned this is a common pattern we're seeing across all the tokens that are being stolen next slide so um sort of as our advice for app developers if you are going to be one of those integrators or you're going to be a developer whose code base is being broken into through those cloud providers we do have some suggestions right encryption at rest um once that attacker gets into your system and dumps all of your tokens please"
  },
  {
    "startTime": "01:20:01",
    "text": "don't make it easy for them to use them make them steal something else that is better protected like your decryption key actually use expiring tokens what we see out in the wild is that there are tons of infinite life span tokens that get committed into code bases um if you are going to be committing secrets into your code base please turn on secret scanning of some kind and get them back out this was a major problem whenever we look at attacks that run through github against victims um if you turn on secret scanning there aren't any secrets inside of your code base to steal and abuse and then finally you know and this is kind of the guidance that we're going to be providing to people who want to integrate against github um just don't have credentials to lose in the first place um if you can use workload federation store keys inside of an hsm please do it because um that is just one fewer thing for you to lose next slide the bulk of what i wanted to talk about today was really what are we as service providers as token issuers able to do to bring app developers and integrators into a better success more successful place there's a lot of easy stuff ish that i think we can get done today um so ip allows us for applications having confidential clients right the focus of most of this is confidential clients um have them declare where they will be using tokens from that ups the game from um i steal your tokens and use them to i steal your tokens and now have to still be within your perimeter to use them that is extremely powerful um limit non-expiring tokens so this is something that certainly we want to get done over at github start limiting the use of infinite lifespan tokens eventually deprecate those and make those go away have everything be proper like oauth rotating"
  },
  {
    "startTime": "01:22:00",
    "text": "access tokens or even better support workload federation and support people not having client secrets not having tokens that they are storing at all we want them to be using their infrastructure identities as authorization against our services and then finally if you are issuing tokens please register those tokens for detection with anybody who will do secret scanning um a little bit of that on next slide this is pretty straightforward right this is basically just upload a regex and then secret scanning teams can find your tokens very identifiably if you add checksums high entropy you can make this very low false positive as well which means that when developers turn this on in their code bases they aren't going to be swamped with a bunch of stuff they want to ignore um just to give an idea of the efficacy of this we've had customers turn this on and find thousands of secrets inside of their code bases like this is a real problem that we actually want to use to combat token theft uh next slide and then um kind of the meat of the topic is token binding right like every time there's a big token theft that hits the news people say so why were they able to use those tokens bearer tokens they suck let's fix that um let's talk about that uh next slide so this is purely from a implementer's perspective that actually is told to go fix this 8705 is dead um we're not going to implement it we're not interested because it doesn't support enough platforms it doesn't work across all of our um platforms that we care about yeah sorry ad705 is um mtls binding right this is uh channel binding um it doesn't work because it doesn't work inside the browser anymore and so if we have to invest in that it doesn't get us far enough"
  },
  {
    "startTime": "01:24:01",
    "text": "mobile apps though and sorry um that is to say though depop seems to be offering us a lot of what we actually care about which is device binding making sure that the device we gave the token to is the same one using it seems to be working pretty well for mobile apps right this is something that we can reasonably go out and implement pretty cleanly today desktop apps you can start using the mac os keychain you can start using tpms to bind keys but because there's no strong application identity uh you get medium il privileges you run as user and you just say that you're the github cli or you just say that you're chrome and you can get those keys and okay that's not great um confidential clients this actually seems fairly reasonable from a technical perspective how we can start um having confidential clients prove that they are still the same client that was sent the token however there's no profile for this there isn't a dedicated way that we have seen to actually go implement this um and then finally web apps um there's like web crypto api which is great um and if you can xss the tokens out you can probably xss out a um use for them as well we'd love to see some stronger device binding there because uh next slide of our risk profile that we're looking at is um assume they've gotten into your database um assume that your site has xss how do we still bind the tokens given those two things um and so we're looking for is really get them uh force attackers to get to the next level which is infrastructure compromise either on the desktop that your application is running inside of either on the browser as an actual desktop app or into your web app infrastructure right they need to be able to stamp a microservice inside of your trusted perimeter that's the"
  },
  {
    "startTime": "01:26:01",
    "text": "bar that we want attackers to have to meet in order to lift and steal and use tokens um and right now for web apps it seems like that's hovering right around like we can do two but not three so next slide uh there's also a lot of stuff here that seems like it's not quite solved yet and again this is basically just please come talk to me i'd love to help figure out how we can solve these one is the i'm calling it referred binding i think there's probably better terms for it but when that authorization code is sent to a confidential client then they have to respond with a cookie that is somehow bound to the same browser that got that authorization code there seems to be some weaknesses there around um is that the optimum time to go steal something and replay it tpm rate limits this works fine if you're like on a windows device right you can get 10 20 signatures a minute out of that thing and everything works fine if you have thousands of users and you need to be creating signatures off of some vtpm as a confidential client you start hitting rate limits really quickly and you start having to do stuff like charge your activity by your tpm as a bottleneck understanding how a profile for confidential clients can reuse certain signatures inside of depop would be really interesting one that is particularly interesting to us is sort of hosted workloads or shared execution um so if you're a ci cd system or github actions and you are requesting tokens for a workload that you are hosting how do you bind those and what do you even bind them to do you hold those and then sign them right before you hand them to the workload do you run a sort of shell around the workload and then sign things on their way out open questions here that we're not sure how to solve and if we can't find the tokens that we're"
  },
  {
    "startTime": "01:28:01",
    "text": "handing to our workloads that feels like a vulnerability and then finally the privacy element here can we actually ship this to all of our users if we're saying hey we want to um durably identify your device there's a whole separate community and kind of zeitgeist right now around making sure that's not possible so not sure where that tension is next slide so call to action really um love to see a confidential client profile for depop um and again very happy to work with you on this and figure out how we move this forward um working with the browser vendors to bind keys from the operating system up through the browser into web applications so that we can hopefully heighten some of the security guarantees there and then finally um really just guidance and standards for like what does token binding look like in a successful deployment um and again we're there to both help move that forward and be a reference next slide i guess that's it okay awesome any quick comments questions mike jones on your last slide what does browser support for depop key protection mean uh right now like web crypto um will have non-exportable keys but it's not clear that those are actually being sunk all the way down into the os and device identity um they're not yeah getting that support would be great um i'll talk to you later victoria um i think that uh doing something with a browser for uh supporting browser artifacts like cookies and similar given the demise of the token binding would be"
  },
  {
    "startTime": "01:30:01",
    "text": "very good for the industry and i think it's a very good idea and i'm looking forward to see how if your proposal develops and you know that we work with browser vendors for other reasons so please feel free to use uh users as the zone as much as possible thank you okay and yeah i had a question so sorry yeah i'm audible yeah go ahead go ahead yeah so i had a question basically uh sometimes we have seen that even for the native apps uh the authorization code flow is used so many times the client secrets are embedded into the desktop apps or the native apps so are we looking also at ways uh to bind that uh client secrets uh in some ways or is it restricted to only tokens uh yeah absolutely not i've um actually i was a little bummed to see that um aaron had talked about removing credentialed clients uh from 2.1 because there was some interest in like temporary credentials that are assigned on a per application basis um had no idea how that was actually going to function but no can i respond to that okay quickly it's very brief i did mention this last uh in vienna or non-event right after vienna at the oats security workshop um about the possibility of turning native apps into confidential clients using other mechanisms that exist so we'd love to talk about that further and see what your interest is in that yeah absolutely awesome thank you very much appreciate it it's killing here"
  },
  {
    "startTime": "01:32:07",
    "text": "hi i'm kelly bergen from the mitre corporation here to talk about token and identity chaining between protected resources can you get closer to michael oh just a lot going on oh really okay all right hey next slide please on it's not moving so some background a couple years ago mitre published some profiles for the oauth 2.0 and open idconnect mostly it was adding security requirements that were optional in the oauth 2.0 is mandatory and we also have in our target environment is a enterprise environment and so clients and servers have pki credentials that they use to authenticate to each other using mutual tls and so now we're working on extending oauth token exchange in a case where you have a protected resource in uh one environment that needs to access a protected resource in a different organization's environment and so next slide i presented this before last year to a couple of people in a couple of forums and i'm back to update you on things so in a single icam environment both protected resources are in the same icam ecosystem so they trust the same authorization server but i want to talk today about the multi-icam ecosystem case where you've got a protected resource in one organization that needs to access a protected resource in a second organization excuse me"
  },
  {
    "startTime": "01:34:01",
    "text": "and so we use make use of oauth token exchange um but in a way that's more of a it's a mix of a profile and an extension to token exchange so the goals for this week are to get some feedback i i sent the document to the mailing list and i can we haven't had a chance to get a public release for our website yet but i'm happy to send it to you anyone who is interested in reading the document so i'd like to get some feedback on the profiles as well as a new claim that i'm defining today so uh some some requirements that we have are that the when pr2 in the second organization receives the access token that it received through token exchange from the first protected resource that token needs to have the client id and the confirmation claim related to pr1 so that it can do some verification in the presentation of the token it also includes an actor claim which includes the subject as pr1 and the issuer of that particular token and all previous actor claims so you can imagine this may uh propagate from pr1 to pr2 to pr3 and so that the final protected resource can verify the identities of everyone participating in in the exchange next slide so here's a diagram that may be helpful we have two options for this multi-icam ecosystem situation the first one the client receives its first token presents it the pr1 in the first organization but pr1 needs to access pr2 because that's where the data is housed for the response to the original request and so"
  },
  {
    "startTime": "01:36:00",
    "text": "pr1 simply does token exchange with its authorization server in organization one it presents that token to pr2 in the second organization and then pr2 uses it validates the token and id with its authorization server as2 and organization 2. and because we have this attribute sharing infrastructure as1 and as2 can communicate with each other out of band so the as2 can validate the token that it received because the first token was generated by as1 and so there needs to be this underlying attribute sharing infrastructure for the two authorization servers to communicate with each other now the next slide please the the use case that we've had problems with in the past and some of you may recall uh we proposed this that didn't have a solution so now i'm proposing a solution so in this case when pr1 performs token exchange with its authorization server as1 instead of off instead of as1 generating the token and returning it it actually pauses uh generates a jot assertion sends that to as2 or as2 generates the token sends it back to as1 who then returns it to pr1 and then everything is fine from there on out because pr2 can validate the token because it was generated by as2 however this requires some custom processing at as1 because i kind of flew past it earlier but we require for pr2's validation of the token that the client id and cnf fields in the token that's being presented to it identify pr1 and if you just do this as is that's not going to be the case because when as1 generates that jot assertion the token it receives should identify as1 in the client id and cnf fields so to fix this problem we are proposing a new claim next line"
  },
  {
    "startTime": "01:38:00",
    "text": "and this claim is chained id which basically just passes those two bits of information the client id and cnf fields from pr1 to as2 and then as2 uh does a little bit of uh uh it populates the the sudden odd claims uh according to the specs but instead of filling the client id and cnf fields with the values of as1 it uses the information that was passed to it through the chained id claim to populate the client90 and cnf fields in the new access token so that when prt receives it it can validate the token should be coming from pr1 and so that's that's my proposal for a new claim and uh next slide i'd like to get some feedback on uh thoughts of whether this new custom processing at as1 is reasonable and whether this new claim could be valid or useful to anybody else we do have some implementations we've got one in ping feder right now that uses custom processing at as1 like i mentioned and we're working on key cloak next and so uh i know we don't have a lot of time today so i'm happy to be available the rest of the week to discuss uh any interest in these profiles as well as uh what you think on this new claim hi sorry the meet you wouldn't let me log in um is there a draft for this yeah so uh like i mentioned we we have to go through a public release process and so it hasn't been posted to a url yet but i can send it to you uh it's been posted to the mailing list i included it as an attachment and i don't know if it got scrubbed on the way to you guys okay uh but i'm happy to send the document out uh to anyone who requests it okay um so my general comment uh when as soon as i saw a client id uh my my thought was what's the format of that is there a"
  },
  {
    "startTime": "01:40:01",
    "text": "specified format or is it ambiguous is it a you know just a opaque string there's no specific no specific so i want to caution anyone who likes the idea of leaving opaque strings uh i want to use the cautionary tale of the subject field of x-509 certificates we had to go and invent a whole another a whole other extension of subject alt name which has a structure which has several different types so that you can partially unscrew the the mess that was created by having something that was completely opaque i think it's perfectly fine if you have uh sort of uh types and one of the types is this is an opaque blob but not having a way to indicate that there is a structure is a recipe for pain going down the road later thank you got it thank you and that was rohan all right thank you any other comments questions sorry rick taylor i should probably use the meat echo app but i've just jumped to my feet um i'm just kind of i'm i'm observing i haven't obviously read this draft because it's difficult to get hold of and i haven't read rifat's draft but i'm you it isn't the solution to this uh this claim this kind of delegated authority that that as1 is saying i um i'm just thinking off the top of my head here isn't the solution to the what should go in this new claim field is actually you're embedding a sub token so there's there's some kind of recursive element here where that delegated claim as you're moving between authentication areas could be captured by the formats you're talking about rifat in terms of it's just a token within a token i i think you guys know i think we should think about this and talk about"
  },
  {
    "startTime": "01:42:00",
    "text": "this that makes it that makes a lot of sense my work is done all right thank you i appreciate it thank you any other comments questions all right okay thank you all yeah hi everyone my first ietf as well so uh thank you comfort for having me on the on the schedule um so uh i'm atul tul shibakwali i'm cto of a company a relatively new company called signal uh and that's my email address and twitter handle there um so let's jump sorry yeah yeah let's uh jump right into it um so what i'm going to talk about today is more of a sort of you know do we need something like this like trying to have a discussion about whether there is a need to standardize something at this level of the rpcs which is sort of somewhat lower level to what what we um are used to talking about and the other question then that brings up is is this the right forum or is there some other you know working group that this should be discussed right um so how rbc security seems to work today in most cases i know some companies do things differently is that at the top level you get an um sort of a service that is consuming an award token and then you know there's a whole bunch of micro services that hang off of that and you know all that context is lost at the top level itself right so what you then do is extract whatever sort of user id or information you need to do from that"
  },
  {
    "startTime": "01:44:00",
    "text": "oauth token and then just you know make an rpc call without any security other than maybe a mutual tls connection between the micro services right a very typical approach uh to doing things today uh the other problem is that um you know when you call a third party api like we had the presentation before where uh you have an api key that is very powerful because when you call that third-party api you're using a token that could be long-lived could could do things on behalf of many users and as a result by using that key you can pretty much do a lot of damage uh on the tenant that you have in that third-party platform that is you know providing that api and the third problem that we see is as more multi-cloud deployments proliferate where you were your own tenants are spread across multiple cloud platforms you still need some security model that sort of crosses those boundaries and maybe you're using your own kind of api keys or you're maybe you know creating some specialized kind of tls connections between your vpcs but again all of these things um seem to have a few problems and let's go to the next slide where where i talk about you know what are the issues right and basically the issue is that you know if you have a vpc compromise you end up with like a you know terrible amount of uh power that you're offering to uh any attacker like for example you know i'm not trying to pick on any one company but uh there's a company called one login where you know an attacker entered into their vpc that attacker was in their vpc for months and that company unfortunately held the credentials for a lot of users for thousands of other companies right a massive compromise resulted from that i mean i don't know the extent of the compromise but i can imagine the the you"
  },
  {
    "startTime": "01:46:00",
    "text": "know the possibilities there um but there are you know various ways in which this can be achieved right you can have a software supply chain attack where you know your microservice within your vpc gets compromised and then the attacker has control over your vpc in that way right um like uh the person from github pointed out that you know you could have a dev chain issue where uh you know your github has been compromised and from there your vpc gets compromised or you know like in the case of one login the privileged user who had access to the vpc at an administrative level their account got phished and you know that attacker was able to enter the vc vpc in that way so i think you know we can imagine there are various scenarios in which vpcs can get compromised and what we would like to do is make sure that even if such a compromise happens that you know the uh extent of the damage can be limited right um and so um the same thing where you know uh if you're using a third-party api let's say you have a tenant and you know your your tenant uses some you know custom process to call a workday you know like an hr api to onboard people onto your system imagine you know if somebody was able to get into your vpc and actually use that same key to create you know fake employee that had a lot of access in your sort of organization right so this uh these api keys are very powerful and you know they you know somehow if you can restrict that power that that would be good as well and you know the other problem like i said in your multi-cloud deployments you know you you may have to roll your own security today and you know there's no standard for doing that and one important thing is that you know the oauth token that comes in has a lot of context it has basically uh a scope it has a user identity associated with it uh and you know that context is lost in subsequent calls"
  },
  {
    "startTime": "01:48:01",
    "text": "right so if you can somehow preserve that context all the way through to the lowest level to maybe at a database level uh that can limit the attack possibilities quite a bit right next slide please and so while i don't have a solution i have seen some solutions previously i used to work at google and really admire what they've done internally but what i feel is that you know a good solution uh would preserve the identity and scope at any level right it should work the same across your 3p apis and your multi-cloud deployments so if you have a standard you can carry that identity and scope you know restrict the scope even further if you want all the way through uh you know across different cloud platforms down to the your lowest level unit database and you know the possibility of compromise through a vpc compromise becomes much lower now because even if you had a fake service in the vpc you're not going to be able to do much until you have a user credential token with a specific scope on behalf of you know so that you can act on behalf of that user now obviously you need at every level you need this to be you know independently verifiable so that the limit you limit the amount of trust you're having between your own microservices and then if i just grab a token i shouldn't be able to use it you know sometime later uh and replay uh that user's uh ability um and because this we are talking about we you know rpcs which are like happening much more frequently than like an token exchange at the higher level you need this to be super efficient right um excellent and so you know what are possible solutions here right so um you know you need to be able to come up with a token that that is specific to"
  },
  {
    "startTime": "01:50:01",
    "text": "users and scopes right and you should be able to you know further restrict the scope and downstream calls right um so so that the service cannot like suddenly switch the context i'm operating on use on behalf of user a and they're uh asking me to make this change to their sort of uh you know personal email address or something like that and i can i can't switch it to be like user b and they're trying to change their social security number or something like that right so um you need to be able to do something like that um in order to limit replay you know you may have to go with much smaller you know much shorter lift tokens and i think something that was proposed before as well in today's session and you should be able to bind these tokens strongly between the originating and the destination services so that you know you cannot reuse those tokens for something else than what they were intended for and somehow achieve interoperability across cloud boundaries by maybe token introspection or sort of having a common route of trust and then being able to verify those tokens when they're received right uh so this is this is all i had i know i may um maybe uh finishing much more quickly that's why yeah and that's uh somewhat by design because i want a lot of discussion even today on this and you know hopefully uh you have some questions okay i see ben go ahead whoa uh looks like i'm talking great so i have several points to make actually um and i guess the highest level one is just to get a common understanding of what we mean by rpc and like in the talk it sort of sounded like you you have as at least the main thing you're thinking about when you've got like a big api that is facing to like maybe the actual users or some other services and then the implementation of that api has to go off and make a bunch of other"
  },
  {
    "startTime": "01:52:01",
    "text": "internal calls to a bunch of internal microservices and you're worried about the you're mostly worried about these sort of internal rpcs that are to these microservices because the the back-end implementations are highly privileged so that they can call all of these microservices on behalf of the end user and that sort of brings up two main topics one of which is the sort of confused deputy problem where you need to make sure that these back end microservice calls are bound to the specific user and scope that you want and that's a problem that we already have to solve today and you know the most common way that we do that is sort of the in-band signal of you know i'm making this api call on behalf of this user or whatnot which is not great but it sort of gets the job done but it's really something that you have to be doing already and then the other topic that i heard was that because this back-end implementation has to call so many microservices it has very privileged credentials there and so if the vpc that's hosting that service gets compromised then you have a very privileged credential that's compromised as well and you don't necessarily have a good way to protect or recover from that and that's also an interesting question to consider um and one of the things that comes to mind as a potential approach to combine a solution for all these would be if the highly privileged credential that the implementation how the service implementation has is not directly usable for calling the microservices but rather you have to go to some other credential token issuing service and authenticates yourself as this highly privileged user but also at the same time present some evidence"
  },
  {
    "startTime": "01:54:00",
    "text": "about the request that you are servicing so that your short-lived credentials that actually unless you call the microservices can then be more easily bound and i know that's going to be uh hard to do efficiently which i think you specifically raised but it's uh it's also the only thing that really comes to mind as a way to combine all these properties and i just dropped a lot on you so uh ask clarification if you need anything and i'm happy to hear your thoughts yeah so you're you're right on in terms of sort of what is the problem we're trying to solve there's one more aspect which is sort of in addition to the microservice rpcs and the third-party api calls there's also the multi-cloud aspect of this where you have you know multiple vpcs and different cloud platforms and so you're trying to solve the similar problem but it's just within your sort of infrastructure if you will uh but yeah you're right on in terms of um like the scope of the issue that we're trying to solve uh or or we're trying to i'm trying to bring up over here um yeah uh was there anything specifically you wanted me to answer uh i'm kind of curious to hear your thoughts about how this relates to existing solutions for the confused deputy problem because i would hope that people are already solving the confused deputy problem and i don't know if you were thinking this would build on top of that or be a different sort of solution right um so i guess i'm aware of a few proprietary solutions today that solved some of these problems but i'm not aware of something that is adopted as a standard okay uh if you want to point me in that direction i would love to go research something"
  },
  {
    "startTime": "01:56:01",
    "text": "yeah i think i'll drop a link in the chat to some kerberos related work that covers some of the things i was talking about but i don't know of anything that's a lot specific sure thanks okay thanks ben george all right yeah i was just going to follow up i threw a couple of links in the chat as well a tool um for work that's very similar to this um i'm not sure that trying to solve all of these problems with a single solution is going to make sense like the third party api one you might want to think about you know uh what kelly from mitre was talking about in cross domains if those are truly third parties that i'm reaching out to um and how that might be a better solution as as it relates to the token management but for sure the the top um two or three things i've seen a couple of things one of them is from netflix one of them was a talk i gave identiverse in 2019 so i think there's thoughts in the space i would agree with you i don't know of any standard that exists today but i think we have a lot of things to build on that's yeah uh that's great i'll look them up i'm i'm obviously not familiar with the netflix implementation but yeah i'd love to take a look um so yeah i think the real question is you know is this something that everyone can benefit from from standardization right and that's sort of the open question i wanted to bring up in this talk today any other yeah yeah i think i think i hope you're here this week so that may give us a chance"
  },
  {
    "startTime": "01:58:00",
    "text": "to like socialize and discuss and to dig deeper into the um into this aspect and see where some of the folks in the room have actually thought about solutions already or have run into similar challenges and so i think it's a good conversation starter right yeah and informally like uh i've spoken to some folks um uh at one of the big platform providers aws and they seem to have interest in standardizing i think peter also uh peter and i also talked about this um there seems to be some interest from google very early so uh things may be sort of going in the right direction but this is a good week for me to have discussions and and sort of bring it up a little bit on in terms of mind share yeah yeah and that's great i think obviously you're welcome to come back after that and and if you have a solution in mind and you want to discuss your sure yeah uh sure thanks everyone uh maybe in uh one of those successful meetings we can sort of go into some detail sure potentials yes if needed we can do that sounds good thank you okay i think we are one minute any other business any other comments questions okay well thank you all we're done [Applause]"
  },
  {
    "startTime": "02:00:00",
    "text": "um all right you"
  }
]
