[
  {
    "startTime": "00:00:50",
    "text": "test one two three test test one two three test test"
  },
  {
    "startTime": "00:15:58",
    "text": "test test one two three test test"
  },
  {
    "startTime": "00:16:01",
    "text": "one two three four test test test test"
  },
  {
    "startTime": "00:19:36",
    "text": "test test one two three test test one two three test test one two three test"
  },
  {
    "startTime": "00:25:04",
    "text": "hey everyone hear me uh this is a test of projecting through my laptop let me know if you can hear me"
  },
  {
    "startTime": "00:28:16",
    "text": "videos [Music] uh talk please test test one two three test yes okay okay can you hear me yes yes perfect thanks a lot thank you testing with this chairs mike online can you hear the chairs mike yes we do"
  },
  {
    "startTime": "00:30:03",
    "text": "all right attempt number 2001 um so very quick updates on documents we have quite a few documents in flight uh we have one document in uh that will be going to isg review which is ash to curve [Music] i think the most interesting is probably next slides about various adoption calls we have um madsen cfrg deterministic signature with noise document was uh um very interesting to watch on the mailing list uh because of um ipr concerns even though the api is expired i um chairs are consulting with irtf chair to decide what the best way for is on this they seem to be interested in their topic uh but a lot of people said that they only they want to work on the document condition they so this is i'm not entirely sure how to as a chair how to process this information um we also have another document hey aegis aegis um in adoption call um there were very few comments until recently there was some support this week um i think uh we are extending glasgow a little bit more to get more information about this so if you have an opinion whether this should be worked on in cfrg plus please comment on the mailing list"
  },
  {
    "startTime": "00:32:06",
    "text": "and with this let's try start with other presentations okay um all right here's our first presentation it's about the bbs signature scheme um we're obviously very tight on time here so uh we'll sort of limit the questions and have the conversation uh bring it online but um here is our presenter the this one yeah so my clients are here and sorry there's some transition so will i just ask just say next slide okay thank you difficulties they cannot see you at the moment right okay that's right um hi all my name's tobias and i'm here to talk today about a signature scene called um the bbs signature scan so next slide please so at a high level what are vbs signatures with a few words um effectively it's a digital scheme supporting uh sorry next there's a couple of transitions um selective disclosure or multi-message signing we call it um next one you can also see down there thank you thank you proof of possession enabled and finally unlinkable proofs through a zero"
  },
  {
    "startTime": "00:34:01",
    "text": "knowledge proof protocol next slide so just a bit of a timeline around where some of this work is originated from the first appearance of this short group signature was documented in 2004 um which is where the name bbs is derived from the work of banaya buoyan and shakum there was been multiple revisions in academia since then in a paper in 2006 and again in 2016 and then as a working group at the decentralized identity foundation we started work on documenting the scheme at the applied crypto working group in 2021 next slide so at a high level of the scheme next slide it is pairing based so it uses type three pairings um two subgroups a g1 and g2 of non-prime water in the current cipher suite that's defined in the specification today um the scheme itself is is curve agnostic but the cipher suit we've defined within the scheme today and the spec is based on using the bls 1231 curve but as we said any pairing friendly curve is compatible provided it's type three in nature uh next slide so uh there's a obviously a problem with this slide but effectively at a high level in terms of concepts um there are three parties involved in this protocol sign approver and verify and there are two kind of core constructs which are signatures and proofs so if we move on to the next slide the highest level summary of how this protocol works is a signer signs a set of messages and headers messages and headers are the two kind of application level payload kind of classes of application level payloads we can sign with the scheme loosely a header is information that must always be revealed and any proof generated under this protocol and messages it's a set of messages it can"
  },
  {
    "startTime": "00:36:00",
    "text": "be in number of messages chosen by the signer and those are selectively disclosable by the approver signatures generated over top of them and then on the proving half of the protocol prover can create a presentation header selectively disclose create a proof and send that to the verifier for verification next slide so just moving into the lower level sort of properties of how sign works as we as i said on the last slide there are essentially two levels of application level payloads here the header which is effectively a message or a payload that's always disclosed and any proofs generated and the messages are selectively disclosable the sign operation is effectively the signer that takes all of that information together signs that and produces a signature some notable properties is the signature is constant size um even under n number of messages and that signature is essentially a combination of a single ec point and two scalars in the context of the bls 12381 cipher suite that is 112 bytes in size um the slide has cut off on the efficiency we're trying to say that it's of n efficiency or constant time if no selective disclosure messages are used in terms of the efficiency of the sign operation and another thing that to note is in the scheme you can either sign just a header or just a set of messages or both so the scheme's flexible in that regard next slide please so verification um this is this is a operation that would primarily be done by approver because the signature itself would never be revealed to a verifier directly that's what proofs are for but effectively um to verify a signature it behaves much like a normal digital signature you take all the information protected by the"
  },
  {
    "startTime": "00:38:00",
    "text": "signature which is the end number of messages in the header and the signature the sign is public key and you get a result around whether or not the signature is valid again that's oen efficiency or constant time if no selected disclosure messages are used next slide please so the proof protocol is effectively the approver receives a signature with a set of messages and a header and then they decide which messages they would like to selectively reveal if they are present in the signature they produce a presentation header which can be used to put in a nonce or scope the proof to a particular domain or audience combine that with the sun is public key and generate a proof so the size of the proof is linear to the number of hidden messages there's an equation there that gives you an idea of how that scales again this operation is owen efficient or constant time if no selective disclosure messages are used and there's obviously a choice for the prover to make around which messages to disclose lastly next slide please so bbs the verify proof so this is the operation that's performed by a verifier they they receive essentially the set of revealed messages and header the presentation header improve they combine this information with the signer's public key and that produces a result of whether or not the proof is valid or not again owen efficient or constant time if no select disclosure messages are used next so the proof just a note on what the proof provides it proves integrity and authenticity of the revealed messages also the header and it proves possession of the signature the presentation header is essentially brown to the proof as well"
  },
  {
    "startTime": "00:40:01",
    "text": "and the proof is essenti is said to be unlinkable because effectively the proof is it's generated under a non-interactive zero knowledge proof protocol next slide so these are some some benchmarks for the for our implementation that we've most recently done for the most recent draft we are still refining some of these but this gives an idea of the characteristics that that we've found so the benchmarks you've got a logarithmic scale on the bottom there so this really shows that performance scales linearly to the number of messages um in the in the smallest case where you are maybe just signing a header you're not using the selected disclosure feature afforded by the bbs signature um you're looking at a signed verified times of you know sub half a millisecond um and obviously the proofs just for note on these benchmarks um as they go up were generated with 50 of the messages were disclosed in the generated proofs and there's some details on the machine those benchmarks are run on next slide please so just a brief section on some of the use cases we see for this scheme the first is privacy preserving anonymous credentials so effectively the ability to leverage the selective disclosure feature to produce attribute-based credentials that include personal information about someone where they can derive proofs of the subset of that information signed by the original issuer to different relying parties free from the risk of correlating themselves in presentations and also providing the ability for them to granularly manage what information they share to different relying parties next slide another application is what we call proof of possession enabled security access tokens so this is a"
  },
  {
    "startTime": "00:42:01",
    "text": "what we believe is a feature that allows you to say issue a security token or an access token under a protocol like oauth 2 where the signer can just produce a access token secured with a bbs signature and then the prover using the proving half of the protocol can generate a unique proof um and send that to say a resource server and a protocol like oauth2 proving possession of that token which is obviously an enhancement in terms of replay attack afford detection for a resource server over say bearer tokens and there's also some simplifications versus other proof of possession enabled schemes that require the approver or the client to manage key material and prove possession of that and get that essentially tied into the token to do something so that's one application and then finally the last one is essentially non-correlating security token proofs so the idea that a signer can issue a single token to a client and that client can then generate n number of proofs for n number of parties um proving proving whatever the presence of that essentially what that token um underlies what that actually proves or grants the client um to different servers without being correlated through the cryptographic layer say a fixed signature because again the proofs are indistinguishable from random and just a quick note there the um indistinguishable from random is it's a it's not an artifact of the presentation header which is set by the client it's actually built into the protocol so just a quick note on why we believe while we're presenting this work to the cfr gen why we'd like to continue it at cfrg we think it fits with numerous existing work items already here which is notably the peering friendly curves drafts which includes the definition of the curve that we're using present and the current"
  },
  {
    "startTime": "00:44:00",
    "text": "syphilis we've defined which is bls 1231 it also bears a lot of a relationship and structure and nature um to the existing bls signatures draft and we also make heavy use of of hash to curve in our draft next slide please so effectively we'd like to call for adoption of this draft and you know we believe that it's sufficiently just um evolved today to describe the scheme however it's you know incomplete as a draft we've got numerous issues still in in our repository um such as a you know we would we'd appreciate a broader review of the scheme um the cipher suite kind of definition that we have within the spec today which allows kind of extensibility and future cypher switch to be defined we uh we still have a series of refinements to do there and then beyond that also clarify some of the extensibility points that we have built into the scan uh a slight please good question yeah is there any itf worth that that will be using this any sorry any ietf work that we'll be using yeah so um this afternoon there is a um uh buff a birds of a feather session for a new a call for what's called jason web proofs which proposes to essentially extend the jose family of cryptographic data representation formats to allow and accommodate schemes like bbs so yes related work at itf uh chris wood also wanted to comment this has potential implications or applications to privacy past as well so there's another one yeah yeah um thanks so i've definitely seen that work and i think that's fantastic and there's there's definitely intersect and overlap with applications like privacy pass so uh in conclusion um essentially it's"
  },
  {
    "startTime": "00:46:00",
    "text": "bbs is an efficient multi-message digital signature scheme supporting select disclosure and zero knowledge proofs um it's had quite a long line of research backing it up improving its security properties um and efficiency there's been multiple iterations in academia over the years as well we believe there are multiple use cases for which bbs can be applied and the current cipher suites based on the bls 1231 curve obviously there's already work going on at the atf sorry at the cfrg using that curve as well um any premium friendly curve can be used and we also believe that the signature scheme itself is also extendable to fit a range of other use cases so that's that's it okay any more questions that's not exactly a question but i'm very interested in seeing this work uh continue especially in the context of securing cyber physical supply chains given the performance and the size of the proofs thank you um lavender from dell the verifier does need to have the public key of the sign right the the verifier does what sorry does it need to have the public key to verify the proof yeah yes okay so how is it unlinkable sorry so how is it unlinkable if you need to have your public key to verify your signature or the proof that you have signed something how is it unlinkable to you because the public key represents the public key of the issuer so you've got heard privacy within it's essentially a group signature so if you say one issuer issues to a herd of you know 10 000 or million signatures then any party that possesses a signature within that group has anonymity to that group got it thank you okay thanks uh we're not going to do an adoption call here but"
  },
  {
    "startTime": "00:48:00",
    "text": "we'll definitely talk with the chairs and we'll decide whether to take it to the list thank you all right uh deer drive hello uh there i am yes cool hi i'm deirdre i work at the zcash foundation uh this is frost uh next slide please uh what is frost frost is a flexible next next please next please ground optimized next please this is two rounds for frost if you're familiar with any of the frostwork that has been published elsewhere the thing that we're specifying in this document is the two-round variant of frost that's where the flexible comes from there were two variants this is the two round variant uh next please schnoor next please only schnorr signatures or snore like signatures there is no ec dsa in this protocol next please threshold next please i thought i would have a clicker for every signature you create you can use a threshold t of n possible signers where those n possible signers have shares of the secret next please and that's the signature scene and the resulting signature is indistinguishable from a single signer schnorr signature like an ed dsa type signature next please so high level you have already done some key gen"
  },
  {
    "startTime": "00:50:00",
    "text": "and then the document that we are proposing is the signing protocol after you've already done keygen first round involves generating nonces rather your contributions to the nonces and then commitments to those in publishing them and then the second round uh you actually use those and all the other signing participants commitments uh with your key share to generate your signature share and then the coordinator whoever that is uh takes all these shares signature shares all these nonce commitments and combines them together into the final signature and outputs your frost signature next please and next one more so this is a high level prior to starting the two round protocol you have done your key gen someone is operating as a coordinator uh we've seen implementations where anyone everyone can be the coordinator in the thing we're describing there is a coordinator next please um as i described you generate nonces you create these commitments to your nonces and you publish them in an authenticated channel not a confidential channel but authenticated channel next please and then the coordinator or whoever who having shuffled around not shuffled distributed all the commitments to these nonces to all the parties who are signing uh uses them to grant to generate your signature share with those commitments and your signing share your key share next please and then you send that signing share back to the coordinator and you aggregate all those signing shares back into the final signature next please so uh fully specified we've been it cranking"
  },
  {
    "startTime": "00:52:00",
    "text": "on it for eight months now um we have four cipher suites in the document the restredo prime order group p 56 uh ed25519 can you go back and uh ed 448 we have specifically specified these cipher suites and the api of the document uh to be compatible with edu5519 and ed48 verification um and we also have multiple interoperable uh implementations several and rust at least one and z uh our proof of concept or reference implementation in python and sage over multiple cipher suites restart that was popular ed25519 is very popular next please um so some of the latest updates there was a optimization that was proposed to frost that would take a order t scalar optimizations for if you have t possible sign of t signers in a single signing uh protocol uh down to one order one and we updated the protocol update the specification to include this but there was some further analysis that showed this introduced a inter-round malleability in the set of signers not malleability of this resulting signature but it wasn't clear that the people who started the signing in round one were provably the same set that finished in round two so we backed this out we decided to eat the cost of the um of the further scalar multiplications and just went back to the one where that was no longer an issue so that is in the current version in v7 of the draft and another big update is um we were having trouble um having a consistent subroutine definition of how to do"
  },
  {
    "startTime": "00:54:02",
    "text": "verification for these signatures because the ed25519 document and other documents uh have led to their verification uh subroutine they don't really have one they have a two lines of text um suggest things that we would rather are uh musts and they're sort of just sort of like anyway we have made this easier by having per cypher suite verification which is basically the same operation you would do for singleton so we're just trying to remain compatible while also like kind of giving strong recommendations of uh some of these cofactor curves of how we would like to see implement implementers implement verification um and then for general schnorr signatures over prime order groups like restretto and like p56 that don't have an equivalent verification equation defined anywhere we give a if you have a prime order group here's a verification procedure for a signature that uses a prime order group so we include that uh next please so we think it's in a good place and we're seeking crypto panel review um and wider cfrd review uh we've been cranking on this and talking to many people in the broader cryptographic community uh but we think we we are ready for the cfrg to really uh really take a look at it now because we've we've grief cranked on a lot is there anything unclear in the draft is there anything ambiguous in the draft uh is there anything technically incorrect it's insecure or unsafe in the specification um is it written in a way that embedding it into higher order application protocols or other things uh is clear and not full of"
  },
  {
    "startTime": "00:56:00",
    "text": "friction um we always welcome more implementations um we do have we haven't added it yet but we're interested in adding another cipher suite over sec p256k1 which is another primary curve that we would like to see supported that hasn't been done yet but we'll see if anyone else is still interested next please and that's pretty much it do you have any questions hi there uh daniel conglomerate from the aclu uh thanks for working on this um i'm wondering whether you think that we need to update 8032 because of the looseness of the uh 255 19 signature verification like do you see a path forward for that is that something that you would be interested in in seeing happen what are the roadblocks to that i mean this is not the only place where this has come up i would be very interested in seeing that happening i don't know what the roadblocks are but i will say that every implementation of ed25519 worth itself and i've worked on a few uh are all they're all doing the co-factor check especially in places as opposed to the as opposed to the co-factor-less check which is one is sufficient not strictly required and the strictly require and the one that uh actually multiplies by the cofactor is the one you need for um for example concept consensus interoperability um and also you want if you are using this uh where low order points or torsion points are an issue which is a lot of places you should just do the cofactor mall and if the uh if the extra cost of that is an issue i i don't know what to tell you like you you can run into serious security issues if you're not doing this co-factor check"
  },
  {
    "startTime": "00:58:00",
    "text": "uh i would highly like to see that document updated uh as an implementer and a person who does cryptography and that gets deployed in the real world [Music] okay okay one more question from inside the room and then we have one from uh meet echo great mike ellensworth um so i'm working on the composite sort of combining pq and classical together for security stuff within lamps uh this seems like it's sort of solving a different application problem but with the same technique so i'd love to chat see if mr were on the same page or sure yeah um a lot of basic group assumptions and the security proof reduces to one more diffie-hellman assumptions so we can talk but there are some security reductions that may not be available to pq systems [Laughter] okay uh the question online is no longer here okay so the queue is closed uh all right thanks dear drive thanks i'm here for today and then i'll be leaving so come find me all right chris wood uh i'm gonna do both of them it doesn't matter which order all right uh so this is just an update on the the keybinding for signature schemes draft that was recently adopted uh next slide please so uh just to remind folks what this draft is all about um imagine you did not have um imagine the setting where you have a like a what we call a proven a verifier approver wants to sign some message and have the verifier check that this particular signature on this message is"
  },
  {
    "startTime": "01:00:00",
    "text": "fine kruger gets his input a secret key and a public key and a message as well does the signing thing sends the message in the public key and the proof or the signature all the way over to the verifier which just determines whether or not it's valid um and sort of uh obvious we we don't want the uh the signature itself to be unforged or forgeable so the approver can't trick the verifier into accepting this particular proof of the signature without actually like running the signing algorithm next slide please um but there are other interesting applications wherein you might not want the verifier to learn anything else about the prover like you might not want the verifier to learn that this specific public key that this specific prover owned was used to verify this message um this application's in tor this has applications in a variant of privacy pass as well as other cryptocurrency settings which we talk about and i will describe in the next slide um so imagine you had a slightly different scenario where you have now two provers each of which have their distinct sets of uh you know private and public key pairs they're both signing a message um they run their signing of them independently and they send them to some with some middleman or intermediary we call the mediator mediator choose a bit chooses a bit at random and then sends the message the public key corresponding to the bit and the signature corresponding to the chosen bit to the verifier and the verified dose the check as before it wants to see if the signature over the message is valid um and it also wants to try to determine what was the bit that was chosen by the mediator next slide please um so uh we have the same property as before the the signature itself is not uh forgeable which is great next slide"
  },
  {
    "startTime": "01:02:00",
    "text": "um but we don't have this other property of sort of unlinkability wherein um the verifier doesn't learn basically which prover generated this particular signature because the verifier is assumed to have as input the public keys corresponding to zero improver one then they could just simply check does this valid validate with uh public key zero or does this validate with public e1 and that determines my bit so that's the property we want next slide please and so this this signing with key blinding extension of a digital signature scheme is basically how we go about constructing this and it basically has two fundamental properties the first of which is that the per message public keys that are produced are independently distributed from what we call the long-term public keys of a particular signer so these uh pk0 and pk1 in the previous slide and moreover that the signatures that are produced don't leak any information about the long-term signing keys and this is what the this construction allows us to to achieve next slide please so i said it was a sort of an extension of a basic digital signature scheme and um the extension basically adds in three new functions on top of your you know classical key generation signing and verifying three of functions are a new function to generate a blind key blind key gen a function to blind a public key and then a function to sign with a blinded key and the uh the correctness property that we want is sort of listed at the bottom so you can run the verification algorithm uh over a blinded public key um uh or against a blinded public key uh using um a the output of the blind key sign function uh with that same blinded key and everything should work out just fine the verifier is totally unaware that this particular procedure took place behind the scenes it just knows that the signature is valid and and well it's not forged which"
  },
  {
    "startTime": "01:04:00",
    "text": "is which is great next slide please there's uh also optionally the ability to add yep another function to this particular construction the the ability to unblind a public key so if i give you a blinded public key as well as the blind key that was used to produce that blinded public key you can invert the operation and recover the sort of the long term or the the the input public key that was not blinded um so here you just basically the inverse of one another um and i don't know what happened to me echo but i'm just gonna keep talking um and so the question uh for the draft specifically was how do we achieve this sort of optionality in practice um next slide please and so we chose to do uh the proposal that we came up with was to sort of contextualize the blind public key function itself so previously it just took as input a long term public key a fresh ephemeral blinding key skp and that was it would just produce a blind public key but now we slot in this additional context string um where context can be entirely application defined so for example in the the privacy pass application it's empty it doesn't have any particular value and that allows this unblinding operation to take place because the person who wants to unblind knows what context is and they can do so in the context of tor there's a slightly different construction for specifying what the context is includes the long-term public key and like a time stamp uh whatever i forget exactly how it's encoded but that's functionally sort of what happens um and that sort of prevents this unblinding step from taking place because you don't know as a verifier either one of these you don't know the long-term public key um the proposed change is up on a pull request it's been reviewed by a couple people um it seems pretty straightforward and obvious so we'll likely land that after we get some implementations and uh new test factors"
  },
  {
    "startTime": "01:06:00",
    "text": "going next slide please um okay and then just to remind people about the implementation status so we have implemented this for all of the signature schemes that are in the draft both edu dsa and ecdsa um and there are different inter they're interoperable these different implementations we do have a security analysis of uh specifically the unlinkability and unforgeability of both eddsa and ecdsa completed it's under peer review right now so not publicly available yet but if we you know ask the right potential we can make it available um next steps for this are to hopefully merge the pr that i just talked about and then start soliciting uh early review ideally from the crypto review panel because if that's at this point we think the document is feature complete um next slide i think that's it actually yeah questions [Music] uh if there's no there's um [Music] queues open online anybody joining uh it seems not thank you chris let's move on to the next presentation hello hello hello i'm john so finally is it rsa or sequence i i got one i got it wrong chris is going to"
  },
  {
    "startTime": "01:08:00",
    "text": "chris chris we'll do another presentation there's one more presentation and then we'll then we'll get to you okay um so this is a really quick update um thankfully in the interest of time for a while now we have the address uh this this draft and the group uh on uh specifying blind signatures based on rsa that have been used in a huge number of applications so it was about time we wrote down how to do this correctly um next slide please uh thankfully uh we had someone in the community conduct a security review of this the scheme in the paper um unfortunately however um it raised a number of uh concerns with the draft that specified um that were not things that were considered in prior analysis of the construction um the main insight was that previously when trying to reason about the blindness property of blind rsa folks did not consider what happens if the signer's public key is generated maliciously because you're trying to define blindness against this person who's verifying the public key and if they construct this public key in a particular way in a malicious way information can leak when the client engages with the protocol which is not great and this is specifically or especially problematic for the deterministic variants in the draft specifically those that use the full domain hash for the like actually encoding of a message and any other sort of deterministic variant like pss with a zero like salt or whatever so it's important to know though this is only a problem with low entropy inputs so if you are using blind rsa for a like a voting application where you know you're there's like five people or whatever in your pool um running a protocol with this type of input could leak your vote um or bits of your vote um but if you have applications where you have a high entropy input like privacy pass that has a like freshly chosen 32 byte nodes as input"
  },
  {
    "startTime": "01:10:01",
    "text": "it's not an issue it's also worth noting that it's also not an issue if you have for example proof that the private or the public key was not chosen maliciously you can in fact like generate non-interactive zero-knowledge proofs that someone can check specifically the client that wants to engage in this protocol to see whether or not the the public key was constructed correctly um but they are not standard um and they only give us sort of like probabilistic guarantees that things are okay so that said next slide um a resolution to this issue and to deal with this emerging security analysis is um basically to uh start stripping away things that we we included for the purposes of you know having the draft be maximally useful for a number of applications um the first of which is we're going to like remove all the deterministic variants from the draft and just sort of enforce or require that pss with a salt length that matches that underlying hash digest is the way to go um perhaps in the future if there's you know some new insight that we get um that allows us to like bring this back or some of the deterministic variant back to do so in a separate draft i don't think it needs to block this particular work and we also added text to talk about what happens if you're an application that has low entropy inputs specifically what you should do to add entropy to your input generated locally on the client um this is interesting because uh if there's like randomness that's generated locally before invoking the signing protocol that randomness has to be like part of the message now that the verifiers will consume and verify um so it does have api implications if you're if you're you know using the construction in that particular mode or you're like generating local randomness or local entropy but if you're an application like privacyfast that has high entropy inputs you can use the scheme as it was previously before without any without any problems next slide please"
  },
  {
    "startTime": "01:12:02",
    "text": "um so at this point uh before we actually merge the change um want to you know do a check with folks to see if anyone had a real concrete application where deterministic signing is actually a hard requirement um such that if we ripped it out it would not break your use case um there's a lot of reasons to to pull this out the security concerns are perhaps paramount um and there are two open pull requests to um uh basically propose a way to to enact the proposed resolution that i just went through one of which um i forget the number and i should have included here my apologies um one of which basically introduces uh like an external api on top of the internal api or on top of the existing api and the specification this external api would be the thing that's responsible for sampling local randomness and then equally or also passing it to the verifier such that it can verify randomness plus message um and that's sort of the approach we're leaning towards right now um minimally invasive um with uh lots of text to talk about how applications should uh use this um in a safe way and uh i believe that's it i don't think there's another slide okay great anybody have any uh feedback on this question for the uh rg i should know also that at this point once uh once we merge this uh resolution uh like the keep lining draft we're basically going to be done we will have the security analysis and and everything complete so whatever is the next step at that point yeah i recommend getting some confirmation on the list with a link to the pr if done okay nobody's in the queue thank you on tbr thanks chris"
  },
  {
    "startTime": "01:14:05",
    "text": "so okay sorry for the night early just now can you hear me yes we can okay well i'd like to give you a short update uh regarding the taste draw essentially maybe essentially we have new content changed in the current draft six which i've uploaded yesterday after the service got open again if you go to slides into the next slide and basically what we have been doing when changing from draft version five to six is that we modified the write-up so and became clear in the discussions that we had on the list and off the list that we probably should focus on a clear set of readers and a clear set of audience and as we have completed the security analysis of the pace in the asia paper which clearly covers the scope and the cipher suites that we are discussing in the draft we decided to focus rather on the on the perspective of the application architecture designer the implement and tester for implementations of cpase so following the feedback that we got for the version four and five drafts we rewrote the introduction completely and focused it on on the implement implementers and that's uh basically the change um which we have applied which covers the new draft version which is online and we would welcome a broader review and uh"
  },
  {
    "startTime": "01:16:00",
    "text": "also maybe a review from a native english speaker regarding the draft and the next step in our perspective would be a review from the crypto review uh users and uh uh people that uh would be using cpace so that's if you go to the next slide that's essentially the message uh i'd like to motivate you to have a look on the new write-up of the draft version 6 so as next step essentially from respect on the content there's been no change any quick comments if not thank you okay fine um right uh the last two presentations are related to recent nist results on bqc contest so hold on okay sophia you can start yes uh thank you hi everyone uh today we're just giving a quick presentation with tom biges about the post quantum cryptography in this process my name is viacelli thank you"
  },
  {
    "startTime": "01:18:02",
    "text": "for having us here next slide please okay so just to give you a little bit of a disclaimer slash preface here um what we're going to be presenting is not a complete overview of the wholeness process because that will take a really long time to do um but we just mainly summarizes the results of the first milestone that this process has actually reached and there's some bias that you're going to hear from us but we try to make the presentation as neutral as possible and we also not uh we're not pointing into actually choosing any specific direction but rather treating this presentation as a conversation we started if you want to learn more um our recommendation will be that you read denise report of the first milestone that they reach next slide and now yeah so if you saw the report uh or some stuff on twitter then you might have seen that uh kyber is the ken that has been chosen for key exchange uh in a post quantum way and there are actually three signature schemes uh we're gonna provide a very very brief overview here and basically just stealing this report and yeah summarizing it very very briefly so if you go to the next slide kyber is the chem the key encapsulation mechanism that has been chosen um there are a few others that are still in the running uh that might be standardized later but that will probably take a while still kyra's quite fast quite efficient and is based on lattices uh something which is important in the context of making protocols and putting this into stuff is that it's important to realize that cams are not the same stiff helmet so while it has been applied into in for example tls very straightforwardly um because chemists are interactive uh you cannot put it for example into"
  },
  {
    "startTime": "01:20:00",
    "text": "signal next slide please okay all right um so term just talked about the key encapsulation mechanisms which are basically the ones that necessarily in order to disturb confidentiality in the face of quantum computers but nist also decided to select certain algorithm for standardization for to protect the authentication in the face of quantum computers and those are the signature schemes and the ones that were chosen were three the first one is dilithium which is very general purpose and it's the main recommendation that this provides but as you can see here on this slide the sizes of the public gear and the signature of the lithium is much more bigger when compared with the classical counterparts on the bright side though the computation times of the dtm is much faster than the classical counterpart that we use nowadays next slide please the second signature algorithm that needs recommends for the standardization no sorry that this is going to standardize and is recommended to be used it's falcom which sizes are indeed better if you compare them with the lithium and could indeed be used in certain protocols the problem with this is that in order to instantiate falcom you have to use a floating point arithmetic and that could be really cumbersome for certain implementations because the implementation have to be done very much correctly so there's the assumption that you could use this scheme but only if you are correctly implementing it next slide please and the last one the last initial algorithm that was proposed to or that is going to be standardized this is things which is a stateless hash based signature that maybe you're already a little bit familiar with because the cfig has already a standardized xmss and lms but as you can see here um the algorithm has nice sizes for the public key but not so much for the signature and indeed because it has a lot of hash function"
  },
  {
    "startTime": "01:22:01",
    "text": "calls it is pretty slow and also there are many parameter sets that essentially are going to be determined by this but as it is currently right now this system algorithm has a lot of parameters and the reason why it was chosen is because the previous schemes as sorry the lithium is based on lattice-based assumptions and this one is based on a set of different assumptions so it provides a wider range of security notions for this scheme next slide please yeah so to briefly refresh or reflect on those current itf standards um it's very important to realize that those are stateful hash based signatures so although they are quite a bit smaller than what sphinx bus is giving you this condition and if you for example uh restore your key from a backup you're completely screwed so that's why it's important to note that this is definitely a very significant difference um falcon and the lithium are of course uh based on lattices and do not have state next slide please so i already briefly mentioned that there's a few more camps that are still in the race uh if you read the report it's more or less feels like uh and this is either waiting for more research uh and waiting too much mature certain schemes a bit more and otherwise is it exactly sure what to do so mclees uh bike and h2c are all based on codes uh classic mccleese has a very tiny ciphertext but probably keys in the megabytes so this isn't sure if that's actually useful uh tell them if you actually do like it psych is very fancy i suggest math but it's very new still quite slow but actually very very small uh compared"
  },
  {
    "startTime": "01:24:01",
    "text": "to geiber so that is very nice and bike and hqc are sort of similar but have slightly different tradeoffs you see that these schemes are all not lattices which seems like uh yeah the main feature uh in these alternates and then there's going to be some signatures as well and that's what sophia is going to tell you about in the next slide yes finally as they say the next process is not ended but rather they reach their first milestone one thing that they also announced is that the aston said they're going to be still advancing certain algorithms for a fourth run and they also are calling now for a next call of proposals uh specifically for digital signature algorithms and the reason is because as you saw the majority of digital signatu uh signature algorithms have like bigger sizes that maybe would be cumbersome when actually putting them into networking protocols so this new round is going to come there's already some hope of certain algorithms like for example mayo that seems very nice but it still needs a certain security checks to actually attest to the correct security of the algorithm and of course also to just uh let you know don't expect any misa standards before 2030 because at least the end of this next call of the process is going to take several years as it did with the previous call nexus live please okay if you are actually very interesting i will want to really start running code there's already all of these libraries specifically livoqs it's a really great library because it performs a lot of safety check of the best quantum algorithms and it has many bindings to different other programming languages there's also pu clean and pqm4 especially specifically for embedded applications nexus live please ah this one i think should have been dropped but yeah so these are some of the questions that you might ask today basically do"
  },
  {
    "startTime": "01:26:02",
    "text": "or a problem can we go to the next slide so here we have a collection of links uh notably this report that we believe and we plagiarize here and also some other fun research papers as well as some blog posts that have a uh real-world measurements and a nice talk from some colleagues of mine that explain kyber for a more general audience if you're not very into lattices these links are all in the in the slide deck so as far as just clicking the next slide and that wasn't already um we were trying to keep this slightly short to enable asking lots of questions or perhaps start discussion um so i guess we'll be taking questions and there's one comment i can make already uh responding to the chat um the non-interactive versus interactive thing with kens is a very important issue and it's important to realize right now that there's no solution for non-interactive postpartum key exchange yet uh that we have a lot of confidence in uh there is seaside which is not in this standardization project and will probably not enter it right now seaside security is hotly contested and although it's quite small it's also very very very very slow even at the very aggressive parameter set that exists today and there's some work that i've seen that suggests that if you want to go conservative for that scheme you're looking at tens of seconds of operation times which is not nice um but seaside yeah it's very new i don't think that will be standardized anytime soon so it's looking that for now we will not have"
  },
  {
    "startTime": "01:28:00",
    "text": "non-interactive key exchange post quantumly there was also one question regarding uh lattice's attacks um yeah it is true that there has been some research about the matter but no drawing conclusion it uh there has been another clear conclusion about the matter and so far it seems that the security assumptions under which the specific lattice problems that are uh used by the quantum algorithms seem to be secure but yes perhaps something's yeah so this is all of course the bare minimum schemes so you need threshold or fancy stuff i think there's a lot of research to be done there still um and i see falcon being brought up and i think it's important to emphasize that nist explicitly wrote that the lithium is the primary candidate for signature schemes because falcon is very easy to mess up uh which is also sort of evidenced by the there's basically only one implementation of the scheme which kind of is a testament to the fact that if just implementing falcon is a technical achievement uh and indeed nist explicitly noted in its recommendation that if implemented correctly which i think is quite telling uh we have a question from uh scott flora this is scott floor cisco systems uh one issue that you did not bring up is ipr concerns uh kyber uh the nist the soleness chem uh fine uh"
  },
  {
    "startTime": "01:30:02",
    "text": "selection uh has plausible patents on it and we don't know if those patents are valid until some judge makes a ruling which will not happen quickly uh nist has uh some agreements uh with the various patent holders but they have not published them so we do not know if they'll be acceptable to everybody uh if that's not the case it sounds like we need to actually have an alternative most likely end true even though it was not selected by nist your comments uh thank you for raising that um that will be true and specifically on the news report uh announcing the selected algorithms for a standardization niche does indeed know this and they think i say i think they say that if the patents have not been resolved by the end of this year um they will be advancing and true instead of kaiba uh so indeed it has been addressed by myth uh yeah i don't know if we particularly want to necessarily wait till 2023 and also uh resolving things to this uh satisfaction does not mean it's as resolving things to everyone else's satisfaction i am we're both not lawyers uh and as she said uh this is the a lot of the information isn't out there yet um i don't think i can speak for any company's lawyers uh but uh i do kind of expect that the fact of the weather is going to be that the next fib standard will require kyber uh whatever if if nist does uh reach some agreement uh i have no idea what that means uh for adoption etc i think it's a worthwhile discussion to have especially as we reach more information but i yeah i really can't say anything useful about that right now uh actually so far uh as far as this is concerned you are perfectly miscompatible if you're using"
  },
  {
    "startTime": "01:32:03",
    "text": "and some other mechanism such as entro that is perfectly acceptable to them yeah for the moment uh if certification if you concatenate a classical algorithm with an experimental one for the key exchange part i think it was fibs 140 it's compatible yes um if that will change i don't know um so maybe this is something that indeed could be started at a discussion um but i don't think we will get a lot of clarity also in the coming months so my recommendation would be to kind of see where the different discussions are going around this yeah personally i'll be putting together a a i submit a draft submission to to the cfrg to for an entry uh uh so let's see if if they can can um uh make that into an rfc as well just in case uh the the nist approval process does not uh uh patent process fails all right last question from the room all right uh i was curious uh you noted some of the difficulties with falcon implementations etc and i'm working downstream on some of the post-quantum serializations more like the jose cose side of things um question about how should we be thinking about non-lattice-based stuff because this was pretty clear that there's going to be steps down some of that pads are there two or three things we should be looking at early just to be aware of on the non-lattice-based approaches and prioritizing some investigations around i think i could comment on that so a scheme that last month uh but rainbow is a refinement of a uov and it's pretty the way that the call for new schemes is written it's pretty clear that uh"
  },
  {
    "startTime": "01:34:00",
    "text": "they're basically calling for a uv submission uh so then you're looking at uh computationally it's going to be somewhat okay for certain values of okay but public keys are going to be massive 400 kilobytes massive um but very small signatures on the other hand uh so i think if you want to play with something right now there's some code for a rainbow still out i expect that uov code will also be available soon and there was already some discussion on the pc forum the nist mailing list about parameter sets for uov so you can get some idea of the sizes there but i think that is the most significant uh thing that will i would bet money on that that will make it into the on-ramp for new signature schemes uh there's some other stuff that we put in the slides and that's much less certain if that's going to make it because the those kinds of schemes are much newer and this did specify that they want mature crypto analysis to exist of your scheme and uov itself um rainbow was a variant of uov and the attack lies in that variant and i think that the confidence in uav itself is quite high so i think that will probably be accepted into this thing okay thank you tom sophia last presentation of the meeting is going to be boss restaurant good morning can you hear me yes perfect thank you tom thank you sophia next slide please"
  },
  {
    "startTime": "01:36:05",
    "text": "i think tom and sophia basically covered everything on this already so kyber is the post quantum key agreement that nist will standardize first it would likely perform great so that's very good news um next slide please um there are so it will take some time before nest actually has it standardized they just said that they will standardize it uh we exp probably in 2024 but we never know uh there are still changes likely to kyber um but we ex i expect many early adopters before 2024. next slide piece that's why uh we would like to have an rfc a draft but we want to match nist's final standard so what's the point then well the drafts can be used as a reference for early adopters instead of everyone doing something different a different advantage is that nist will not include a machine readable specification and we intend to in this case python but maybe something else also it will um have the idf closer to the standardization as the cardboard team is very eager to help out on this draft and it will unlock early usage in for instance tls because code points are cheap next slide please so any questions uh concerns is there interest for this"
  },
  {
    "startTime": "01:38:01",
    "text": "phil baker yeah i'm very interested in this uh i i don't particularly care if it matches what nist comes out at in the end because what i'm concerned about at the moment is not doing pqc cryptography i can't because i'm doing threshold but i would like to be able to set myself up a safety net of shared secrets and routes of trust that a pqc hardened so that if there is a quantum computer produced i can then back out and recover my security context uh you know i can do a new release that is secure but i'm only looking to establish those shared secrets for uh you know a limited application and the other thing you can do is once i've got a shared secret i can create a global mix in that will pqc and all the other stuff like russ housley did in his draft okay uh any other questions in the room we have one online and then we'll take the question in the room um i think i'm online that's you um hi um i'm flo from uk ncsc um i can say it's really really good to see this happening i'm kind of following on from nist's announcement um i guess as a contrast to the earlier comment i think it's really important that eventually we do match with nist specifications um i appreciate all the reasons to start doing this work now but i think in the end it's critical that these match up and we don't end up with fragmentation between standards bodies thanks okay thanks um and uh i want to remind folks that the tls meeting is uh later in the day and uh i don't know if the tls chairs have anything to share about opinions with respect to um early definition of pq algorithms for key exchange in cfrg whether that's"
  },
  {
    "startTime": "01:40:00",
    "text": "helpful or not uh if you have anything to say no nothing to say at this time great okay well um uh we are ending a little bit early despite the uh delay in the start uh folks did kind of push through these presentations pretty quickly so thanks everyone for attending and uh thank you rich for taking notes and we will uh have the have the notes up sometime soon um thanks all for coming [Applause] um yeah i'll just surely try to catch up with danger it's really just reducing is [Music] um"
  },
  {
    "startTime": "01:42:13",
    "text": "um that's the"
  }
]
