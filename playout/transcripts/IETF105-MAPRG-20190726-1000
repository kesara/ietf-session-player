[
  {
    "startTime": "00:00:12",
    "text": "we\u0027re just testing the mic but we can start anyway so welcome to idea Friday morning met Reggie session I\u0027m Mia criminal and I\u0027m Dave Wonka I thought marrying you that but other people might not know yeah so this is an IIT of session we also have basically a note well which is kind of aligned with the ietf note well but slightly different terms but I guess you\u0027ve seen that what this note well is actually not pointing at is the code of conduct and harassment policy but it\u0027s also important things to note so please be aware of all these things this is the user site which is in there for your convenience with our mailing list and and also the jabber room and the link to this session may need to look it up later and this is our today\u0027s agenda just a couple things I wanted to mention about the status of the group and things that are really healthy and mapper G as well you know we have a lot of people we always have a packed agenda as we do today and we\u0027ll work our way through here one of the things I wanted to mention is the last couple meetings I\u0027ve went to PRG the privacy enhancement and assessment research group that\u0027s measurement related as well and they\u0027re doing some interesting things so look for opportunities I think to sync up with what they\u0027re doing and watch what they\u0027re doing especially when it\u0027s about privacy with measurements the the next thing let\u0027s go the next slide so this is the way I was greeted when I got my badge this week none Blanca and it may be they are anticipating that I won\u0027t be there at the next in Singapore so so we have one Blanca this time in non-pom Connect next time can you go to the next slide so Mary and I were going to call for an alternate or additional map our gt-r chair we\u0027ve been running for about four years I\u0027m not meaning to step from it but we could certainly use help with another person when we\u0027re covering meetings in different parts of the globe so if you\u0027re planning to attend IHF 106 in Singapore and are interested to work with the two of us on preparing the agenda and and vetting and reviewing the candidate work there let us know we\u0027re especially looking for someone who is in academia not because it\u0027s a credential that you need just because we\u0027re both in industry right now and you might have some contacts that we don\u0027t have oranges upon so it\u0027s not a requirement but but if you have that you might like to join and then like I said you\u0027d be able to curate the upcoming program which is kind of fun and it\u0027s a little bit of work about like Miria this time for instance did a lot of legwork contacting people to get to get the program filled out so feel free to write us about that I think is that the last thing we had there yeah that\u0027s the last of that so "
  },
  {
    "startTime": "00:03:13",
    "text": "our our first presentation is going to be is going to be a remote because Yoder song is in New Zealand and it\u0027s very late there so we\u0027re gonna let them go first and let\u0027s see your are you in the I see it in Java are minute sorry in the Medeco huh hello hi we can hear you fine we\u0027re you\u0027re ready to go hi everybody my name is Yoda so I will be presenting the paper understanding evolution and adoption of up level domains in DNS sick this paper was recently published in the 2019 I Triple E imminent measurements in the networking conference and other co-authors of this paper are Surya Ramachandran and dr. Anika mohanty both also from the University of offered an excitement so we focus from the top-level domain and speaking about domain in the das hierarchy and we also provide a brief overview of the ASIC implementation in New Zealand specifically generic top-level domain or ggod is one that does not represent a country for example calm organi and a key motivation for this study was the new GT program introduced by egg-cam which introduced over a thousand theories through the internet since 2013 next likely so that\u0027s a timeline of the generic top-level domains and when they were introduced so in this presentation we will classify generic top-level domains into three groups owed for the ones created in the 1980s early for those created between 2001 and 2012 in a new generic top-level domain for our thousands of new generic top-level domains introduced since noon we will also look at country code top-level domains OCC Tod later on next slide so in terms of related work let\u0027s paper exchange upon the 2015 paper in analyzing top-level domain and second-level domain adoption we also later on briefly look at being s implementations specifically in context of the New Zealand public domain NZ next time for data collection we used two different sources for our data so we collected packet traces from the campus network of University of Auckland which is the largest university in New Zealand was over 40,000 users in addition we "
  },
  {
    "startTime": "00:06:15",
    "text": "also collected metadata for an internet NZ the manager of NZ New Zealand national top-level domain for three national level data sets meaning classified the generic top-level domains into the three categories old early and new based on the time of introduction and finally we carried out pub level domain and DNS sick analysis in it looks like so moving on to the results next slide for trace over you we found the number of DNS queries to grow between 2013 and 2018 was the proportion of requests to early G theories new G theories and country code top-level domains are increasing while there has been a lot of growth in the new generic top-level domains compared to 2013 they still only account for around 0.3 percent of total requests in the local campus network next time in terms of the top 10 most queried top-level domains we find some common generic top-level domains that calm donate door and in terms of low country code top-level domains the NZ not a UN dot cm for New Zealand Australia and China or regularly appeared in at opt in every year there were no new general public domains at all in the top 10 in any of the years and for early early generic top-level domains the only one to appear was not info in 2014 and 2018 next time so for unusual top-level domain usage we notice the country code top-level domain for Slovakia or SK 2 ranked 4th in both 2017 and 2018 we found over 99% of these DNS requests to actually be machine generated as part of antivirus software used by the University so different from the fully qualified domain names of usual websites such as say wo comm we find a fully qualified domain names of these machine generated requests to be much longer and containing seemingly random combination of alphanumeric digits so while there was rapid growth in the number of queries to Slovak and SK TOB we found that this ghost was not actually driven by human activity but rather just the machine generated DNS requests next we also found in terms of unusual clearly usages that the country called pop like the country code top-level domains which were not restricted to only the citizens of that region may be often used as generic top-level domains instead for example the thought IO country code "
  },
  {
    "startTime": "00:09:15",
    "text": "top-level domain for the British Indian Ocean Territory was topped here most most query in both 2017 and 2018 and this was not due to interest in the region itself but rather because the top-level domain is quite popular among startups and open source projects online so the IO and several other similar country code top-level domains they are now heavily used as general public domains instead are now classified by Google as GCC Tod or generic country code public or domains similarly we also found that you the usage can be used by domain hack or concatenating sections of the fully qualified domain name to form a different name so for example as is commonly seen in youtube link shortener YouTube PE which uses the top PE country code top-level domain for Belgium so what may now be a country code top-level domain may be more accurately described as a generic top-level domain in the future so the top level domain classifications we identified earlier will need to be regularly updated to reflect that changes next time in terms of Tod grows from 2013 to 2018 we find a number of queries to new generic top-level domains to far outpace the old generic top-level domains and there was also some spillover effect into the country code top-level domains where the generic top-level generic country code top-level domains as classified by Google grew at a much faster rate than on generic country code top-level domains next slide for these ranked frequency plots the top one is fall 2013 and the bottom is for 2018 so between these weekend most notably see growth in the number of new generic top-level domains we can also observe there there was this zips though in old and early junior top-level domains and let learn new Jeff top-level domains are much less concentrated as shown in the flatness slope in the graphs next time in addition to top-level domains we also looked at second-level domains so in this example here the dock hole for company before that ends it New Zealand country code top-level domain it\u0027s the second-level domain so similar to the new gTLD program by I came the second level domains in New Zealand were open to public registration in 2014 so this means that instead of choosing from one of 15 predefined second-level domains a company can theoretically use any name as the second-level domain so the second-level domains account for over twenty percent of all new domains but "
  },
  {
    "startTime": "00:12:17",
    "text": "only account for 0.6% after a total requests in the local network next time so right so one of the potential problems from opening both general top-level domain and second level domain registration to the public is that there could potentially be overlaps between the tree for example QE is a term used to describe New Zealanders and currently there is both a doc Kiwi top-level domain as well as a doc Kiwi the NZ second-level domain and the New Zealand in the top-level domain we found an unusually high number of Kirito NZ domains registered shortly right after their release as shown a spike in a graph which we think could be due to cyprus for him from those who already have stock kiri domains and wanted to avoid others registering for similar domains today all next time so for the SI being a sick overview we found a spike in the DNS SEC implementation in only 16 likely from cloud Freya\u0027s introduction of universal dns sick we also found that since noon and secured that equations are slowly growing again and now while the proportion of signed domains with expired signatures has decreased quite significantly since the invaded district holes still remain high with little improvement over the last few years next slide so fulla takeaways next time the takeaways for the domain names so we find nearly introduced your top level domains and not ends it second-level domains to grow in popularity but still not very heavily used yet advantage of new general public or domains is that the cause is this publicly concentrated in the most popular domains cyber cyber squatting on a large skillet theoretically that\u0027s feasible compared to cyber squatting on the older generic top-level domains however a potential problem we found is the overlap between similar domain names and the top-level domain at second-level domain level which will require further coordination in the application stage to prevent this from happening next time so for Dina is sick we found the implementation in New Zealand is growing but chain of trust is incomplete main problem being that the incorrect Deus trickles without which it is not possible to authenticate the public TS key records in the chasm we found an improvement in reduction of expired signatures as a proportion of the total signed domains which we see as a growth for a basic implementation in New Zealand next time so finally the "
  },
  {
    "startTime": "00:15:20",
    "text": "concluding remarks so in this paper we presented a longitudinal analysis on the adoption of new generic top-level domains and the deployment of DNS sick in new zealand new generic top-level domains are growing in popularity but still not very popular yet we believe there needs to be more work done on exploring the top level domain topic further before the second round of new generic top-level domain applications in the future thank you thanks eurozone so we\u0027ve got time for Q\u0026A and I see George\u0027s up at the mic over here so let\u0027s go there hello hello George Michael from a pinna cue can hear us yes so I think a study like this needs to explore the concept of D G a or generative domain names hash names which are a mechanism for command and control in botnets and so aspects of behavior in the new gTLDs relate strongly to compliance checks on how you justify your need for a name the old gTLDs and ccTLDs tend to be harder places to get a name and the newer younger TLDs often are attractive targets for bad actors you talked about ESET using hash names and machine generated names for good purpose but I think you have to address the question of the use of hash names for bad purpose the second thing is that many of the new TLDs have no intention of offering delegation services to wider public industry dot iBM is not an open domain it is a private closed domain and its growth will be a function of IBM\u0027s mission it\u0027s not a function of a market process it\u0027s a function of their need for labels under IBM the third and last comment is that you typify cctlds as if there was some normative behavior that is prescribed by ICANN but the quality of a ccTLD is that it reflects an independent economical nation-state that in effect is a UN chartered entity that has standing as government it\u0027s not about a contract binding or a belief of meaning it\u0027s about the fact that they have an army and so they are only different in the quality that they own tanks and not specifically that any legalisms govern what they do but your paper was very interesting and thank you for presenting it thank you "
  },
  {
    "startTime": "00:18:23",
    "text": "Tim Tim welcome back actually quickly ask you to step over to the other mic because that\u0027s where the video is currently okay hi now I\u0027m on video so thanks thanks for your talk what I would be interested in and maybe you can a little a little bit elaborate on that is where did you did that get the data or which kind of data did you have for your analysis so you talked about you had a collaboration with the top-level domain registry but what did they gave you and how did which which kind of measurements or analysis did you do doing that data so know what\u0027s in this so they were in the presentation that was specifically the DNS SEC overview graph as well as the second-level domains created over time for Kirito ends it\u0027s like okay you\u0027ve got like these numbers from them or yet like I suppose you didn\u0027t get any zone files or something like this way you did measurement okay and as the last remark I just wanted to drop the ripe Atlas Project if you want to do again as like measurement stuff you can also do like any kind of DNS records they\u0027re also limited to direct geographical regions or something like this that might be interesting for future research Thanks thank you now this is a lush pizza from the Swedish in the registry first I want to say to George we are not a government agency we never have been you never will be the Swedish government has not been involved in the running of the dot se registry and for the record we don\u0027t own tags so it CCTA these are really very wrong over the map so but back to the topic you said that there is a lot of domains more domains signing the NSA then there is domains of sending India\u0027s records and in that I would like to know do these newly signed domains support the CD s CDNs key because that is then something that registries could actually use to foster the adoption of the NSA further metadata we were available I don\u0027t think this information was available for us to see so I can\u0027t comment on that thank you thank you Ben Schwartz Google you mentioned that 10% of DS records you "
  },
  {
    "startTime": "00:21:27",
    "text": "you saw were invalid you clarify what you what you mean by invalid here so for example are you saying that those domains if I tried to resolve them through a validating recursive would fail with surveil I believe it\u0027s the recall don\u0027t match with the hash of child child the ASCII records but I\u0027m not entirely sure on that okay thank you thank you Cosmo could you enough room Jake could you could you speak up to the mic Ilocos Sur Cosmo could you honor whom JP arias about top ten mostly low tier de Graaff I get a Sameera I know I see you by using loo talks about data under which data do you use the most queries will based on local campus network local the campus network we okay I see thank you and if I could ask one last question your song the you mentioned in your takeaways the problem of the overlap for instance the doc heated at TVNZ I was just curious how effect how effective can one be it at identifying that those are essentially clones or what are the methods that you use or you can imagine we can use to determine that are there essentially aliases do you have to do active measurements or can you determine it passively from from the registrations we actually didn\u0027t look specifically into the domains that appear in tough human doc here we go ends it but I think that we can look into that further in the future alright um well have a good evening thanks for staying up late and presenting this work to us and let\u0027s a round of applause for Tommy Tommy we\u0027re ready for you [Music] thank you so Tommy\u0027s coming back to update us I think for the third time maybe on yes if this is hi I\u0027m Tony Polly and I\u0027m gonna be sharing some client measurements that we\u0027ve taken I\u0027m at Apple and so this is information that we get from looking at sampling from iOS and Mac OS devices previously we\u0027ve shared a lot of information about how we see ipv6 adoption I\u0027m gonna be sharing a little bit about that now but one of the new cool things that\u0027s happening is that we see a new protocol version TLS one "
  },
  {
    "startTime": "00:24:29",
    "text": "three coming out and ramping up and I wanted to share some information about how we see this happening so as some background on what we\u0027re looking at here TLS 1/3 was published as RFC eight four four six last August been in development for a while but things were not really I\u0027m deploying it at marked scale yet now on our clients we enabled TLS one three by default for pretty much all connections in March and so this was my OS 12.2 had Mac OS 10 dot 14.4 so at this point we were able to start collecting data about how often we see TLS one three actually being negotiated with servers that our client devices are talking to and you know anecdotally we had heard and I think many people are aware of TLS 1 3 being rolled out we were aware that many server deployments were turning it on in some percentage and increasing it and ramping up support but I at least I have not seen a lot of data I\u0027m shared from either server side or client side of exactly what scale we are seeing yet I would love to see data that anyone else is willing to share but we thought we would kind of take a step here and let people see how the adoption is going so the questions that we asked before looking at the data was first how much server adoption are we seeing of TLS 1/3 from the perspective of the clients like how much is this growing and then also one of the key things about TLS 1/3 from a measurement perspective is that it\u0027s supposed to have some performance benefits in almost all cases it can shave off one round trip from the handshake time so we want to see is this actually playing out in reality how is this actually improving the overall connection establishment time that we see from the client so before I get into the data just some notes on the methodology we are collecting this data on a per connection basis and in this case it\u0027s a per TLS connection we\u0027re not looking at for this data anything that\u0027s not a connection that has TLS enabled on it and this is all going over TCP what we\u0027re collecting is the TLS version how long the TLS handshake took the alp n value that we negotiated over TLS is this HTTP 1 or HP 2 as well as information about the address families are we going over v4 v6 and our v4 v6 enabled on the network that we\u0027re connecting over and so because of all of this what we\u0027re representing in general is the amount of "
  },
  {
    "startTime": "00:27:30",
    "text": "TLS 1/3 that is being used this is not strictly the number of servers or the percentage of servers that have enabled 1/3 but rather like the percentage of popular servers or how often our clients really using this in practice we imagine of course that there is a long tail of things that have not turned on new protocols but that users don\u0027t often access and the data for this presentation is being collected over around a three month period this represents April to June and it\u0027s a random sample of for those devices and those users who have opted in to data collection point zero five percent of all the connections so there\u0027s a large scale of connections and this is getting a lot of data but it\u0027s a small sample of it all right so here we see a trend over the three months we are sampling the green is the amount of teal s13 that we see compared to the overall amount of TLS and I\u0027m only showing teal s12 and TLS 1/3 the older versions are there but they are extremely tiny they are less than half a percent it\u0027s not interesting and so we see a really steady march up 10% 17% 22% the the amount of servers that are turning this on is definitely growing but we had other data in here and so we want to start correlating it and it\u0027s very interesting if you look at this same trend when we look only at connections that established over ipv6 and here it\u0027s a similar trend very similar line so we see that the numbers are kind of being ramped up on these servers but the the absolute numbers and absolute values of TLS 1/3 are much higher and as of June we are already up to a third of all of our connections when we were going over v6 we\u0027re using TLS 1/3 so this is great so looking a little bit more into this kind of correlation because this is an interesting thing we noticed in the data so we wanted to dig a bit deeper we ask the question how likely is a server to support TLS 1/3 if it is also upgraded to support ipv6 so this data is now only looking at connections that are established whenever on a dual stack network so we have both before and v6 available we\u0027re asking for both a and quad 8 queries and so we\u0027re essentially checking does this server support v6 and we see that of the arguably larger number of v4 only servers only 12% of them by June had adopted and enabled 1/3 but of the servers that had previously already been rolling out ipv6 "
  },
  {
    "startTime": "00:30:31",
    "text": "that number is of third it\u0027s at 36% so there\u0027s a fairly strong correlation there and kind of looking at this the other way around when we look at the overall amount of server enablement of ipv6 today when we\u0027re on dual stack networks we\u0027re seeing about 40% of connections go over ipv6 but when we sample only for those that are going over to us 1/3 so if people have been upgrading recently we see that at almost 70 percent so if you\u0027re looking at you know new TLS connections these are almost all happening over ipv6 this is an interesting correlation here in the data similarly we look at the correlations with a OPN so how likely are you to support TLS 1/3 if you have also upgraded to support HTTP 2 and again it\u0027s actually really really similar number so if you remember for v6 and before v4 had I think was 12% and v6 had 36 or 38 and here we see almost exactly the same numbers if you are kind of a legacy server if you\u0027re doing HP 1 there\u0027s only 13 percent of them have upgraded their TLS but for it should be 2 connections over 40% of them are already on TLS 1 3 and again looking at things the other way we actually see overall from our clients about a 50/50 split of connections use it should be 1 or hb2 on any given network but if you only look at the connections that are using TLS 1 3 we see 75% of connections are using HP 2 so again interesting correlation and really quite similar numbers here and then the last thing we want to look at was the performance to validate that other question are we seeing the performance gains that we were expecting with TLS 1 3 and here the answer is pretty clearly yes so this is a PDF showing the number of milliseconds that we see for both GLS 1 3 and TLS 1 2 handshakes and we definitely see a much stronger skew towards the faster handshake times with TLS 1 3 than we do with 1 2 so it is clearly a benefit all right so that\u0027s the data we have to share so the overall observations we\u0027re making is that TLS 1 3 support has more than doubled over the past 3 months the performance wins that we expected are being clearly demonstrated and we\u0027re getting a lot of value here and then the other kind of correlation insight is we\u0027re noticing trends of "
  },
  {
    "startTime": "00:33:34",
    "text": "these kind of two populations we\u0027re getting bimodal populations of leading edge servers that are doing TLS one 3hp to ipv6 and then some straggler populations that are doing POS 1 to HB 1 that are ipv4 only and so you know client developers are looking at needing to optimize for two different populations here and they should be kind of expecting these general groupings and it\u0027s also I think interesting when we\u0027re looking at new protocol adoption I know that these three protocols represent definitely different groups of people at the IETF and I think it shows that we have a lot of common interest in common patterns that when we look at the devices that haven\u0027t been upgrading to v4 those are kind of common enemies of apathy or inertia that all of these protocols are trying to fight against and we\u0027re kind of all in it together in this so questions please hi George again can I just check is this a sample set mediated from your experiment in the client or are you doing some traffic measurement at exchange points or are you bound on the server side what can I might have missed it mm-hmm it informs the the next question so this is your mechanism in Apple for measurement of client view of server behavior right so this is on the population of both beta users and users who have opted in to data collection on across the world on just normal devices that people are using so generally Wi-Fi cell networks that users are on so I\u0027m not using skew in the negative kind of pejorative sense I\u0027m just observing capital exclusively we\u0027re in the domain of the apple handset in mobile distinct from say Apple and Android you would have some question around is this a function of update cycle and availability of things because this is a client negotiation right the server proffers and then you have a downgrade so it comes to a question is if only newer devices you recruit is there some skew here that affects the relativities because if you were across a random population v6 would naturally have a higher uptake of 1/3 because it tends to be present in the newest software releases on carriers and 80% of everything is v6 in mobile and that was really the only point and it\u0027s not to criticize yeah this is great data but I just wondered if that was an aspect yeah that\u0027s a really good point and so I will add to kind of like methodology in the population here we are only gathering this data from the people on operating systems that have upgraded to enable 1:3 so these are not going to be older "
  },
  {
    "startTime": "00:36:35",
    "text": "devices these are of course not devices that we don\u0027t own so it is an early adopters on the client side let\u0027s just one thing to interject we\u0027ve got about four minutes and five of you in the queue so let\u0027s let\u0027s drain that out and then please remember to ian\u0027s what Google thanks for presenting this data it was a similar question about selection bias actually I was wondering if you had attempted to do a hold back experiment where say half users like did not have TLS 1/3 enabled but recorded when TLS 3 1/3 was was available and one the other half like actually used it to get a better idea of like because I would remove the server selection bias that might otherwise be present and we found it for quit that\u0027s kind of important but that being said I suspect your data is still largely in the same direction I just asked ya so earlier on before we turn it on by default we were doing probing essentially detecting when TLS 1/3 was available even if we didn\u0027t use it kind of just as a product decision we decided to turn on full-blast for everyone because there\u0027s the performance when we didn\u0027t want to hold some people back but yes that\u0027s a very good point young lute regarding your last measurements with the PDFs um I was wondering you physically showed TLS 1.3 server\u0027s seem to be better connected more up-to-date what kind of what is the reason for the PDF being so low is it that they are just better connected or is it the protocol that is better yeah that\u0027s again a very good point there\u0027s lots of biases that get introduced this data and I think the other information that we showed of like hmm these are clearly servers that are adopting on the leading edge means that they\u0027re probably optimizing other things more yeah I think you know at least isolated measurements we definitely do see the benefit of one RTT this particular data is only looking at the handshake establishment time right so the RT T\u0027s are a major factor in this so I do think that does dominate this but you are correct thank you professional jobs you I really like to remark about there being as sort of the leaders that are deploying new stuff and I was wondering could you if I can put in a request look at whether or not these people have their domains DNS excited and whether it was validated by just looking at the ad bit in the DNS response that your clients get we don\u0027t have that but that\u0027d be great yeah could you do that I\u0027d be great thanks well noted all right Brian Trammell not as Banerjee Koecher despite the Hat the with another bias question which I\u0027ll actually freeze it as a comment because I think the answer the first question we know there are a few things that you could look at in their responses that might identify server "
  },
  {
    "startTime": "00:39:36",
    "text": "software so that you could essentially Coralie some of these sort of like you know early adopters and stragglers by what the software the earth are running cuz one of the things that we saw like for example with ecn is um nobody actually cares about turning it on is just that the defaults are better and one thing that would be really great for this community to know is to what extent is the work that we\u0027re doing toward driving the defaults forward driving some of this deployment into what extent are these people who are making actual decisions based on the merits of Chios one three Thanks yep that\u0027s a great question there\u0027s of course concerns for us about trying to do too much explicit fingerprinting for privacy purposes but I think developing ways of doing this that our privacy preserving would be a great idea gen-i and god I was actually going to say something very close to that which is have you have you considered looking into fingerprinting the the SSL libraries that are being used at the service to see there\u0027s a correlation between you know if it is possible that maybe does one SSL library that happens to have bad defaults of that and you turn it on and performance actually sucks which is why people have it turned off I don\u0027t know but that\u0027s one other correlation that I was interested in but I don\u0027t know how easy it is to fingerprint SSL server libraries or how feasible it is for you to do it even if it is possible right right and of course there would be the concern about kind of ossified on the fingerprint that the given implementation has you know we\u0027ve been seeing actually issues on the other side of certainly firewalls our other measurement devices expecting certain patterns of our own TLS implementation and having changing behavior based on that so it\u0027s tricky area so just a very quick question I did not see any breakages mentioned in any of this yeah the we weren\u0027t sharing breakage numbers but it\u0027s essentially very low and pretty much the same as one two we don\u0027t see anything that\u0027s really specific to us one three that\u0027s lovely thank you [Applause] thanks Tommy Cory\u0027s next we have now talks to talks related to quick as you expect us to have usually hi I\u0027m Corey first and yep we\u0027ll talk about quick I\u0027m going to talk about a some results from a test bed we built we use vagrant to configure some computers virtual machines we ran tests with quick we use quickly and thank you for the people who helped us do that and we put a real Hardy light path between our two endpoints this work was jointly done by me and Tom we\u0027re all from Aberdeen University so this is a summary plot this is a summary plot of a lot of "
  },
  {
    "startTime": "00:42:38",
    "text": "experiments and you can see maybe look at me right left box first and you can see red which is straight using TTP to do a W at this time of 100 kilobytes across the satellite link and you can see they takes six seconds to transfer there are some outliers and this is working across the hall access path from the satellite and the internet and doing the measurement across the path they could go to the green box in the left one the green box shows with TCP using TLS 1.2 to get the same thing and the green box shows the advantage of a pet the performance enhancing proxy which the satellite operators put in their network and you can see it\u0027s much better on the left with the green than the red so the real question is now what does quick doing what quick has to use the full path it it has to go over the satellite the satellite can\u0027t benefit it so it does quite well it does almost as good as the pepper so which is really good news thanks everyone for working with quick however it\u0027s not quite as good as per happen and you know little bits make a big difference when you do web performance let\u0027s look at a bigger transfer that\u0027s the one on the right though you see the red plot goes a little bit wild why because when you\u0027re using TCP and you\u0027ve got a high bandwidth delay product and something goes wrong the pathology can go many different ways ssthresh can get clamped low and all sorts of things can happen so big variants on the right quick did well but not quite as well as TLS 1.2 uh-huh just a minute and we had a previous talk what about 1.3 let\u0027s put 1.3 in here quick he\u0027s using 1.3 so it should be as good as TLS 1.3 and now we see the gap because TLS with 1.3 is much better than TLS with 1.2 thank 0 for didn\u0027t doing TLS 1.3 that worked really well and quick isn\u0027t yet as we tested it as good as the TLS 1.3 and that\u0027s all right let me tell you more about what we are actually using as our link it\u0027s a satellite access link it\u0027s a commercial offering we\u0027re using an engineering platform the forward link has a bit rate nominally of 10 megabits per second and you can easily get 8 point 5 megabits using iperf across this reasonably good service for an isp returnin link is 1.4 megabits pretty good and not as much here symmetry as many people will have at home if they\u0027re using soft light but I mean that\u0027s ok bandwidth delay for the very large majority of packets was just over 550 milliseconds however let\u0027s look at some long-term measurements of what actually the RTT was you\u0027ll see the blue "
  },
  {
    "startTime": "00:45:40",
    "text": "bar is the average RTT and it\u0027s a high bandwidth delay product link nearly always at around 5 50 milliseconds but there are many outliers this is because there\u0027s a radio link underneath it\u0027s trying to merge the resource occasionally you\u0027ll get resource requests which can\u0027t be satisfied because got a contention busy time to the day work badly occasionally resource requests will fail you have to try second time to get resource or your partner traffic changes abruptly so these are single packet measurements you see these large outliers so I assert this is a wonderful test case for testing a quick implementation if you want to push this implementation in a weird way check all the time is work check all the transmission stuff work this is a great environment and it\u0027s real so we\u0027re carrying on doing this we\u0027ve done a lot of measurements and one of the things we\u0027ve done is plotted and TCP as sequence number divided by MSS because that\u0027s a thing that\u0027s roughly equivalent to the quick packet number and you get plots that look like this and okay I mean there\u0027s lots of these there\u0027s lots more in my dataset if you\u0027re interested in these come and talk to me and maybe next ITF will organize a side meeting to go through this sort of data with multiple people I\u0027m not going to take the question to the end because I\u0027m going to another a rather different notes where I\u0027m going this is the download and yeah all the time quick does well sometimes it didn\u0027t do as well remember TCP has this pathology as well so what we need to do now is look carefully at exactly what\u0027s going on the link logging is wonderful we can get a lot about lot of logging out we can really drill into it and lists are not wonderful so we have to figure out how best interact how timers are interacting yeah okay so we got lots of data what did we conclude I thought I might get some people to like so I\u0027m going to be quick quick quickly was wonderful it was a great experience using it Thanks right and no complaints about this and we have you to the logging available but then we\u0027re always already talking about what we should log TLS 1.3 had a two RTT advantage one or two RTC advantage always always better always good the fullness of quick or satellite was not as good as for TCP with a pet but it\u0027s within shooting range so I\u0027d really love to try and get the NIT signed out so it is as good downgrading to TCP is not a long-term solution for these people these people are currently actually pushing you to use their pep in many cases because it\u0027s a predictable environment that\u0027s got a stop we\u0027ve got to enable quick of this network to do that we must make quick work well do that we must understand quick well and understanding performance issues falls into three parts so this just probably implementations I mean we\u0027re getting there but there are things that could be changed maybe small changes to the spec in areas of transport maybe there\u0027s some "
  },
  {
    "startTime": "00:48:40",
    "text": "simple new mechanisms that could really help I\u0027d love to explore that I\u0027ll bring that up to the ITF as we find out we just started I\u0027m trying to say how exciting this is we just started will continue doing measurements we\u0027re going to talk about logging and tracing because we\u0027re currently building our own tools they\u0027re not bad they\u0027re passing JSON they\u0027re taking peak ups for the TCP putting them together the Q log stuff could be really fun and if you\u0027re a satellite person or a radio person and somehow we can run our tool across that using a virtual testbed that\u0027ll be cool to get a wider spectrum of results my slides have quite a few extra bits but I don\u0027t intend to go through them unless people really want me to thank you questions thank you going it\u0027s actually a great great thank you did that and it\u0027s particularly great to of the tresses of people can do a simulation and chunder starts the the one the little caveat I have with your talk is that you are mentioning quick when in fact you are testing a specific version of a specific implementation and I about human we add the 19 implementation of quake a truss interrupt if we tried all 19 of them would get 19 different results I think at the moment I\u0027m not sure I completely agree that when the 19 have finished there will be that different on that sort of test if we push harder on better tests which we will go to I\u0027m sure we\u0027ll get very different results so yeah yeah okay but on network tests and things like that so so it\u0027s very likely that we would get and the reason I said that is that I did spend a lot of time tuning an implementation of quake for this kind of scenarios not exactly as you know you mentioned which is cannot the edge kind of tuning you do at the end of your beauty tune for the main cast first and then you go to the other casts and and that tuning involves things like checking the way you compute timer or shaking the way you manage spoken for windows checking the way you do to recovery and so all that is work and so basically that the typical process that people get dissemination they look at what\u0027s going on as a haha and they tune her concern here and there and eventually they get to the level but I think that\u0027s the great conclusion of your talk is that if we do do that work there is no reason to believe that will not get to the wizard okay I checked from that a couple of points and one of them being that maybe next idea people who are doing similar things with all the other scenarios or other quick implementations let\u0027s get together let\u0027s spend some time actually drilling into that not at the protocol level really at the results of what we\u0027re seeing to actually find out why and how yep thank you let\u0027s make call on the end of this "
  },
  {
    "startTime": "00:51:41",
    "text": "queue then I just so we can stay nearly on time denying God Wow so many parts first I\u0027m actually super impressed at how close quick is to the pep PCP and how far away it is from the VPN TCP which also makes me super skeptical partly because and I do to Christine\u0027s point you\u0027re using quickly so I will say you\u0027re using the right implementation I\u0027m biased because I implemented the loss recovery in and so I must partly partly responsible for some of the performance you are seeing which is also why I\u0027m trying to understand what the pep is doing so if the pepper is so I don\u0027t really understand that that itself that is being deployed is it a full Terminator is it a or is it just doing retransmissions HACCP its BC I know that\u0027s obvious from the tracers if you look at the traces yes obviously they receive traces if you want to be clear so I think I mean thank you for us for doing these experiments is awesome in terms of the I think what do you at least part of what you are seeing is the difference between Reno and cubic because quickly correctly implements and deploys Reno yes and this is a really high large B DB link and so there will be your difference there and for the for the transfer sizes here you have I expect that cubic is going to ramp up much faster I we have looked at that and I think that\u0027s partly it and I think also limped some timer issues and minor details that could be fixed so yeah this is not a complete piece of work is the starter yeah so do big things one I\u0027m super happy to continue working on this and you know this is happy to take this back and sort of fix the quickly implementation for any bugs but second we you might know that we have Martin Simon and I have been working on a simulator environment for testing these things so it\u0027d be good to actually validate this stuff with the simulator as well to see if we can replicate the same results there excellent thank you yes right Google I want thanks for two things one for not using the chrome quick implementation because so many papers and benchmarks have been based on it and you know there\u0027s always my personal concern that that\u0027s overfitting and implementation and maybe we have some bug that turns out to be a performance enhancer number two thank you for picking an implementation that as close as I know actually implements the most current version of the recovery and congestion control spec and so this is sort of validating that what\u0027s in the spec largely like works pretty well and then the it\u0027s just you know I think there are some approaches that probably could allow quick to perform better here namely like and maybe TCP as well PBR would be one of them there\u0027s a lot of "
  },
  {
    "startTime": "00:54:42",
    "text": "good events that perhaps are detrimental to BB are not helpful and so that might mitigate this but I\u0027m sure there are other things we could do as well but thanks okay great work gory please keep it up do you know the weather aqm is in use in the networks or any of this measurement they were not in these experiments okay so I\u0027d be interested in seeing how that compared and the other thing that I\u0027d be interested in in your future work looking at is comparison of fairness on the tour the terrestrial network on the far side if how quick behaves compared to sort of stock TCP yeah we should we probably would put an emulator in series with the actual satellite link to let let\u0027s do some wireless effects and odd things to concatenate two paths hey Colin Perkins with no hats and first of all thank you great work I can a slightly like to echo the point Christian was making ever seen a lot of or heard a lot of people having read particular papers which talk about particular quick implementations wildly over extrapolate the results and it\u0027s something that that I don\u0027t know whether it\u0027s this group or the quick group might consider putting out a statement on how to benchmark quick and how to report results and how to how to describe what you are benchmarking what you\u0027re evaluating because I think it is harming the perception of quick and what people are doing experiments with very early implementations of quick comparisons are very highly up to most TCPS and writing what looked like very general conclusions when they\u0027re actually much more specific yeah that\u0027s one of the reason want to stand up here because we pull quickly and compile it and run the test so we want to keep it way up to date and anything we do is patching that because I think that\u0027s the only way to do it if you\u0027re benchmarking it now please don\u0027t use a G quick benchmark as your baseline it\u0027s different thing the things that moved on we really need to make sure we track the latest version in the quick way and we really need to make sure that publications writing about quick are very specific about what they are testing and what are the implications of that yes I agree with that thank you thanks much Cory okay next we have eaten from Google actually talking about the immigration to ITF quick which is also quite exciting yes what I\u0027m here to talk about our migration towards ITF quick from Google book so so where\u0027s "
  },
  {
    "startTime": "00:57:45",
    "text": "chrome now be 46 which is a Google quick version that has no real meaning but I\u0027ll explain to you approximately what it is is now a default enabled we also enable be 39 and 43 on the server at the moment be 46 is in variants draft three and below compatible and it has transport draft nine packet types which I think actually are maybe but still the same packet types that we have now a future version will be in variants for compatible which is the new but that changes everything by like one byte which is a rather relatively minor change technically but I\u0027m sure when we deploy that a bunch of other things will break before t6 does if a quirk that actually only supports Google quick style eight byte connection IDs in one direction and zero in the other that was just a convenience because it allowed us to delay if we went to call the variable in connection IDs which took a ton of time thank you david scamozzi i don\u0027t know if you\u0027re here so the rest this talk is all about b43 to be 46 so if if you\u0027re wondering like what versions are what transition is you talking about that\u0027s what I\u0027m talking about the first thing is the change from public reset so Google quick has an unauthenticated public reset that sent anytime you want to close a connection and you don\u0027t have any state or you\u0027re too lazy or whatever but basically it\u0027s like a TCP reset - connection closed during the handshake and a stateless reset post handshake and idea so why is this hard it turns out in our code we have lots of spots we send a public reset like well over ten I think it turns out and each one had to be fixed and tracked down and it just took a while also why does it matter so handshake timeouts and I all timeouts are quite a bit longer than getting an immediate reset in particular idle time out of our default is 30 seconds right now that\u0027s extremely user visible so you know getting this wrong even like a few percent of the time is a is a major source of user visible like problems you really want to close the connection as quickly as possible any sort of circumstances so when this and what and do you quick if we have no state we just send a public reset that\u0027s pretty straightforward ITF quick header in their state you send a Salus reset if it\u0027s a long header no state and its initial we try to start creating a connection it\u0027s a long header in those state and it\u0027s handshake we send an initial close and hoping that they still have pinche our initial keys which is sort of a you know I hope but it works in G quick at the moment and the Persians not supported it\u0027s worth we send VN there are definitely some quirks here where you can get into this situation in the final version of IETF quick where you start receiving handshake packets and you\u0027re like there is nothing I can guarantee to send in response that might close this connection quickly and so you just have to send initial because it\u0027s the only thing you have a keyboard hopefully that will not turn out to be a "
  },
  {
    "startTime": "01:00:45",
    "text": "practical issue so let\u0027s talk about quick identification it turns out at least one major company and increasingly more large companies are trying to do quick identification sometimes this is to just give you network monitoring tools like your network is this much HBS and this much you know quick in this much DNS and this much although other stuff and so it\u0027s also for selective traffic you know and enforcement basically like I want to disable quick or I wanted disable quick if I can\u0027t get BS anion which we\u0027ll talk about later so be 46 when you started rolling about at a very large increase in post hinchik black horn so it was about 2 X be 43 I\u0027ll explain it a little bit later how we determine post and shape by : but again this is one of those things we draw closely because it\u0027s very user visible and waiting 30 seconds for your connection to timeout before reissuing the request of our new connection is really really painful it also turns out historically these have been user focused so a given user will experience this all the time and the other 99.9% of the population will never ever experience this so it\u0027s not like every once in a while a web page is slow it\u0027s like one user is having like a bad month or a bad year so understandably we don\u0027t like this the confounding factor is sometimes the network just goes away right like this happens and so we have this base level of noise that we assume is the network going away but we have no real proof it\u0027s it\u0027s a very you never getting this to zero right like especially with mobile devices like Android and other things and this height is higher on Android for understandable reasons right however on April 13th that suddenly fixed itself like a or basically fix itself I mean there\u0027s a little bit of a gap there but it went from 2 X 2 like 1.1 X over the course of a weekend and we didn\u0027t change anything because we don\u0027t things out of our weekend let\u0027s first talk about what what too many are tears and then we can tell you what happened on the fifth RTO ng-click we found that that was a reasonable signal for closing the connection kazoo is black hold as I know at the bottom 5 RTO is and our current implementation is kind of comparable to seven RTOS in you know linux which i think is some time\u0027s configured as a default so you know we\u0027re not so far off we sort of stumbled upon this by accident because we were just having some unrelated issues but it\u0027s enabled by default on Chrome desktop definitely your heuristic we could probably do better but it turns out if you bump it up to six you\u0027re so close to the idle time out it doesn\u0027t really save you much and if you make it any shorter you do occasionally just kill a connection that you don\u0027t really mean to so Bates we\u0027re rare but this is "
  },
  {
    "startTime": "01:03:48",
    "text": "sort of our proxy for like black holing and it\u0027s it\u0027s worked out to be a pretty good indicator and in this case it\u0027s a better indicator than we thought it was so when you started debugging this we\u0027re like how can we get more data about when this is happening in a connection and one of our team members have the great idea that we should add the number of connections sorry number of packets that have been sent or received before closing the connection due to too many RTS and on the client side the receive metric was was very indicative as you can see all the extra black holing energy is all between two three and four packets so basically if you got in packet five like the rest of the connection is set you are all good to go and if you didn\u0027t get pack at five then you\u0027re gonna have a bad time wait 30 seconds unfortunately this typically happens after you\u0027ve completed the handshake because four packets is more than enough to complete the handshake in Google quick so typically the difference between two and three and four is like this is a case of you don\u0027t do zero RTT you get a packet and then that\u0027s a reject and then you do burn a round-trip and then it sends you to the cert and that\u0027s somewhere between one and three packets just as an explanation so that\u0027s why sometimes two three or four but they said basically this is like two flights of packets and then like whatever it is is blocking traffic okay yeah so it turned out to be quick them in a box identification suddenly improved when a vendor updated their identification software so happily most users of the software update or middle box update weekly we recently found out a few do not be aware of this because you know it\u0027s been fixed for a few months but some particular individuals did not have it fixed in their local instance so you know if you\u0027re in the future if you\u0027re debugging random quick issues and you\u0027re like well I think this was fixed by this vendor like months ago be aware that there is sometimes a long tail of updates and it was impossible to diagnose because it worse like we talked on tactic the vendor and they\u0027re like law it\u0027s fixed and we contacted the user and they\u0027re like my life is terrible and we appreciate both perspectives and so let\u0027s talk about how to block quick if I was gonna ask you to block quick because like this is sort of a favor and I\u0027m not saying anyone\u0027s going to change what they do but you\u0027re really cool if you did if you\u0027re gonna block it can you ensure that all packets in one direction or the other are blocked like doesn\u0027t matter which direction like don\u0027t care you can let all the client packets go through and drop all those server packets that\u0027s fine just try to drop all of them because anything else is really user visible and chrome now has some heuristics to try to avoid networks like this and avoid using click on them but it basically goes from consistent and utter failure to flaky hangouts or hangs and connection like loads followed by like it works suddenly and you don\u0027t know why so it goes just basically from failure to flaky it\u0027s not fixing the problem it\u0027s like a flaky test for our only test so the other one "
  },
  {
    "startTime": "01:06:50",
    "text": "I wanted to talk about from V 43 to be 46 is antivirus blocking suddenly we noticed that the usage of quick on window has dropped we eventually actually traced it to a single company at the time we found this out v 46 wasn\u0027t blocked so we\u0027re like hey maybe they read the CFM variants graphed and said not to drop the idea from variant correct but decided to drop Google quick I mean this is like me like thinking my most optimistic thought this was not true this was just completely false we turned on V 46 and two weeks later they like stopped started walking that and the last thing is anytime you change the location of sni you\u0027re gonna break a bunch of stuff there\u0027s a few more versions left so we\u0027re gonna probably break some other things and I\u0027m really sorry and it\u0027s a hard transition but we we\u0027re trying to do this as fast as we can okay so I\u0027ll stop and give questions is that your last leg yes that is my last word thank thank you heal from bringing this bringing this here this is something that I been sort of sitting on the edge of my seat for a while to watch what happens when the G quick format changes and as expected it\u0027s already been ossified this is this is like this is exactly what we were afraid of and this is exactly what you worried about and it seems like maybe some of them are actually doing updates which is promising but that\u0027s something that you mentioned so the the point that you are showing there with the energy in too many audios is is in the early key packets there so that suggests a particular type of behavior in middle box right where they allow a few packets to go through and then die and we\u0027ve we\u0027ve seen this before and I wonder if this is actually it\u0027s important enough now I mean missing this again here and then one thing this is important enough that this is the sort of thing that you want to protect against meaning that if if we keep thinking that all packets are going to go through or none of them are going to go through it may not be it may not be compatible with the way that middle boxes build their architecture because I imagine that some of this is basically like you know the forwarding thread sort of parting it passing it on to a different thing that actually goes off to do detection of what type of packet this is and by the time that completes and comes back you already forwarded a few packets I\u0027m not sure exactly what\u0027s causing it to happen but we\u0027ve seen this it may be worthwhile two things it may be worthwhile actually documenting some of this detection for others to also be able to use I think it\u0027s super useful for non chromium clients to be able to build this sort of detection the second I wonder if this is something we can specifically detect as behavior and use for failover in Chrome yeah we\u0027ve thought about specifically detecting it but it turns out the so far we\u0027ve only observed one vendor actually doing this and so we can email them and ask them to "
  },
  {
    "startTime": "01:09:51",
    "text": "fix it usually and so we\u0027ve done that and because the concern is otherwise we\u0027re aspiring the protocol on this weird thing where we have this workaround feature and then we\u0027re stuck with that sort of forever and so and everyone else has to build it too so I mean I guess I\u0027d be okay if Kron had to edit because you know were early adopters and we\u0027re pushing things but like if every quick client on earth had to add like a bunch of extra heuristics for the rest of time that because sad thank you so you don\u0027t have any insights why this behavior of this window showed up sir because you said it\u0027s one winter but you don\u0027t have any insights what the intention was there and the intention in this case was they had a Enterprise policy they had both monitoring consoles and an enterprise policy and enterprise policy allowed you to drop quick and the way they drop unknown protocols is they give it a few packets in each direction and then if after two or three packets they haven\u0027t figured out what protocol it is then they start dropping it and so quick classified in be 46 as an unknown critical and then get into this weird bin so that\u0027s the other thing I was going to comment is this isn\u0027t good for like any transport I seriously like like unis people are thought like whether it\u0027s you know SCTP over DT OS like all these things have the property that like if you complete the handshake and then black hole it it\u0027s going to be like a really bad time so for the general interest of transport innovation on UDP across the entire internet I would like people to try to avoid doing this it\u0027s not just for quick I mean this isn\u0027t just for us this is a little bit more yeah it\u0027s basically it seems like so what I what I\u0027ve seen people talk about is that there are console on the console of the middle box it basically says packet received cannot classify cannot classify and then it drops it so it\u0027s possible that it\u0027s you know trying to do some classification on a separate thread and that it eventually times out or gives up and then says I am done and then it decides black hole but yeah this is a huge issue unfortunately the people who can change this aren\u0027t necessarily in this room which is the frustration we cut the line and we would like to people to hurry up a little bit and if you\u0027re quick you can still join the line but please be quick all right thank you for sharing this I\u0027ll try to be quick here first of all a question when we see the black holing is it only on a per connection basis it\u0027s not really like across the whole device to the IP typically it is actually across all the connections for that like basically like every quick connection on that hoe starts getting black hold yeah okay because because it\u0027s a middle box we\u0027re all quick connections are going through yeah so it\u0027s it\u0027s sort of a host-based not based thing yeah cuz you\u0027d mention the kind like the background level because we\u0027re always gonna have some things that happen to drop off after four packets but it\u0027s not really possible then to look at kind of all the parallel connections and you can see if oh if this other connection "
  },
  {
    "startTime": "01:12:52",
    "text": "dropped off and it was further along maybe that\u0027s just a network impairment but that would not work in this case yeah I mean we don\u0027t really mean our client side is not fine enough grain to actually even prove that back but yeah I don\u0027t know yeah unfortunate I think we can say for sure whether or not that\u0027s true yeah I burn a trammel quick question I\u0027ll take the rest of this offline probably as the co-editor of the quick manageability drop I\u0027m gonna follow you a couple of issues to take this presentation into that draft the so you said that the dropping all of the packets for a connection is much better than dropping some of the package for the connection from user visibility that\u0027s kind of obvious and I think we should actually kind of write that down and yell it from the tree cops in the IETF and then you said that like so for one of these you\u0027re basically you get the first three packets and then you drop are you seeing other things where you basically get like intermittent drops after you have actually like spun up data um like so data channels like you get like you know you get some actual data it\u0027s like oh great great now quick works and then you drop every other packet or I mean like look can you characterize a little bit more the kind of drop patterns that you\u0027re seeing we we do see some level of other like well post inject dropouts yeah anecdotally it seems to mostly be not related we\u0027re out in nap times I was in that poor and the fix for that is if you like typically happens in the following circumstance it\u0027s more like the than that times out the server is sending a bunch of packets to the client the client stops sending for some period of time and the net timeout instead of being a you see timeout is a fixed timeout so it\u0027s like yeah you know at 60 seconds I\u0027m going to time out this UDP like a couple instead of after 60 seconds of idle I will do this I then then just send a ping to reopen it I mean totally seem quite a bit of that on non quick traffic as well so there\u0027s like there\u0027s some there\u0027s something that\u0027s in Java and the college is another okay cool it\u0027s not super common but it does happen yeah okay so we see that you guys ask a lot of great questions we have 50 minutes of content this fit into 45 minutes right now so we\u0027re gonna ask Alexander who had 22 maybe 17 minutes or something like that and similar for Roland afterwards you cut a few minutes off so that people can ask you questions but otherwise uh up next is Alexander about packet loss signaling for encrypted protocols thank you so I will we have as an operator we have the day to take issue of what to do to locate losses it may seem a bit strange but we are not completely aware of what happens in all nodes of our networks that\u0027s a day to day truth so maybe for many people networks are just dumb pipes but it\u0027s only true if someone can look at all the nodes and if you get some Oh am I\u0027m something that tells you okay "
  },
  {
    "startTime": "01:15:52",
    "text": "I got some drops on that interface and you have app sorry no blind spots it might work but in real world in the real world it\u0027s not the case so what we do is typically we do some liquor to me we use some observer in the middle of the path and try to say if some problem occurs upstream or downstream from that point and then reiterate we do this in a day to day basis and have been done so fro 30 years on TCP obviously using sequence number and X that\u0027s trivial but what do we do with the encrypted transport very quick that\u0027s a much tougher of course so quick as you may know has something for RT t measuring RTG with the spin bit but nothing for us for now so we\u0027ve got a proposal to to solve the problem which is that based on only two lost bits which we named Q and L the Q which is square signal and L which is the lost Evan signal the square signal is just one toggle every n packets so that\u0027s like you\u0027re the one the alternate packet packet marking also something mentioned by kazuo yeah a year ago about earlier in earlier loss detection discussions so the U cube it is just flipped every N packets so when you look at it at a middle point you just count the sequences and know how many packets have been lost upstream from the points the last seven bit is fed by the stack itself and set to 1 whenever some earlier packet has been considered lost so by counting both of them you get the upstream loss and the end to end nose so the nth windows is just a fraction with l equals to 1 and the upstream loss is the the difference between the expected number of Q and the observed number of Q\u0027s and of course the downstream loss is the difference roughly the difference so we\u0027ve been doing running an experiment an orange network through real customers with thousands of Relkin customers in four countries with Akamai CD ends so Akamai CD ends use a quick stack which is based on chromium with proprietary patches and in it we implemented the the QL bits as you might know in the current quick specification it\u0027s not possible to add those bits anywhere in the packet so we did use some ugly level free trick you poked those bits in the IP header in the TTL high high order bits this a lot this is completely ok it\u0027s a hack of course not supposed to be generalised but that\u0027s a way of doing a real-life experiment today with negotiation with version negotiation in quick it\u0027s absolutely not "
  },
  {
    "startTime": "01:18:53",
    "text": "possible to do a large-scale experiment for anybody but people holding a browser or popular application we do not have any of them so we were first to do that okay the topology of the experiment is the same for all four countries you typically have an observer point which is just below the Akamai CBN\u0027s serving the access network the suspected usual suspect in network troubleshooting of course the internet paths and the access network paths so whenever we start an investigation in our networks even today with TCP we\u0027ll always start at this point because the key question to us first is whether the responsibility is ours or not the implementation details are as follows so the quick stack patch by can I just compute the qnn bits and insert them in the in the TTL high bits and then we had to put the value to a out of the qubit so you you which was 64 as a trade off based on a few audio experiments and we will see that it\u0027s not so bad so this gives that also not another constraint wisdom from the the hiko the high collar used which is the TTL high bits of course since we use to high bits it means that TTL must be below 63 but is no real constraint as the Akamai server just set the value to verse 63 so what we do to gather data at the observation point is we do traffic capture with step aggregators and we only capture the downlink pass we don\u0027t need the lag flow and also we truncate the full payload is completely useless so we only gather packet summaries with timestamps IP ports the qnl bits and the payload size for debugging the segmentation works by top hole and time out obviously because the we have no insight on the exact signaling going on in quick and this allows to capture in one compact text file per flow or would that happens and then do various tests with the post-processing of the signals so both based on those files you will see that both statistical and unit analysis can be performed so here before going into the analysis I\u0027ll adjust the plotter CDF of the flow size because the flow size is important as you know that the cubit just counts sequential consecutive packets with the same value of course if you got fewer than 64 pi packets in the flow you see nothing so what I\u0027ve done here is plot the value of 640 which is 1010 1/2 K odds of Q which gives a boundary and the under image that the last chunk the "
  },
  {
    "startTime": "01:21:53",
    "text": "importance sadistically importance of last rank which obviously is incomplete so if you just clip the data set above 640 you get roughly 10% in the in the worst case but that\u0027s enough given the number of point captured so the review the refusal we used to exploit the desert are of two kinds first is statistical you watch which calculus kattappa lots so we just give the very dimensional distribution of the loss derived from the queue and from the l bits and then the unit view if we want to debug what happened on that point on this connection we just plot an individual cumulative display of the Dallas\u0027s which is over the time in the connection how many packets were lost other Angwin or upstream so when we look at the first kind of plot when it is we analyze it this way we\u0027ve got AB sum of stream runs against end-to-end us the diagonal of course means pure upstream loss because when the packet is it is plus upstream is also appears in the end to end the vertical is pure downstream loss it appears only on the end-to-end measurement and not at all on the cubit but the bottom right is supposed to be impossible right so we will see well then when we do unit analysis you you select one of these points and look at the the way that the laws are spread over time in any one given connection in this example you can see that both are completely synchronized which means that it\u0027s pure upstream loss which is every time there\u0027s a cat which is lost it\u0027s also lost end-to-end in this other case you see only end-to-end absolutely no upstream so it\u0027s pure downstream this case you see something which shouldn\u0027t happen which is no end to end and they\u0027re completely insane and concentrated upstream loss which this case was based on miss segmentation the heuristics applied to the signal counting the qubits is supposed to do something about reordering I get rubbering and in some case it does Polly and also miss segmentation happens when the timeout doesn\u0027t is not enough to detect that some connection was reusing the center pole in this extreme case you\u0027ve completely uncorrelated losses which happened to be due to observer pass saturation observers now we are going to look at the four countries so these are the four scatter plots quickly you can see that there\u0027s one of them which is an outlier the bottom left is "
  },
  {
    "startTime": "01:24:53",
    "text": "completely insane and we see why the first one country one as you can see is concerned the main energies on the vertical axis which is pure downstream loss there are a few points on the diagonal which are really under they\u0027re gonna which in some events of upstream loss and there is also some noise in the dragon area which after unit analysis have been shown to be due to reordering so again that\u0027s due to when reordering happens the boundaries consecutive packets with Q equals zero and one must be somehow accounted for one country true it\u0027s much cleaner the noise from razoring is very low and we also seem some energy on vertical axis and a few spots on the diagonal winning events of strong actionless you can also see that they are not exactly on the diagonal below and this has been accounted for by the fact that in quick you sometimes have packets going down which are expandable these as for example ax coming from window update so we have corrected them to do that country 3 3 is come insane 1 and after after correlation with other flows on the on the same capture pass from TCP it was shown that it was completely saturated capture chain so the method also is able to detect something that TCP could allow which is detected when your measurement purchased doesn\u0027t work country 3 is a cleaner tool and a few outliers due to musical notation now you see the CDF the summary or CDF films from downstream loss ratio which gives you the difference between the X and y axis and the earlier drafts but plotting them at the CDF gives us a quantitative view that\u0027s better than the the scatter plots okay so to summarize what we\u0027ve seen in the dragon area sometimes you some noise is due to imperfection of the denoising heuristics some is due to miss Eckman session and some is due to observe all\u0027s which obviously should not happen in your life and the summer is all these noises can be disambiguated by unit analysis so we are not at all blind and also the heuristics have been quickly developed and could be improved and reduce the noise as a wrap-up the unilateral deployment was made possible only because we use we didn\u0027t use the quick vague vehicle itself that\u0027s all hack to that and that\u0027s the only way to do a large-scale experiment the signals were designed to restore "
  },
  {
    "startTime": "01:27:56",
    "text": "disappear like ability to debug networks and they do worse and short of direct quick support we are not we do not expect to get in anytime soon and other vehicles is needed so any suggestion are open and the mechanism can be applied to any other protocols quick centric the only contribution that is expected from the stack is the Elbit which is the the mechanism by which the stack tells that it has rust buckets so in the summer is a tree it works and we really do need something like that to keep the beginning networks shot of that with just the current state of quick and no extensions we are we are completely blind all right we can squeeze in like one or two questions yes but Google thanks for this data this looks certainly interesting one question I had is for the spin but there\u0027s a fair amount of analysis of what happens if one or the other one of the two end points like tries to for lack of a better word subvert the signal it seems like it would be very easy for either end point to sort of convince a middle box like of whatever signal I wanted as well as some pretty crazy signals have you had any thoughts about the ability to reject implementations that either are buggy or just basically like you know not complying with a with this scheme or how to figure out when it\u0027s negotiated pertaining the client server in their heart leap spinning and so it\u0027s a server-side right so for the for the downstream path only the server sets the bit and in the it\u0027s what we are interesting in because it\u0027s where the congestion now networks okay so the most valuable signal is unilateral my module at the fact that right now you\u0027d drop half of these packets because the removing header protection with them just to add possess site or native for TCP is also unencrypted and it\u0027s also subject to gaming right somebody wants to send you interest in TCP interesting-looking disappear stream nothing stopping them all right thanks so much Alexander so up next we\u0027re gonna have Roland talk about the RPI and you\u0027ve got twenty minutes for this and then we\u0027ve got one last presentation okay thanks Dave so I apologize to the people in the video in advance I\u0027m gonna walk off the video to start their animation because my laptop didn\u0027t work with the presenter thing right this is a talk "
  },
  {
    "startTime": "01:30:56",
    "text": "about the RPK wayback machine my name is runs fresh Sakai I work for an omlette labs and I hope if it\u0027s that somebody recognizes this Logan Ziggy says there is a 50% chance we\u0027ll end up in 2011 hands up who knows quantum leap there we go good right so I don\u0027t know it labs we make rpki relying party software which is called Bruce inator and this has already seen quite a lot of uptake in production for which we are very happy but we also want to test ourself for ourselves and to see how robust it is and we found out that the ripe ncc had a really really nice dataset that had all of the rpki our away data for all of the RI ours pretty much since the inception of our PGI and so this is really cool data so what we wanted to do was use that data to test route inator and we wanted to run all of it through our validator software now we got dumps into our gzip files and towards the end I\u0027ll tell you where you can find a mixer now open data and we wrote a tool that we called siggy to transfer this back in RPK time so interlude L telling holding Ziggy and telling Sam where he\u0027s gonna go next right so what does Ziggy do it is a Python script again open source URL will be in one of the later slides and you give it a date and what it\u0027ll do is it\u0027ll try to figure out which of the RER repository data needs to unpack it needs to recreate the towels because the towels are missing in the tarji set files and then it uses a tool called fake time to run the route inator for the correct date and we did this for data starting January 2011 until February 2019 at least that those are the graphs that I\u0027m going to show you but actually during this week I did some more processing and I have some really nice animations that I can show you that go until Monday this week right for those of you are less familiar with rpki a quick recap of some of the jargon that i\u0027m going to be using throughout this presentation the rpki is the resource public key infrastructure this allows you to set what are called route origin authorizations and basically this authorizes a certain AAS to announce certain prefixes this is cryptographically signed so he can sort of validate that some a as was allowed to originate that prefix and what we typically deal with after validation are called verified raw payloads yes that is an acronym in an acronym and this is a cryptographic a valid statement about a prefix from an hour away right so first of all if we look at our PKI it started in 2011 but it uptake sort of started slow and then as you can see it is escalating towards "
  },
  {
    "startTime": "01:33:56",
    "text": "today it\u0027s really growing exponentially what is also interesting is that the historic data that we got actually turned out to contain some flaws you can see little arrow on the on the graph that says first day right data validates before that date something was off in the manifest files that we got from ripe ncc and it didn\u0027t validate and it is on a road map to use old or pti software to also validate that data and see if we can make this graph cover the whole period another interesting thing that you can observe here is the DES spike in the Green Line from AP neck and these were three asses that suddenly D aggregate all of their rows which basically means they disable their max max length attribute and rather than say oh this is a pretty role that\u0027s valid first say sixteen prefix and you can have sub announcements of stretch 24 they remove the max length and basically converted that into a whole bunch of slash 24 rows and apparently this was a mistake in some software and it got fixed pretty quickly but the impact is quite visible of course we also have to look at ipv6 because that\u0027s a modern protocol and again you can see more or less the same trend it\u0027s it\u0027s far fewer prefixes for which there are rows but the trending growth is pretty similar it\u0027s still growing exponentially is growing up towards today and this is good news so people are really starting to apply this technology so what I also looked at was the prefix size in VR PS over time because the hypothesis behind this was that as ipv4 exhaustion continues you might expect to see fear piece for smaller prefixes and actually as you can see in this this graph that is true so this is a CDF for five years and in 2019 the fraction of prefixes that are 24 is sort of let me see let me say that so that\u0027s going down mmm I\u0027m trying to figure out what I wanted to say there so the and the same is true for mix lengths right the the when mixed length is used you can again see that the max length becomes smaller and smaller and smaller over time so there it\u0027s going down toward slash 24 for a v6 we see pretty much the same thing again the prefix size is decreasing over time so we see vir PS for smaller prefixes and this is also to form X length there are by the way at the end of the presentation there are many more graphs that I didn\u0027t have time to put in because I would be way over time if I present all of these but again "
  },
  {
    "startTime": "01:36:57",
    "text": "the takeaway from this is max length is decreasing for post v4 and v6 right now this one\u0027s a little bit interesting right because there\u0027s been some debate about whether or not you should use max length or water or or if you should prefer 2d aggregate and have lots of rowers that cover smaller prefixes and because the reasoning behind this is if you have a max length but you don\u0027t and now announce all of the sub prefixes that are underneath that you\u0027re expose yourself to high checking attempts and what you can see is that max length use over time was going down until about a year ago and it\u0027s slowly starting to rise again and while we can\u0027t confirm this we think this might be because people are now validating a lot more and it turns out that people were announcing sub prefixes that we\u0027re not covered by the by the rowers and their announcements were getting rejected because of that and then to fix that if you\u0027re announcing sub prefixes it\u0027s easy to add a max length that is valid and then your stuff will validate again so it seems I mean this is kind of a hand waving a bit but it seems that this is a trend now again so average prefix size so there\u0027s a note that says compare this to Jeff story in in routing that was because I use this slide at the right meeting in Reykjavik where Jeff Hewson presented something about the BGP table where you also see a decreasing prefix size in the announcements that are in there well no surprise we see a decreasing averaged prefix size in the VR P\u0027s ball again max length and the prefix size that is in the row up right again for ipv6 we see pretty much the same thing right we see the average prefix size so more rows for smaller prefixes now I\u0027m gonna go so this is interesting so this is something that I did over the week we I\u0027m gonna walk around so I can store as Mary you\u0027re gonna start it for me no okay so I\u0027m gonna walk around and start that story on the video so what we did here was I we have a tool that will exist a video that one no oh okay I need to click harder there we go what this shows you this animation is the coverage so this is the what we took was route fuse data for announcements that we see in BGP we took the veer piece for certain dates and we use the "
  },
  {
    "startTime": "01:39:57",
    "text": "NRO stats to map those the prefixes that we see in the announcements and in the rowers to certain countries and what is animation will show you is coverage so these are this is the fraction of announcements that are covered by Aurora and this animation runs until this Monday and hopefully it\u0027s kind of visible on the screen what you should be able to see in this is that it\u0027s slowly getting darker and that means that we see more and more and more announcements that are actually covered by Aurora and what you should also see is that we\u0027re now April May June that this is accelerating towards the end and I apologize that the screens kind of mangle the the nice colors but if you want the graphs I can give it to you sorry can we play it again of course we can play it again oh and yeah you\u0027re from South America they\u0027re doing really well in South America as you can see right now all of the tools that we use to create this our open source so he there\u0027s a slide at the end where he which shows you where you can grab them now there is one more graph that I like to show you and this one I like even more what this shows you is accuracy and accuracy we define as the fraction of covered announcements that are valid right so and what we graph here are those countries that have an accuracy of 90% or over so I cut off everything that has a lower accuracy because this will show you again that even there is already a high accuracy so all of the countries that you see right now on April 1st 2018 had an accuracy of over 90% for the announcements that are sort of for the prefixes that are assigned to those specific countries but what you will see is that the quality is really improving a lot again what you see here is a some kind of a disappeared oops but the takeaway from this is that quality is really really getting good so if you are not filtering based on our PPI you really should be doing that because the quality of the data is very very good right the accuracy is high and can we go back to the slides I have to stand here sorry America Mario\u0027s very strict right so those were those two let me "
  },
  {
    "startTime": "01:42:58",
    "text": "figure out if I can somehow if people are interested send me an email I can I can get videos for that right to conclude we wanted to test route inator and it turns out that RPG I took some time to standardize properly right because we couldn\u0027t validate some of the older data the data raises lots of questions but it also shows you that you could deploy our PTI filtering in your network today because the quality of the data is actually very very good the next step that we want to do is to compare this against routing information in more detail and we actually already did that we submitted a paper to the ACM measurement measurement conference that was accepted so we\u0027ll be presenting that in October in Amsterdam and also this is some free advertising as the one of the general chairs of IMC please come to Amsterdam in October to the wonderful internet measurement conference with that I would like to give a big thank you to the folks at the ripe NCC for actually collecting and keeping this data to a meal from ripe to help us make this data available this data is now available as open data so if you are a researcher you want apply play with this rpki data go grab it off the ripe FTP server and and also grab our free OSS tools ruminator the secure reading stats which is what I used to do all of the computation for the the maps that I showed and I turned the repository for city public earlier this week so you can grab that I will actually now download the data from the ripe archive see what you do is you run it you give it a date it will grab the right data it will run routine a tour if you have a local copy installed and it will give you the vir piece for that date and with that I\u0027m open to questions can I now go back to you Mike I Robert Kiyosaki ripe ncc one suggestion I don\u0027t know about the exact incremental value that you would get out of this but if there was an option to say disable manaphy check then you could fill in the gap that was before 2014 for the MC the reason is the manifest is there to protect the integrity of the whole set and if it\u0027s fair that it was not compromised you know in that time period yes there is a footnote there but if you assume that it wasn\u0027t then you would feel that yeah yes well it\u0027s certainly something we want to do is it\u0027s just that with all the work we\u0027ve been doing on the tool set we haven\u0027t had time to sort of add the flag but we\u0027re certainly gonna do that and we\u0027re probably also gonna make all of the validated the data that we file a date it will probably put it up somewhere so that people G can just grab the vir piece rather than having to do run the validation themselves Oh Brian Trammell former rpki skeptic thank you very very much for putting all of this stuff together like so the entire stack one of the problems that we\u0027ve had and sort of trying to figure out what the "
  },
  {
    "startTime": "01:45:59",
    "text": "state of deployment of a lot of these things are is that from the research standpoint it\u0027s like oh you go and you read a paper from 2011 and then you read another paper from 2015 and you have two points and are very hard to draw a line and here you have data points from every day and the line is very apparent one of the the reasons I was in rpki skeptic is sort of the the base rate fallacy problem that you have when you\u0027re enabling optional security um there\u0027s a risk to turning validation on because the data is you know there\u0027s problems with the data you might actually start projecting routes and you have availability risk um and you\u0027re creating African security risk the number of it so if I were trying to come up with a risk number from your data set that would be one over accuracy correct yes okay because the things that are uncovered or are you know not going to be a problem anyway okay and those and the scale on the geographical data you\u0027re showing was like ninety nine ninety percent to 99 percent of aggregated prefixes or at announcements announcements announcements and you\u0027re kind of hoping yeah and then the other quells unique prefixes in announcements yeah okay yep perfect thanks okay thanks alright let\u0027s do this one last question and we\u0027ll go to the last presentation Montgomery it\u0027s more of a comment on the last question is that when you look at those accuracy numbers you have to be careful to see if the route that\u0027s invalid might well be covered by another route that is valid or not found we\u0027ve looked at that and the coverage is pretty high the coverage that uses the same path as pretty high coverage then ends up in the same origin is pretty hot okay thank you Thanks so [Applause] so we had we had a little a bit of juggling with the agenda and we decided we decided to have you on come and talk to us about his recycling large-scale internet measurements for about 10 minutes and so we\u0027re gonna do is bump the hackathon report that I would have done but all is not lost there\u0027s a link to the three minute hackathon talk that I gave you can find it in the agenda so in the in their updated slides for that so take it away on thank you so this is work we already presented at Pamela\u0027s here and basically this all started out well I\u0027ve been doing intuit measurements for the last couple of years I was here at ITF in London and presented some stuff about Google quick deployment so yes this is a talk we\u0027re also quick ISM but we also scanned follow up more we do DNA scans TLS scans HTTP to scans and the thing with these scans is we are looking for something very specific but if you for example do a tcp/ip v4 scan on port 81 percent of all IPS will actually give us an EXO answer what about the remaining 99% right so the "
  },
  {
    "startTime": "01:49:02",
    "text": "idea basically that we had in this paper Wars is there actually something we aren\u0027t scanning for but we could have scanned for all can we reuse the data that we already have and to find that out we basically captured all ICMP traffic that words ingressing our measurement networks as we do not generate ICMP traffic for ourselves this is either background traffic or traffic that is actually generated by our measurements so the idea was okay let\u0027s study ICMP and let\u0027s study what actually is in there and we were surprised because within a week we got what is it six hundred thirty seven million icing penises from a whole different number of Phi piece over 53,000 autonomous system and you can see a plot basically where you can see that yeah well they correspond to our measurements which typically start at midnight if people correctly set their cocks and yeah that\u0027s not going to detail into this and we basically first thing that we looked in Wars okay do it does it make it different what we\u0027re actually scanning for and yes it does so if we do like a TCP scan on port 80 you can see yeah we don\u0027t get a lot of destination unreachable or port unreachable messages which is obvious because you would use probably reset for that but there are still some of those it also makes a difference whether or not we\u0027re simply doing ipv4 scans we also do dns-based scans where we have a lot of zone files for example it\u0027ll be an Essbase and you can see that we get a lot of destination unreachable because host is prohibitive messages if we do scans based on DNS data and on the right you can see a little table it summarizes basically what kind of messages we go out and I\u0027m going to from some of those types and see what we actually found when we looked into those messages I think it should start with redirect messages so if you don\u0027t know what redirect messages are they are basically used to signal a better pass in your local network so RFC 1820 1812 actually states these three things and the most important one it\u0027s in the middle it basically says yeah well if the source address of the packets on the same logical network is you you may send these things however we never are but we still got 18 million of those messages over the public Internet 100,000 of those are network redirects which the RSA says you must not send Pickers they don\u0027t make sure we make any lot of sense if you look at them well but the majority of host redirects and they affect roughly two thousand asses and these are roughly four thousand in these redirects roughly were 400,000 destinations and within all our DNS data that we actually have we found also that 900 of them had an a record and some of those actually redirect you to private addresses or bruised left address spaces "
  },
  {
    "startTime": "01:52:02",
    "text": "okay this is not good next we looked at source quench messages so if you you probably remember that I didn\u0027t actually really knew what source quench was when I started this and it\u0027s basically like easy ends grandparent so the idea was when the route is congested it\u0027s and the source bunch message to the sender\u0027s however research found well it\u0027s kind of unfair if you do that and it can lead to blind attacks and you guys said please don\u0027t send those in 1995 and you also said please ignore those messages in 2012 and most actually do that since 2005 yet we still found that they\u0027re roughly 2 now a thousand I piece located in over 350 asses that still issue those messages mostly those messages were issued in the destination access but we also found some that were issued on path I mean we look into some vendor manuals of yeah some some router manuals and we found that most of those vendors also state that they removed it in between 2000 2010 but you can still see it takes decades to get stuff out of the internet so whatever you\u0027re standardizing people will use it forever if I once activated it all right yeah we also looked at another thing fragmentation and if something gets fragmented because as I said we also scan for quick so our packets are roughly 1000 fluid bytes and it as it turns out or is we use a map default in our scans and they don\u0027t set the don\u0027t segment bit so we got some fragment time exceeded messages from another couple of scans where we do set those bits we see that there\u0027s a bit of fragmentation so yeah another big bunch of messages that we actually got were TTL exceeded messages which were kind of what when I first saw them because I was well those internet paths actually so long what are our TTL values and it turns out it\u0027s the max in Z map and the otherwise we take the Linux default so we ask yourself other paths very long are the many middle boxes tweaking our titi outs or are there maybe routing loops and we basically performed a whole bunch of trace routes to see if there are loops and we were able to basically defined roughly 440,000 / 24 from roughly 20,000 SS that were unreachable dgo loop yeah interestingly we thought we could find 130,000 that or the pass where we had all a piece of all involved routers and with simple checks we could see that roughly 5,000 of those actually also cross is boundaries of loops however you can definitely make that better yeah then we also looked at whether or not these loops are persistent so we compared basically trace routes from two "
  },
  {
    "startTime": "01:55:03",
    "text": "weeks apart and we found that loops from roughly 150 Isis disappeared but they were still 404 subnets unreachable we also found loops in our upstream ISP in the German research network which was nice because we can contact them and they confirm the loops which they didn\u0027t really like they fix the loops and for her so we\u0027re specific they could tell us basically it was a static route they had once in one router then they removed it another one but it was basically still forwarding packets to the other router but this rotor had no knowledge about the drought anymore and send it to its default route which was the first router and yeah so it\u0027s that you loop all right this brings me to my aunt so as you probably know the internet is full of defecation badly configured systems we\u0027ve got more things in our paper the nice thing I mean we all know that but the nice thing is we could find that without actually doing any new measurements these were all done from apart from the trace routes we\u0027re all done using data that we already cooked what never looked at yeah lots of routing loops our data set is partly available until last month\u0027s will visually our drives run full yeah thank you all right we\u0027ve got four minutes left in the session yeah questions for Yann or if we don\u0027t have those any other comments on work for the group would be great along the lines of ice for the last three times we\u0027ve participated in the hackathon so we\u0027re welcome to Meaney what can I talk about that anything thanks young okay so um along the lines of the invitation to talk about topics for the group Mary and I have been encouraging people to do measurement work here not just report it so we\u0027ve been doing that and the hackathon and the large-scale scope of it is what I\u0027ve been trying to do in the hackathon is create an anima anonymity sets for world wide web clients and there was some really nice work at the nrw if you haven\u0027t seen the program there check that out about what can you learn from an IP where they surveyed the server-side IP addresses and what you can learn from those and in PRG and I\u0027m also interested in doing this creating anonymity sets somehow of the service side so people can\u0027t profile encrypted traffic and determine what you\u0027re talking to so if that\u0027s something you\u0027re interested in get in touch with me or talk about it on the list or you might be able to do it in Singapore the hackathon feel like that\u0027s all we have thanks so much "
  }
]