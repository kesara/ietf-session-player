[
  {
    "startTime": "00:02:07",
    "text": "Yeah. Should we get going then? Yeah, I'll do I'll do a quick hello in the macro view. Alright. Hello, and welcome everyone to CFRG. If you you were expecting something non CFRG, you, may want to go to another room. So, we're gonna kick this off with, some slides As you know, Stennis Love, who's online, is one of the chairs as well as myself and elect your human person. We've got a big long agenda today. So I'm gonna hand it over to, Alexei for the chair slides. Okay. Right. Okay. As a reminder, this session is being recorded. You all should be using Hey. This, local site tool or the full meta call access This is just a reminder that we are car covered by IRCF note well, and Ferris procedural documents, If you've seen them before, great. If not, of these, have a look. Again, privacy is kind of conduct, Yeah. Just a reminder. So this is an are irtf. Research a group. We don't make standards. We cannot make IETF do anything, but luckily, they like to use put off our work. Okay."
  },
  {
    "startTime": "00:04:02",
    "text": "Very quick agenda bashing. From Cheers. So, We have an optional talk at the end if we have time. Cheers would like to swap. Mike on was and John Bradley's talk because, we'll have open mic just which we'll probably talk about CAM can combine us, Any other agenda bashing. K. Going once. Going twice. Okay. I'll try not to read everything on these slides because this speaker mic is turned off. Can can folks hear us online? Please confirm on the chat. That the speaker might can be heard Yes. It is. Yeah. Okay. Great. Okay. So, very quickly going through document status is we have a couple of RFCs published since. We have 1 in RFC editors queue. I think waiting for one from Oh, I RFCF chair. But other than that, there is nothing in ISG or review at the moment. We have plenty of active documents. We have four documents, which are either in the research group plus call some of them weren't officially closed, and there is there are follow-up discussions and in in one case, There is a separate follow-up action."
  },
  {
    "startTime": "00:06:01",
    "text": "And various other documents got updated. If you find anything wrong on slides, please tell chairs. We have one recently adopted document on RSA Guidance guidance. We we Also in an interesting position where we adopted a topic on hybrid PQ cams. So there will be a design team form to compile requirements, but we don't have a document yet. And the rest are some documents. Some of them are expired and probably need to be. Revived. Any comments on this? Oh, I gotta Okay. Sorry. That's alright. I'm not Christopher Pat. Chris Patton and Nikkew. Can them? we approve Yeah. How do you do that? On on on Okay. Christopher, your first Yeah. I was just curious about, if if anyone in the room is an author of the cryptography specification guidelines document. Can you comment on where you plan to take this? I'm I'm personally very interested in this in seeing this revived. Thank you. Yeah. This is Nick Chair and, coauthor And, this has been delayed a little bit, but the next steps here are to open up this discussion around special characters. And,"
  },
  {
    "startTime": "00:08:00",
    "text": "overall, just add detail to the document and, push anyone to the list for discussion. K. Them, Yeah. Thank you, Dan Harkins. So, what happened to my, dnhpke draft that was thought I was in the past call, and then I didn't hear anything about the results and I wasn't on top of this. Needs Shepherd combo. Got it. Okay. Sorry. Sorry. And and I'm I'm doing the shepherd. So That's alright. Well, the, making this slice was useful for chairs. Because we actually find out the things we haven't done either. So so Okay. Okay. And, Just a very quick slide, would you mind people periodically about that, There is a, CFRG is kind of managing a crypto review panel, which is used for reviewing documents for CFRG security area and sometimes independent stream. In fact, pretty much anybody can request reviews and ask for advice. So it was 1st formed in 2016. And now we are on the latest iteration, We'll have the new members listed and Well, Steven Farrell, was it lurking, but he is now officially a member. So, yeah, this, panel is selected for 2 years term, Okay. And with this, I think we can start presentations. Hi. Hi, Colin Perkins. Just to, echo my thanks to the crypto panel members. This is a a really important"
  },
  {
    "startTime": "00:10:03",
    "text": "activity. So thank you to the chairs for organizing it, and thank you to the, the members of panel. Very much appreciates it. So Okay. John Mattson First. Okay. Which one do 1st Is your mic on? Yeah. Mhmm. Is this this one? Yeah. I think this one's dead. Should we? Yeah. It's okay. Grab this for now. Excuse me, chairs. Do you want me to signal for us to get another microphone? Can you yep. You can hear me. Sure, Scott. Yeah. So this is, hedged Okay. I think which, would you want me to take this first? That was the command. You know? Yeah. That"
  },
  {
    "startTime": "00:12:07",
    "text": "So will be about the working group document hedged ECDSA and hedged EDDSA signatures? Yeah. Next slide. And a big discussion topic on on the list was, what name it should have. Now we have taken a decision and changed the name to hedge ECDSA and hedged EDDSA signatures. This aligns with, with most academic late academic literature also with Nyst, several changes. We have added a second 0 padding block. That separates the context from that. Hash of the private key. This was requested by several people in alliance with BS I, incorrect or incorrect statement in eddsa about the context, fitting in the block, in edDSA. USA, we changed the order and added a 0 0byte. Two reasons for this, we noticed that the old order collided with the old, RFC 8032. I don't know if there's any, vulnerability based on that, but it seems good designed to avoid collections by design, And then Bernstein recommended to put said the random said before the context. In ECDSA, we clarify that said was different in the two steps, but then then he raised that this breaks the design principle in the deterministic ECDSA that it aligns with HMAC DRBT. So we change it back. The reasons to have 2 Brandon would be to have improved security against"
  },
  {
    "startTime": "00:14:02",
    "text": "2nd order DPA attacks, but we think this gives this protects about 1st order, and I think that's good enough. Then change some of the K MAC link output link requirements and some editorial. Yeah. Ex and while in the last, NIST has published, or it's a 186 file. So ED DSA and the tunistic DSA. Which this draft builds on and also needs standards and Nesta's published the hedge MLDSA, which was the hedge moved by. A draft version of it, anyway, that use the hedge signing by default. And we also know now that NIST has not included a hedge mode. That's also a request for, 800 and 86 stopped. Slash 5. Next slide. And this is how the eddsaconstruction looks now the order and the padding, and then the green things are new. So now you have 2 different bite strings that should be padded to align with, the blocked length. To cipher. And here is the contraction for the domestic EDDSA. The only thing new in EDDSA is the second padding block or, first padding block. Yeah. And then some additional information. Next slide. Some open issues Bernstein. Yes. Allows switching the order of the random value and the prefix such that you should use, the the If you are concerned about R&D failures, you should put prefix first, and"
  },
  {
    "startTime": "00:16:01",
    "text": "If side channel are the top, then you should put said first. I think both as reasoning makes Since that I'm not sure we want to take the effort to have 2 different modes I think the focus here is Side China attack. So I would I feeling would be to keep it as it's, but open to see it. Then second question is deterministic interface, burn summary that we should have a deterministic interface don't want to speculate, but I assume he means an internal interface in the in the draft. Not an external interface where the user inputs set. And also writes about test factors. I think I think test vectors that fixes this. Next Then, what's comment about the FIPs compliant mode where you put random as additional input to the, It was a suggestion from Cisco. There's a link to a Cisco blog that talks about this. I don't know if there's anybody wanting this draft to to write something more about this as a compliment, don't speak up. And 7 Alwy naming, Simone. So he has new names for the new modes. Today, there was a discussion on, on the GitHub saying that they don't I don't remember the name of the person, but they don't think that's a good name as good ideas. These are passable with existing verifier. So I think my current feeling is that we keep calling. We calling it hedgeeddsaorhedged ED, yeah, with context similar to randomized ECDSA and"
  },
  {
    "startTime": "00:18:01",
    "text": "deterministic ECDSA, which is, and this will be hedged eddsa. And then comment from Renee once must instead of recommended also comment on GitHub that recommend is probably the way to go. So well, unless there's more comments, I will close that. Next step is to include test vectors and as of yesterday, we actually has test vectors than any generated them already and published on GitHub. So they will be included in the next version. These text vectors include message key and set, and then you get the signature. And then there's been people requesting that we should have So proves for the constructions. Of course, always welcome. I think you can discuss what should discuss what kind of proof this should be. I think the deterministic ECDS that doesn't really come with any proves. And I think my high level feeling is that the proofs for EDDSA applied to be also. But that should be investigated and looking to I mean, any people wanting to look at this is very welcome in all aspects. No. We don't have a mic. Okay. Yeah. Any questions? Yeah. Crystal, Can you go back to slide, I think, slide 5 issue number 3 on GitHub? So, I'm I'm Kai, I'm somewhat concerned about adding this, like, kind of bit of flexibility flexibility in the in the in the, recommendation. I'd like to under I'd like to have a clear understanding of the motivation in particular second point here. Put whatever is least likely to be attacker predictable first. This makes intuitive sense."
  },
  {
    "startTime": "00:20:02",
    "text": "Do we know that this is actually an issue in this particular case. It seems like good general advice, but I'm I'm wondering if it applies to the signature schemes for which, this this draft affects. I don't know. Now we decided to move, the random value said before the context anyway. But the main reason for this was that the old construction had some collections with RSC 8032 you thought it would would be it feels like the the different notes in, 8032 avoids so we thought it would be a good idea to avoid continue to avoid collisions even if I don't have an attack. The yeah. The other the other is suggest something I get the feeling that, we should not to allow switching the order we focus on side channel tech 3 That that outcome makes sense to me. Yeah. Good. Yeah. Loading Yes. So this is new presentation Gallowing Council mode with secure short tax. Next slide. It's also presented at the NIST encryption workshop in October. So if you looked at that, This is very similar. So DCM has been used because it's attractive performance and probable security. Human standardization, Ferguson identified 2 weaknesses One is that, this forgery probability increases with the message length. And second is that's a"
  },
  {
    "startTime": "00:22:01",
    "text": "well, forgeries allows the attacker to calculate the authentication key he suggested to go back to encrypting the tag, the authors argue that this would lower performance. As a comment, Denise, neither at all, said yesterday, an alternative construction where you just do You do. G hash with 2 different keys and they also show that, this has already already done 20 years ago, this was had already been proven to be be secure in in in several papers. Needs to not follow this. Instead, they put some requirements, and now they're actually removing the whole annex B at least they are planning to do that. So short tax with DCM will disappear. There are other CCM has very good security with short tags, but it's not very used. So people generally But people want ETM performance. Next, even if they have to. So there's many use cases of short tax. One is, radio link players, which often have 32 bit tags, including 4 g and 5 g. There, it's no to change an IP address and the encryption is mostly for for headers. The information is encrypted with HTTPS anyway. Then there's another big use case is in in media, especially for audio where you have very small packages and four year of a single pack. It doesn't matter. At all. It's barely noticeable because it encodes so little time of speech"
  },
  {
    "startTime": "00:24:03",
    "text": "And due to its weakness, CCM is simply not used And then you have the alternatives is to use CCM or CTR with HVAC which is slower Yeah. Next slide. So what is this draft it's called, GCM SST, GCM would secure short tags. It, strictly, it follows the recommendations from nearby. It's just an implementation of that. Nearbay does not specify everything. So there's some options for how to drive the the keys, for example, we have to do this as a key stream. ECM like e ECM SST like ECM is defined as a general construction, so you can use any key stream generate not just, block cipher. 3 differences. There's an additional sub key it. DCM has 1 authentication keyh It is at another the fresh sub the keys are freshly generated with each nons, and then we use polyval instead of t hash. And Polyval is the little Indian version of which is more in efficient in basically all modern CPUs. And you have some polyvoline can actually be implemented in terms of each other. And this node GCM SST has been shown send by SSH and 3 dPP for use in 5 g, and very likely 60. And that decision is already taken, and this is, it's not the same specification, but they are technically compatible with each other. And there's been strong interest in IETF for media encryption applications. And I also have a presentation is in ABC Core. There. Tomorrow. slide. Next"
  },
  {
    "startTime": "00:26:04",
    "text": "So how does it look like? Everything is green, is is the new things. So The difference is polyval, as I said, and you, key to you, and then there is always 2 key generations, Jay Mann here and Jay age, both age and QR, deriving fresh every instance, And that's the differences in performance wise. There's 2 extra AS invocations in the beginning, but that is compensated by the faster polyval. So in speed testing, they're basically identical was what my performance tester set. Next slide. Decryption is, basically, yeah, same changes. This is just this is if you know, GCM, this is GCM with this these changes, Next slide. So what are the properties you get. So performance very similar tag sizes, this can support short tag sizes, and for short tag links. In GCM, the security of short tags is the tagline minus the logarithm of all the blocks, number of blocks, the plaintext and, AAD plus 1. In GCM, as it tea, it's proven that you get it's, instead you get TB security as long as, your plaintext and AAD is not longer than your tag is shorter than 128 minus the same, we have adopted the AGCM allows"
  },
  {
    "startTime": "00:28:00",
    "text": "very big AAD. We have we have decreased this and also the plain taxes decreased because we have 3 keys instead of 2. The AADs decreased so that we get the smaller logarit input And with that, with with these requirements, you can get To 96 bit to 96 bit up 5 bit tags, you get almost perfect secure If you decrease the plaintext and so on, you can ramp this up. This is so this is mostly useful for short tax. For long tax, the the security pro. If you have 128 b tax, the security properties are very similar to ECM. And it does not solve non reuse. You get some you get some more terms in the equation. So you get like you need for full tax, you need 3, non collisions instead of 2. And as, Scott wrote on the mailing list, if you have short tag. So, of course, you need, more, like, 9 in the example. Okay. Next slide. So, this is a quite small modification. Old known construction with proven security. It does not only work for AES. 128. It works for any, cipher that can generate key stream, So, Randall, 256 bit blocks or a stream cipher. Performance wise, it's very similar to GCM. And we think it's not only useful in 5 g, but also for media encryption, especially audio. In, for example, SRTP and as free. So would CFO be interested in working on this? I think the the draft is in the very It's it's age has worked with this for"
  },
  {
    "startTime": "00:30:00",
    "text": "several years. I think The draft is in a quite good condition already. Have a microphone now. Yeah. So, Any questions from the audience comments on this? If not, we'll discuss says the chairs. There's sort of a lot of work going on in different directions around block ciphers and block ciphers modes. So Yes. Okay. We have Chris in the queue. Chris? I could ask less questions if that would be preferable. I I I have two questions on this one, though. The first question is, are there, working groups at IITF who need this, or we think we might they might need this. The second question is, how much, how much room is there to change things that if, like, CFRG decides it wants to I don't know. Tweak it so that we can have a 128 bit knots instead of a 96 not. I'm not saying that's a good idea. I'm just saying, like, what's what sort of changes, are on the table, if any, Yep. I I I I think this is in a good Chase if see if audio wants to change something, then see if audio changes something. I I there's no need to be compatible with with what is used in 5 days. Yes. That, that's a good basis stand on. I'm going to present this innovative core tomorrow. So then we know a little bit more about s o to be impressed. The boss definitely there was interest, on the mailing list in SRT and S Frame. And also some, quick mailing list, media or quick, I think, in the past, but nothing formal, but we'll figure that out. Yeah. Cannot Yeah. It'll just be good to know like, specifically what purpose this will serve. Yeah. think main media encryption, mainly audio, I I would say. Yep."
  },
  {
    "startTime": "00:32:05",
    "text": "Anybody else in the queue? Deer, let's see. We've got Chris. Deirdre's up next. So, alright. Thanks, John. Let Deirdre represent remotely? Okay. Let's see if it lets me control these slides. I'm I'm sorry. Hello. Can I my, my cooling was down can I make a quick comment about audio in my experience, audio doesn't need that high performance? And so the traditional solutions of using HVAC works quite well. That was Scott Florerer for the record. Me good? Ahead, dear. I saw you. Cool. Great. I'm gonna go pretty fast. So I'll just jump right into it. This is a proposal for a way to use mlchem, as a pure post quantum cipher suite for HBKE, public encryption. This is Someone came to me and asked what would I use a toolset, software implementation, a specific construction, for how to do public key encryption in a post quantum way. I didn't have a ready answer for them. So I basically went looking for ones based on HBKE because it is modern and good and nice. And I didn't really find one. So you may know the X wing draft that we have proposed a couple of weeks ago, which is a hybrid cam. And we could look at that one. But I we know that ml cam is about to be, finalized as a FIP standard as FIP's 203, Knockwood. It should be landing in the summer. Maybe. So I wanted to just write that down. It was like a"
  },
  {
    "startTime": "00:34:00",
    "text": "if we wanna do something clean and straightforward and just slot mlchem in where the Diffie Hellman stuff in HBKE is it should be really easy. Right? So I wanted to just write that down, pair it with HKDS Shatu and AES GCM and just write it down and just say, Here is a thing one could use and feel pretty good about it if you completely trust mlchem for your post quantum asymmetric epography. And this is this is pure base mode for HBK. We're not doing anything with off mode. I didn't even look at pre shared key pretty sure he may work fine, mostly because we don't really know how to do auth mode with some of these post from Cam's, so I just didn't look at it. So so you're here, the requirements of the HBK, RFC, basically, it has to be the cam that you're slotting in to your Cypress suite has to be shaped like a cam with a a key gen and end caps and a decaps. Signature. And that cam has to be in 8 to secure in this way, in this imageability and their chosen cyber tax or, you know, adaptive chan shows a separate cyber tax attack. You'll see later that in the chem setting, this has been kind of lifted over from looking encryption, they they just say in CCA full stop and that it just means the same thing. So if you get confused about that. I got confused about that too. It means the same thing. So these are our requirements according to HBKE. So we look at the draft for Mochem. It says that it has these security properties elsewhere in the document. It has the the signature for the team. It seems like we're good to go. Right? Do we have to do anything else? Well, there's been a recent work in the past 6 ish months, maybe. About, reencapsulation attacks when integrating chems in higher level protocols. And the paper that has formalized security properties that are and the written down in kind of more of a symbolic model, what these re encapsulation attacks kind of look like in different settings."
  },
  {
    "startTime": "00:36:03",
    "text": "Has been done by Cass Kramer and his group. I'm gonna be talking about this paper a lot and the first one, first work to really make it salient to me that we have to be careful and we have to look under the hood of some of cams that we're integrating in higher level protocols, is using some of the research from premers at all in analysis of signals new pqxdhuhhybridpost quantum, secure session set up that they did migrate to with help from analysis from the crispin team. Of the first version, of their, PQ, key establishment. Relied only on having an NCCA cap in it. And they basically found that depending on the implementation, the specific FO transform of the chem. That that would be vulnerable to re encapsulation attacks. Luckily, signal specifically made change, but they were also specifically using a Kyber, not MLP, but Kyber. And so they were secured from this attack, and I'll get into that later. So, this is a little bit more detail about that and there are links in these slides. For you to to run that down. So I wanted to look specifically at how DH cam, which is the current default of all the cipher suites in the HBK, spec actually was constructed and basically do, like, a dirty analysis of its properties to see if if ML cam would be fine and match up. And so this is looking at was actually in the spec. And then the key schedule in HBKE, which consumes the output of DHchem. So it's looking at what the actual cam is doing. And then once you've spat out a shared secret at the end,"
  },
  {
    "startTime": "00:38:01",
    "text": "what is HPK the rest of HBK doing with that or any other information? So in here, we can see that when you do an end caps, you take your ephemeral public key, which is basically your ciphertext for any other chem, you take your long term public key, PKR, you serialize them and you concatenate them together and you shove them through your extract and expand, which is basically your KDF inside this this chem definition. And what this does is it gives us the strongest binding that you can get under the formalizations, from premers at all. And we don't have to worry about an attack who can manipulate anything on the wire, to if they manipulate the cipher it'll have a different shared secret. If it manipulates the public key, it'll have a different shared secret. So the shared secret binds the ciphertext and the public y and bind is has a specific you know, definition in the paper. Against a malicious adversary, a malicious adversary, they can manipulate material and and basically try and do whatever it wants. The key will change. So this is related to other, earlier properties like contributory behaviors or something like that. Other times, people called the sort of thing robustness. So that's that's that's what DHchem gets you. And then if you look further down to HBKE, Basically, HPKE relies on the shared secret and nothing else about the chem. Nothing about the public key. Nothing about the ephemeral public key in in DH Kemp's case or the ciphertext in any other Kemp's case. They just take the shared secret out the back of of the cam. It just proceeds and doesn't bind anything else into the key schedule. This is different than, say, your TLS 1.3 where it's just you know, hashing, hashing, hashing, everything in the transcript like, the message transcript of the protocol, into the KDF. So I just wanted to to kinda point that out here. So,"
  },
  {
    "startTime": "00:40:04",
    "text": "This is kind of lining out what I just said, which is that the binding properties, of DH CHEM mean it is safe to take the shared secret out the back of Dhm and just not have to do anything else, like hashing in the public key or hashing in the cipher which is in this case the ephemeral public key into your key schedule. Because the shared secret already tightly binds. The public aid, the ciphertext, it will change. If those 2 things change. So That's great. That means CH cam is great, and it's a very good cam, and we really, really like it. But ml cam is not the same. Spoiler kyber is the same in terms of these binding properties, but mlchem is not Kyber. So looking at Kyber, what we're doing when we're doing a decaps here, this is some of the the draft for fit 203. We take the hash of the public key, the encapsulation key, which is part of the decapsulation key. And we commit to that, we'd we hash that in with the decrypted message here, And then if we, correctly re encrypt the message and do all this stuff using the key material, and confirm that we actually everything passes. Then we have tightly bound to the public key. Butts. Whether we bind to the ciphertext, the ciphertext, see up here is completely dependent on the oh, I I don't know if you can see house. It's completely dependent on the properties of K PPE decrypt and encrypt, which is like another layer of non obviousness. And I'll I'll skip to the end and basically say, that also on the x wing paper, but other and kind of other areas that mlchem is chosen ciphertext resistant. Which is equivalent to a slightly weaker binding property than"
  },
  {
    "startTime": "00:42:02",
    "text": "than the kind that, that DHChem has. It's leak bind The the shared secret binds in a weak resistant manner to the ciphertext, not in the malicious against the malicious adversary. Leak basically says if all the key materials leaked, but it's not manipulated with. Are you resistant from an attack? And it is but it's not as strong. As say, a malicious adversary that's doing whatever they want to it. And this is due to the properties of the public key encryption algorithm underneath, and I'll I'll show some more citations for that later. This is not catastrophic. But it's not exactly the same as what we've been relying on with DH cam. And I will make an argument that In ml Kemp's case, This is not the end of the world, but it does matter for other chems. Because all the HBKE standard says is it needs to be an in in CCA CHEM. Right? It doesn't say anything else about any of these other binding properties. So, for example, one other post quantum chem that been in the competition. And this competition and kind of floating around is based on security properties is classic. This is, from the submission standard, for classic McGalese. It is an NCCA cam. Oops. Okay. I don't wanna do that. So that's fulfills HPK's requirements. It does a, like, tightly bind the ciphertext out in here. So on line 4, We hash in the ciphertext, which is passed in, So the shared secret and that's the shared secret is is the result of that hash or deal be late anymore. So when the ciphertext changes, the, the the shared secret will change. But kind of similar to, to MLchem, the binding properties of it binding to Publicy are completely reliant on the the public key encryption"
  },
  {
    "startTime": "00:44:01",
    "text": "algorithm properties. The the robust which is kind of, you know, explored by by grubs at all in kind of one of these citations here. And what they found in their analysis of HBKEs that for, sorry, for mlcath. Like, there's so many acronyms. I'm sorry. For classic Nicholas, for any plaintext m, use possible to construct a single ciphertext c that always decrypts to m under any Private key. Private key. So that means I can encrypt to, you know, Chris Patton's public key But it will decrypt under, you know, Nick Sullivan's private key. And that's just a feature of classic McGalese, and it doesn't violate the NCCA security game. So what this means is that there is no binding at all to the public in classic McGalese as a chem. There's no leak. There's no mal. There's no even honest. There's, like, if everyone behaves, there's there's no binding at all. And, basically, this is, like, you know, you do you slot this into HBKE, it obeys all the, you know, requirements of HBKE. And it'll blow up in your face because it'll just let you decrypt to somebody, encrypt to somebody else, but it will decrypt to me or something like that or anybody else because those are the properties of the cam, and HBK relies solely on the properties of the cam for this sort stuff. And I'm gonna keep going because I'm running out of time. So, basically, what I'm saying is that the way it's currently written the chem properties really matter. So if we want to use MLP come, and matched basically saying, D DH comes really great. We just wanna match it. Let's try to match it with Manuel Chehm instead of using it raw. And you just Do another extracting expand. You put in the ciphertext along with the shared secret you already got out the back end. And you're done. This is this is doesn't pull in any other requirements from any other part or any other things. It's really straightforward. You get your, your malicious binding to the ciphertext"
  },
  {
    "startTime": "00:46:02",
    "text": "you get your you already have your malicious binding to the public key. You match DH cam, you're done. But this is kind of hand rolling it for mlchem specifically. An alternative is basically changing HBKE to do what ideally it would have done on the first place, which is to always have it bind to the the public key and the cipher No matter what chem you're slotting in there. The down is this is already a fixed standard. It's already been deployed. It's already been implemented in all places. The pros of doing it this way is that you would avoid having to look at any other cam and finding properties about whether it's safe to use it in HBKE. Would just really bullet proof you and belt and suspenders you and get you nicely in line to what your expectations were for DHchem. And you can enforce it for for any of these. And it would basically allow you do classically safely. If you, if you, wrapped it like this if you did them all wrap like this. And I go go back. And basically, I talked to one of the Kate coauthors, and basically they said they agreed, and they thought they it should is kinda one of the design principles that HBK should have, and it does have just for DHchem. But we it's written in there that they want to allow other site suites and other chems in there as well. So Yeah. So I have a draft that's specific for mlchem. But we have an alternative of how we might want to approach this and and welcome questions. Okay. Thank you. This this, this type of, analysis is is super valuable. Even if it's post publication of the RFC. So We have Chris in the queue. And, Chris Kotowski, in fact, this is cool work. I really like it. The somewhere in the in the draft, you say that you mentioned that, you know, the MLCAMS and history rejecting. Mhmm."
  },
  {
    "startTime": "00:48:01",
    "text": "Do you need also those binding properties for when the, there's a encryption failure? No. It it includes are you talking a description failure for on the HBKE level or in the Emilco. All of these binding properties include modeling explicit implicit rejection. There's like, a subset of the full assortment of finding properties that can exist that basically fall out if you are implicit rejection. But all all of the chems that are would accept DHM, I guess. Not no. No. Not really. These are modeled explicitly and looking at whether it's implicit rejection or explicit rejection. So if if decryption fails inside the in that step, It returns the the whatever the rejection value or something like that. And that is included in these notions, these formalized notions. I hope that helps. Cool. Yep. Okay. Thanks. Hi. Rowan Mae currently unaffiliated. Of Thank you very much for, this analysis. I I had one comment on the use of MLchem without without hybrids and, in general, which is I can think of two specific places where I think this is useful. 1 would would be NTLS where you don't need the the guarantees of HPK, which would be like, an mlcam VPN like, a you know, at mlchem TLS VPN, to cover traffic that's otherwise already encrypted with classical cub classical crypto. And the other would be I've got classical crypto, and then I wanna do I wanna do, jots of various kinds, you know, off tokens and things like that. And I could use HPKE with mlchem inside there"
  },
  {
    "startTime": "00:50:00",
    "text": "encrypt those tokens so that I get both you know, both classical and and, post quantum resistant. But, this kinda comes back to the, like, What is like, what are the priorities of CFRG and particularly what are the property, what what are the priorities with respect to things that that people in the IETF need And you know, for MLS, I'm, like, really good love for us to get X wing, done, so we have some concrete easy to use. Kim that we can reference in a cipher suite It's really tempting for people to come and say, Let's go and do a generic solution. But unless you can come up with at least an existence proof, for generic solutions, Let's go and do the concrete obvious thing first, and that doesn't prevent you from going and doing a you know, RFC 9180 biz to to patch HPKE. Later. Thanks. So as a as a reminder, HPK e codepoint registration is only a document required. Review. So whether or not this particular draft would be exactly compatible with 9180. I I'm I'm not sure maybe, Deirdreight you you would know exactly what the differences would be. It may be possible to get a code point with this draft without it being adopted. 2. I think because it's defining its own cipher suite, and it's explicitly defining, like, just a little bit of extra. Like, I could just write down MOCAM with a tweak"
  },
  {
    "startTime": "00:52:04",
    "text": "like, in full and be like, here, here's my new site for suite, but it's, it's just a shim layer, basically, around Mokam. I think that would be fine as his own cipher suite. And then later, like, doing you know, a patch version of HBKE that would make it moot. Could also be a thing that happens. They're not blocked by the other or or something. So, Yeah. There's there's a number of different directions we can go in, but 9180 biz as suggested by this presentation is not out of the question. Thank you. Thanks. Chris, Chris Patton, you're up. Hold on. Alright. Can I control the slides? Yeah. Okay. Alright. Sweet. Okay. Changing gears here. So this is an update on a draft, a new draft that was presented at, 118. A protocol called Mastic. This is kind of fitting into the VDAF framework and, the step we're working on for the PPM working group I'm yeah. I'm gonna give a quick update, and then I wanna have, like, a broader discussion about how CFRG should, govern the development of these of these protocols that are kind of new, I think, to a lot of people in the room. So, if you fall asleep during this, please wake up for the last a few minutes. Okay. So the problem, that we're solving with this protocol is called the heavy hitters problem. So this is something that PM would like to have a solution for. We wanna do is, we have Clients and their uploadants, And then we want to count, we wanna find out the bits strings that occur at least t times for some threshold t. So just as a running example, imagine that we're we wanna know, the most popular website"
  },
  {
    "startTime": "00:54:00",
    "text": "visited by, the the users of our web browser. So, it comes up in a lot of different ways, but that's kind of the canonical use case that people think of. So we already have a solution for this. In draft IRTFC of our GVDAF. This protocol called Poppler 1. So what we're doing in this, this is, this is basically multiportic computation, secret sharing, multiportic computation, In this protocol, we have 2 aggregation servers, that we need to trust to be non colluding. We need to trust that one of them is honest. They are computing a secret shares of this thing that we call the prefix 333333 The prefix tree, for a set of strings, is basically build a binary tree, where, each path is a distinct, possible input and then each node is the number of inputs that are prefixed by the path to that note. So, for example, for a bit strings are 0 0010110. 111001 we can see by inspection that, 3 of these strings begin with, 0. So that's why the left child of the root has the, the label 3 on it. So yeah, we're going to traverse a subset of this tree while we're doing this computation, and we're going to reveal the prefix counts along the way, so the values of the notes. And so that's the basic idea of of of how popular works. It's a very, it's a, it's a very efficient solution to this problem. We need, an additional property besides So what this guarantees is that we only learn aggregators only, well, the attacker only learns, this, the the subtree that we have traversed along the way to computing the heavy hitters. Another proper property we need is called robustness, where we wanna make sure that no clients can, like, poison the input by, for example, voting twice"
  },
  {
    "startTime": "00:56:03",
    "text": "for, for 2 possible inputs. And we have this 2 round, little multiparty computation that the aggregators run. With randomness provided by the potentially malicious client. So, Poplar is nice. It's it's a it's kind of like a proof of concept, that shows you can really do this computation quite efficiently However, we've been working on an alternative. The reason, basically is What we're finding with VDAS in practice is need to get as much functionality out of each protocol that we can. So for this new VNAF, we call Mastic. So we wanna support first for 1st and foremost, this use case called heavy hitters, But we also wanna support, a more general variant we call weightedheavy hitters. And the goal here is, very much the same. We have, each is each user some submitting a string, but it's also submitting a an associated weight. And what we wanna do is find the strings with the highest total weight. So for example, extending the URL example, imagine we wanna know what are the highest engagement websites? So which users are, like, how many What are the websites where users are spending the highest average time on, I don't know, on TikTok or whatever the the website is. And then there's this other use case, that's actually how pre 3 works. Pre 3 is the other feed app that we have in this document. What we can do with Mastek, we can get 303 style metrics, which is basically like any, I don't know, any any aggregation simple aggregate that you would wanna compute. You can compute with pre and we wanna be able to group the metrics by specific client properties. So if you're a web browser vendor, you might wanna know your you might be tracking some health tricks across, versions of the browser. So you have, like, Imagine you have a dashboard where you're seeing like the the the the"
  },
  {
    "startTime": "00:58:01",
    "text": "you know, the load times for a specific for a specific page across all versions of your browser. So MasTec not only solves use cases. It's also more efficient and it has, a much simpler, and I think nicer design than popular. The this has kind of 2 core components to it. First, we generate this print generalize this, primitive that's used in, Poplar, called an IDPF. We generalize it into a VIDPF. And this is, based on a paper, that's a couple years old now from EuroCrip. This is, you know, it's it's it's it's pretty lightweight. It's based just on hashing and it's sort of, like, implicitly provides a property that we need from it. And we also replaced the 2 part 2 round MPC thing I just described before with just, evaluation of a fully linear proof. Well, that is a distributed evaluation of a fully linear proof. This is the core component of 3. This is a primitive we understand very well. And we have a nice implementation of and it also just requires one round. So the overall MasTek is more round efficient and also, yep. And I'll skip that point altogether given time constraints. So, where this is, so what we've done since this the it was presented, in Prague last year. We began security analysis. There's a paper you can read here, that paper is in submission. It has proofs of privacy and robustness basically for the composition of its primitives. We still need a little bit of work work to do to get concrete balance for the primitives themselves. A lot of this is already kind of puts it in prior work, but we do need to put it all together for MasTek. And then we also, are working on a, an implementation in Rust in, the LibriO Library, if those for folks who are familiar, with, the code that's, we're developing in in this space. So the goal for the next draft is"
  },
  {
    "startTime": "01:00:04",
    "text": "we're working on aligning the reference implementation with the security analysis and also kind of improvements that we've come up with while working on the actual Russ code. So kind of just feeding this, feedback loop. So, we think this will be ready for an adoption call at the next IATF, not this time. So what do we want? Well, Yeah. I I basically want 2 things from from you all today. I've kind of two questions. So, this first one is, really just helps us kind of pick a lane. We've already talked a little bit about this on list and off list with various people. Question is, should we replace Poplar 1, with, Mastic in the base feed draft. The the the idea being that, if we have if Mastic is clearly better, then we should just replace up a Poppler with it so that there's not 2 solutions floating around for the same problem. The cons the consensus so far is, seems to be to adopt mass the domestic draft and then only consider removing popular 1. Once we, are more comfortable with Masic and are more certain about, that it's it's it's it's always going to be better. So I'm happy but I wanted to give anyone who has strong feelings about this, an opportunity to to push back So, cheers. If you wanna open it up for questions real quick. The queue is open. Questions or comments about the direction for MasTec. Okay. Nothing in person. We can take this to list. I think it's an important question Alrighty. I mean, I I I think, yeah, that sounds fine. I think we've"
  },
  {
    "startTime": "01:02:01",
    "text": "people, I think we've already settled it on list. But, okay. Okay. Yeah. That's that's that's fine. Tim has just entered into. Info. Thank you. Don't have anything too substantial to say. Just wanted to express my support for MasTec. Particular as I think Chris might have explained. Sorry. I was distracted answering questions in the chat. Besides the features that Mastic has, if popular doesn't, It is also way, way, way faster. Papa requires 2 rounds of communication between aggregators to prepare a report into an output, master just one. So even if it were, like, even if it had no features beyond popular, it would be an enormous performance win. Okay. Thank you. Okay. Chris? Is there anybody else in the queue? No. Okay. This is the last question I wanna ask. So, So, yeah, I I mean, so I've been thinking about this problem, of, with talking about this with various people. So I wanna ask what people what people's thoughts are on how we should govern all of this work. So, there's a lot of like, protocols that fit into this Veda framework. 2 of which we've adopted, Mastic, which is is, might be adopted, and this other one called which you'll learn about in the next talk. And we expect that there will be others. Well, we at least we hope there will be other Not all these necessarily need to be endorsed by CFRG to get deployed but nonetheless, the the the the week we hope the feedback will be useful. It's also note important to note that DDAF is kind of a subset of this thing we call multiparty computation."
  },
  {
    "startTime": "01:04:01",
    "text": "And there may come in need very soon. In fact, to develop, other MPC techniques that don't fit neatly into this framework. So I think there will be a a question here for CFRG to answer with like, what are we going to do with all of these, different protocols. Is it up to see if we're g to, develop applications for, or rather develop documents for for for for for each of these. Okay. I don't think you're gonna get any more comments from the room today on this. But, thank you for the presentation. Okay. Well, then I'll just end with a plug for, Martin Thompson's side meeting tomorrow to, to discuss a related topic. It's called m p other MPC for measurement or something like that. Alright. Thank you. We have Junior Chen. Thursday, 2 pm, not tomorrow. 67p67, I believe. Hi, Charles. I just, requested to control the slides. As possible Cool. Can everyone hear me okay? Yes. Okay. Here, everyone. Continue from the VDAF talk from Chris. I'm going to give a quick update of a new new new VDAC called private inexpensive,"
  },
  {
    "startTime": "01:06:00",
    "text": "Norman Enforcement. Which is a new VDAF to support federated machine learning use cases. A new use case for the PPM Working Group. And this is a joint work with Chris Patton. I'm going to briefly talk about the use case first. Here, I have a list of devices on the left. Each of them trains a machine learning model denoted as f here. With this local data, Each client then generates a model update vector, which we also call it gradient. And to improve the machine learning model, we have the central server on the right that collects the gradients from the clients. The server then irregates the gradients and applies the aggregation to the model and then we get an improved model called F Prime. And we'll send this new model back to the clients to repeat this process in the next iteration. It is great this process doesn't require to collect the client data directly, but the model gradients still leak very sensitive information about the clients. So we want to see if we can better protect client privacy here. One solution is to leverage the VDOT framework to this trust on the central server. So VDAF is a secure multi party, computation protocol that can arrogate, client measurements with privacy and robustness. And the base VDAF draft specifically provides a spec for 303 which uses the idea of a fully linear proof which is basically proof system to verify certain properties of the client measurements. And this ensures we don't learn anything about each individual client measurement, but still able to, robustness of the system."
  },
  {
    "startTime": "01:08:04",
    "text": "Therefore, we have proposed this new Vida called Pine which is based on the recent paper with Guy Rothblum. Iran, Omry, and Kunal Tower, time. This new Vida is to compute an aggregation of decline gradients, expressed as vectors of real numbers. And to ensure the robustness of the allegation We require each client to have a bounded out to norm which is defined as taking the square root of the sum of squares of all gradient entries. It uses the similar idea of a fully linear proof like, pro 3. And let's see how it works in Here, I basically replaced the central server with 2 interrogators, one leader and one helper. And the collector will collect the allegations from Both irrigators. What client does is it encodes it's gradient into a vector of field elements and generates a proof to prove its Alto Norman is bounded. And then secret shares it's gradient into a vector and signatures into Secretures for the 2 aggregators. The aggregators then compute the results over its secret over their secret prospective secret shares in a sense, their shares to the collector and the co the collector will, combine the 2 aggregate shares and produce, and and the aggregated gradients, And as before, it applies the aggregation to the model to obtain a new up up improve model F Prime and send it back to the clients again. So why do we need a new Vida to support this? Why can we leverage crew through directly?"
  },
  {
    "startTime": "01:10:01",
    "text": "Because computing is squared out to norm, in the field operations can overflow the field modulus So, for example, if my altonome bound here is 10, and the field module is q is 23. I have this clearly invalid client gradient. That is 9907 taking a squared out to norm molecules of fuel size is only 6, which can cause the aggregators to incorrectly accept this client gradients. As a result, we will corrupt our machine learning model, and we certainly do not want So the central challenge here is to prevent or detect this wraparound effect. One solution with Pro 3 is we can ensure each entry of the gradient is sufficiently small for example, by, representing each entry of the grading as bits, But communication cost of such approach would be extremely high So which is feasible for some large dimension. So Pine provides this innovative idea of checking for this wraparound effect. The high level idea works as the follows. A random vector will be sampled, and each entry of it is a negative 1 1 or 0. And we'll compute a dot product of the random vector with the encoded gradient, And if the squared out to norm of the gradient wraps around the field module is this dot product is likely to be large. And our and our paper approves the check and correctly detect wraparound with probably a half. And in order to achieve the desired soundness error, we pay repeat this check, many times, And we want to know this is incompatible with Pro 3 because"
  },
  {
    "startTime": "01:12:00",
    "text": "we want the clients and aggregators to independently derived this random vector on their own. And finally, some performance comparisons. Suppose our optimum bound is 1 and the number of fractional bits to keep in each entry of our gradient is 15. And on dimension, a 100 k Pai's communication cost is 15 less, fifteen times less compared to a solution with pro 3. And finally, some next steps for this draft. Currently, we have finished the core design work. Have provided some reference code and test vectors And And as as a next step, we want to complete the draft checks and finalize the parameters to minimize the communication costs. And in the meantime, incorporate feedback from the implementers. And our, questions for the research group is is CFRG interested interesting and adopting this work because this can enable a new use case for the PPM working group And we certainly welcome more security analysis from the Research Group. Thank you. Alright. Thanks. Any questions? keeps dropping. Hi. I have Roman Sorry. I can't get my Roman. Yeah. My my Wi Fi So the question about support for PPM, has the working group decided that they wanna work on this particular use case, I'm kinda wondering chicken egg. Do does the group here need to work on support that use case, is the use case already agreed upon and we need CFRG can you give us a sense for that?"
  },
  {
    "startTime": "01:14:05",
    "text": "I think we're, this is a new use case and we're We basically just proposed this in the mailing list for PPM, and we're asking for feedback there as well. But, We just want to, sort of start the draft that can enable this draft, enable this use case, but we're also asking for for feedback from the PPM working group as well. Thank you Chris, you have any Collins. Yeah. To to be very clear we haven't got a, the PPM group has not asked us to adopt this this work. Yeah. The, there's there it's not in the charter right now. This is this is a new use case for a VPN that it still needs to discuss. Absolutely. It's something that, Junior and I and I have had lots of conversations in and out in in and outside of PPM. But, yeah, it's not formally part of the charter. Martin. Yeah. the So, Chris already plugged the talk away the side meeting that we're having light on this week. A this has some interesting implications for the the balance between the work that a client does and the work that is done by the servers. There's a couple of points in this space that we've been exploring ourselves. For this. And so it would be good to have a discussion about that. I think, before sort of finally concluding this. But I think that what that discussion needs to happen in in PPM rather than here. And once PPM has decided on the use case and and has some thoughts about maybe how it wants to approach the problem then we can have a better discussion"
  },
  {
    "startTime": "01:16:03",
    "text": "Sounds good. Thanks for your feedback. Thanks for the presentation. Our next talk as mentioned, we switched the last two speakers. So Mike, you're not up. It's John Bradley. Yeah. Yeah. I could leave o There's not much room here, though. Yeah. We're not one of the most decisive bunch. Hey. I'm Am I supposed to stand on the x? Please. These cameras can adjust so you don't have to. Okay. So I wanted to introduce, this draft to get people thinking about it and provide some feedback you can So The first thing I'll I'll admit is that synchronous remote key generation may not be the best name for this in the long term. But that's the name that I inherited. Based on the research we can go to the Next slide. Okay. You don't So have a clicker brand. submitted an update. The the origin of this comes from web auth and slash Fido where we're we're originally looking for a way of doing credential summary, That work, kind of went by the wayside when Apple and Google, and others decided that sharing public keys amongst devices was a good thing. Which meant that"
  },
  {
    "startTime": "01:18:03",
    "text": "Relying parties were unlikely to do a bunch of extra work to enable recovery of credentials. Because they just copy it between devices. So our original work on being able to Have a cryptographic method of being able to recover credentials without sharing the private keys across devices. Came to nothing. Then we got involved in the, European Large scale pilots for for credentials? National IDs, etcetera. And it turns out that You know, when you invent something for one thing, Sometimes you discover it's really useful for something completely different that you never originally anticipated. So we're using the same techniques one of the problems with verifiable credentials is that you need a proof key for each instance. So using this, technique it enables you to generate A. Public key and a blinding factor, which can then be used to generate then be given to a wallet, which can then generate any number of public keys for creating instances of verifiable credentials. And a secure element. And then take the, key handle and the public key and generate the appropriate the appropriate signature for the verifiable present if you follow the the flow verifiable credentials. So this enables This technique enables wallets to essentially manufacture any number of public keys, that are non correlatable So the properties of this are that the"
  },
  {
    "startTime": "01:20:04",
    "text": "Key the public keys that are generated off of the public key seed, are Noncorrelatable, and there is no way for the holder of the public key seed and the blinding factor figure out what calculate the private the associated private keys. We have, some feedback, and a number of formulations for making this post quantum resistant for those that are interested in that. The our the draft has all of the appropriate references. We've worked with a number of universities to get formal proofs of the security and privacy properties of this formulation. So, so It could be called blinded public keys or something else. This has been you this is wide this technique is widely used in Bitcoin. Which may or may not be seen as a plus by people. But this we have formal proofs for a generalization that work with any elliptic curve. And can, again, work with Other. Other algorithms, So we would like people to have a look at the draft and provide us with feedback. We're the 01 stage now. And at some point, since This could potentially wind up being used by the euro by a number of countries in the European Union for their wallet instances, And by Fido is an extension, 1 of the One of the interests we have is providing a raw signing function for Fido. So that You could have a remote Secure element,"
  },
  {
    "startTime": "01:22:03",
    "text": "That's one of the problems that the European wallet projects will have is that essentially, there are phone 4 phones for sale in Europe that can actually run the European the proposed European wallets to the standards that are actually required for national IDs. And that's going to necessitate either network based or external secure elements where you have to tap something to your phone to be able to do the actual presentment signing? So this is one of the the potential ways of dealing with that? Are there any questions So just to clarify, you are not asking to adopt the but you're asking for reviews. Right? Yeah. I'm asking for reviews. So so my Right. Maybe. Panel. Maybe crypto panel can help you out. Yeah. Yeah. Yeah. So at some point, Once we've got some more reviews and cleaned it up a bit, Yes. We would like to ask for adoption, but this is probably a bit early for that. I mean, unless you love it so much, you want to adopt it now. I mean, you could twist my arm. Since I I think chairs are awake enough not to doc documents on the spot. This time because we're so jet light. You know, don't don't use it again. Okay. I get it. Any other questions? No. K. Okay. Is, Mike Onsworth. In the queue. Oh, he's here. Oh, Mike, So just to set you're up. this up, we had a call for adoption for a topic at the CFRG, which,"
  },
  {
    "startTime": "01:24:04",
    "text": "that's is not the typical way that this is done. Usually, it's a fully fledged document being adopted and then modified and adapted, but, there were so many different approaches and questions about, how to do a hybrid, classical, and post quantum safe cam that we brought it to a list. We had a pretty active discussion And, feedback on the list was was plenty. And part of the reason why not having a full draft, was useful is that it raised a bunch of requirements that were may not have been note ahead of time. So some of the feedback we got included A lot of folks said that we need general advice for why your combiner of PQ and classical Publicy cryptography for key agreement and offer signatures. Why your construction's unsafe. So there there were multiple different constructions, such as the one from apple, the one from signal, the one proposed in TLS, and it's not just a chem plus a chem. And not all of these had the exact same properties. So there there was a need for general advice to protocol developers about how to do this safely. There's also, request request for specific combiners to be specified. So that, implementers at the higher level level could just plug things in safely, and have a CFRG reviewed implementation, combining your favorite PQ, which, in this case, for a lot of folks, is mlchem, but There's other ones that are interesting, to folks and your favorite classical, key agreement, whether that's eeliptic curves or RSA depending on what your deployment requirements are. And and this is something that we do"
  },
  {
    "startTime": "01:26:03",
    "text": "keep in mind, at the CFRG is to try to be as helpful as possible to IETF protocols and what's, what their constraints are. And provide specific outputs for that. We also had a bunch of other notes, including some sort of tricky questions about mlchemmlchem and IPR, which, have not been been dealt through. So, Mike was Mike was happy enough to, to, to volunteer to, to come give, a small talk about, one aspect of this discuss discussion. So I'll leave it with you, Mike. Alright. I feel like I'm gonna need sunburn cream after giving this talk. You've set me to be the whipping boy for this topic. I personally am not advocating for totally so, like, hybrids so generic that you can choose from a drop down at runtime. That sounds terrible. I'm sure the think the the the combiners do need to be tailored to the properties of the underlying cam. Right. And I don't want anyone to think that I'm still advocating for that. I was 4 years ago, but Right. And CFRG documents typically don't, tell implementers which interface to use or how to implement cryptographic agility one way or the other. But, yeah, from, from a generic perspective, there are ways to do this incorrectly, and we've seen a lot of recently, Yeah. Okay. So the topic that I'm here been asked to talk about is really quite specific. It's why basically yx20519 isn't everything that we need, why why I think we should consider other classic. Things to hybrid with. I've just got really one slide here. Can I can Next? Basically, that my point here is it's a, it's a migration stepping stone. So I work for NTrust. Most of you who know NTrust know us as a publicly trusted route CA."
  },
  {
    "startTime": "01:28:00",
    "text": "Web web PKI. We, of course, do other things. We've sold on prem PKI since 1990 2. And there's lots of ecosystems out there that use on on PKI for all sorts of weird custom bespoke Weird, custom, old, custom, weird, old things. Like predating CMS sort of old custom bespoke. They're still out there. They're still running. We've gotta think about how to move them know it's easy to scoff and say, well, if you haven't done any modernization in 30 years, it's also our problem as citizens of that's your problem, but the world because there's lots of money that traverse these things. Right? I could also have make an example public S MIME, which to 3 significant figures is a 100% RSA. As a thing that's you can't just trivially say, well, your smart card should now just do a lptic curve. Like, okay. Yeah. Cool. So there are things out there. Right, that are on RSA that may still be on PKS 1 encryption RSA, and we've gotta think about to move them. And Yeah. They maybe shouldn't, but they are. So these, you know, we've got some customers who have on prem PKIs where, like, the QA lead time is 2 years. You wanna implement new crypto. It's got a soaking QA for 2 years. That's that's that's after it's step certified, which by the way, ml cam is not. Don't even have the FIP spec yet. And then people have to write code, and people have to get in the, like, 18 month or longer fips queue to get a testing lab. Like, So I don't know when we're actually gonna practically see you know, large deployments of interoperable fips and common criteria certified implementations of all this stuff. A figure the start gun goes in a couple months, and then people get in queue, and then maybe we'll have them in prod by 2026. And then it then it's 2 years of QA, soaking time, and then it's a year of pre rod, like, it's entirely possible that even if we move fast, we can't get the stuff into prod until 2030 at the earliest. So the point here of of hybriding the old stuff with mlchem is to try and shortcut some of that lead time."
  },
  {
    "startTime": "01:30:00",
    "text": "If you can take the crypto, you already have certified compiled deployed and just bolt that into mlchem, you can get mlchem faster by cutting off some of that lead time. I think I've got one more slide. So that's that's really the sales pitch here is that it's we're not saying that, you know, our SAP KCS 1 is good. We're saying that it exists and we need stepping stones to help to help gracefully migrate it to the the newer stuff. So we have this discussion about, you know, should we make hybrids for for RSA chem versus RSA OAP versus RSAPKCS 1, what stuff RCP cases 1, I know, is a tricky one because it's no longer certified as of or if it's approved as of, like, 4 months ago, so it's weird for me to be spending up here, advocating for it. But, like, it losing FIPs approved status doesn't make it any less used everywhere. So these are, yeah, these are sort of the questions that I have. These are the emerged use cases that I'm trying to advocate for. If if we can have this stepping stone, it actually helps us get off the old stuff faster, in some weird counterintuitive way. And that's really all I'm up here to say. Can I go away before the firestorm starts? It doesn't fit very well. But that would be no fun. Rowan may. Could you go back to the previous slide, please? So, I understand the, you know, like, Total, totally understand the 2 years of QA in the 18 months to 2 years for the Fips queue and all that. And then Then around the bottom bullet of the slide, there was a statement that Deploying mlchem with RSA. Was faster than deploying ml cam with Another you know, with with another"
  },
  {
    "startTime": "01:32:01",
    "text": "another classical chem. And I don't see any evidence of that. I see evidence that it takes a long time to touch these devices. And I see no evidence that it makes any difference What other Okay. So what other algorithm you use? What other additional security improvements you added same If I, if I only have an RSA code today, time. My entire ecosystem is RSA. I I don't have an elliptic curve code base. I now have to right one and I'll have to get that FIP certified I then can do an ML cam with a lift at curve. Like, you you have the same 5th certification lead time. If I don't have a lift at curve today, That's assuming that your code base even compiles today. That's assuming that your code base still compiles. My code base compiles. Well, you're you're not talking about you, you're talking about the bit. I'm doing something very hand waving, but Yeah. That, that's, that's a real, that's a real issue. Like a lot of the pea, a lot of the peoples that have this problem, Their product doesn't still you know, cannot compile on any code base that will compile ml camp. I think that's a different problem than looking for. I'm advocating for not having to touch my the inside my fibs boundary I don't know if that maps till I can't compile my code at all. Or if those are different problems, I'll just conclude by saying I'm I'm I'm skeptical of the problem. And I think it might be useful for us to try to get more data about you know, who thinks, you know, who who at some more specific use cases. I'll also say, on behalf of my employer into the camera, we've been asked to provide, you know, we're we're talking to our customers to see if anything will come publicly with this. So slides are super you know, not naming names, it could be useful if these customers are willing to put their use cases publicly, but at the moment, I'm quite they Right?"
  },
  {
    "startTime": "01:34:02",
    "text": "Richard. Hi. Salzakamay. Rich Address the last point, we might be able to be a blinding factor to help present some of the use cases. I think this is really cool. Think it's important. I think it's gonna be sort of interesting for CFRG to talk about deployment in Commerce so I'm kinda curious where the chairs and you think this would go. Yeah. This is this is a a really big open question from the adoption call. And, my hope is that we can work together with a design team that we, put together, so waiting for for people to accept in the next couple weeks and put together a, a reasonable set of requirements that is, justified with practical reasoning. Behind it, for for what the outcome of this adoption call should be in terms of documents, whether it's one document that has advice on how to properly combine things as well as some examples or 2 documents. 1. Providing general advice and when providing specific specifications. So, this is still not decided. And, our hope is that as the design team comes up with a list of requirements, we can discuss it on the list. And, tear tear it down and come up with something that we can agree on, moving forward that, the CFRG can provide something that's useful to not only the IETF, but the industry and and fill this particular gap, not replacing NIST or replacing other standard groups. But, providing help where there's disagreement in terms of how to do this."
  },
  {
    "startTime": "01:36:04",
    "text": "Okay. Yeah. Friendship with Winterside. I wouldn't say this is helpful to the industry, but at any rate, yeah, I'm I'd be in favor. I'd be happy to help work on some of this stuff. Scott Fluor Sysco Systems. I would support, we have exact it's just we have the same problem that Mike is running into that we will, Once, ML chem is standardized, we'd like to ship it, but we can't be it's not gonna be fip certified. Mike says that the queue is 18 months. I believe it's actually a lot rather longer than that. We, yeah, we need a work around And, yes, our code does compile. Steven. Hi, Steven. I I I'm really confused by the talk about competitors. I don't care about Do we want to have every possible option defined? How do we so I'm I'm I'm also confused about how we stop anywhere. How do yeah. How do we stop stop anywhere? Yeah. Yeah. The question on my mind is I think a re a slight rephrasing of that is, I think, a re a slight rephrasing of that is If we have good reason to believe people are going to do stupid things, Should we try and help them do it less stupidly? Is that a fair rephrasing of your question? It's it's a it's a a on, you know, not unexpected reaction, but not a fair rephrasing of the question. No. The question is, you know, I mean, It's entirely I understand you have, you know, enterprise PKI customers, going back a long time, doing all sorts of weird things, There's a lot of possible algorithms in the world. There's a lot of possible ways of combining things. I don't know where, you know, where do we stop? Do we ever stop? Do we get Yeah. I think we stop, and I think we have to. And a lot of the feedback on the list"
  },
  {
    "startTime": "01:38:01",
    "text": "was that, more choices beyond a very crisp set is detrimental to, Well, giving people too many options that they could they could do incorrectly, I think TLS 1.3 was an example of a protocol that tried to balance these 2 things, reduce the total number of selections and cut it down to, you know, RSA ECDSIP 2 56. X25519. And then the 2 or 3 signatures, and that's it. And mapping those to what's expected to be deployed. And the long tail would be not something that the the IETF would comment on or the CFRG would provide suggestions. So I I mean, I think where do we stop? I I I think our previous set of documents that had, number of recommendations things like OPRF for hash to curve had relatively limited number of well defined co code points and, allowed other people to come up with different code points if they want as long as they fit the general construction. I think that's this is something we could potentially do here. The, the risk of having only one construction is that this is not what what what everybody else is gonna implement. So, I mean, in TLS, people needed P256. Right, where people needed RSA. It wasn't like we could go to require only X 25519, although some browsers were able to do that. So, it's a balancing act. That this is, from the the chair's perspective. Okay. Next and the list, we have online, Ron? Hi. Yeah, they fully agree with this. I think we need to understand what we want, the different"
  },
  {
    "startTime": "01:40:03",
    "text": "construction for each can combination, or whether we want a generic combiner that we can it and allow certain combinations. But, my my my the reason why I'm here in the queue is I think I would like to put the sir sense of urgency forward because we have several protocols that they're starting right now ready to go with their own cam combiners. And if we don't start, like, if we don't do it now, we risk of them encrypting data ready to those can combine us and then having to persist them not being able to change anymore. Okay. Thanks for the comment. Jonathan. Jonathan Huylands, cloud player, Perhaps it would, get more consensus to adopt these documents or adopt whatever documents come out. Alongside a die, die, die document that says these documents are valid for 5 years. And after that, they are not CFRG approved whatever that means. You know, just like literally say, these have fixed lifetimes. I think I think that makes more sense in the IITF than the IRTF. I mean, the IOTF can do it over that lake. Right? But we are not the protocol police. We are not even the, standards writing group. At all. So Although, CFRG does sort of straddle the line more so than other IRTF groups. Thank you. Okay. But there's a nobody else in the queue here, we'll we'll continue this discussion online with, with the design team and"
  },
  {
    "startTime": "01:42:00",
    "text": "Thanks so much, Mike. And we do have an extra presentation that We, was last minute and We have the slides here, and and we're gonna We don't. We don't have the signs. On an analysis of Ratls. Half of them. Hilton list. This is the last minute one. Yes. Yeah. Muhammad is Thomas Centaur, are you in the queue? Yeah. Hi there. Can you hear me well? Yep. I I sent the slides. I see that they are uploaded should I share from my side. S. Yes, please. We're having issues on our side. Okay. Sure. I think I need some approval. We're able to do it. Yeah. Yeah. It's just a moment. I'm selecting taking back. Okay. Do you see the slides now? Yep."
  },
  {
    "startTime": "01:44:01",
    "text": "Okay. Perfect. Yeah. Sorry for the last minute, slides. So I saw there are some some some time, and we have done some analysis of, combination of remote attestation with TLS and we took it to the TLS working group. We didn't get any feedback. So I was Just wondering if folks here from crypto background can give us some feedback So a little bit of a background of why we are doing this and why this is relevant the key idea basically is that I have, like, there are few problems in TLS, which we are trying to solve by using remote attestation. Of them is that TLS inherently does not validate the security state of the endpoints Whether it's server or the client Both sites, their software, and their platform. And for many use cases such as confidential computing, we need a more comprehensive set of security metrics. And second issue is that, TLS is inherently very complex. There been there have been like 15 different exploits so far already. And the question in our mind was that is all the complexity, for example, the key satisfied. And for the first point, for example, like there are 3 ways of combination of attestation, which gives a more comprehensive cytosecurity metrics, with the TLS and this diagram is showing time point to the right. And in between, there's this TLS handshake protocol and the two parts like key exchange and authentication here. And one can do the signing of the evidence, which is for remote attestation. Continue signing all the creams. It can be done before the TLS even starts, which is this lipid has pre handshake attestation and one can do it during the authentication phase when the certificate message is, being sent at that point in time, that's called as the intra handshake attestation"
  },
  {
    "startTime": "01:46:01",
    "text": "there is an IT of craft, in the deal is working group. Not yet adopted. It's, still an intermittent draft. Which describes how this is going to be done. And we started the formal analysis for that specific protocol, which was also the also an ongoing work. And where we hit this TLS kind of thing where we, which I would just describe And the final option is to do it at the very end when the nearest handshake has been completed one can then sign the claims and submit an evidence at that stage, which is the post handshake all of the solutions in the confidential computing domain are valid, like, intels are at TLS, for example, is prehandshake at the station, the IETF draft is in the intro 1 and, the confidential containers project from the CNCF is one example of post handshake and is also why to use So our focus for this presentation, for example, is on the formal analysis maybe started with a validation part of former model, which was developed by India, like, 6 years ago, and then we saw that the the core idea was to see Whether it matches the specifications and whether the model is good and So this part, the formal model that we took from India we basically wanted to integrate the motor station part into it and then validate it and use it for our properties and the tool we use is preventive. So all other blocks are currently irrelevant. I just want to focus on this validation part, which is relevant for this. And"
  },
  {
    "startTime": "01:48:01",
    "text": "very simple validation framework. What we did is we took India's artifacts each and every key which is generated by that we just simply compare it with the with the TLS 1.3 specs taken from the, RFC and we compare each and every key coming from both of the sources. And if there is a match, we have a cost and success to represent that And if there is a mismatch, we have a constant failure to represent that. And we keep repeating this procedure for each and every key starting from the first key going up to the end of the key schedule. So with this, what we saw that there is there were a number of failures within the key schedule. Right from the very beginning and then we try to explore what the reason is why this is the case. And, you might be wondering where the right side for the scale is 1.3 specs come from how the keys are generated there. So from the RFC what we did is the RFC in that figure. I think section 7.1 describes the 1st stage We entered the 2nd stage from other sections and so on. And then we compared each and every key which is coming from the India artifacts to this And we found that the issues were basically related to the handshake secret and the master secret. And because of that, actually since, like the handshakes secret is here, the master secret is here. So once these keys are wrong, so Any other key generated afterward is also wrong. And the key point that I would like to discuss or get feedback is basically what they have done in their artifacts is The derive secret stage for both of the direct secret states, the stages"
  },
  {
    "startTime": "01:50:00",
    "text": "which is actually here. The sort for the HKDF expect for the generation of handshake secret As well as the derive secret for the generation of a which is which generate called for the HKDF extra, also for the master secret in both of these cases, Instead of taking a 0 here which is according to the specifications, they have taken an extra hash Over that, And this is something we have not yet been able to figure out why it is extra hash would be required, for any purpose, like, if we have explored all the options and exhausted all our thoughts that we could, for instance, even from abstraction purposes, And we check the times time of verification for both cases, if this would in any case help them save some do some abstraction. I mean, do verify it quickly or somehow something like that. He found that the time Again, in both cases is nearly negligible And in the second case, the second issue is that which is kind of we we have no idea here again. The master secret generation is being done in such a way that it skips this the life secret? I completely understand that at symbolic level, There are one way functions, it makes sense to I mean, combine these two blocks, represent as 1, even ignore this kind of drive secret completely. But my question is why this specific derived secret is being ignored and not and not and not and not This one So these are the kind of questions that I am Still struggling with And, we haven't been able to find any"
  },
  {
    "startTime": "01:52:03",
    "text": "could reasoning for this while this would be the case to skip this and notice in the same artifacts And this is something that we took over to the TLS Working Group, as I said. And, we mentioned all the Not all. I mean, some of the issues, the both of the issues that I have described above And I was wondering if someone has any thoughts or experience regarding this to to share with us some insights There's one person in the queue, Jonathan Oylan. Jonathan Oylan cloudflare. So, So I'm I'm not one of the authors of this paper, but I did study it quite closely. And I was under the impression that they were implementing draft 18. Not draft 20. Achieve like, would you put in your email? So that might explain some of the differences. Yeah. That that's right. So so just to clarify, So the paper, public published in S And P was draft 18. But the later extended their artifacts to draft 20 And since, like, we were we would naturally take up the draft 20, which is closer to the RFC So that's why we have taken a this draft 20 rather than draft 20 artifacts rather than draft 18. Did you check whether the draft 18 artifact to capture draft 18 because I was I think they do. I think think the draft 18 artifacts are pretty good. Actually, the change for this derive secret was actually done between draft 18 and 20, which is more precisely in draft 19. But this is still question for us, like, why would you implement"
  },
  {
    "startTime": "01:54:02",
    "text": "draft 20 in a way which takes this drive secret and not this one. Right. So And then I went through all the discussions which were done for draft 19 in milling waste But that kind of still doesn't make much sense to have this one and not this one Even if they have any kind of like, I I assume it was just a piece of work that didn't quite finish but the other the other thing with the strong hash, I believe they have in their paper a whole section on modeling what you what if you use TLS 13 with week property oh, with week primitives, And so it's probably just a way capture to whether you can use weak primitives, and it will still be correct. So I I wouldn't read too much into it. Yeah. Yeah. I I mean, that's completely fine. I I I went through that thing well. So so that's completely fine. But the question is, I mean, you can model we if you had wanted the groups, that's the, I mean, the caches, that's fine. But the question here is that according to the specs, it would not be has shared. So my question is not about, like, strong why this is strong cash or weakness. The question is that it should have been according to the specs, it shouldn't be any We also looked at the their implementation of the derived secret function, we thought that maybe So it the dive secret by definition itself has the hash over this. So we we we we also looked into that maybe they they took a hash over here because they change the definition of the life secret so that the hash is taken inside the derived secret So it's not, like, looks like this from the outside, but inside, it's it's equivalent to the specifications"
  },
  {
    "startTime": "01:56:01",
    "text": "but it was not the case. I mean, they have a double hash is one is insight according to the specs. And additional one hash is here. Which is which doesn't make sense to us. Yeah. As I say, I'm not one of the authors of this but, you know, if you if you want a model that's much bad, each code, you look at the Tamara model. Not buying bias, of course. But, yeah, I I I wouldn't read too much into the draft 20 stuff. I think they just never published a paper on it as far as I know. So I think just assume it's not finished yet. Alright. Any other questions? Thanks. To Mohammad, Alright. We have, 4 extra minutes. Any other business? Going once, going twice. Alright. Thank you so much. For this point. Great session of CFRG, and we'll we'll see you again online. Find exit Yeah. I want to say"
  }
]
