[
  {
    "startTime": "00:03:19",
    "text": "okay good morning everyone I think we are getting ready to start so welcome to the Orem cat session so I\u0027m on a bernstrom my coach Colin Perkins is sending the blue sheets around so please fill those out you have see all seen the note well I hope so take care it applies of course to this meeting as well I misdirected points do we have a note-taker volunteer note-taker Allen so thank you and we have anyone on Jabbar thanks you said okay the agenda has been circulated so we have not such a long agenda today so we\u0027ll start with the status update on the different documents then we have a discussion on the feedback design team and the progress there and then we would like to discuss as we are now starting to finish up our initial experimental candidate algorithms and the associated drafts how we should proceed from there to take our algorithms further to propose standards and evaluation steps that we see there because that is the next things that is coming up for the for the working group and where we need to kind of agree on how to evaluate and assess those algorithms as the working group document status so we have the candidate algorithms so the couple the CC algorithm has been submitted to the highest iesg the share the bottleneck detection has passed the working group last call and there\u0027s been quite some discussion from the feedback that came there and I think all those issues are now resolved and David has updated the draft also earlier this month so this one is not progressing into write-up The Scream draft is ready I don\u0027t know if Martin has pushed the button yet but everything is done there okay so but I saw that Ingmar or you submitted the DRI update to fix the ID needs that were there so it\u0027s yeah ahead yeah we fixed I Denise and I think I could to mark and then it will be ready for pushing the next button yeah Martin already did the writer for it so it was just that it gets stuck on those parts then we have "
  },
  {
    "startTime": "00:06:21",
    "text": "another algorithm that was out in in working group last cool for a while we now have captain some feedback back on that algorithms algorithm so the waters will make an update based on that and then that draft is also ready to proceed and then we have the GCC congestion control and that is still waiting with the waters to continue and resolve the issues that has been with with that draft for a while related to the handing of laws our requirements draft is still stuck waiting in the editors queue but I guess eventually it will be moving forward and then we have the evaluation drafts and those I think are now the ones we need to start also get out after the candidate algorithm drafts have been moved on so the eval test we discussed this draft at the virtual interim so the eval test was ready for working group last call than the eval criteria there was some outstanding issue with the TCP port there that you were looking at maroon why don\u0027t thing yeah so I think we\u0027ve changed the evaluation for the TCP model for the short but we\u0027ve not published it yet I think we\u0027ll probably have to send it to the mailing list just to see if everyone agrees with in the new model will do this today or tomorrow okay and very good because so beyond that I think the there\u0027s nothing else open in the in the draft as far as I remember no I think this was the only outstanding point that needed to be resolved and we wanted to send the eval test and the well criteria together for working group last call as they are so closely I did so that people can look at both of them and see that they are in NSYNC okay so that\u0027s as soon as we have the DAP date for that bomb we can move both of those two to work in group last one and then we will of course need some reviewers for those drafts so we\u0027ve had a little bit of not so much review enthusiasm in the the last drafts that have been circulated so please try and help review those and I think it would be very good also we have different authors on the - and they need to be in sync so if you can also review each other\u0027s drafts to see that yeah I think the review is appreciated I would say the eval test of course has been implemented by several people\u0027s I would be much more confident about the the "
  },
  {
    "startTime": "00:09:25",
    "text": "correctness of actually both the drafts because removes things between so people have to actually implement part of the eval test without of it by reading eval criteria and some things have been pointed out in the past based on implementers experience so I would believe that they\u0027re much more I\u0027m much more confident in the correctness of those of those drafts but reviews are much appreciated yeah I think they are both in in good shape but I mean also even you know when we send a working group last call even if you have used them and implemented them and and seen that they worked well just confirming that on on the list this is good input yeah Mia Kulemin as a for mature so those have been around for a while and are pretty ready so I think we got already a couple of few years in the past say just checking in last call\u0027s probably the right thing to do I wouldn\u0027t be too worried that\u0027s what I\u0027m saying no then we have the the wireless tests and this one there are also no open issues in the draft as far as we have determined but that one has not been used as extensively so I think there we\u0027re looking for for reviews and also for people to try it out and use it okay Randall Europe but doesn\u0027t matter the rail Joseph Mesilla might be this one person might be good to try to look at the wireless tests that was a person who presented the ICC RG about the improved lower latency Wireless paper maybe he has some relevant experience if we can get him to look at it that might be very useful okay I assume he\u0027s talking about the AKM stuff from token right yeah I can certainly check with you yes and the last draft on the evaluation part is the video traffic model and this one is also [Music] known own updates waiting from the water so this one is also ready for review and should then be fairly close to working good last code from Zephyr sorry Mina one of the author of this drop is saying he believes this is very for working with Moscow and now you\u0027re "
  },
  {
    "startTime": "00:12:25",
    "text": "talking about the video traffic model Whittaker okay there there is a new version I guess pushed oh yes it was updated since we had the interim yeah so maybe we can have that one go along without the two for for working group times call it if the working group is happy with that so what I\u0027m saying I was just gonna say that we\u0027ve implemented at least half of the traffic model in I think the trace based one we haven\u0027t done the synthetic codecs but I think there was some rumor by Cisco that they were implemented in ns3 so if anyone\u0027s actually done that would be good to see if someone\u0027s actually independently run the code base and gone through it and seen but the results seemed good last time that we saw them yes we had some discussion about that that virtual interim as well where they were presenting dentistry and I think some people were going to look at it but I don\u0027t know if you have had time so as I said like the first the second part of the draft we\u0027ve implemented so we\u0027re confident that it kind of works as we expected to the first one it\u0027s not been independently been verified so there\u0027s of course like results that we\u0027ve seen but no one\u0027s confirm them so it would be nice if someone can confirm it I had described mmm Segawa again saying like they are working on getting the code ready for open source then what they want to mention make sure is likes a good quality code so they\u0027re almost there so the code might be getting open source pretty soon so people contract I think in stock here\u0027s referring to this ns3 Park because I believe this traffic model is already open source I think so yeah incised in 3-4 weeks if you know okay yeah so it would be very good of course if someone would like to try it out and I think if once the code is available that should also be a good opportunity for people to play with the different algorithms then we have some "
  },
  {
    "startTime": "00:15:25",
    "text": "draft related to interfaces we have the CC codec interaction and we also have the individual framework draft so we put this up for for discussing later how we should bring these documents forward and how we actually should structure the work around this because it we started with some documents then the framework document was kind of asked for but then we were working on the algorithms and the feedback so this work kind of stored a little bit but now that the other things are moving out we should revisit this and see you know do we need to start with the framework document to have that and then see what we want to fill in or do we need yeah what is there actually the relationship between these these documents because this has been moving around a little bit then we have the work around the feedback and that is also something that we have on the agenda there so Colin will present an update on the feedback overhead and then we have the associated feedback message draft that the design team has been working on and the feedback message draft is also on the agenda for apt Concord yes these are the milestones so we are now I think going to be able to send out the drafts in July so we are on track for the the next few milestones we of course updated the dates for these so that\u0027s why we now are in track on those and then at the discussion at the end we will get back to to the other milestones and how to approach the work around the evaluations of the outer ears now that we have the candidates because our next milestones are several milestones related to that actually okay so next up is the feedback design team so how do you want to say anything is introductory remarks so status update okay so I have a very short status up this pondus and design team design team what we have been doing I think in interim we have been showing like we almost resolve all the issues we had only thing that was there was like whether we need a commission in terms of compression or something else there was some purple jobs and during this in "
  },
  {
    "startTime": "00:18:26",
    "text": "tremendous meeting didn\u0027t even have been meeting several times and what we have been doing you have been analyzing and comparing like what is already deployed compared to what would be the artistic advantage determine if somebody deploys this dis current format and the idea was like if we can compare our numbers with a workable solution and we have like certain margins to see like how how efficient or non efficient it is we have an idea like well how how Hamas did the compression would matter and we have kind of come come to a conclusion I think Colin is going to present the comparison results from the design team that we have been looking at and their current conclusion so far we have is like we don\u0027t see a great increase in or do sorry a decrease in our on the requirements on artistic bent and if we do some sort of optimization so I we believe from a design point of view this conformant is pretty good and after Colin\u0027s comparison I think I\u0027m going to ask whether we can conclude this work in our MCAT and movie to a geek or 4/4 instrument track so I\u0027ll ask that question so keep bear that in mind Colin\u0027s presentation where the tasks discussion to the working group okay thank you is that okay so I\u0027m Colin Perkins I\u0027m speaking from the for make some knots as a chair as a design team member I want to talk a little bit about the congestion feedback in that TCP and the the overheads of the different congestion feedback schemes so the obviously when you work out when you think about the overhead of sending congestion feedback in our TCP there\u0027s a bunch of things it depends upon right it depends upon how often you want to send the feedback it depends on what information you\u0027re feeding back what type of feedback is needed how you\u0027re formatting that information whether it\u0027s big sensors a compound or a non compound packets how many media streams you\u0027re sending itself we\u0027ve got two approaches to sending this this feedback which I\u0027ve considered we\u0027ve got that design team feedback which is what I presented in I guess the sole meeting I went through the details of that and I also just did an analysis of the overheads of using the mechanism google has been using or one of the two mechanisms Google have been using in the the Aaron Katz transfer wave congestion control extensions which I think was presented here a year or two back okay so we "
  },
  {
    "startTime": "00:21:30",
    "text": "looked at two scenarios first one is voice over IP second one is a two party videoconference so beginning talking about the voice over IP we\u0027ve got a two party VoIP call so you have two SS SES in the session each participant sending and receiving audio no video flows or anything anything like that we\u0027re sending speech frames every TF seconds usually there will be every 20 milliseconds and which we want to send a congestion feedback every n R frames and we can tune how often we want to send the condition feedback based on how much overhead we\u0027re willing to accept and how quickly we\u0027re on the feedback as a result of that you want a reporting interval which is the framing interval times how many frames you want how often you want the feedback and you can send the congestion feedback either in the regular compound rtcp packets along with the sender reports and the esters or you can use a non compound packet and send it in between the regular regularly scheduled packets and we\u0027re sending N and sub MC non compound packets between every compound packet and we see that sub parameter indeed the analysis later the format of the packets which looking at the format Stefan Stefan has proposed the format of the feedback it was like this you see there\u0027s a fixed header with some SS our C\u0027s based sequence numbers packet counts reference times and so on there\u0027s then a block of what\u0027s labeled as packet chunks and then a block of what label those received deltas the packet chunks IVA just a bit vector of V of this packet was received this one wasn\u0027t or there are alien encoded if you have long runs of received or lost and the receive deltas are just times and Delta\u0027s relative to the reference time and they can be either 1 or 2 bytes obviously the size of this packet varies depending on how many other packets you\u0027re sending feedback upon and it varies depending on the loss patterns because you\u0027ve got a loss RLE or a bitmap and you can choose whether to send that is entirely or a bitmap but the overheads were there if you\u0027re only sending a single stream this works if you\u0027re sending multiple RTP streams is designed as as a transpose got a transport wide sequence number so it relies on putting an additional sequence number in an RTP header extension and they then send one feedback packet from one SSRC which reports on those additional sequence numbers and there are a bunch of issues about how that fits with within RTP in general which I\u0027m not going to talk about and talk about the events but for the voice over IP case you don\u0027t need this header extension you can just use the regular RTP sequence numbers so what\u0027d you end up sending with the system what what what the Google implementation sends as I understand it "
  },
  {
    "startTime": "00:24:31",
    "text": "you have compound packets which comprise the UDP IP headers send a report and the source description packets if you use the to get what the RFC number is the the our mcat recommended cname formats you end up with 18 octet cname send a report with a sender into a block and one receiver report in it and you end up with compound packets which are 108 bytes you also send nan compound packets which just have the congestion controlled feedback a TCP packet within a UDP packets the of course varies depending on what you know the packet lost patterns and so on I\u0027m am analyzing a couple of cases if we start with the the best case which is that no packets are lost therefore you can just put a single aryl each chunk in which says everything was received like that takes two bytes for the packet chunks and the receive deltas the best case is that all of the deltas are small enough to timestamp Delta\u0027s small enough to fit into one bite therefore you need one byte for each packet you\u0027re reporting on if you\u0027re reporting on NR packets you that means you have n R bytes for the the deltas and you\u0027ve got 28 bytes for the UDP IP headers 20 so you end up with the non compan packets being 50 bytes plus NR which is number of Records I don\u0027t sing from clarifying question so the CC feedbacks not send with the with the compound packet the Google implementation doesn\u0027t as far as I understand it doesn\u0027t send it with the compound pack but they could but they could but the analysis of both the proposals do you send the CC feedback only with the non compound or yeah I\u0027m doing the analysis that this way with the CC feedback for the Google proposal being Center in the earth non compound the design team proposal I\u0027m sending it with the compounds as well just to match what what as I understand it the Google implementation does and if I\u0027ve got that wrong and I\u0027m sure I\u0027m sure Stefan will correct me by email he\u0027s not on meeting me thank you okay so you\u0027re sending some compound packets between them you\u0027re sending some number of non compound packets you can work out the average a TCP packet size very straightforward way plugging in the number of Mon compound packets being sent just by averaging outs I sort of compound and non-combatants "
  },
  {
    "startTime": "00:27:31",
    "text": "if you troll your way through RFC 3550 you find that the the expression for the reporting interval reduces number of number of participants number of SSR sees times the size of the rtcp packets divided by the tcp bandwidth you plug in the size of the the average size of the packets based on the size of the compound and non compound packets you decide you want yes voting interval to be framing interval times number of packets you\u0027re reporting on there\u0027s some algebra and you end up with this expression for the TCP packet and the the erm cat CC feedback draft walks through that in a bit more detail and if this is exactly the same thing as I presented at the soul meeting so if you take the Google proposal second proposal what this chart is showing is if you\u0027re sending for example audio where you\u0027re sending a packet every 20 milliseconds and you want to report at every second and frame then in order to do that you need an a TCP bandwidth of forty two point two kilobits per second assuming this feedback format and assuming the best case where you\u0027ve got the better the best case you know the smallest to pick values as you increase the number of frames on which you\u0027re reporting new you report less often then the rtcp bandwidth goes down similarly if you\u0027re if you\u0027re increasing the size of the packets again the rtcp bandwidth acoustic in the best case you can get down to 1 point 8 kilobits per second nor that number but in the best case you hit your sending one our tcp packet per second for one point they kilobits per second of a TCP which yeah it\u0027s a pretty good overhead so just confirming again the the one point eight is when you\u0027re sending every 960 milliseconds of every one second or feedback right it\u0027s when you\u0027re sending feedback every every second but just to make sure that people understand what those numbers need to be computed yes yes so if you\u0027re sending if you\u0027re using 60 milliseconds worth of audio in each packet and you\u0027re reporting and every six sending an rtcp packet every sixteenth packets which works out as once per second and if you want to report faster or if you want to have smaller packets on lower latency then your you know your overhead goes up to the more faster you report the more about you guys and tables are right yes "
  },
  {
    "startTime": "00:30:34",
    "text": "you see these match the old table I presented and soul I updates in the slides and mists yep so if you get back so this this is what what you get if you\u0027re only sending come power so this this I\u0027m not sure it\u0027s quite right it\u0027s the next slide and this actually matches the Google thing where your alternates even compounded Nam and and the numbers came out as distraction less okay you can do the same analysis for the design team proposal the design team proposal sends packets with an extended report and the extended report yeah has the design team format for reporting on the time stamps and packet loss our Legion and the the ECM feedback and if you work for the analysis and I had all the details of this in the presentation so you find that the sight of these is a hundred thirty two plus twice the number of reports similarly you can work through this for a non compound packets and it actually works out to be the same size you do the analysis here she come out with slightly bigger numbers to the next one and again when you\u0027re alternating compound and long compound again you come out with these movies so if you put a list together what you see is that for voice-over-ip the proposal that\u0027s much rtcp bandwidth yellow and we see that the proposal that stefan made eats the design team proposal in this case now it was two caveats on that one is that stay hounds proposal is not reporting on ecn feedback or as the design team proposal is so we\u0027re sending more information so we can expect us to have slightly higher overhead the other is that I\u0027ve made best case assumptions for the size of the packets however even if you make the worst case assumptions and they did do this analysis that they haven\u0027t put it on the slides it still wins out well they\u0027re not by as much so for the voice-over-ip case Stefan\u0027s proposal does win but report some slightly less information - suppose also requires the header extension and the midnight not for this case because there\u0027s only one street oh it does for the the video conference case so in this case if you don\u0027t need the EC and feedback then this shows that you can optimize things and get away and "
  },
  {
    "startTime": "00:33:34",
    "text": "whether we use Stefan\u0027s proposal whether we use something else we do there\u0027s clearly a little bit of benefit in in optimizing this and yeah in the most extreme case you\u0027re going down from two kilobits to one point four kilobits about TCP it\u0027s a little more significance at the higher okay um the other case we looked at was a video conference point-to-point video conference two participants each one sending audio and video so in this case we\u0027ve got four SS RCS - for each participant one for the audio one for the video we\u0027re assuming they\u0027re all bundled together into a single five tuple so this is all a single at RTP session we\u0027re seeing a video framing interval of TF so one over TF frames per second we\u0027re trying to get the rtcp reporting into ought to be some number of video frames I said there is you maybe want to send on a TCP report every video frame or every second frame or if you fall frame or something like that and we fit in the audio reports along with the video reports and again we can send this as compound or okay so quickly walk through the design team proposal which we had last time and then I\u0027ll go into what we got with Stefan\u0027s proposal so for the design team proposal we\u0027ve got two SS RCS therefore we need to aggregate the feedback into a single packet we\u0027re assuming we using the reporting groups extension to reduce the overheads of doing that we\u0027re assuming we\u0027re using all of the various optimizations the designated reporting sauce and avoid having to send each of the reception quality reports twice and there\u0027s a draft which is in the RFC editor qulet that describes how to do that and everything is aggregated citizen that which whether it\u0027s the audio or the video source which is chosen as the reporting source the non-reporting source in this case ends up being an empty sender report because if Ella gated it to the ever SSRC see me and the grouping optimization I\u0027m assuming a juicy name you end up with 20 28 bytes for the SS back the reporting sauce ends up as having a sender report with to report blocks one for each of the audio and video SS RCS it has an S that\u0027s packet with C name and the reporting group identifier and it has the X sub block which contains the congestion feedback if you work through the analysis walk is 2 bytes reach of the audio packets and 2 bytes of each of the video packets it\u0027s reporting on plus a fixed header and you do the sums when you come out with 156 "
  },
  {
    "startTime": "00:36:34",
    "text": "bytes plus twice the number of audio packets plus twice a number of video packets for the size of the compound what that\u0027s in along with the size of the UDP packets the nan cup of the non-reporting SS sv packet and you end up by its plus all of that and because you\u0027re aggregating to SSR sees into one packet you you end up having it and Magnus had the the draft which describes okay we\u0027re then making a couple of assumptions we\u0027re assuming everything is constant rate video because the voice the analysis is not practical we\u0027re assuming all the frames are equal size so we\u0027re just chopping up the bitrate into equal sized packets audio is assumed that being sent at 50 packets per second 1500 byte video packets same rtcp calculation as for the voice case you just plug in different numbers for the sizes of the compound and the non compound engines okay and what you get unless this is a little trickier tricky to see what that of this slide means I\u0027ve worked with a set of different media rates so we\u0027ve got the media bandwidth on the left and a set of different frame rates and that gives you a number of audio and video packets that you\u0027re recording on so if you\u0027re assuming 100 kilobits per second video and 8 frames per second that works out as one video packet and 6 audio packets per report and you see which is 1/3 of your your media rate if for example you think while megabit per second 30 frames per second video joined up with free earth free videos like this very thoughts - audio packets and we need 122 kilobits per second of rtcp which is taqwa sends with a media rate so that\u0027s working through a bunch of different media rates a bunch of different frame rates and we\u0027re only sending compound packets here we\u0027re not filling in with non compound packets and you see the the overheads vary from about 30 to 35 percent of films about 6 percent you can also send reduced-size packets and this lets you omit some of the details just send since just on the feedback next slide if you do that you think you reduce the reporting overheads from these are about thirty thirty five percent to twenty twenty five percent so you get a win by sending long compound packets in between as you would expect because there\u0027s less header advanced "
  },
  {
    "startTime": "00:39:35",
    "text": "okay okay all right okay let\u0027s go back okay so this right right anyway let\u0027s let\u0027s keep going yeah yep so this I think is looking at the and it\u0027s trying to work out the overheads of you know putting the compound packets in for the audio and the video for the Google proposal which is just a straight forward you send the reports requesting group extensions and so on which would be that in the most optimal way of doing it if if I understand right this is not quite what the Google implementation does I think they do something slightly different I think this is actually lower than what they\u0027re doing and it matches what the design team proposal is to do for the compound packets if you get to the next one think sorry can you end up with two 220 bytes and you have a aggregation the Google proposal also for the month that sends non compound packets so the compound packets are just regular packets and the events and non compound packets that have the congestion control feedback in them and that\u0027s the only congestion control feedback it sends the non-compliant packets here we\u0027re assuming again if we start with assuming the best case there\u0027s no packet lost the deltas all fit into one byte you end up with the non compound congestion feedback packets being 50 bytes Heather plus the number of audio packets plus number of video package surety in the best case the NA and the env come from the received timestamp deltas and then it\u0027s 48 bytes heather plus 2 bytes for the asura this ends a single feedback packet which we pass on all of the SSS so you put those numbers in with the Google proposal and you come these TCP bandwidth requirements and again we\u0027re assuming best-case timings and were currently excluding the overheads we require that TP header extension it\u0027s like if you assume the worst case so the worst case we\u0027re assuming the packet loss is unpredictable so you need to send a bit vector rather than the rles and we\u0027re assuming the timing variation is large enough that you need two bytes per packet for each one so that\u0027s that\u0027s the best case in the worst case you come out with a slightly different expression "
  },
  {
    "startTime": "00:42:37",
    "text": "divided by 17 fits seven into each each two by chunk round that up for the worst case twice the number of audio video packets for the receive deltas next slide and you plug the numbers in and you can add with see what expect slightly higher overheads okay so to compare them all if you just look at the rtcp overhead comes out with the blue column the Google proposal comes out between these two yellow columns and you can make best or worst case assumptions and looking at this it looks like the Google proposal wins out however the Google proposal also needs an RTP header extension because it uses transport wide sequence number and that takes up eight bytes per packet for the header extension if you include the overhead of that header extension wins out in most but not quite all cases and it all depends on exactly you know how many are RTP packets are in each report and etc but we\u0027re doing that worse a general slight win for the design team proposal and this so what what we\u0027ve done here is compare the performance of the two approaches to sending feedback the design team proposal and well the voice over IP case the Stefan\u0027s proposal Google proposal wins out provided you don\u0027t need ecn feedback because it doesn\u0027t send ecmt battery for video once you take into account the extra cost of the RTP header extension it requires the Stan\u0027s proposal losses slightly in most cases to the design team proposal although there are a couple of cases where it wins out the bandwidth the rtcp bandwidth for the video especially for earthtones proposes very dependent on the data rates packet loss patterns exactly how many packets are reported in each case so you find some cases where it wins out some cases where it doesn\u0027t the design team proposal also has the adventures that conveys ecn feedback or a second proposal doesn\u0027t and it\u0027s more compatible with the way RTP uses ssss so my I think my conclusion is that yeah in general given that I think we\u0027re mostly interested in the the video conferencing in the more the more we won\u0027t be flexible and support a wide range of "
  },
  {
    "startTime": "00:45:37",
    "text": "scenarios I think the design team proposal is the right way to go especially if we want to include ecn although if we do care about highly optimizing the voice over IP case and don\u0027t care about ECM we can clearly get it with jump backs I don\u0027t think the design team feedback also if anybody ever comes up with a multi-point congestion control algorithm would extend to that naturally right yeah definitely and I\u0027d say one thing I haven\u0027t done is you know simulated a for example 15 person conference and see what the overheads are there so that there\u0027s a bunch of scenarios we haven\u0027t looked at here but for the two simple cases the design team has been considering I mean what one of the things we had in the in the sole meeting is that there are other savings you can make you that this is making assumptions that you\u0027re sending 18 bytes sorry 16 byte are group identifiers and 16 bytes C names and that sort of thing and you can certainly send smaller identifies in some of those cases and you can get wins in that way you can play with a number of non compound versus compound packets and that sort of thing so there are other optimizations you can make okay that\u0027s all I have any more questions or comments no Sayed you want to go get back to your question yeah and jihad again and so as I was saying in the beginning I mean who from designed a point of view who have been doing discussions around it we have been looking at other factors and that has been proposed like early and completion and separation vector and also we now have compared this with a deployed solution with some of some of the numbers that in Colin mentioned might not be as exactly as how deployed but I think we we have included the numbers that\u0027s not like it\u0027s a bit bigger than what is what is there but still I mean I think we have done a we have seen this compression we have our conclusion I was like well there there could be optimization as Alan mentioned like if you don\u0027t think of little bit of information ours is information but we didn\u0027t feel like it\u0027s out doing the optimization so the question is like are you guys also agreeing with the working or making with that conclusion and if he is then the next phase will be moving it to every curve as I like DTR mcat a bit "
  },
  {
    "startTime": "00:48:38",
    "text": "eco trapped and asking for a further process Proceedings of this district so any comments on that one and I also heard like somebody has implemented this thing at is it works for in trouble yeah that\u0027s likely to hear but any comments well I don\u0027t think so I had a question given that the could you go back to the or wipe scenario where you compare the the two proposals avoid scenario right yeah so the reason I bring it up is to to see because that\u0027s the the scenario which is where we can be the most optimal right and there if the savings are not in the congestion feedback because I can see it it\u0027s about 20 percent off right so like the the yellow is 20 percent lower then then the blue now the question is do we want to make some savings if I do do do we want to engineer some more to make the 20 percent savings and if it\u0027s not in the feedback message then where could those where could be gain the 20 percent because in the video scenario you know they\u0027re off by you know they\u0027re they\u0027re not off by much so you can\u0027t engineer much in that case scenario but I see I see why people would care about the audio-only scenario and want to optimize that because you have only one or like one stream and the proposal is like the numbers are close to point seven compared to three point five but if you put them and it to algebra it\u0027s about ten to twenty percent off now the question I have is does anyone care about the twenty percent lower and if people don\u0027t care about it then your questions worthwhile like should we should move on but if people feel that we want to make the twenty persons having the question is where can we make the because it\u0027s not in the feedback format as far as I understand I think calling does not apply to high count Parkinson\u0027s flow Mike I think partly the difference is the feedback I mean one is reporting any CNM one isn\u0027t although it that certainly doesn\u0027t account for 20% but there is a difference there I think also partly it\u0027s the one is sending feedback in every packet and one is sending I mean the compound packets that will make a difference yeah and we\u0027re also slightly formatting the things slightly differently one uses an Excel block and one uses a regular a TC packet type so that\u0027s the slight differences in header overheads I think a combination of those things is probably what accounts image overheads yeah I think I think you can play with "
  },
  {
    "startTime": "00:51:38",
    "text": "the numbers and actually get close to this as members eventually it is not in the feedback message something else right so you can in deployment you can try to see like how whether where it comes comes from and I think in other previous meetings Colleen and Josh also showed that is the head of thing that\u0027s actually bothering us more than right so the the one point four that I see here is the non compound version and the 2.0 is come around and non compound right yeah so we say it\u0027s a bit discipline oranges but we know the but but this is like compound I mean you can you can I think you can play with this non component combination it can get like pretty close figures here also but we want to I think that Paul Collins point or I want to say show that like this is the this is the case might be there and this is not that far off so my question was just to confirm that people understand that the twenty percent is there and if they are willing to live with it yeah I mean we can we can do things like changing the X up lock into a regular our TCP packet error reports for example one and safe yeah like four byte sourcing which probably accounts for most of the it may perhaps sure so John flexagon sorry um this is I\u0027m just writing to make sure you so you\u0027re sending you\u0027re doing a VoIP call where you\u0027re sending a compound back at every other every other voice frame essentially no no so how often ask are you sending compound in your case so in so in the so hopefully the microphone if you pick it up the fatal it\u0027s removable okay so in in this in this bottom case for example with every sixteen packets when sending which at all I think we\u0027re alternating compounded non-compliance it\u0027s still sending a and they\u0027re still sending a compound packet I mean Oh in the assumed in the google case they are sending compound packets sometimes right yeah what you didn\u0027t count at all here if I did it right there included okay I mean in the innocent someone should check whole my cuz I\u0027ve and because presumably you know they should they\u0027re you know they feel like it\u0027s further you know they don\u0027t need to send compound back it\u0027s more than every yeah whatever I know what they\u0027re doing so then there\u0027s only they\u0027re not pulling that completely out of nowhere so no sure uh and it just feels to me like you\u0027re something compound packets more freely than you need to do with hurt your numbers here yeah it works what definitely yes so if I remember these slides correctly the the yellow number is actually the the better number for "
  },
  {
    "startTime": "00:54:41",
    "text": "Google\u0027s case and that\u0027s actually only the non compound if I remember correctly right because the other number is one point eight at the lowest one so yeah yes and and one thing we do need to do is you know this was my calculation I think nobody ever than me has checked it so we need to check like done this correctly so I mean I\u0027m not hearing like people who are super super be sad about this but our what I am hearing like we can do engineering around it to get actually make the difference and this figure says the slide what we\u0027re looking at is not even at the same kind of values yeah alternative reduced compound and we have only reduced size there so yeah I was just gonna ask are we going to document so just going back to what Jonathan said before are we going to document strategies on because you could do a lot more savings by not doing or like just doing non compound or deterring the compound back it\u0027s further apart and putting more non compound because once this draft moves to a VT core I guess we would need to come back once in a while to be to be discussed here but some of these strategies might impact congestion control algorithms so before we give AV decoder the thing do you want to do we want to document like what the instructions from the candidate algorithms are for yeah I think in designing we discussed this okay and we decided like okay we we can have a guideline for how to use this I mean there are other things that that was on the discussion on like adaptiveness of artists if we feedback and all this thing and that so the decision was like okay we\u0027ll have and the packet format the design team is will be there will be producing these at adjust and packet format and go to the decoder and publish it and then if design team continues doing this adaptive thing or the guideline thing this ending can produce another document so separate okay yeah that\u0027s a separate document IDs because otherwise it will be over complicated and it\u0027s not going going farther than because those are the more complicated thing and that people need to agree and do some more experiments so I believe is better to be answered this one so - Westland I haven\u0027t participated in design team and I but I mean for my perspective I\u0027m Cedars bit rates difference is that\u0027s significant and I mean you can tweak more I think in the space we\u0027ve been talking about how often you send compound etcetera there are I think but the important thing here is I think is to have a structure which aligns with "
  },
  {
    "startTime": "00:57:42",
    "text": "our to CPS were normal just be used so any future extensions in other dimensions also be able to who come to your on on those known compound packets etc one then you need etc and them so I I would expect that when you write this as a packet extends you actually need to in enmity core you\u0027re probably at least discuss a bit on where it\u0027s transmitted etc and because its intended use so I think you\u0027re gonna get to go into part discussion maybe just on guidelines level but about how you use it for being being reasonably efficient so Agnes ed what I wanted to say I call Perkins from Islamic so I mean we have the errand cat CC feedback draft which currently just works through this analysis or an earlier version of this analysis you in some ways I think it would probably make sense to put the discussion about how you tune a TCP and how you use the the packets format into that draft revelon into the draft that just documents the performance especially since that\u0027s when I run cat and therefore more like draft and therefore more like the end up as an EBT draft I think that makes complete sense to me I mean this and that graph is supposed to be having an requirements or like behavior of RM cat feedback so that makes me the one pushback I might make on that is the promise it\u0027s not how much you tune it as because the algorithm and the feedback happened at the tops at endpoints this if an algorithm needs something specific about the feedback it\u0027s going to need to be negotiated so and I think that might need to be in the deep draft that\u0027s gonna specify it whatever you know signaling negotiation you have to do along with this message I would imagine : programs too too far to walk to the floor my cake that my clothes will give you a little yeah so sorry Colin wagons too fast walks up to the floor Mike but from the virtue of war Mike um I I don\u0027t believe this needs any additional well I think the only additional signaling this will need will be one to say use this feedbacks I have everything else is just the existing tuning knobs we have n STP there\u0027s things like tuning the rtcp bandwidth fraction and they bring different features which are already have singling extension all right I mean I guess it\u0027s it\u0027s a little indirect just my worry and you learn some we\u0027re gonna need to say you know if I give you this artist we feedback fraction I intend you to use the whole thing and things like that so just going back in the end there\u0027s a sequence number that you report on I don\u0027t think there is a way to say that you report on fewer sequence numbers are faster anywhere in the mechanism I think you just live with whatever the other end points actually so it\u0027s the sender "
  },
  {
    "startTime": "01:00:42",
    "text": "decides to like to report on whatever it\u0027s receiving yeah those feedbacks Andrew decides whatever so then the congestion control just needs to adapt to if it\u0027s reporting on the 16 thing on the 16 frames and you want it to report on four I don\u0027t think there\u0027s much the the media sender can influence on the report sender so then my question to the chairs like shall we take a ham on light occurring on like we\u0027re done with this and we want to move forward to a bit ago for further processing of this draft or yeah we can do that I mean impression is that we have some discussion but most people seem okay with the proposal but to formalize it let\u0027s take a ham on it so if you are in the question is gonna be if you\u0027re in favor to move the document forward gravity call and see yes for the first time and then if you\u0027re against moving it forward in the second hum so first if you are in favor of moving the document forward to a Vedic Cole please come now and for the record that was a pretty good supporting ham and if you have objections for moving the document forward to a Vedic or and have concerns about it then please hum now okay and there was no hummus against so I think we\u0027ve read it okay thank you so I think that\u0027s it I think then the I think the working wall also have to decide whether the design team still will work on the some of the other thing that we discussed like guidance and all these things but I think in every decor it will be again assured as a design point of view right so with the iron country design team will again have to do some revisions and to incorporate every decor things yeah I think I mean depending on the feedback that is received yeah the design team will be the recipient of that feedback in orange sauce but how do we do and one question I\u0027m like I\u0027m really not don\u0027t know answer like how do you do I mean we keep their draft in arm cat and this have discussion a bit core or do we submitted another draft in analytical how long do I think currently this is an individual track yeah Charlie this is an individual submission so I mean we\u0027re go in whatever it\u0027s on the agenda for a boutique or on Friday and a BT Court will decide again it wants to adopt this as a milestone and you know we\u0027re obviously we can say we\u0027ll use the the fact that it\u0027s currently and draft DTR MCAT is does not stop a BT car from "
  },
  {
    "startTime": "01:03:42",
    "text": "saying this will be the basis for draft a BT car feedback message or whatever yeah if it wants to don\u0027t then we\u0027ll take that up and maybe people are okay and my my assumption as as working Creek chair is is that we will be less calling this improvements working group I would assume it\u0027s adopted yeah okay then Thanks okay thank you very much to the whole design team for all the work on the feedback and very good that that is progressing yeah say it say yeah okay then we will move on to the last point on the agenda and these discussions on the outstanding parts of our milestones so first we would like to revisit the work on the interactions between the applications and the RPP flows so some of this started quite early and there were some different document circulated also some of this before the current chairs were active and that we have a number of related draft so there is the orange cat CC codec interaction draft which expired some time ago we have the framework draft which was an is an individual draft still at the moment but that came about as a request from the working group and then an even older draft on orange cat app interaction so we would like to ask the working group how you think we should what documents do we actually need here and in what order do we need to produce them because this this discussion has been kind of going for us so I\u0027ll start with the last one because I think the last one got split into CC codec interaction and the other idea was to keep some part of the app interaction there I\u0027m not sure if it\u0027s been overtaken by events because I think two years have passed in between and like whatever recommendation RM cat had wanted to make for the app interactions probably already been done I don\u0027t know if you want to document that still in the interaction between the application and the congestion control and then the CC codec interaction was the lower bit so we had a document which was all inclusive we split it into two parts the app interaction of course because most of that guidance had to go to w3c we worked on some of that and made that happen so there\u0027s some set parameters get parameters that the application can do the question is do we still want to in like document that bit here in a simple document or just say that it\u0027s done somewhere else so you\u0027re "
  },
  {
    "startTime": "01:06:44",
    "text": "saying that the first question really is is this still useful or not yep and opinions on this I think I had I think the framework document kind of like try to and try to create a ground where the other draft can see like things are happening but in by doing that it almost explicitly or implicitly a implicitly excites like what kind of app interaction you trying to do I don\u0027t know like this it\u0027s a bit interesting if I say that like if app interaction framework document should have all these kind of things again because that was that that was where we started like in the beginning saying like well we have one document doing all the interaction thing and then we split it up but I do believe framework document has a other purpose so basically we will have three kind of experimental as I believe three kind of experimental condition control algorithm will be there and we need some sort of framework to actually see how how implementers can use those things so stream of document definitely serves another purpose but I don\u0027t know like will it be good to just say something about app interaction or CC interaction and because there are like modules like what this is that and then you talk about and the idea was like when a framework will have the modules of codec and app then app and CC a NAPA correction will come and say like this is how we interact with this frame or maybe that that\u0027s it maybe we will do work we want to work with framework document and say like how much we need to put in the other interaction documents that might be wanna prospects let\u0027s work on this frame or document and come back to their other interactions when we have a bit clearer idea and if the working group really things like discovered and framework then we can make you live in just one comment so the reason why we splitted it up is because it\u0027s really for a different audience like the framework is for somebody who wants to work on congestion control mechanisms the Kotick interaction is for somebody who wants to work on codecs and the app interaction was like for ya developers for like its turn of people so but if like the feedback is already where it\u0027s needs to be we don\u0027t necessarily have to push it to an RFC okay so now I\u0027m going back to the framework thing because I believe the framework work started and I think when it started the idea was for the framework to be adopted by the individual documents that refers the congestion control and that never "
  },
  {
    "startTime": "01:09:45",
    "text": "happened and we were last calling all the documents without actually any conformance to the framework now so in some sense the framework then actually makes claims or like besides guidelines on things that have not been adopted by the congestion control algorithms and that\u0027s an interesting question because now we have an a chicken-and-egg problem because the framework is there but no one\u0027s actually used it in that exact way so again since it\u0027s not being used it\u0027s the veracity of the claims made in the document unsubstantiated Kulemin and the reason why this didn\u0027t have missed because we didn\u0027t want to hold to work up any further right so I would as an individual contributor I would argue for if if the group decides to move more than one mechanism forward to propose standards to also take up the framework document again and make sure that those documents are lined then yeah so this clearly is an opportunity right for the next steps that we need to take if we now have three candidates if we now going to move down to we don\u0027t know the outcome of that process yet if there will be one if there will be multiple or you know how the evaluations turns out but if there will be multiple than it could clearly have a benefit to have some common terminology for the audience that we propose yeah I I think maybe I said what I think we discussed a bit in you know around authors and also with the chairs like what is the future of this framework if everybody\u0027s already going forward with the condition contract India the idea was like okay when is like it going for a standard track track it happens this document would really and the the final output of our mcat will conform with this framework so there is a point of what she\u0027ll work in with the framework I think the framework didn\u0027t get adopted last time we presented now talking as author up on the frame or one of the author of the framework document is like there was some terminal the RTP terminology that was brought by very own I guess but we we actually communicated it where and anywhere and also in the mailing lists like if anybody has any kind of idea how to solve that those issues we didn\u0027t get any like don\u0027t think we get any responses on that one so I mean from from author boundary as we are putting more or time on it and it is a outcome of what working you wanted us to do I would further think like if you want to work on it out of this "
  },
  {
    "startTime": "01:12:46",
    "text": "document in the next update perhaps we work with issues updated and then work on it so otherwise I mean I to heal to layer on as the individual and then we spend quite a lot of time on fixing all this thing and then might be no use so this is one reason we wanted to bring it up here right so that we can see what what path we think we should take because I think I mean we have an option to now say we have three candidates now is a good time to work on the framework and and see we agree on terminology and we are whether the other documents are still needed or not I mean that so the I think we need someone that find them users and otherwise we should so the first one is the most mature of it so I think we just like need to figure out what\u0027s missing from there and maybe even get a round of review they think they were it was quite ready when the 0 1 and the 0 to submit it so I think the first one should move on the the only thing I\u0027m concerned about the framework is that since it\u0027s three documents don\u0027t take the terminology are not conforming to framework I have this feeling that it might be overtaken by events again like you would have a framework document which probably defines everything in a nice way and and then maybe the burden is for the new algorithms that come that may be proposed later on for four experimental they might that might be guidance to them to use the framework document because so that I feel there\u0027s work for framework to be done I\u0027m just afraid that we will get this thing done and then maybe be overtaken by events and no one\u0027s actually going to ever use them I I guess the other question would be whether we should be doing it now or whether we should wait a while to it got more experience with the candidates and do all of this work you know in in a year or so this time maybe when when we\u0027re thinking about okay which candidate should we bring probes so the documents already that I think there\u0027s nothing stopping them stopping the authors from people like updating it to based on practical experience yeah sure I mean you know the question is did you do we do we just keep it you know bumbling along for a while and while we get some experience or do we try and finish it now that\u0027s a good question like I think the question as if it\u0027s if you\u0027re ready to finish it now then why wouldn\u0027t the the documents that are already just going out like why wouldn\u0027t they make a comment on the but I think I mean it will take some work to finish it up of course and we will need to have the agreement between the candidate algorithm authors that this makes sense for the algorithms so I "
  },
  {
    "startTime": "01:15:46",
    "text": "mean certainly it\u0027s it\u0027s not ready at this point it is work that would need to be done sure and I understand the overlap between the the authors of the proponent proposed proposed algorithms and the framework document so assuming they can\u0027t work on both of them simultaneously they probably prefer to work on the algorithms themselves and like taking them to completion but that\u0027s what they like it\u0027s an energy question it\u0027s like if these documents do not need it then who are we writing it for well I think that if we are gonna push multiple in the end if we were gonna push multiple algorithms for a proposed standard and using common set of terminology for the same things would be required right I mean now they are all experimental and means in some sense not so aligned but if we gonna push multiple documents then I think you could see a there is a value for having them consistent so that is just my concern and I just want to I said so yeah but I mean at this point we don\u0027t know the outcome of that process so I mean that could be an argument for saying that you know we keep it around we can update it but we don\u0027t put the effort on completing that now when we know more what we think will happen with the candidate algorithm somewhat to push forward we also see more what we need in terms of aligning the terminology and and the framework for the documents regarding the the codec interactions I understood that this was also held up because it should relate to the framework right we wasn\u0027t is why this work at some point at least in the discussion why that stopped that is at least then no I don\u0027t think so they were like the framework came much later I think it was the app interact like it was full app interaction app codec interaction or something like that was called before it got split into two documents all the app stuff got removed from the original document and only the CC codec was kept I actually do not have any recollection with the framework maybe I had Ken since he is the common author between the terrific and so I mean I clearly the other document is is older but the CC codec interaction has not been updated for a while and I think that was because it it was decided to have the framework document and the relationship between the two were kind of keeping the other one on hold as well I think there has been long-standing history behind these documents especially the foredeck interaction I have interaction I as far as I remember the codec interaction was was pretty gritty but then again what you said like we started to work with our mcat framework document and we would like to I mean the app interaction coding interaction doesn\u0027t meant I mean "
  },
  {
    "startTime": "01:18:48",
    "text": "they would like to interact with the framework so the idea was like we should use that common terminology and or stuff like that that was the only thing I remember actually that but I don\u0027t know like I thought like the coding interaction did in its expired because of lack of enthusiasm from the working group to work with it and that was my thinking thinking but yeah it was there was discussions like okay having that interaction and having the interaction with the framework makes a lot of sense so let\u0027s work on the framework because at that time it was the urgency from the working group to work on the framework document like let\u0027s work on it and align the other and the performance to the framework document but then they\u0027re working moved aside realized like it will be a longer process to actually finalize the framework document so we we said like for sake of progress we don\u0027t hold up the proponent algorithms just because of the ephemeral definite move on and that\u0027s why we didn\u0027t really work a bit more with framework document so this is the story in short so which is holding of what it does to me doesn\u0027t really make sense now it\u0027s like now we need actually the working group need to decide again what we\u0027re gonna do with this so it doesn\u0027t really matter who is holding up what now I mean do people in the group seat benefit benefits in finishing these three turns some of them just become you know obsolete by now I think the CC codec one was always important because it actually says what the CC and the codec would do like like are you gonna increase and decrease and that\u0027s why I think the reliance on the framework is only between those components in the framework we can of course go back and look at the framework I haven\u0027t had the look at the framework document recently so I can\u0027t really say if it touches more than one component or more than the two components that are named in the named in the title every worthwhile to check it out maybe he\u0027s ahead and I can take some time off line this week and actually come back with what we think may be the next steps for this Jan again yeah III personally don\u0027t believe there is like huge mismatch or huge it heaves need a huge amount of work to actually align the codec interaction the current framework document but the thing is like we are not working none of them that\u0027s the whole point I think that we\u0027re discussing whether it is needed and if needed shall we be again resurrecting these drafts and started working and maybe from the very beginning and I don\u0027t believe like all the three and the framework it has he it needs his want of work to just align as I was a common author I know like coding interaction and framework framework it "
  },
  {
    "startTime": "01:21:48",
    "text": "was like we we have been thinking about like this alignment from the beginning might have stopped it and that might have been the fec thing because that might be the that might have been one of the things that the Sisi codec says that when you have to up like increase the bitrate you might want to do it opportunistically or something and then it probably interacts more with some thing like an FCC thing as a map as an extra component which perhaps the framework wants to like have some ideas about right so maybe the the point we can take is that maybe is I had an eye on some other folks who are interested in this game Natalie sit down sometime this week and and come back with what we think and then maybe if there\u0027s other people who have opinions yeah I mean you know if if some of you are willing to look at these and come up with a concrete proposal for what to do going forward that\u0027s that seems reasonable mm-hmm you know obviously if there\u0027s no there\u0027s no one willing to do a working progressing these then we\u0027re not gonna go anywhere with people don\u0027t fancy fancy they need enough to do some work but if you\u0027re willing to work on them and still still feel there\u0027s a need for them then yes again maybe maybe you can ask this question like there are a couple of implementers here I\u0027d say like they\u0027re working with some sort of implementation of this they can actually say like whether this kind of document is useful for them for implementing already component so that would make quite a lot of sense to me actually to stop start working on these things so anybody implementing and this condition control from this company I can say like what they\u0027re thinking I think me a clear not an implementer but I also don\u0027t think that\u0027s very useful because people have an implementation already so just adopting it to something it\u0027s just overhead I think this document is really useful for open the floor to make it easier to develop new mechanisms confidence I also think where we really need this is this as we said earlier if we take multiple drafts to trigger standard we need to align terminology and so on so that that\u0027s where I think this is going to become more critical what I\u0027m saying again from since no one\u0027s saying that we don\u0027t need them I think we have a way forward since no one\u0027s saying we don\u0027t need every probably need it and since we think that we probably need it we probably should keep the traps and like keep working on it and and we can figure it out what the actual outcome if it\u0027s going to happen this year or or MA I don\u0027t know it March 2018 if you\u0027re looking at a more forward-looking like if this is going to "
  },
  {
    "startTime": "01:24:48",
    "text": "be guidance for picking something for proposed standard then maybe maybe you should move the dates further yeah the dates clearly can may need some adjustment but I think that the main question we had was if if this is a work that we should push forward now or if it\u0027s something that we should you know put on hold for later or what the feeling from the working group was from need but I mean if school ensured if people are willing to work on it and see that it\u0027s useful done still it\u0027s not so many people involved in the discussion so in that sense if if the ones that are interested want to have a further discussion on it and come back on the mailing list with with how you would suggest for these documents I think that would be be useful and then we can have a further discussion on it and I think for the for the implementers if you have implemented it it\u0027s true that lining would be an overhead but it could still be interesting to know if you feel that something like this would have been useful when you implemented it to kind of see because that kind of relates to to further implementations in the future right okay anyone that wants to add anything on on this topic [Music] okay then we move on to the second the discussion item we wanted to bring up in terms of how to move the work in the working group forward and this is we now have a number of candidate algorithms that we are pushing out this experimental and the next steps for those algorithms of course is to have evaluations and see if some of them are suitable to be pushed forward as for standards track and as a working group we need to kind of agree on what type of evaluations and how we should follow up on this work so that is the point that we would like to have some input from the working group on what you see there we have a number of starting points that can play a role in this of course we have the evaluation drafts and you can we can go back and and check against those at some point each of the candidate algorithms are identifying aspects that should be evaluating during the experimental stage part and of course in some sense we also need to look back at that and see how that turns out we will need some deployment experience from the algorithms and we also have the the anise and the three open source module that was discussed at the interim that could also allow experimenting in a controlled environment with visual algorithms possibly so in my opinion I think the "
  },
  {
    "startTime": "01:27:52",
    "text": "last one the industry or mcat or open source module is basically for taking things from a draft or an idea to to experimental I think most of the most of the candidate algorithms have done something similar either like in a benchmark test bed or inside ns3 or ns2 as I would definitely remove the last one because I think that\u0027s what we used for the first round of evaluation I think there are a lot of papers been published for some of these paper these things so that\u0027s also another so I think both scream nada and GCC have had some track record on that and I guess so the first draft of evaluation results is that like just pointers to to results because people have been presenting their results and some of them have been doing it and in some form of test beds and so on so forth some curious what the working group thinks the first thing is and like what would be the what would trigger to the second bullet so I think that that\u0027s the discussion we want to have I mean some of this can clearly be or could clearly be formalized evaluations using a set of criteria we\u0027ve come up with some of it could just be reports on deployments and new real world experimentation and the discussion we\u0027re trying to have is to what extent we need you\u0027ve a formalized criteria for this and what extent we say okay come back in a couple years when you\u0027ve got some experience with some documentation of that experience and then we\u0027ll have a discussion me a cool event so at this point I would really like to see some results from using it like in the wild on the internet not only in a test bed and the point about having a draft about this and publishing the draft is also to give then when everything is finalized give people some kind of additional documentation about which candidate is the right one to choose and that definitely needs also to relate to like real world experience and not only test fit and I think also one thing that would be still interesting is comparisons between the algorithms because I think a lot of the results are still being done individually for each algorithm and even if you know the test scenarios are specified there may be some differences so it\u0027s not always easy to compare those results I think there is also still room for actually comparing the three and they maybe also with the "
  },
  {
    "startTime": "01:30:52",
    "text": "common feedback format and so that is still a space right right so just getting back to the so we\u0027ve done out of the three algorithms we\u0027ve done to the varying levels of success on this and so I think the thing that I want to like push a little bit more on is like figuring out what the wild means because it might be too high a bar for some people if he like setting the bar for what we mean by while is actually an interesting point to make because because I think media said like testing it in the wild and setting the bar on what we mean by wild is like an interesting discussion I think that\u0027s more interesting then because if you set the bar too high then some some people might not be able to meet it and if you set the bar too low then we don\u0027t have a comfortable results just grabbing from the jabber room Savio Mena it would still be useful to compare the algorithms with the same simulated environment yes that test fairness 3rm cat is useful I think he meant RNs three NS they aren\u0027t cut implementation days just cuz implementation flops so we have Randall in the queue so Randall you\u0027re up I agree that having a the same environment a testament is a useful double-check but I also agree with Maria that we should definitely be trying these in the wild and Mozilla is willing is willing to help try to integrate some of these so that they can be tried in either in private builds with Firefox in the wild potentially if they\u0027re stable enough and and such we made something that\u0027s we flipped on and off dynamically in regular roles but that\u0027s that be a second step but certainly we\u0027re willing to work to let that be used for actual video calls and for this before you go away probably you can also help with like what other rights to measure this stuff right figure out that everything went right yeah you know getting it in is it\u0027s the first step and make sure it works and second step is then once you have it in you know how do you then evaluate how well it\u0027s working in the in the real world which is not necessarily a "
  },
  {
    "startTime": "01:33:52",
    "text": "particularly easy thing to do beyond simple subjective comparison perhaps something whereby yeah being it\u0027s an interesting area of discussion so yes I think I had a different color I wanted to say I think the bar for getting a document or an experimental RFC getting something something documented internet experimental I see shouldn\u0027t be too high but to get it cheaper Poston that we can actually set the bar rather higher because there is already documentation people can try and work with it and kind of moving into proposed on that is really saying kind we\u0027re sure that this is something that works well and we can recommend you to use it so what I\u0027m saying and this time I\u0027m gonna probably use call stats I hope and not only as an individual but as a company that measures things I think we can definitely help in this so if Mozilla or Firefox or any other browser would develop builds that could be put in the wild we could definitely help build an app which could measure and we have ways to do that and like of course the question of subjective quality is an interesting one and I think that we can look at as we move forward but this is something that we could really help with given that we have large-scale instrumentations of apps and so on so forth so if there would be a way to control this from the application to choose a congestion control or whatever mechanism the browser vendors can come up with we definitely be able to help devaluation okay thank you Khan Perkins from Islamic and one of the the other things I think we we need to make sure we take into account when we we do all this evaluation is making sure that the candidates have converged on feedback formats and the feedback formats are working for them which is something we haven\u0027t done any evaluation of so far we\u0027ve designed a feedback format but I don\u0027t think the candidates have done experiments using it yes I would like to thank a modular and Hollister Deo for saying that they\u0027re willing to help and and for your information we have for a scream I\u0027m talking now about this room we have open source code so it would be I think very interesting to see like if somebody in triggers is that one introduced that one and try it in a while and yes I would like to second what Colleen said maybe "
  },
  {
    "startTime": "01:36:52",
    "text": "the implementation knit I think not maybe you need to use the on the feedback form I do we\u0027re trying to see to see like if that is also working but yeah I mean we\u0027re talking about setting and setting bar for from experimental to standard we\u0027re talking about setting some high bar or low bar or mediocre bar to do that I think getting from an experiment to standard is a biggest type I think we should really be careful about it but I don\u0027t think likes wanna meet up and was setting bar we\u0027re talking number of user and all this thing is just like we would like to see some uses in the wild and say like it\u0027s working it\u0027s not making the whole you turn it down or I mean III heard like we\u0027re setting some bar but I\u0027m not her I\u0027m not hearing the discussion of like how what are the things yeah um I\u0027m I don\u0027t think that\u0027s a discussion Mia could have in the discussion of setting a bar issues for right now we\u0027re probably good to see some first results and then rather have a discussion about what\u0027s the right metrics to track and then you can actually say this is ready or not ready I don\u0027t think you can make a decision on this right now yes was certainly very positive that we have a number of people that are willing to work on trying out the the algorithms and a first step is of course to collect results and experience with the algorithms in in various forms so I had another question which was so I think a few years ago we presented FEC based congestion control at the time I think the the feedback from the group was to pursue a generic path which we never actually got to like there was not enough excitement with the proponents and the proponents have moved forward so I think just as a pointer we would be submitting one more experimental draft soon for a fact-based congestion control it will perhaps not be generic I think we can always have that discussion if you want to keep it general we\u0027ll probably bring an experimental draft which we\u0027ve been working on for the last few years and so my question that relates to this is that now that we\u0027re seeing the proposed the old proponents moved their candidates forward into working group last call are we still going to have the RM cat meetings in case we bring this forward as far as I\u0027m "
  },
  {
    "startTime": "01:39:53",
    "text": "concerned if we have things to discuss we can meet thank you and I think it will also be valuable to report back on the experiences in those meetings so when people I mean both if there are new proposals coming of course that can be discussed at the meetings and also if we have experience reports from the algorithms that are deployed that would also be interesting for the working group to hear about I think and with my other hat on submit your experience report papers to the applying networking research workshop so do we have any more comments or thoughts about this part that anyone would bring up to the working group okay so then we would encourage people to bring your experiences with algorithms and if we can get real implementations and real experience that will be very helpful of course but I think otherwise we close this discussion for now if there are no other comments and if so we also close the meeting so thank you all for attending and for your input and this also help finish up and review the drafts that we still have in the pipeline so we still also have some of the old work here to finish up while we talk about the next steps so thanks oh and the blue sheets everyone signed the blue sheet otherwise it\u0027s in the back okay "
  }
]