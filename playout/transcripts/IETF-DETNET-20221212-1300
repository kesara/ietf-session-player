[
  {
    "startTime": "00:00:04",
    "text": "Come With Me Now uh yes but very low uh thankfully we most of us will have a volume that we can turn off but you're a little low yeah thank you okay okay excuse me we're gonna get started uh I'm Lou Berger I'm here with Yana schwarkas and uh our secretary Ethan said he was going to try to make it although it is really early on the west coast of the us but we do expect to see him uh thank you all for joining us at this interim um it is a formal ietf meeting so our note well applies I think everyone on the list here is pretty familiar with it or else you wouldn't have made it into the interim if you're not uh just go to the note well um um please do try to practice good mic control make sure you're muted we name you you if you're not supposed to be talking we also are going to use the queue to manage the discussion foreign biggest item here is that please joining us in the collaborative note taking uh I'll drop in the chat in just a minute the link to it our Janos may do that um but please join us to make sure that we capture uh discussion uh all the material is online from a working group status we've uh had a recent RFC published thank you all to those who worked on it it was quite a bit of work and we're close on a couple of other documents we do have one recent adoption it's helping Drive the"
  },
  {
    "startTime": "00:02:00",
    "text": "requirements for the discussion today uh we are going we did ask to rename from uh in naming the the sort of file name we removed uh large scale and put in scaling some of the discussion during the adoption and before the adoption pointed out that a number of the requirements are just about building um scalingup.net and rather than argue more about what is large scale and what is not we're going to just use the term scaling we do expect that to also um propagate down to the title of the draft at some point we have one uh liaison when there's a draft response that has been discussed on the list um there's been very little discussion but there that discussion did result in a change uh in Reading over the the text um and I proposed the most recent change I think it addresses uh both my comment and the comment of the other comment that was raised we do ask that everyone take a look at the response and see if you have any um additional comments without any comments we'll we expect to send this on the 16th we'll actually make sure to say the same thing on the list River pity packed agenda we are going to stick to the two hours I'm going to keep going uh so what are we doing excuse me so why are we here and what what's the focus of the discussion um well we do have the enhanced debt net data playing requirements document that's what came out from the um that's what's in our chartered Milestone uh and you know we're the scaling document we think fits that bill um on this session we're going to really focus in on uh queuing addressing queuing related requirements that are in that document that document has lots of"
  },
  {
    "startTime": "00:04:01",
    "text": "different things in it but we're really just going to focus on on one aspect of it queuing um and any type of uh changes on wire changes that are necessary to support the queuing we've had a couple of proposals on that and we expect that to be the scope of uh of this discussion um there are a lot of good individual drafts we need to choose how many proceed are they combined are they maintained separately or some things more interest some are less you know that's the normal working group processing so we're going to do that normal processing but also to help move things along we expect to form a design team early next year the uh the charter has not been written yet we expect this this group I'm sorry this meeting to help inform an offline Charter proposal that we'll be working and once that we have a proposed Charter that will be sent to the list as well as a call for participants we'll open it at that time and again this will be in the beginning of the next year um thankfully David black who has been acting as uh tsve advisor for the group for a long time has agreed to act as design team lead we think that's super appropriate because his working group has done much of the queuing work within the ietf so we're really appreciative and lucky to have him and with that we are I think ready to move to the next uh the next presentation so first up is paying if you want to join and we're going to hand off um uh cue control side control to you and see how that works"
  },
  {
    "startTime": "00:06:00",
    "text": "if it doesn't we can pull it back so Pang you should be able to click next on the bottom of your underneath the slides you should be able to control them and control the advancing yeah I think it's yeah all right great um yeah perfect perfect thank you so much and um if we don't have any questions on that first lot we'll hand off to Ping okay please proceed thank you okay thank you um and it's the requirements of large-scale determinist network and for the name maybe the security is a better choice and we can see more comments from the quarters because uh before that we don't really know the the changes of it so my person personally uh will is that it is accept acceptable yeah so uh first thanks for all of the people who review and support this draft it was adopted and got so many good comments the latest version was submitted always only the data and the file name changed and the courses has been addressing the outstanding comments um so this presentation is to look through the motivation structure and comments from uh that has balas Kiran and the two and so uh the motivations aiming at the latke and in fact we have changed in it to address heterogeneous characteristic with long hopes large Pearl hope time variation great number of flows and multiple demands without the same cells and this document describes the technical requirements and the"
  },
  {
    "startTime": "00:08:00",
    "text": "data plan requirement yeah so um this document has absorbed some requirements of assumes large secure enhancement and but this draft I mean I mean the the draft option also updated and into the next version so some content of it is changed and um she also also proposed some new points we are also working on this on this work here wow so let's have a recap of the key attributes of the large scale then that we Define the large scale the network as the network that requires and then a solutions for typical one or more of the following attributes including the clock synchronization or no clock synchronization in different domains the combination of short and long distance hops of the end-to-end path and the virus transmission rate and the large number of flows with different levels crossing the multi-domains and more common scenarios of topology change and the videos of Link and this mechanisms used to ensure the boundary latency maybe multiple or have different configuration or parameters in multi-domains so we don't we see that such demands are normally with a single administrative control Network or multiple collaborating administrative network with within a closed group of the administrative control and the overall technical requirements are closed closely aligned with"
  },
  {
    "startTime": "00:10:01",
    "text": "attributes it is a latest submitted version including to support the timer synchronously large large single hope propagation latency higher link speed massive traffic traffic flows videos of links and on those and topology change and the enhancement of killing mechanisms related to the configuration of multiple killing mechanisms and switch over causing body demand and following balance comments we add a new requirement so we can find them in the next slide Maybe and here that data plan enhancement requirements to support the aggregate head flow identification matter information used by functions ensuring the data music latency returns in related files explicate past selection some of them is to show the requirement for the alphabetic to support the enhancement of the functions hmm and here's the summary of the outstanding comments first one adding the explicit requirement regarding flow fluctuation and so second is to some scenarios attributes are referred to the heterogeneity and not necessarily the skill itself and it is better to discuss how current Internet solution doesn't support large-scale scenarios and whether this is still in single demand or multiple demands and clarify using C graph is not limited to two queues and it refers to the article in section 3.52"
  },
  {
    "startTime": "00:12:01",
    "text": "and some phrase that are marketing and does not prepared for the of the publication and the considering ads time of public publication when stating current numbers as they are likely to be outdated at some point and crack the mirror errors in section six and now most of them has been addressed in a proposed new version of the draft but it has not been posed to the Middle East most because of the third one because there are some texts in ASU comment sub section to see what caused by the large-scale damage and the single or multi-demand details however the authors think about maybe some paragraphs to brief to see before the comment section will help people to understand more about it so we haven't decided this one and once we address this we will send the new proposed version to the mailing list foreign to address the comments from adding explicate requirements regarding flow fluctuation we propose to add a new requirements after request for a common dates flow fluctuation in aggregation because more kind of traffic flows will cause more Dynamic join or leaving of the flows which will further cause more flow of fluctuation as well as more brake stability of the Daniel flows for instance there will be more aggregation nodes which receives flows from more Upstream nodes which adds undeterministic delay of the package"
  },
  {
    "startTime": "00:14:01",
    "text": "treatment once one of the node makes the mirror Minor error of the package element it will has and begin blood effect of the downstream nodes moreover considering the node and the link failure are more common in large scale Network which requires dynamically traffic during in another alternate path it will also is a easily caused flow blockages fluctuation and the besides it is almost impossible to identify the individual IV Flows at the download plan because of the large overheads and the results for the vision for a massive number of flows so um the with cards and value range used in the identification may have to change in order to ensure the aggregated flow has a dynastic characteristics and it is proposed the change of the requirement and for the others comments to be addressed we write them in the new version of the um of the of the draft so um The Next Step the proposed updated version will be sent to the mailing list and I hope to get more comments and we'll find together of it so thanks and Link comments David please uh sure one one small comment in adding the new requirement on accommodate flow"
  },
  {
    "startTime": "00:16:01",
    "text": "fluctuation please distinguish the concept of Toleration where the mechanisms continue to work in the face of flow fluctuation from recovery or restore where the mechanisms are expected to undo the damage that has been done by fluctuation restoring uh some sort of correct scheduling of packets yes thank you uh in fact we just got comments with a few words and we will really work more about the flow fluctuation of it thank you foreign please go ahead uh hello can you hear me okay thank you as Tom mentioned the Gap analysis of date plane was submitted uh hope that will help to make a clarification for the scaling networks and until this uh displaying The Wider current that letter should be enhanced and so we could continue to discuss later of nine to say which can be added in requirements draft thank you foreign to hear more comments from the WT"
  },
  {
    "startTime": "00:18:01",
    "text": "Greg yes thank you um I wonder do you see that it's possible uh to quantify um what uh can be seen as a large uh latency so um one of the requirements I believe refers to uh tolerate large latency so uh you mean tolerate uh latency for example how how um how large the latest thing can be tolerate um I I think that probably uh it might be beneficial at least to explore uh if uh some bonds can uh can be set to the requirements and to latency that can be tolerated by queuing because I believe that certain applications they do have some requirements and uh it's probably not practical to have to try to solve the problem for unbounded latency well again um do you have anything you want to add uh when you're done responding just ask David to go uh hey okay so I think the request is just to in the document describe quantify um what you mean by uh large latency I would suggest um think about it in terms of round trip"
  },
  {
    "startTime": "00:20:00",
    "text": "time right yeah so at least think of some bones thank you uh what David and I was going to briefly add Jitter to Greg's comment as unbounded Jitter is even more difficult than the unbounded latency I think this falls into the new uh requirement area about uh account about uh accommodating flow fluctuation uh thank you with simple examples in terms of you know this industrial wants to have Jitter and latency below I don't know one millisecond and wide area applications I don't know eight millisecond or so um improved by example first and then see how further on because otherwise it's not quite clear what we would want to write porless I think I I heard the first comment and I'm going to invite Greg to come back if he wants but what I heard from Greg was he was talking about link latencies what I think you just said was application latency okay so I might have misunderstood I might have misunderstood uh Greg and Andrew rejoin the queue if you'd like but I I heard uh like right yes uh thank you um uh no I was not about application it was about their uh propagation so the Communication Service um so the distance also suggested uh later then uh it could be looked together latency and Jitter um thanks Carlos are you still in queue nope okay great"
  },
  {
    "startTime": "00:22:07",
    "text": "okay I think we've uh uh reached the end of the queue so we're gonna move to the next I believe it's true song and you two should have slide control uh hi can you hear me well um three percent we hear you yeah oh great example yeah I knew that it's a little slow it's okay it's okay uh this is Jason and I will give some introduction about our work on your hands then data plan requirement um this page is for some background about this work um in Ikea 104 and 105 we collect some suggestions by working group to split the document of 10 and enhance the data plan into two independent documented because the contents contain two parts the first one is a plan requirement and the second one is solution so I I just also see some comments in the chat box to ask whether these are the same topic actually there are two separate Topics in this uh committing because we we are trying to separate these the documents into two parts and my presentation is for the first part um first I I'm trying to provide a rough understanding of what is enhanced than that from my perspective it's just um give some thoughts"
  },
  {
    "startTime": "00:24:01",
    "text": "um in that data plan framework for wording sub layer is supposed to provide resource allocation and explicit routes and before we Charter it is supposed to reuse the existing ietf and the attribute mechanisms of to provide boundary latency and zero congestion loss in forwarding sub layer there is a good summary in RC 9320 which has just been published in the document it gives a timing model to compute end-to-end latency and a bond different and the bonds of different queuing mechanisms there are a lot of common queuing mechanisms that have been discussed in the document including firm prescription time aware shaper credit based shaper secondly queuing and forwarding which are well defined in attribute work and um after the working group has discussed about the Rich Hunter and we are reading A New Concept of the net emails understanding uh enhance them that will Define new layer 3 mechanisms for resource allocation for boundary latency and zero congestion loss in order to satisfy new requirements for example the requirements has already been discussed by pong in previous presentation and which will give some modification or update or new mechanisms based on existing work so if we are on the same page about what is enhancer.net maybe we are ready to discuss what is the requirement of enhanced data data plan um as defined in RC uh 93 23 mechanisms are requested to provide money latency and zero congestion loss the first one"
  },
  {
    "startTime": "00:26:00",
    "text": "is resource allocation which is supposed to configure and allocate Network resource for the the Daniel flows exclusively and this part I think is supposed to be covered in the net controller data plan and also the encapsulation to identify the resources to be used by a given the net packet in the data plan and which is uh also very important is to provide queuing shaping and scheduling mechanisms which is the the detail out behavior of allocation resources it determines the transmission queue selection and the the hardware in the device considering that as I have mentioned the resource allocation is for the controller plan part and the queuing mechanisms will be a lot of different variations and a lot of possibilities for different use case and the scenarios so in this document we will pay more attention to uh to encapsulation to answer the question what type of metadata will be requested for the enhanced Dana data plan um basically we we give three kind of cases the first case is that no new metadata is requested because they're in our previous work data flow ID has already been defined and that is could be used by both sub forwarding sub layer and service sub layer so it can also be used if the the enhance the then that mechanisms is flow based also some existing capsulation in ITF could be reused for example the sap so in that case we need no no more new metadata"
  },
  {
    "startTime": "00:28:04",
    "text": "and there are also two other cases um the first one is we can explore metadata for enhance the data that could use the by different QE mechanisms or we could Define different metadata for different query mechanisms we think the working group may prefer the first one because on the the the other option will bring difficulty for interoption when we introduce different kind of mechanisms queuing mechanisms especially there are a lot of different query mechanisms that are being discussed in working group and there also a potential new mechanisms that will be discussed in ITF uh So based on this goal we try to design the the enhancement data plan encapsulation uh based on RFC um oh I I noticed Janos is on the Queue do you want to ask now or after I finish the presentation you seem to be going now into solution and the previous slide or up to this point was about requirements and now we have a working of the document recently adopted about the requirements so it would be really great to incorporate the requirements and have Consolidated working group document so it would be good if you could discuss it quarters on the list on how to include the requirements but oh yes I think that is um a very good question actually uh this I have also asked in it woman five uh what is the relationship of the requirement of enhanced death net and uh the requirement of large-scale uh"
  },
  {
    "startTime": "00:30:01",
    "text": "large-scale deterministic networking so um yes if we think the the working group believe the the uh the the right method is to combine the all these documents we can we can try to just work on the the only document that has already been adopted and the working group think may we need another document for example for the a design team that has been mentioned by Lou I think we are also happy to contribute on a new one it's it both work for us it's like new newer requirements I would say for that that we can call it scaling and runs and so on let's see what David wants to say sure they will go ahead please here we go Lou did you want to jump in or do you want me to go ahead not knowing what you're going to say sure I'll jump in it's really brief um so we have a a working group document now that covers enhanced data plane it's called scaling right now if uh if you if anyone has any additional requirements they'd like to see brought into the working group please just work with the uh work with the the the the working group the authors the mail list put put your suggestions on list you have a document here it's great work with the authors and come up with a proposed um Edition discuss it on the list and once it's agreed to on the list we can bring it into the working group uh document the as always authors have a lot of flexibility on uh authors of"
  },
  {
    "startTime": "00:32:00",
    "text": "working group documents have a lot of flexibility whether they want to incorporate text and then bring it to the working group or discuss it on the list first and then publish it there's a lot of flexibility there so uh she's talking I think you have a a open door to bring your requirements into the the working group and into the existing working okay sure okay so I was uh I I was hoping Lou would say something like that I'm going to set second lose remark which is there is an adopted uh requirements document for uh for scaling uh I would expect the design team to work from a revised version of that of that document is is early version of revision expected and another the other comment I wanted to make is um the lower left post to the side we use existing encapsulation field like dscp that is something I'd like to see working group discussion on whether dscp should be used for detnet or whether net ought to Overlay existing uses of dscp of uh for other purposes this really gets into how does detnet relate to the to the diff serve architecture and that that that that will need to be a working group decision and I I'd like to jump in here before before your response your song you have another slot where you were for talking about um solution I think this spot if you stick to requirements and then move to Solution on your your second slot no that would be great and uh yes any type of metadata any type of mechanisms those are all about Solutions thank you uh Yes actually um we are very willing to combine the all these requirement considerations into the existing working group document"
  },
  {
    "startTime": "00:34:01",
    "text": "actually I'm also the co-author of the working group document uh the only only uh reason why we reach this topic again as a separated top as a separate separated presentation is only that uh do we think the enhance the that's not equals the uh scaling than that if we think it's um it's the same concept I think it the there is no uh no reason to to to have another work if we think there is still some difference maybe we should discuss how to define the enhanced then at the end the scaling tenant that is my confusion here actually foreign I think from a chair perspective let's try to have one requirements document and if we end up having too much in there or think that it's really two separate uh discussions we can separate it out but right now at least from my side and I think as Jana said earlier from his side and maybe even from David's side it seems like these are all overlapping and so let's put it in one document and if we need to separate out later we can do that keep in mind just because it's one requirements document doesn't mean that it's one solution document uh I am going to uh not pronounce uh the person's name in Q correctly uh I apologize for that but uh yeah thanks hello so I as one of the course so Clauses after uh the the the the recently adopted requirement requirement document I I would be happy to take this uh relevant part into discussion and to be included to be combined into that document I think that"
  },
  {
    "startTime": "00:36:00",
    "text": "would be a as one of the contributor of the working group I think that would be a great choice I want to clarify a little bit why it is uh scaling related at the first uh Point that's because we are kind of think the scaling actually the death net is already a very good solution right now but the scaling is we think is one of the root cause that we probably need some enhancements to that so uh in that case the enhance the data plane is part of the considerations in addition maybe to the control playing so if you are if the causes of this document thinks there could be some other root causes other than the scaling I think we can have discussion offline and to see whether um what's the appropriate text can be included there thanks yeah actually uh I think behalf of the co-orders we are totally fine to uh to combine the contents into the existing working group document and the reason why we we make it separately is because it is suggested by working group to to separate this from the solution part but I think we are have the we are on the same page about um we will work on the same requirement document first and to combine this part into the existing uh scaling that net document and if we are coming out some other points rather than getting we can work for other extra document or extra work I think we are on the same page that's not great thank you"
  },
  {
    "startTime": "00:38:00",
    "text": "all right thanks I have a very very clear comments uh if we change the large scale to scaling I think it brings more chance for us to work for more requirements unless it will be open for for the future update yeah great thank you uh is it okay to it seems like the rest of this presentation is moving on to Solutions you have another 10 minutes to talk about Solutions is it all right if we table the discussion here move on to uh tourless and then come back to you is that okay oh sure maybe I can give a very quick to to go through all the slides because these slides will not be mentioned in the solution part it's it will be very quick maybe one minute yeah yeah this page is for um how to design the enhanced data data plan we we are trying to introduce them the upon the latency information which could be uh divided into two type one is for resource one is for requirement which aligns with the the definition ERC 86 and 55 and also for all these designs for different types of of the query mechanisms that will be discussed in the following sessions and this is some proposal which I think also have already been uh discussed uh which maybe we need we need a design team and we are very happy to see that will happen in the next year and happy to have David deleted discussion we are willing to contribute yes that's all"
  },
  {
    "startTime": "00:40:01",
    "text": "okay thank you um so this is um a little bit of food for thought given how um you know the the past git itfs we had several really interesting uh queuing mechanisms that um seem to require uh new data to be carried with packets and um how that is I think a great thing that we would want to have but how do we do it so next slide let me hit okay I can do it right so um I guess yeah through the design team we'll get to the overview comparison and how to move forward with them um but I think what I'm worried about is that we cannot afford separate header for each of the options right so um we we have for example in the IPv6 forwarding plane uh just you know one uh routing header that we can have um and we would also want to have of course pre-off be combined with the queuing mechanisms um and if every mechanism starts with its own header then then we have a problem with that so now the process I guess the way I predicted is that we can perfectly well Define in-depth net the functionality that we want um but when it comes to the encapsulation packetization of anything we want to have new in the package then we'll need to work with and give up control most likely to mpls six men or a beer working group if it's for you know a multicast so there are working groups that are doing packet headers for their realm and um that basically means that we also need to explain to them or what the packet header needs to do and so I was starting to reverse engineer from the packet header thinking as to what we can deliver for that and so I thought that maybe if we write a draft that is the information model that describes the elements that we want to have in in a packet for for different mechanisms um without finalizing how they would actually get packetized in the different forwarding plane mpls IPv6 multicast or"
  },
  {
    "startTime": "00:42:01",
    "text": "so on then we would have exactly that piece of work that um connects what we get out of that net from what we need from these other working groups and so here is a kind of example rundown which kind of is what I would then drive up for next year to write into a draft and so was it just they're trying to give a quick rundown of this to give an idea of what we might want to end up in in such a document right so obviously for the non-latency things we already know pre-off we have a sequence number we have a flow identifier we have right now an mpls header in which it is is that good enough can we use it in the other forwarding planes but maybe most easily just at least you know re-specify exactly what the attributes are of sequence number and flow ID that we want to have in that net package so that we can perform our pre-off um then um an example of a queuing mechanism to reduce end to end a Jitter is that we really just have a play out function right so that may be another you know even draft to write out but we haven't done this so far if we use existing queuing mechanisms um that are available we'll have um pretty much unbounded uh Jitter and to reduce that you could even do that in the last top router or receiver device by knowing the um timestamp when the packet was sent or when it needs to be played out right so that's kind of a Stein timestamp information field and that would be only processed on the last dead net hop um then when we go to um hop by hop so um there are things that we can use like the um uh experience delay on a perhap basis um and that would basically be updated by every router and that would allow the receiver to also perform um layout buffering but without having"
  },
  {
    "startTime": "00:44:02",
    "text": "um a network-wide clock right so that is then different in so far is that it gets updated on every router hop um changed there is something which is a novel and would be similar to some other Advanced queuing mechanisms um so this is also for a mechanism that hasn't been written as a draft right so I was not having the time to get through all the drafts and look at their information elements but here obviously um uh the delay time on a per hop basis that is what the damper function would require so there is one damper draft out already there may be more damper drafts coming um and um then obviously for tcqf which you have heard me talking about repeatedly um that would be a cycle number um both of these would also be Rewritten on every hop um and um then um priority um to to distinguish kind of the latency on a perhap basis that's been used in a couple of queuing mechanisms um and that actually is I think the the most difficult one if if we're getting into uh this one because we already need to have some steering header um in in that net if we don't want to do that on a per flow basis um and those steering headers like an srmpls srv6 or prte would need to be you know able to support such priority element um so there are even been proposals to have perhap deadlines like in jakov Stein's proposal which is currently expired but which to me would be the the hardest one to figure out how to do a packet header for um because it would need a much longer per hop information element uh that that is a deadline as opposed to priority which is just four bits which would fit perfectly fine into let's say srv6"
  },
  {
    "startTime": "00:46:03",
    "text": "so with all these type of information elements right it sounds like overwhelming but I think when we start to think reusing the basic idea of the dscp a code point which is not fixed semantic but which is a configured semantic right we can start thinking about having an information model that says well in the packet we don't really want to have explicit semantically hard coded information elements but we want to have code points that are covering the semantic that is configured on a per class basis for example and that concept actually already exists in srv6 SRH headers um and of course we are all aware of how this works in dscp where you're configuring red Pi scheduling queuing parameters for each dscp to achieve the desired semantic of the dscp so here's basically for everybody who is writing one of those um queuing drafts as we call them right now to think about what kind of the you know maybe shareable set of information elements in packet headers that would be sufficient for you right and so I was really thinking if we get a way for the queuing mechanisms with 64 bits which could be subdivided in two times 32 bits read writeable and then we would also have only the sequence number and the flow identifier in 32 bits then you know that would get us to a point where we have 128 bits for dead net and that is as much as an IPv6 address so that would be well feasible for the amount of overhead that you know networks are happy or not happy to accept um and might give an idea of how we could converging right so that was basically it and this is far out right but maybe start to think about what you do in your queuing drafts as in information elements and then maybe we can start thinking about merging them"
  },
  {
    "startTime": "00:48:01",
    "text": "and thinking about how we can have emerged encoding for them thanks uh Carlos you have David thank you feel free to um sort of run the queue during your discussion time go ahead David hey Carlos um so I like what like I like a lot of what I see here though I would suggest that you've got you're trying to do two things one of which is one of which is incredibly useful now and the other one of which we're going to be arguing about for a while to come the one that's really useful now is the taxonomy of all the things that could be carried between nodes or uh that taxonomy trying to divide things up help structure the sort of what are we what are we going to choose to do I have a feeling that the encoding is going to be a very long discussion um and first of all figuring out what we really want to carry probably comes before how uh uh we go about encoding in how many and how many bits to uh to uh to spend on what so if yeah I completely agree and I was only giving um the encoding stuff and I wouldn't want to have that encoding unless we really agree that you know we have an information model and we did Define in the taxonomy you know these derived information Elements which are code points are how we want to call them right at a later stage but you're right I would like to start with this semantic information elements that are required uh by by the different queuing mechanisms and then be as explicit as knowing okay they're rewrite uh in the beginning or on every hop on the receiver and they need to have the following number of bits for the following size of network so and that's painful enough work that I would very much hope that we agree that more of the"
  },
  {
    "startTime": "00:50:00",
    "text": "queuing document authors could help together um collecting that information okay thanks yeah sorry please go ahead folks well actually can't hear you uh actually I I want to provide some consideration about David's question because as I have mentioned we are also trying to introduce some metadata for the for different kind of queuing mechanisms so that is also a question we have considered that is why we we Define two types of boundary latency information one of them is requirement one of them is required ID because for the requirements if we consider boundary latency it will be a Time start a timestamp which limits the the how many bits that will be requested and it is a resource ID it can be an integer depends on the query mechanisms so that so that's why we do the classification and we think that will be uh may could be considered if we try to bring or design metadata when we Face different kind kind of chemical mechanisms"
  },
  {
    "startTime": "00:52:02",
    "text": "so I I think because we have quite a number of queuing mechanisms being proposed here so talking about the metadata it would be great that and if we can kind of identify what are the common parts or so-called fixed fields uh what are those probably is already being uh showed somewhere but I I didn't know uh and what other uh mechanism each each of the mechanism specific information so that part would be more like extensible so um the fixed parts and external extensible parts for the metadata um there are two types and also I want to Second the opinion that metadata the formatting is is important but before that we still need to make sure we reach the group reach some certain level of consensus that the the the the mechanism I mean the the query mechanism is to be useful here thanks yeah just a quick comment from uh from me I I completely agree the the only thing I always have in the back of my mind is that we do have limits on how much we can carry in packets um the more we want the more pushback we'll get and so I I keep that in the back of my mind when looking about different proposals that we have how much would they need in the packet header how difficult would it be and I think that's not too unprudent if we uh we keep that in mind foreign so uh as I also added to the chat window so I think simplification is a very good direction so I I also would like to have a solution that we are limiting the"
  },
  {
    "startTime": "00:54:00",
    "text": "information we are adding to that net packets regarding the Bandit latency whether it is only one or two parameter or just a class information I think this is up to discussion but uh I definitely intend to to limit the volume of information we are adding to that net packet so I think that that's a good direction okay it seems that we are ready to move on to the next slot I'll ask you to please respect the time the last uh slot on this uh on this draft went over quite a bit uh so please uh stick to 10 minutes including your question q a Time thank you go ahead hello can you hear me uh yes you sound good and you should have the controls so please proceed uh yeah yeah from Huawei I will introduce the data Network display of minimum and maximum and to understand the internet plan does not use metadata flow ID to identify the detonate floor but didn't Define the metadata to guarantee the network and lendency that there are other requirements of that standard display in draft requirements for that scale date matter including you place the inclusion of the mixed data used for traffic treatment capability to different underlying Network technique Technologies to to meet the requirement of full boundaries there have been several mechanisms proposed to that net the list"
  },
  {
    "startTime": "00:56:02",
    "text": "on the left is the exciting mechanism and the right is the draft information that is currently in that plan used to facilitate that net transit in order to guarantee the bonded entity as shown in this figure bonded let's say is for information is transmitted across multiple detonate translate node and used by that title follow your supplier so this page discusses the design principles firstly facing the body language information should be carried in data plan to facilitate.net flow to map user forwarding on the cellular cellular resources not focus on the local mechanisms secondly basically is a good is good to have a uniform formative to accommodate a various other scheduling mechanisms secondly the class of the classified as a mechanism mentioned in the previous page into two categories why is requirement which summarizes the requirements from a Detonator service on the map to the resource uh another is resource which indicated the resources in the entropriate data field we use the VR type to indicate the type of boundedness information and the BR format is used to indicate the format of a boundary elements information for the field of flag currently we we don't have any definition for it"
  },
  {
    "startTime": "00:58:02",
    "text": "for future study if there are any mechanism or algorithm accepted by that and boundless information is different with insertable editor so falter rpv6 based.net that player will Define the new IPv6 extension Hardware operation called the Bri operation operation jspr Operation can be encompleted in Azure IPv6 hope or hope I hope audio is extended harder depending on the processing Behavior and there may be more than one pound difference information can appear in about VR opinion uh same uh similar to activate6 based on that United player for the rpr space data plan we Define a new Mac obvious retention Hardware called API extension header the processing behavior of rprs is very similar to IPv6 so the PRI can be also in capital selected in rpis over UDP or IP based Detonator display uh for the use of photos of understanding we partied some examples to give instruction on how boundaries information is used in different algorithms Australian fish figure below this is the comments from itf1 cell for the first Common Warehouse supported separates the requirements under solutions under principles on this media"
  },
  {
    "startTime": "01:00:02",
    "text": "for the second comment which Queen mechanism is 2 it depends on the Node for example the timer resource ID can be a computer present or second mechanisms is used on when that's not node if there are several several clean mechanisms supported by one node the markings scheme depends on the Node local mechanism foreign so any comments or suggestions are welcome thank you Daniel please go ahead for you I'm I'm a little confused can you hear me yes please go ahead yeah okay okay I'm a little confused about the uh suggestion from this uh draft that um and end to the end to the delay budget that will be encapsulated in the data plane and I I don't understand why it is necessary for the trending load to level exactly what is the end-to-end delay budget to use it as the as the parameter to to to to relay and and forward the packing and the flow because the end of the end um service requirements the parameters it could be uh it could be it could be utilized and incorporated into the and the policies through the control plane so um I I do not think it's necessary to put this kind of information as metadata"
  },
  {
    "startTime": "01:02:00",
    "text": "into the data plane.net thank you uh secure uh for the another to another delay is your spell for uh for example if one of the uh use two uh too much too much time to forward this package under the uh follow next node will will try to use a little time or forwarding it first of all we we carried that under 200 delay in the data plan uh for uh to buy buy cooperator with different nodes in their in their own forwarding time they can uh foreign value uh yeah I I can provide some uh additional information for Daniel's question uh actually for the end-to-end delay budget it is um raised by some of the mechanisms that has already been discussed in rwe ITF or some papers um the consideration of how to use"
  },
  {
    "startTime": "01:04:02",
    "text": "end-to-end uh delay budget you know Transit node is open to different type of mechanisms but the basic idea is that if we have um uh end to end did they budget and when we Transit through a packet a Transit through a node the the the latency that has already been expensed in this node can be um can be minused and the the end-to-end delay budget will be reduced to the left delay budget and which could be used to direct the packet forward in the next node that is the one possible method to how to use this parameter the most important thing is that our proposal is now to carry some specified or some particular parameters in the encapsulation the the motivation is similar as a tourist presentation what we are trying to do is just to find some common metadata that could be used for different mechanisms and we are also agree about the balance suggestion we we should try to simplify the the metadata we are trying to carry and the metadata can be for be further discussed by working group foreign some of the proposals with the deadlines um I haven't seen and I don't think even jakov or so was claiming that they are deterministic latency calculi for them right so they're basically based on heuristics and I think that's another uh point we need to discuss whether we want to include them in our mechanisms um it may be useful to to share packet headers but it may not be called deterministic"
  },
  {
    "startTime": "01:06:01",
    "text": "so that that was what came to mind okay let me go on to the slides here so this was just a quick follow-up given that the nodes from itf1415 said we ran out of time and wanted to continue the discussion um I actually didn't want to repeat anything much except that I felt there was good feedback and people like the benefits of the tcqf mechanism and I think the main distinguishing aspect is that it is the only queuing mechanism that gives us a low Jitter without the need for new packet headers because its metadata can fit into the EXP and the IP dscp if I'm wrong or not I'd like to be reminded what competing mechanisms we have that fulfill these requirements of no packet headers um and of course that is also meant to work for a high-speed large-scale networks all those good things I had a great talk with David black afterwards so there is some to do for me um with respect to trying to generalize the pseudocode that I use as one form of defining how the mechanism works insofar is that it's too much really doing queuing NQ and DQ and he felt and that's certainly true that it is more constraining than it needs to be so that we can have interval implementations that use whatever internal mechanisms they want I actually also just send an email to the mailing list about should we call these things queuing as opposed to scheduling maybe that's the better term to use and and then I'll try to figure out what to do in the pseudo code to achieve that um what what I actually wanted to to to maybe ask um for all of us in in our documents is right what what should a queuing document have right because I was trying to figure out what it is and that's what um the tcqf document has um configuration data model A textual description of perhap processing and the different Ingress and egress processing"
  },
  {
    "startTime": "01:08:01",
    "text": "as necessary description of the calculus to determine the latency um of course if that's a predetermined one like in our case I'm pointing to the uh the pre-existing predefined calculus but it's very simple of course the pseudocode then as as as a more formal method to describe that could I guess in other drafts be algorithms math formulas and then describing the scalability and performance aspects um and you know specifically for for this document of course that's right is do people feel there is anything missing um to to actually um adopt it right given how it doesn't uh require different packet headers um so what what are people missing and are not liking about the text thanks David I'm done bringing the audio to click on so so Charles um let's see with respect to use of dscp I'm having a I'm having an immediate reaction uh on the lines just because you can doesn't doesn't mean you should um I think it would be better to describe the advantage of this draft as needing a relatively small number of bits that might fit in a dscp or the working group might decide that detonet really ought to be orthogonal to diff serving hence we need to find a place to somewhere else to stick that small number of bits I don't think a decision um that says we're not we're going to make debt net orthogonal to uh uh to dscp changes in marriageless draft I'd like to see the merits characterized in terms of number of bits needed as opposed to as opposed to where to put them right yeah yeah I think in in general I agree but obviously given how I you know know how long things take to build into"
  },
  {
    "startTime": "01:10:01",
    "text": "Hardware um when it comes to different packet headers um and and the pushback we've been getting with new packet headers in other working groups for other purposes right I think in practice there is a relationship to reusing even if if it's considered to be a temporary app use of some existing packet error field yeah right as I said I would like to separate what needs to be done and here I I fully agree that that that there is a that that tcqf gets a lot of Leverage out of a relatively small number of bits from an assumption that oh we're going to have to put in the dscp is I believe much as you're correct about implementation constraints that is a fundamental architectural question um that I'm sure you'll recall in the first round in the first go around for debt net the decision was made not to use dscps for debt net you're asking the decision to be Revisited it should be Revisited as an a priori decision not a concept not a consequence of implementation constraints thanks Susan gray with uh David has mentioned I think our dsap is a good choice but we we should also consider the the Deep serve meaning of the sap whether it is really um appropriate to use it internet just the two I think that is also one point we should consider yeah I mean we do have um you know from the history some interesting attempts to leverage even within the same um flows a different dscp like you know AF 41 a42 for the guaranteed amount of traffic versus the extraneous amount of traffic for partially congestion-controlled traffic flows right so this wouldn't be the first time when we logically think it is appropriate to use more than one bscp for a flow for different purposes so I I"
  },
  {
    "startTime": "01:12:00",
    "text": "don't think I have an architectural um you know reason why why it would not be good idea to use dscp but yeah happy to have that discussion in the design team there all right so I apologize you just said that like after the business question in the design team like uh you are asking what is and within adoption I think it's available the input to the design team of the document like that I guess the design team should discuss uh develop the way forward foreign yeah I uh I I said the same thing but while muted so thank you um and can we move on to the next uh presenter please uh hello this is ejo actually I think uh guangpang has some emergencies to attend to so I will be presenting on behalf of him ah thank you so much I was wondering what where he was and it wasn't showing up so thank you you should have control of the slides thank you let me see it okay uh this is about the a variant of cqf and and it's a data playing uh data data playing enhancements so I think this has been presented before so I will try to move quickly for the first few slides uh basically uh the first few slides is a brief introduction on the basic or fundamental cqf which using which uses two buffers and if we"
  },
  {
    "startTime": "01:14:03",
    "text": "click on it uh the buffers will rotate for each of the cycle we call the cycle time t or TC in some of the slides so it's click here they need to move the next hop then the next hop until uh going to the sink so this is a well-known feature I'm sorry well no mechanism for the called the fundamental uh two buffer cqf and uh it has a formula here let me see yeah this page so the total queuing time the minimum should be the number of hops minus one times the cycle time plus a DT then the maximum is the number of hops plus one times the cycle time TC and minus DT so the that time the DT plays a important role here um because the DT basically is used for to absorb all the types of time variance uh including the uh the the variations in the propagation delay and uh in the Pro in the processing delay and other other variations like the clock drifting something like that uh so that's the fundamental cqf does um so uh the uh we're thinking the fundamental cqf has a potentials to to be widely deployed uh because the end-to-end boundary the latency can be easily calculated uh it's just related to the number of hops and the the cycle time so uh giving given the condition that we have more and more um link bandwidth the link speed is higher so there is potential that the cycle time can be decreased uh and so it can be decreased from a few"
  },
  {
    "startTime": "01:16:02",
    "text": "hundred milliseconds to a few milliseconds so that's basically what the left hand side talks about and in that in that case let's try to revisit the that time record that that time is the time required to absorb the time variation so uh even with the decreasing of the cycle time the dead time it Still Remains uh there so basically uh the dead time will occupies a large proportion of the cycle time so that's that means that that time will eat up the cycle interval when the cycle time is more that results in the low link utilization or sometimes it becomes impractical because that that time is so large that that uh it basically is 100 or even more of the um cycle time for example when we take the cycle time to be like a 10 millisecond then the dead time is normally like a a few a few milliseconds like three or five then it would becomes impractical so uh that gives her some requirements of deploy the cqf the fundamental cqf in large that network uh so there are one two three four there are four requirements um we are trying to address the number three and number four here number three is the longer links introducing longer propagation delay and number four is the larger processing time variance variations uh because um there are large number of nodes and the node types has great diversity they're all different types of nodes so the variations um we don't know up front so that would"
  },
  {
    "startTime": "01:18:02",
    "text": "be something would be hard for the fundamental cqf because it requires a shorter cycle time T in order to achieve the lower end-to-end boundary latency at the same time it required to maintain a relatively large dead time in order to absorb the time variation and also to maintain a smaller ratio of that time over the cycle time in order to achieve better link utilization there are some existing ways actually we call it a cqf say cqf variant that to use more than two buffers so fundamentally the three buffers would work in the rotation in the rotation manner uh that is a quite straightforward way uh I'm not going to uh introduce it in great detail here it's available in the I think in the in the in in some papers uh we don't think uh the new standard would be required in order to provide how does simple secure variant would would work so in general we think it is feasible by increasing the number of buffers uh by revisiting however we will take a closer look at the cqf variant there is a Time ambiguity window problem uh if you look at lamb has signed there are green and uh and the red small boxes so basically the idea is um because there there exists the time variation in processing so the receiving window uh will swell so when the cycle time is so small the receiving window time swells"
  },
  {
    "startTime": "01:20:00",
    "text": "spending across more than one cycle then that will give the difficulty in identify which one is the right side right buffer right output buffer a particular received the package should be put into this is because the fundamental cqf use the time demarcation however the time demarcation may not work so well because uh the the the the processing time variations gives the uh gives it hard and to make the right time demarcation unless the DT is so great so that we can clearly separate the swelling receiving window time so the the way out is to make sure that the packet carry the carry the cycle ID metadata so that at the receiving part it use that cycle ID to determine which one which output buffer is the right one to be put so that's the help to help remove the time ambiguity so uh I think this one is uh some simulation result uh if we look at the left hand side the picture we have a flow generator then two routers two physical routers basically a flow generator is the both the source and the uh and the both the top talker and the sink for the flow then the Red Arrows means the best effort flows as a background and the green arrows is the high performance or the dead net flows so uh there are the the the best effort flows has much larger Buster size and basically the experiment try to show uh try to examine actually whether the large burst of the"
  },
  {
    "startTime": "01:22:01",
    "text": "best ever flows will impact the net flows so the right hand side it shows that uh the the Jitter between the sorry the children of the Dead net flows using the the cycle ID based sqaf actually is is bounded so even when the time cycle is so small it's as small as 10 milliseconds so that's basically show the uh it can work when the cycle time is more uh so here is the summary uh we are thinking the secret of attractive features and the potentials for wider deployments and the cqf Orion is straightforward layer 3 extension from the fundamental cqf uh by using more than two buffers and you and employ the explicit cycle ID so the thing is um uh last time uh there was a question that uh there are IEEE has a project 802.1 qdv which trying to enhance the cqf uh so what we look at it and also why uh this we this we still want to uh we put the secret variants in the death net this is one of the reason is that we think this um the second ID based cqf actually is a simple extension sorry simple cqf variety to the current published standard which is a fundamental SQL 2x4 3x4 cqf it doesn't require it does not require uh the major change of the current cqf like what's being proposed in the ongoing project"
  },
  {
    "startTime": "01:24:00",
    "text": "IEEE 802.1 qdv so that's the that's the reason why it was put here I think that's all they ran out of time so I'd like to take your question at the end to leave time for the others if that's okay yeah sure okay thanks um and uh next you know uh um so do I have the control okay I'll just proceed okay yeah um the title is asynchronous deterministic networking uh with the ADM we try to avoid slot operations and strict time synchronizations um oh I do have the control yeah here the the framework tries to focus on decoupling the latency guarantee problem from the G2 guarantee problem first we can guarantee the latency on end to end the basis not per hope since the E3 latency and latency bound of a packet could be less than the sum of the bounds per hop here the two solutions are introduced regulators and The Meta Meta metadata based forwarding the metadata could be the so-called Global finish time it will be introduced in the next slide and the Jitter guarantee the cheetah"
  },
  {
    "startTime": "01:26:01",
    "text": "guarantee solution can utilize the already guaranteed LED latency guaranteed Network and time stamping function and buffering okay so the latency guarantees Solutions the first stone is the ATS it is very well known uh all of all of us is very well aware I guess so I will just briefly introduce it is it is composed of the two uh two-part FIFA system and IR the key here is that the interleaved regulator does not increase the the maximum latency of a FIFA system the FIFA system usually is a combination of an output Q uh scheduler and sitting fabric up until an input Port module so it encompasses just one hub and the next solution I would like to introduce is the fair which is the flow Aggregate and IR is combination of flow aggregation and The interleaved Regulators in between so here the the whole network is divided into several aggregation domain we call at and the only at the boundary of the AED the IRS are placed the IRS are operated per flow aggregate and in a sense it is a general generalized ATS as you can see in the table in the bottom left the fair framework uh even encompasses the interserve which is which doesn't have a regulator but the scheduler is based on the flows the cues are assigned per flow"
  },
  {
    "startTime": "01:28:03",
    "text": "and the queues are served accordingly but maintaining flows maintaining Q's per flow is too complex so if you do not accept in service we do not accept into surveys our Solutions so here fair is a compromise in between it can it can have a complexity of the ATS but can be performed even better than the inter serve as you can see in this analysis um these analysis is too complex so I will just briefly go over so the basic idea is is that the network is very large they are divided into several parts called ad and the the topology and the flows Are all uh completely symmetrical and the flows are all identical in that case if we divide the network into proper ads you can have a test Affair can show the best performance in terms of the latency bound and the latent bound so the ATS Dean's serve and the fair that's the order of the the value of the latency bound and our as the the last solution with the regulator uh it is introduced the port based flow aggregate regulation it can be placed at every node or a critical links to break the cycle so the the cycle in the topology plays a very bad role that accumulates the burst and feed for the burst accumulation"
  },
  {
    "startTime": "01:30:01",
    "text": "and he can break the stability of the network so breaking the cycle is very important so in that regard the protobase fa regulation is focused on that respect so here the keys that regulate the flow aggregate itself that individual flow if you remember the IR maintains only one Q per flow aggregate but its regulation is per flow so it has to maintain the Flow State but here uh forget about the flows forget all the individual flows just maintain the flow aggregate uh parameters that is the the sum of the bursts of the older flows and some of the average rates of those flows so it has the best scalability but but it turns out that it works uh quite good um so here another topology it is also symmetrical but has uh only nine nodes the flows are yeah you you can you can take a look how the flows flows around and here the um in a single Arrow or single line there are many microflows so the single line for example the red arrow line is flow aggregate actually which has a number n microflows inside so by increasing the end we can control the utilization so in the graph on the right now you can see the end latest bound as the function of digitalization"
  },
  {
    "startTime": "01:32:02",
    "text": "here uh atheists IR performance best but the portable is following regulation also works fine yeah and uh the latency current framework with the global finish time so the theory is very complicated so I will skip all of them because I already introduced them in the 105 meeting the key here is that it carry the metadata to metadata one is the global finish time and then dhp this is the this is very hard to name but it is related with the the delay experienced by packet p in node age these two metadata can be kept and having these metadata inside and use them as Global finish time we can have very good performance so here virtual class reference is a very old classic scheduler but it is very complex in in this in the sense that it has to keep the flow state in every node but here the proper solution does not carry the Flow State inside the I mean does not maintain the Flow State in kernels it maintains only at the source node and it it carries as the metadata and using that metadata the the scheduling is performed so here the the flow is protected yeah up until here we we we have seen the solution for the latent guarantee"
  },
  {
    "startTime": "01:34:01",
    "text": "and we can also guarantee the Jitter um using the timestamping metadata it has a very good feature that the end it in perfect latencies is bounded yeah okay um thank you oh well just one more okay sorry so we have experimented with the embeddy system and even even with the clock drift between two physical embedded nodes which is uh six micro seconds per second um using a draft clock drift compensation algorithm we we could maintain that the overall Jitter is bounded by 10 microsecond thank you tested okay thank you very much we'll move over to shafu please please proceed we don't hear you your mic uh looks like you're still muted shafu please try to unmute"
  },
  {
    "startTime": "01:36:07",
    "text": "uh very well yes okay thank you oh okay uh I'm a software City and uh I will presented the the name based 14 scheme according to the next page the screen is not shared sorry we see your second slide it says motivations if you'd like we can drive yeah yeah I couldn't get the second but uh yes we see the slides changing okay yeah okay so the proposal is to fund uh attention Q mechanism suitable for largest networkers to provide the deterministic cruise as we know the money existing scheduling algorithms such as CBS ATS certificate priority and EDF but they cannot be directly used in iPhone network to achieve deterministic accused so this proposed is to discuss a values of EDF with enhance the latency conversation uh other three key attributes of dominant crew the first one is the countdown time"
  },
  {
    "startTime": "01:38:00",
    "text": "which indicates the countdown waiting for scheduling and the second is the rotation timer interval which indicates the decreasing step of countdown time and the third one is authorization table which indicates the continuous sending duration when the queue is scheduled and the the two scheduling modes and the types of queue that is in time under until uh the main technology information of the packet is the Poland residence time and the residence type of valuation uh we can't calculate the the allowable current delay based on the planet resistance time the resistance time evaluation and the 14th delay inside of the node turn up the packet to the key with appropriate countdown time range so these are examples are below I'm not described in the title The Operators can take a specific plan that resistance time as a specific class firstly we conquered the secretary condition for in time opponent info internal mode uh if if each class has a place the perfect constraint function for example the liquid bucket then we can suggest a blender lose this Time Warner for easy class always term for untimer mode we suggest a loose end time schedule into also meet the schedulability condition and take the anti-musculating and igles node uh we think that the receiving a in intermediate node is not necessary for"
  },
  {
    "startTime": "01:40:00",
    "text": "the screen for I mean in the white useless it can allocate the bundle Wise from the traffic constantly in the function of a specific class as just mentioned and at the English node take admission controller are incoming Port undertake or just treating and output Port according to the allocated bandwiders uh so there's a another example I also know the described in detail so this is partially upgrade scenaries in some cases if the traditional surgical priority a network for example with three of hops which can provide the bonded delay then it can only upgrade the support of data and several border nodes to control the delayed data of traffic uh we hope that the Border nodes can get the the information of the package for example the Legacy device ma mercy and insert the data Knight information into Power case uh even the on the support and basic security or maybe other means for deployed considerations a small plan devices time is used for low latency requirement service we can't take a loose on time scheduling under Transcendent nodes and take a triggered anti-musculating and the Eagles node"
  },
  {
    "startTime": "01:42:04",
    "text": "so this page is for encapsulations uh uh is uh that an information called in the IPv6 package which is foreign Networks uh later we we are further evaluated the art that ability of this proposal to the large-scale network deterministic requirements document uh for 34 aspects I will not described in detail so uh we welcome to any comments on the cooperation please go ahead so now I have a couple of questions the first is um could you go back I think go back go back two slides and if um please or three slides the point of considerations please um yeah this one"
  },
  {
    "startTime": "01:44:00",
    "text": "um I'm trying to understand the suggestion are you suggesting that we enter that there be two detnet Services um one of which is is effectively bounded latency and which you you which you've referred to as in time and the other being the strict uh on-time Services essentially delivers at the Pres at the precise deadline yes foreign unbounded latency we can just leave as a question as a question to the follow-up follow-up on on later and the other question I have is if we can go forward to um go uh go to the evaluation two slides forward um I don't understand the comment on deployment considerations about uh once I I do not understand the first come under cost the time secrecy she's not required between Network nodes surely you're assuming some form of global time based given your use of absolute deadline times so what exactly is the Assumption on time synchronization because the world mechanism is based on the an offset of time there is a local time and maybe we can't discuss in many stuff okay let me make this question on on the mailing list because something there's there's something that's not making some making sense about this picture"
  },
  {
    "startTime": "01:46:06",
    "text": "okay we can't make a qualification are very least thank you and with that we're going to move to the last slot the last presentation Quant ity yeah we hear you thank you thank you I'm from the tea and is the king-based enhancement um okay and that's first uh do the recap of the cuning based enhancement and first from September in 2021 the working group agreed to discuss the queuing-based topics and then in its 112 and 113 we continue to discuss the queen-based requirements and now is the scaling that led and then in April in June 2022 we submitted the killing based deep plane solution for IPv6 and mplc encapsulation and then in June 9 2022 the working group realized that new Champion the milestone for the enhanced diet so we updated the drugs to align with the terminology of the requirement and then we discussed or get analysis and the enhancement of the natural database framework and answer itf-115 and so this time this win"
  },
  {
    "startTime": "01:48:01",
    "text": "meeting we plan to discuss the um the queuing based reading enhancement uh uh for the scaling.net so first uh we just want I want to present the Gap analysis uh uh as mentioned just now the primary goes for that queues is to support the different levels over the Deathmatch queues for multiple services so uh we we must first discuss the characteristics and statistics over the data mystical networks we just defined that into two parts one is the the large-scale dynamic flows with requirements for different levels of different cues and the last one is match scale Network topology including let's not number of laws and Link links and so we try to summarize the Gap list including the function questions to provide the aggregated flows and the education in service thumbnail and the this team this deterministic agency in forwarding something for the aggregated flows it requires large amount controlling signaling to establishing the meeting that letter databling that's not performance or aggregate flows for deterministic latency it may need to support the enhancement of queuing mechanisms including the metadata is the their focused on on this at this meeting"
  },
  {
    "startTime": "01:50:05",
    "text": "so what's uh there was there a Killian uh based enhancement uh we we just provide the uh queuing Pro uh based enhancer functions and the metadata um we proposed that the packet treatment should indicate the behavioral reaction ensuring the deterministical latency after definite laws including the queen magnificence and the related metadata and first of all it is necessary to list all QE mechanism out and discuss which poem mechanisms expects to carry query information as metadata so for some queuing mechanisms the queuing based information should be carried in metadata for the correlations between knows and some other curing mechanisms don't lead to carry under a metadata for example the cycle and the deadline information should be carried in the metadata and and of further uh under um in it is supposed that the resources over the data plane should adapted the different demands of the data service so the format of the data playing enhancement must be a generic and the format must be applied to the order queuing mechanisms so and the data plane encapsulation needs to be simplified as just like mentioned and further implication implementation consideration and so we"
  },
  {
    "startTime": "01:52:01",
    "text": "Pro we proposed the necessary information and is is expected to be carried and the other information could be configured on the control plane so we proposed uh the uh IPv6 extension solution to provide the encapsulation or further information of the download flows to achieve the the end-to-end the dynamistic latency um we also agreed to define the format into two parts and the fixed part and the reflexible part the fixed part is needed to be generic and be applicable to all queuing mechanisms and particular Q information metadata is a specific for each query mechanism so the part and this part is flexible and during variable lens so we Define under um new option for the next two signal under uh deterministic latency we defined the dynamistic latency action type to indicate the actions we defined the type the direction into two types the first type is that indicates the behavioral action type over packed treatment it is related to different service requirement and under an alternative option may be reusing the ds3p field and the subtype indicates the type of functions ensuring the determinist latency including including the queuing mechanism and the related metadata"
  },
  {
    "startTime": "01:54:03",
    "text": "so uh it is the same with the uh mpls uh Mercury mpls encapsulation we also uh proposed the mpls extension solution and for the queuing based on metadata um but these extensions are aligned with the ongoing work in mpl's working group for example the SP labeled as SPL is added to indicator the deterministically latency action DOA the bsbo spltu indicates their presence of their mpls and network action and their mpls DOA stack foreign we have discussed uh um some solutions for the queuing based enhancement um I think it is first uh necessary to list all queuing mechanisms out in data net and discuss which queuing mechanisms expect to carry queue information as metadata and then and then to design the common format for the queue information so that should first gather confirmation from a working group first which queuing mechanisms should be enhanced in the red um and then we will form a follow the children based on of the deadline and the line twister terminology and so comments and their questions are appreciated thank you"
  },
  {
    "startTime": "01:56:03",
    "text": "okay thank you um I'd like to open the floor for any uh comments I'll point out we don't have a lot of time for discussion but we do expect that the discussion will be centered in the uh design team are also our expectation is also when the design team is formed that all design team meetings will be open um for even those who are not on the design team to come and and participate foreign just two quick comments um so one was one Gino's a great uh slides on um derived from ATS I think the one issue about scalability um unless I misunderstood it is what I had in my first slide about cqf that I still have to have with a hundred uh Ingress and egress notes a hundred times a hundred ten thousand flows that I need to carry um in these type of mechanisms unless I have something that allows me to aggregate on midpoint hops which you know the cqf solutions for example do and the second point was uh something that I think is applicable to all the queuing things we need to figure out how much we think it is feasible to have that um perhap per flow steering State on the 546 Tuple of that net flows is still there when we do need to have the queuing be better scalable right because the proper flow steering state may also be an issue and I think that that is not at the core of of the queuing mechanisms but it's kind of a a question for the scalability so I think you know on behalf of the co-authors hopefully we can get more input on what the the working group feels about that"
  },
  {
    "startTime": "01:58:03",
    "text": "foreign now or just just quick answer then yes uh you are correct the scalability on ATS we still have to carry the I mean we have to maintain the Flow State stage of the flow individually yes so that's the main actual value of ATS so the the fair and P far is the mitigation I would say especially the the pig flower the perf The Pork food flow aggregate regulator doesn't have that issue and the the last solution c-score I would name it um it is even uh simpler it is more scalable now the detail I could I could elaborate maybe in the email or yeah thank you mail and until we have the uh design team when we hopefully have more time for discussion thanks so there are some similarities in some of the um drafts there's no reason why authors can't get together and start talking uh uh inside author groups while we're waiting for the design team so so don't wait uh please feel free to meet informally uh feel free to use the mailing list if you'd like to use ietf uh resources working group Resources contact the chairs Alias and we'll set them up and you can have informal meetings between now and the the formal start of the design team um with that we are pretty much out of time I'd like to thank everyone for participating uh clearly a lot of energy"
  },
  {
    "startTime": "02:00:01",
    "text": "in this topic which is what you would always like to see uh in a you know a contribution driven uh organization so that's really great uh yadosh do you want to uh anything I would like to just thank everyone also very good discussion and let's continue and happy holidays yes absolutely absolutely well uh thank you all and look forward to seeing you uh uh virtually and uh eventually physically uh hopefully in uh one of the upcoming meetings thank you all thank you bye-bye"
  }
]
