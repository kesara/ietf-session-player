[
  {
    "startTime": "00:00:40",
    "text": "It shouldn't be too rushed for this meeting. So wait another minute or two. So david, do you think we're okay to start?"
  },
  {
    "startTime": "00:02:19",
    "text": "We can't hear you, David. Are you muted? Hey bernard. This is Allan. Not only is David muted. He is also under a cloak of visibility is we cannot see him at the front of the room. Okay. Well, How about this alan? Would you be willing to sit up at the front? Just for the sake of being the I heard Mock has chocolate for volunteers. Oh, okay. Yes. Yeah. Yes. I can do it. Okay. Thank you. Alright. So we'll Yeah. Alright. Thank you very much Alan. And Okay. Just a few meeting tips, hopefully everyone has gotten used to using be echo by now. And we're gonna use it for the mike So please take note of that. And if you're not speaking, Please keep your audio video off. We still have the mask mandates in place, so please wear those unless you're actively speaking at the microphone, Right. And If you're remote, we urge use of a headset or a echo canceling speak about. So Just a few reminders on how Works to enter the queue. There's this hand tool you click on the hand with the slash that puts you in the queue And then we call on you"
  },
  {
    "startTime": "00:04:00",
    "text": "and you enable your audio or and or video, and then you leave When you're done, you poke it again and that takes you out of the queue. Please don't speak without getting in the queue so we can manage queue in an orderly way. And of course, video is separate from audio. You don't have enable a video if you don't want to. Although, of course, if you're at the Mic, we're gonna see you But if you remote, that's up to you. So here's some info about It one and sixteen, and the agenda is up an accurate agenda is up on the material site And if you're having any technical issues, there's linked that tells you how to report them. Hopefully everything's working okay? So here's the note well. You've no doubt seeing this at this meeting, previous meetings. But it's basically a reminder of Igm have policies that are in effect on topics like Patents, coat conduct, Alright and By participating, you agree to follow these processes and policies And in particular, if you're aware that you there's a contribution that's covered by patents our patent applications I must disclose that act or not participate in the discussion. Here we are. David. Rush. We're just in the note well. If you want more detailed info, we have pages and pages and pages of dc, at agreed. But please don't do so now, you'll probably call asleep. Then we also have a guideline for conduct. An anti harassment policy and the anti arrest procedures. So if you have any concerns about that, talk to the Team, But we hope that we maintain an environment in which we treat people with dignity, decency and respect But if not, please. The. Persons."
  },
  {
    "startTime": "00:06:03",
    "text": "And also feel free to talk to your chairs. We're happy to take anything. Luckily, we haven't had problems in this working group because everyone's able to be nice and friendly. Yep That's cool. We have mask policy is an effect. I'm sure you're used to this by now. But it must be one of the meeting rooms and a recommended for common areas. You can roll it for eating and drinking and also active speakers can remove their mask. But otherwise, no exemptions. And We're asking for Five, F fifty twos are better. And there's a blanket that details. Alright. So a little bit about this, meaning the agenda up there. There is notes. Up there, we would like you Once we find who's gonna take notes, please take him in the note app and the link is there. And so we get to the part of the meeting where we're gonna beg for note takers? Or maybe not if someone owns here quickly, There's someone out there who wants to volunteer here to take notes. This is a part where I stir awkward into the audience. I noticed that everyone is staring in san at the laptop. Come on folks taking minutes is really easy. That will not accept you though. But shares all. Bernard is actually in the sea. Yeah. But we have to go and I asked people. I think I think we've reached that stage. If do you wanna pick a victim? Thank you. Much appreciated. Okay. Thank you very much. That it's much appreciated. Okay."
  },
  {
    "startTime": "00:08:02",
    "text": "Alright. So here is the agenda. So a lot like, previous agendas will have a three update from well Well, I think it's will. And then we'll talk about it transfer issue should two and then web turns of h three. And then to the wrap up in summary. Alright. So of be a three update. Well or Test test. Okay. That's better. So thank you. Good afternoon. My name is Will Law from Mac my chair the w web transport working group along with Yan Var from Mozilla. So we're gonna give a short update on progress over the. Our status is working draft still. We've updated it last one was published March fourteenth just a few weeks ago. And it's current on the website. The links are available there. Our charter was standard for an additional year. It did expire last December and being renewed until December thirty first of twenty twenty three. We've set a rough timetable for the year. This is perhaps aspirational, we're hoping to get to a proposed recommendation by September, they're not Too many large issues outstanding, we have a milestone which is called candidate recommendations we've got nineteen open issues, but eighteen of them are labeled ready for P r. So if we can get those down, I think we're in a good position to achieve the timetable shown here. And I also wanna welcome Nit, judging from Google, who's a new editor for the specification. I think it's the same knit taking notes. Okay. Hey good to meet and person actually. I'm never seen him before. Okay. A summary of some of the major updates that we've had since"
  },
  {
    "startTime": "00:10:03",
    "text": "our lost f report, which was back on November tenth. So firstly, web transport send and receive stream make transfer you can now send them over to workers. This is actually restoring and all the behavior that that got broken. You can open streams in parallel to the connect request. You do not have to wait for the connect to complete before you can go ahead and say add a uni directional stream, you can ask for the connect create the stream and it will get queued up and execute immediately. Send order and stream creation, we added a two to the sixty fourth send order integer it has a knowledgeable state right now, higher the send order the had priority that the queues get drained on the ascending side. In speaking of Victor, there's two issues he'd like to resolve One is that it's not applying to buy streams. I believe the Api will need to fix that And the second is you would like to make a send order of zero to be the default. He's rake okay he's writing the issue right now. So it's the latest information. Thanks for bernard. We've got a Web codecs echo and the samples directory. Can grab the code and start playing around. Where transport error adjusted slightly. This is a change for Chrome because it's They they went early. We've got a... We're basically aligning with w guidance for the construction signature, and we have a code sample shown here. The W draining promise will now resolve once the web Transport session receives either a drain web transport session capsule or the go. And so your application now gets notified that training is active. And then lastly, by y readers for data you can applications supply own buffer for a single data, and there's a recommendation of a max buffers size of sixty four kilobytes Next page, please?"
  },
  {
    "startTime": "00:12:01",
    "text": "Yeah. Just one thing I wanted to say well is is we're also supporting on the client was even go away and drain web transport At least at the moment, we don't support sending it. And so if people feel it's important to send it, I guess they should pipe up I think either slide coming up on that but not cool. Yep. So update number three is Firefox have released web transport support. Now is is Randall on the line. Or uni var, either one, I was gonna give them a chance to speak to this. Hello? Yes. I'm ready hey at? Hello? Would you like to present this slide since it's your work? Sure. So firefox one thirteen nightly, supports web transport, the prep on by default for nightly. Is literally building as we speak here. It merged to central a few hours ago. This includes data support. Congestion control is cubic it's largely written in rust. And we passes almost all of the web platform tests and we've added several in the process as well. And the last few will be getting too very shortly. Did the few features that we haven't landed yet like send order support? And outgoing day tie outs and a few other details around that. And by y o b readers for data since I uploaded the spec before we added The y v readers for data. So I'll have to update that. And some other small details. And if you take the current nightly and make sure you've updated it and if it's from"
  },
  {
    "startTime": "00:14:04",
    "text": "it's not the one that's building at this moment, turn on the enables for web transport. You can go to Mo q streaming at university and see how it it's just fine. So That's be it. Fun. File bugs? Thank you very much, Randall. And it's great to have two working versions now. So we can start to look at inter interrupt being one of the next items there. And then I just wanted to present some of the current issues of debate within the group. And if there's any feedback or guidance from this group is per the direct then we we welcome and appreciate it. Firstly, we're debating adding a send rate estimate to our web transport dot get stats method. This is an estimate of the maximum rate at which the application can write data and avoid queues building up we have a lot of questions on this, which I won't read them for Verbatim because you can. But there It's a step that is easy for to get different from different user agents. Whether it's null or not is an issue of debate right now, what and is the value going to change as you're sending? So there's guidance from this group or any experience and implementing such an Api we'd had appreciate. And not should I posed for issues after support for questions after each issue or think it's more relevant. Otherwise, we got a multiplex our questions as register. People are in the queue you can call on America. Yep. I can't see the web key though. I can only see the room Yes. At eric here okay. I can now. Helps if we turn to microphone on. Eric can near from Apple. I think this is one of those where it's kind of hard because there's all these interesting questions. And that makes it a slightly harder problem to solve, and it's tempting to just try to kind of punt that"
  },
  {
    "startTime": "00:16:01",
    "text": "But as we've seen a whole bunch of people who are trying to use web transport, especially for media related applications. I think this kind of feedback is one answer to the give us essentially Udp packet. We wanna build the same thing all over again. And so it's really tempting to try to actually solve these. Rather than just anton on it. So yeah. We can without spelling it and make it null or not null and and all of those things. But I think the overall question is is it worth spending our time on? I think the answer is yes. So... And I think we should do it and update it over time and we can answer all those one questions. Online. Okay. Thank you. So so this is kind question for from there so about the implementation of so we basically t three price on you implementing. So we... They are or improvement implement. So Chrome implementation based on the draft therefore of Ppp data but we are going to change out to the. So... Yeah. I'm wondering what pardon you are using. I'll be honest with you and I do not know the diagram code was the network side, data code was implemented by another engineer here. I'm happy to check on that for you. So feel free to send me an email or Jess at mo mozilla dot com? And I'll happily Up You. David? Yep. David Speaking as a data ent. I might know the answer here because Empty was telling me that for a fox almost has the diagrams we're not quite yet."
  },
  {
    "startTime": "00:18:01",
    "text": "So the answer is neither, but I think they're almost there was what you're saying. May I ask that you cc see the list on that email, because I think we'll I'll be curious. And I wanted also apologize to everyone. It was my fault from the mask side, but took a while to get to the Rc version of Data, but were there now at least and Like, if I let let's make sure that we have the same versions in Firefox and Chrome. So that it makes... It encourages websites to just do one thing and and not start using agent sniffing. Thank you, david. Okay. Queue empty I'll move to the next issue of debate, which is client initiated drain, and this is what Bernard just mentioned. So we have notification to the browse client that the server initiated draining, but the quest if you if you read, and I quoted here, the web Transport back either endpoint can send a drain web transport session capsule. So the question is should we allow the Javascript application and Api to send a drain notification to the server. And we gave an example of a client archive cap app that might not want to lose streams in flight it was terminating. Its connection. We don't know if that's valid, but we appreciate your input from this group as to whether we should exposed that Api to a javascript application. Yeah. Alan, do you have an opinion on this? Also, Eric? Go ahead. Hey. Lucas party cloud. I would rephrase the question is is there a reason not not to provide one. So generally the less Apis we provide applications to hang themselves the better, so that would be the reason, but in this case I. What what Alan had pointed out, and I guess, Alan you can speak to this is"
  },
  {
    "startTime": "00:20:02",
    "text": "the danger of doing this in the case where you're doing frame stream transport, because essentially in that case, if you're saying don't create streams, you're basically saying don't send anything. So that might not be what you are necessary. Name with Eric. Interesting. Eric. So I was gonna say this this kinda of comes back to one of the debates that we're were having and in Land around, you know, if I send a fin while I'm in the middle of receiving something from a server should the server continue to send it to me or not. And the ambiguity there means that we've essentially had to just say and that's kind of how people treat even if that's not really what we wanted. So the temptation from a specification standpoint, I think is, yes, please expose this, Let's have that kind of explicit it signal so that we aren't later left guessing about what did the clients really mean and half the people use it this way and half of music it that way have to do the most restrictive things so we don't have a better answer. And I think that specificity is probably more important then keeping it as small and less foot than as possible, but that I could go either around now one. Alan for, I am a client initiated go away enthusiasts. Although my the main reason why I wanna visit originally I've push for it needs to be three and then also push four to web transport. Has mo to do with proxies that are going like, that need to shut that that are acting both as a client end server and want to send that capsule Okay. Directions. Actually. Rather than thinking about a client specific case, Although I think you have a perfectly good example here of why client might want to send it and I echo Lucas point, like, Why not, I'm I'm not sure it's that bigger foot gun that the the text in an Id after draft says, just you know, it's very it's sort of fuzzy. Right? Terminate the session as soon as possible. Sort of application dependent. What that means and you're the web transport application running server side, or"
  },
  {
    "startTime": "00:22:00",
    "text": "you know, you're doing frame per stream, like, maybe it'll just terminate abruptly in the middle of something or maybe it'll all know like, I'm in the middle of I'm gonna go to end of the next logical stopping point. It's just it's just an indicator. So I don't know I don't think there's that much risk that People are gonna So that use case would be like a mock relay or something like that, Did you're sending it to and you're expecting it'll forward it on the the subscribers. Well, the reason why I wanted the in the but transport draft is more like that relay going away and wants to notify both endpoints like I'm going away. Like, you guys figure it out what you wanna do. And so I wanna start, you know the idea of draining a server with Go away, it's not just like, one direction, like server going away. Client needs to know some you wanna. So needs whoever's generating the data. Thanks. Thank you. So k empty, can we go last slide for the w Yep. Okay. The next item, a Max stream limit. So there's a default limit built in I think to save memory was the the reason for order to protect the stack on either side, And as we start doing frank stream at fifty sixty frames per second, we're very quickly going to get over this limit. So the question is, do we need an Api at the Javascript level to request that the user agent increase the number of allowable incoming streams, or should the browser automatically handle this in the background and the application just opens streams as it wants. And the browser takes care of it. Okay. Hey, Lucas Part."
  },
  {
    "startTime": "00:24:01",
    "text": "Speaking from somebody who has an implementation that does stream limits. Dot for web transport but just for quick. That the Apis are really simple. It is configurable. But it's not dynamic and responsive to as much as pick a number and that slot the highest thing. We wanna to improve that. We wanna come up with min. Watermark like Max watermark levels, some ability to do stuff, maybe make it literally dynamic so keep asking all grunt signal credits like I think you probably don't want the automatic thing. But what the shape of the Api is, I think is gonna maybe take some moderation on So That's not maybe good input, but So use you see it as more of a hint to the user agent to then do something to give it a hint that I am gonna be opening up upstream at a certain rate or expect discount of spreads in flight Yeah. I don't know. I think if you don't let This is a own configuration on its magic and it's a take different between implementations like gonna cause paying for for users in my opinion. True you might not hit it day to day, you Now we we see this in Http. It's like, okay. Well, you can make it under concurrent request for most places that's never hit. That any when you do, it's Not game over, but it's awful. It's terrible. And can you do then and wait for the next browser update to ship or wait for a new spare like today? So would like to see maybe like the Candidate Api of how that might look. And then and then maybe give that some review. Sounds good. Thank you. Randall? I mean, if the the downside of hitting this it's can be very bad as was just mentioned. You know, how much are we saving by making you know, having the having the limit there. What are we... You know,"
  },
  {
    "startTime": "00:26:04",
    "text": "our thing... Implementation you really using fixed size tables for this. And it and if they are, let's set the limit high enough that It's relatively safe. For almost any conceivable application unless you know, there's... I I don't I don't see why it would be using that much memory in there, and I don't think that our implementation does. Alan Yeah. I think like, the previous folks have said, like, you know, this is about the server being trying to send frames to you and going hitting a limit and not being able to, And I think that's gonna be a very sort of ugly failure mode when it when there's not a knife and if that. If the application knows, I'm I really need a thousand or ten thousand or whatever it it knows that in advance and can avoid that, just by saying, like, please give me this many or fail me now before I try. That's probably better. Then... Yeah. You know, it not... Yeah. Doing automatically. And not catching up. In particular, Alan I think we we looked at the scenario where you're trying to do something like conference saying like frame per stream and you've got you know, there's this kind of arms race. We have these grids, seven by seven or twelve by twelve or whatever. So you have a lot of streams then you have concurrent streams open. So it's pretty easy to see how you'd go over a hundred, for example. Yeah. I think like, the sort of the the way we've been treating three moments in each So far has been like a, a hundred seems to be okay, but we I think that's probably gonna be applications that are gonna be much larger for web transport. Jo, Yeah. I agree Alan"
  },
  {
    "startTime": "00:28:02",
    "text": "with side. So just concluding at the very beginning, it seems like a good option. And also just a data point, I implemented the case that the media sends frame okay. Creates a quick screen frame. And this only applies to the in flight themes. And even if you have a latency around two hundred milliseconds, there number flight streams it's usually ten twenty. So it seems that also putting that limit so sorry of better connection. It seems putting that limit to something around a thousand. It's also a reason trade. Thank you. Harold? All this. The important thing is that there has to be limit. And we know we have to test the behavior when we hit that limits. And when we increase it or decrease it or whatever. Because and wherever use say from, someone is going to say super fragment again Ex I mean you will always get something bigger than you expect. And you need to have defined behavior when you hit it. Okay. Thank you I think Queue is complete. Or do we get randall back? But it looks like Just un quick response to Harold. Okay. But what is the reason we have to have a limit I mean, why what why is there a limit in here in the first place. If there is a strong reason for a limit, okay, fine. Let's just set it high. And find a way to, you know, the application tell us if they need more than the the than the default. But it's very hard for the application to to figure out what what it's going to need ahead of time."
  },
  {
    "startTime": "00:30:03",
    "text": "There has to be a limit. You will not know what it where is until it until it tests. When we did web to see data channels. We put in a spec that you should apply sixty to four thousand channels, well sixty four thousand channels channels out resources in a surprising amount interesting places. So better have a limit and ability to see it because otherwise, you run unexpected behavior. Can I ask tell you? Where you go away. I just have a question for you. Do you trust the the browser to... The the application to set that limit? That needs to be as I said before, there needs to be defined behavior when you hit that limit. And people are going to hit it. No matter where is. No matter whether asked for more or deaths. Before. Jumping in as Chair. I'm gonna ask you a clarifying question how I think what you're saying is that if we don't put a limit in the spec implement are gonna put decide their own limit, and then you won't know what it is. Is that what you're saying? That's exactly what I'm saying. Awesome. Thank you. Are we is Lucas, the next one in the queue. Or is it randall? Lucas. Okay. Cool. I just wanna clarify, like, nearly everything we're talking about here is quick. Right? It's it's quick. You have to give an initial... Stream limit and then it increments to the course of the connection based on effectively credit granting from from one end to the other and and opposite directions"
  },
  {
    "startTime": "00:32:02",
    "text": "don't think we need to lit how quick works, because that's the sole problem, but we know what happens when he went out a credit. It can't do anything. Like they we we have different application protocols that have to deal with that thing. It's... I don't think that's that's the problem. The the consumer would be that limit is effectively about the resources for the receiver. So you've got how much takes to to maintain potentially the the your Ps sending that many streams which per streams is that cheap, but then you've gotta to think about the flow control, the potential that what you say is street stream. I'm I'm willing to receive one megabyte That's a hundred megabyte commitment. That's that's actually okay on a client. I know. But if you say sixty five thousand, And then you can open up multiple web transport streams. As a comment just in the in the Zulu like you could across a client And so the the the absolute number probably needs some care, but that I don't think we need to, like, over rotate on solving solve problems. Yeah. I think someone asking, like, why do we have a limit I think that's the answer. It's like this limit is to protect the receivers from a perspective because streams have an merry cost associated with them. I guess, I'm curious a little bit how the Api handles and and also the point we need to define how it fails what it's gonna look like, there's two potential failure modes here when you an application wants to make a stream, but it's at the stream limit. One is it can fail immediately and be like, sorry. There's no streams. And another one is an implementation might do it. Look like a promise where it's like, yeah you you can make a stream, but there's not available right now. I'll just wait. Until one closes and then you could have it. And just they need to get hung out on that. So anyway, things to think about. Okay. Think that q is now drained."
  },
  {
    "startTime": "00:34:04",
    "text": "We gotta go away. We had one item left there, but I guess if we're out of time. Just I just go a lost item item for quickly. We've added send order for streams, but The relative priority of streams to data currently undefined. And I think Chrome as it stands data games of the highest priority. So if you're pushing a bunch of data down your data grams you'll never actually end up sending a stream no matter what send order. You give to it. And I had a hallway conversation with Martini nieces a similar situation inside Firefox currently. So the question is should we allow data kilograms, something simple like web transport dot data grants don't send order. And we apply the same send order value relative to streams for data grants to allow the application to side. How it wants them to be prioritized. Anyone see a problem with that approach for a reason it should not be done. Got a thumbs up from Eric for the minutes. Okay. No two. Thank you very much. We've taken our time We appreciate the chance to give feedback then. Okay. So Eric right now. For Alright. I'm American here, we'll make this reasonably quick. We've done a whole pile of fun updates since we last talked, which is cool. Because got lots of good feedback in It one fifteen. So we have landed all of the fun capsule design team changes which great because it feels like we've talked about that for several meetings now. We've also done a number of other issues that I wanted to just highlight here and talk through feel free to go through in the Pdf of these slides and click on these links. But"
  },
  {
    "startTime": "00:36:03",
    "text": "all of these have now landed and the text should be in pretty good shape. One of the big changes that we talked about in one fifteen was that we wanted to be able to send anything that was allowed by flow control before you get back to connect response. This is actually going to come back to an issue that we're gonna discuss a couple slides from now. So it's worth thinking a little bit about, but this is rather than waiting a full round trip to say, hey, I'd like to talk to you using web transport and then have server say. Yes. That sounds great. I totally talk web transport. Let's talk. I can say, hey, I'd like to talk to you about web transport, and by the way, here's what I'd like to say. So That's a fun one. Other things we change some status codes if anybody looks at that and choke on it, like, now's the time to speak. It's just a number pro requests or cheap. We added some initial flow control windows. We ended up spelling that as a single web transport in it header field. So rather than having all the different I can have stream level flow control, web transport sessions slash connection that will role stream limits, all that kind of thing. There's now just a single header field trying to be reasonably a model that just says, hey. By the way, these are the initial limits that connects back to that. One earlier we were just talking about because you can only send the capsules that are allowed for full control. So it helps if you're allowed to send something. And then the last one which we kinda of went back and forth on over the past couple of meetings. We had a setting for enabling web transport. And we also had a setting for sessions and one of them only had to be sent from the server. So we had two settings, and that seems crappy. So we now have one and if you say settings web transport mac sessions is zero, then you don't score web transport. And if you say it is anything non zero, it's as if if you said settings enable web is one. Next slide, please. So this is the particular spelling of of we ended up landing the web transport in it header field. So it is a dictionary."
  },
  {
    "startTime": "00:38:00",
    "text": "Structured field because structure fields are great. And it gives you the initial flow control limit for both directional streams. Coming from the remote side. As well as bidirectional streams in each direction. And so you can essentially just pack that all into the same header field. And at that point when you say, hey, I'm would like to talk about Transport, by the way, if you'd like to start sending me stuff right away. Here's how much I would appreciate you sending me because otherwise you're gonna be very sad. Not allowed to send me very much. Alright. Next slide, please. There are a happily short number of remaining issues. Either means that we need some more people to read the draft and file more issues or we're actually making some progress those issues are for some examples that need to be updated, which is mostly editorial. If anybody gets excited about that. Holler, but we'll do that. The other two we actually should talk about. One of them is error handling. Because that's one of those things we don't wanna forget about. And the other one is flow control because Flow control is on. Next slide, please. So for error handling, the proposal that we have is that any kind of violation any other fatal error that you run into can cause us to just reset the H two stream and destroy the whole web transport session. And that's kind of a fun and interesting thing because the last time we talked about this, we all nod it and said. Yes. Yes. We want this to look just like we do it for h three. Is not quite exactly how it works for h three because H three has a bunch of other streams that are part of that web transport session and we're about to spend some time talking about how much fun it is to try close some of those streams. In this case, we're a little bit lucky because h two is packing everything onto two stream. So if you need that h stream, you're done. This is not quite the emotional equivalent of what we were doing before, but I think it is far cleaner and simpler. So barring any objection, I think the the proposal sounds good, which is the instance you do something wrong, we say great your session is gone. Wonderful I don't see anyone running to the microphone to say, no, I want more complexity."
  },
  {
    "startTime": "00:40:02",
    "text": "Alright next slide, please? Alright. I'm gonna para phrase a little bit from mt here. But we've got two remaining issues. Other than everything we just talked about, and these are so far the last ones. I am tempting fate here. About the initial limits for Flow control And so the problem that we're trying to solve here is I was just talking about earlier, you can send any capsules that are allowed by flow control. Without waiting for your two x response back from the server. That's really nice because I'd like to be able to send all of the started on whatever protocol I'm running over Web transport, start talking to server about whatever it is I wanna say. But in practice, this really means that you can send flow control frames, which is nice because you can get things starting to open up so that the server can send you stuff. We now have the wonderful web transport in header so that you can actually get your initial limits right there. And you can also send data frames because aren't super controlled because you can just drop them. But if you actually wanted to send useful data on useful things like web transport streams, It's wonderful that we could said you can send any capsules allowed by flow control. But what we really emotionally wanted was for you to actually be able to send useful. Capsules. Next slide, please. Nothing against Grams there front too. So the opportunity that we have here is if we were to define non zero stream and data limits for flow control. We could allow both your client and your server. Right? Because here the the server is usually allowed to to send back. Actual reliable data in that first time trip. And seeing as we spend a lot of our time trying to ourselves into updating protocols to few around trips, it seems a little bit silly to cheerful say, yes, we want to extra round trips in this one. Next slide, please. So we have"
  },
  {
    "startTime": "00:42:00",
    "text": "two kind of options here. I think this is where we really wanna feedback and some guidance. So one option is we could just say we're gonna have a default limit for all implementations. And this comes back to what where was saying a little bit ago of, hey, if I'm going to have my browser have a limit maybe we're just gonna have everybody pick some limits and we'll just hope that works. Or the other option is we can communicate some of those initial limits in settings. Next slide. I see a q building. If you if it's relevant to this slide go up, otherwise, this whole section. So. This is the last section. So if you wanna go stand in line. Alright. So if we that slide and then we'll take questions. Okay. Yeah. Okay. Go to the next Yeah. Thank you. So we're gonna just talk briefly about each of the two options and then we'll discuss. So if we do have a default limit for all of the different implementations, that basically just says that now everybody wants to put my web transport has to support at least as much base. And that's potentially hard if you're on a constrained device or you're trying to do something that's a little bit more specialized. It's a little bit awkward to say, hey. So I can't support one megabyte across a hundred streams. Because that's a hundred megs, and maybe I don't have a hundred megs to give out. Also how do you choose the value? What are you gonna do for that? If we're gonna choose the value, I think Mt had a suggestion that we could always base that on some values that sent for h two in settings. So we can either pick, like, hundred streams is what you get, or we could say, it's some multiple of what you allowed for your too, like, you're allowed to use half of what h two gave out something like that. So there are ways to do that. But if we go to the next slide, We could also communicate about these things and talking to each other is occasionally a good plan. This allows anybody to sign up for only as much as they're really willing to to give the resources"
  },
  {
    "startTime": "00:44:02",
    "text": "to. The place where this becomes hard is that the h two settings are shared across every web transport session that's happening within your connection. And that becomes extra extra fun because we don't say the magic word intermediary. And Now you have to look at who is upstream of you or down stream of you and make sure that what you're going to advertise at the very beginning of everything matches what you think is on the other side of you. Or sign up to buffer. And I believe that brings us to our discussion moment. Lucas Part, for this initial limits thing. The idea of a default sounds horrible. Because it's just gonna... It's gonna spend for forever trying to pick it. The seems more like equivalent to what we can do in quick line already with transport parameters. If If I remember correctly, you have to look the climb would to wait for the service settings anyway. Is that correct? Be able to send the web transport request Yes. It would then send the additional stuff on. So not to introduce any any additional waiting or blocking that. So seems okay. I think this create a setting and figure out solving that problem with my inclination. Just Yeah. Ian. What. Yeah. I think I think I definitely default this not what I would prefer. Setting is fine. Infer it based on the existing issue setting seems a little ugly. Like, I'm not really sure I like that. So I'd rather just have a dedicate one it doesn't seem particularly hard? You may not have been thinking about web transport when you set your h two settings. Yeah. Victor. Victor so Google."
  },
  {
    "startTime": "00:46:01",
    "text": "I am generally opposed to using the settings. They're many reasons, but number one is like this, I think is for multiple sessions, which might not have to... Anything to do with each other. And number two this generally goes against the design of flip over H where almost everything is communicated like band and like, you could run it over a tran and and think with just more time. And that's the property that this makes implementation much more easier. I am moderately directly positive on the sounds reasonable in the show limits? Oh, force finish of f. But it sounds like sauce here do not far accept. So we might be just stuck with without the initials. I do think that the fact that it shared across different sessions is an interesting challenge that we're gonna have to figure out if if we do decide to go that route for purely the layering in the one pieces. That doesn't concern me quite as much because if you did wanna to run all this over one, essentially, what you're saying is I'm willing take an extra round trip. Because I don't have those values available and so I can default to zero. Right? But, like... Okay. So you're saying that whatever would do here is, like an extra layer. Couple. We're I don't know if I call it an extra layer, but like, optimization. Essentially we're we're saying that if you assume that you start at zero. If I choose to tell you a non zero value upfront so that you can send me things four we've had a chance to exchange the header. That allows you to get started sooner. Okay. Question, Eric, since I'm hearing some difficulties with settings with using header with the response be an option here. The challenges that I have not yet sent anything to you on this stream, and I'm sending you a connect request. And before I get a response from you, I would like to also send a bunch of things on that"
  },
  {
    "startTime": "00:48:01",
    "text": "same stream that our web transport capsules. And so before we've had a chance to say, Hello, here's how I'd like to talk web transport. I need to know how you would like to not web transport. So the only cost here is the round trip. Correct. Okay. Which is something that we as an option, I'm not spit seeing an opinion, we could accept that cost and say, for h two, you eat a round trip. Like it's two is not the fast one. If you want fast, you get three. We could do that. Ian what. What about using a study saying it's still requiring matter basically saying if you, like, back header value that violates the setting then, like, you know, don't close the connection and, like, where away. I mean, we're good at resetting the stream. So so. Yeah. I mean, because that just that me get some of the know maybe any proxy things because it's still bend. You basically... Like... You're basically making an optimistic assumption about what the peers is going to do that they're not gonna reset your stream. Right. And and if you are an intermediary and you are, sending these things. Like, if you do sign up for more than person on the other side of you is willing to deal with you are now start holding that bag and your choices are either throw on the floor and say oops I screwed out. Or pay buffer that data and either is fine. So as chair here. I'm not seeing a whole lot of consensus in the room, but it would be nice to figure this out. This is the perfect kind of thing that doesn't matter that much but that we can rat hold on for hours. So to kinda reframe the discussion, like, is there one thing that everyone could live. Because end of the day, these details for each two, Like, if we get them horribly brown, it's bad, but I think there's probably an answer here that people can live with. It sounds like we had significant sentiment against just signing everybody up for default as part of doing web transport."
  },
  {
    "startTime": "00:50:02",
    "text": "And I think narrowing the the set of options by removing that one would be helpful progress even if nothing else. Is that a thing that everybody would be comfortable. Removing. I see. It comes up and I. Lucas who was kind of maybe not really in queue. But bernard I see you locked the queue you do you... Are you thinking we don't wanna do this now and you wanna punt it to later or Yeah. Wanna make sure that we we could come back to it. But I just wanna make sure Victor gets the time that he needs. I know we probably have a good amount left. But how about this very good putting. Yeah. I... Let me unlock the queue very briefly. And take Eric suggestion of Can we live with not having them be defaults. And then we'll figure out how to communicate them is that something that the room can say okay with. Alright. I see Victor nodding. I'm. That something's up. Okay. Great. Alright. That is progress. Thanks, folks. Thank you. Is very helpful. if we move forwards in our Alright? And slide deck, I believe. Okay. I think that's that's it. Because the next slide is victor. Victor. Yeah. That's me. And the. How do I I just tell you to change slide? Yeah. Okay. So we actually this time have a lot of updates since F one fifteen, We merge a lot of p that this is the first time I don't think I even can fit everything on one slide. First one, Eric Already mentions a capsule design team."
  },
  {
    "startTime": "00:52:00",
    "text": "The second one is There was a long standing issues that's we're missing test for describing how to do of upcoming web transport sessions. So just have tags it... If you're browser you should does that? Live it up to implementation. We cla there were some clarifications regarding how settings changes behave across zero. But that we don't support Rt, you cannot go from Z rt t t where you supported what transferred to priority where you don't. There is We... There's ninety free and the other we now require you to explicitly enable connect protocol that means you'd have to often... Like, before it was implied because there was no Websocket is over free defined and now we require you to explore very opt to extend connect and there is a similar one for explicitly requiring and opting into h three data guns. There was some updates around the origin header we previously prevail this said it's always required to now say it's on records to send the fuel browser. Next slide. So there is a kind of a big one ninety six So we had the magical frame called trans stream, and whenever you send a frame, your so streams that used to be Regular, each free stream transformed into to a web transfer train. We changed this a little bit. Now it's not a frame. It's just special values that you have to send the very beginning of the stream, and you can no longer send it in the position others at the beginning goes the stream."
  },
  {
    "startTime": "00:54:00",
    "text": "There is more pending clarification tax from Lucas that I was assume... I still need to finish for viewing. But there is And the last big one is there is clarification on how to do the situation where you have one session web transport session on a connection and you close that connection, but you want to close the web transfer session and make sure it's that the custom application error codes that you that stream has arrived and there some recommendations out you. If you're doing that, you should wait until it's acknowledged before you tear downs the entire connection. So so sars may this. Now let's go to issues. And the next slide. There is Issue seventy sevens that is related to the fact that if you open the stream, that's where transfer data stream and the reset it's too fast, then pierre will never know. It's a of transfer data stream. And this has unfortunate implications that don't want to deal with with. For today we are not going to discuss this because this is currently blocked on quick working group reviewing an extensions that will solve to this assist. So I put the time for a quick working group meeting and you're are welcome to attend if you care about that. And add to that for anyone who wasn't in the room last Idea, we discussed this in what transport in the consensus from our meeting was to have this happen quick, and we reached out to the quick chairs who agreed. So that's what's happening now. And if you care, please come. Yeah."
  },
  {
    "startTime": "00:56:03",
    "text": "Alan from. My question is, maybe is premature. to say this extension is adopted and quick. Let's Do we think web transport will require this extension or can you run web transport without it? I would expect this to basically require it because There are, like, If you don't require it, ninety nine percent think will happen, but sometimes the things that are supposed to be reliable and consistent with just suddenly, like disappear So this is like I would not attempt to opens the pandora door docs what would happen if you do not acquire this. Actually on before you sit down, We hadn't realized that that was comment... Or something that could be contentious. Are you okay with requiring it? Because like, That was that kinda reminds, but Let's pull the room. What do you think? You're asking me you're asking room. I'm asking you first since you were the one who brought it up. Then anyone else who cares to join the queue. I've seen comments on the mock mailing list, indicating that quick stocks are not as malleable as we maybe think they are. For example, expanding how the new prioritization. Some people think that's gonna be really hard. And if that's true, then I also potentially see implementing Whoever has that opinion probably also is the opinion implementing this change. It's hard since I no. I'm not gonna make comparison, which is easier or not. So I just think if we... If we're gonna require this extension for web transport, then that is raising the barrier to having what transport a little bit. I think. That's a reasonable point. Thank you. I guess, then what I'm realizing is the implementation difficulty will depend on how complex the extension that happens in quick is. So maybe we need to put a pin in that until that happen. So... But"
  },
  {
    "startTime": "00:58:00",
    "text": "if you're in the queue show your opinions, but maybe we'll have to wait until we have a bit more clarity on happen quick. Just wanna point out that we discussed this last time in at the One fifteen. And we said it should be a requirement. As an analogy. Are also not strictly required to do web transport. You could do web with just streams, but we still say that the data extension is a requirement if you want to do a a web transport, And we said this would be the same for this extension. Whatever maybe Okay. Let's let let's figure that out once we get it. Yeah. Because all good. So I just wanted to ask the personal... If it's going to be optional thing then why we do we have to make changes to quick. Lucas Parties speaking as an individual. I think requiring this makes sense and I wouldn't use it's as, like the potential difficulties there's the reason not to do it. I would use the desire to have this thing to improve web transport as a baseline for people to build on as input into trying to make a design of the other thing. Sufficiently implement by quick implementations and and have people who implemented state how hard or not it is about doing things in quick not hypotheses. So is yeah Yeah. Just one thing I wanted to say, which is that there are a number of issues with the handling of debts that will affect things like Mac relays. The draft that's been proposed handles some of them, but not all of them. There's more variability and quick limitations, I think that will potentially by implement of Mac relays. I'm not sure whether that says that it should be discussed in Mark but there there are dragons out there that aren't installed by the draft. I wanted to comment that"
  },
  {
    "startTime": "01:00:05",
    "text": "we already require one quick extension and like at least three Http t free extensions. So I of think we should be particular we can concerned about one? Next slide. Bernard? The first one where we actually get to talk and this is and results. We currently define an eight bit error code space for r streams And the way we define it is we take, like, a small chunk of h free error codes space and then we'll like remove the Grease codes points and it's like continuous minutes except for those. And people have complained that eight beds is not enough. And I kind of with that statement. So there are multiple options we can delicious. One of them is we can just leave it to this. Another is we can use the greece algorithm to just take like, their free erica space like sixty four beats... I'm sorry. Sixty two bit long. So we can take two to the thirty two of it and that would be It's relatively a small fraction of it still. The other options that this pointed out to me and I believe it's works but I'm not mature is We can just use the facts that we know in advance that it's a web transport stream. And this requires like us to do reliable resets. But if we know that, we can just can ignore to facts as there is potential name space collision."
  },
  {
    "startTime": "01:02:03",
    "text": "And if it's like web transfer data stream will always take the it's error code and assume it's a but transfer to our codes as opposed to a free error code. That should work. And also depending on the outcome of tomorrow's discussion on our level research works there might be another way to attach metadata data. What do people think Ted? Sorry. It's a long way around. I just wanted say, I I think using the entire space and not worrying about the collisions is probably suboptimal. And so I would I would be quite fine with the reset code in the metadata blog where the expand the space, but I think using the entire space and ignoring any potential overlap is I I I I see dragons in the future there and I would like to avoid them. So so the way we, like, try to avoid dragons is this is safe as long as we guarantee whether we know if something just a web transport stream or free stream. And if was also p, I should I to look it always out. Yeah. And so my my only concern there is that If if there's something that want to signal at the at the other layer at some point in the future, This is a web transport stream, but your error is actually in each three error related to it you you're gonna have to double the you're you're going to have to create an error at the web transport layer that says, you have an h three error. And it's this one. Right? If there's something at the at the h three layer that you need to communicate and web transport"
  },
  {
    "startTime": "01:04:01",
    "text": "you're going to have to recreate that capability in the in the web transport ones. And that just seems like let's let's avoid that whole thing. You've got other options. Let's pick one. Okay. It's that's sound understandable. Thank you. Any other people who have opinions? I do. I opened this issue. So I don't particularly carry if I have sixty two bits or or thirty two bits. Just having eight bits is not enough, especially if you're doing, right, more complicated stuff on top of web transport. You you really run out of road codes very quickly. Is there any downside of just expand the space to thirty two deaths in going this today? Mean That still gives us what billing. Just asking. It's... I I would have to write my more test coat. It seems like the most straightforward forward option, but that's my first. To answer that from a policy question. I was looking through the In policy for the for these are codes. And what what I'm reading is like asking for an unreasonable amount of space is discouraged. But if we ask for thirty two bits in the eight byte space, which is like a two to the six sixty two minus two to thirty two. That's just negligible. Maybe the eye people will get annoyed at us because that's gonna do something ugly on the page, but I think that's fine. So I I think we're okay on from my policy standpoint. Yeah. Special this registration is going to be really funny because it's like from a to b except for every twenty first error code. Lucas."
  },
  {
    "startTime": "01:06:01",
    "text": "Yeah. I I put it the I agree with Ted, like, I'm A very nervous about to squat over the top of Hp three because the potential alias thing and conflicts like that. So taking us a bigger chunk than eight bit seems okay, I'm sure we can work with our honor to try and make this better strangely, from from the cat registry that where the experts on David. I had a somebody I was speaking to who but intentionally purposefully used rallies from the reserve range. To to do actual things and they shouldn't have. So I was already speaking with her honor at this week about how we might be able to improve the descriptions of these, like, grease ranges make it even more obvious what they're supposed to do. So this might just fit into that too. There's a piece of work I was thinking of doing anyway. So Yeah. I I'm not against taking a bit more space in our registry, subject to the the designated experts during the review, and I'm willing to to help with that. And one of our expert experts actually... So I see Alan, and mike bishop is the other. I don't know if he's in the room. But And if you think this is a buy a bad idea, please stand up. Give us a thumbs up or a thumbs down. Or words. Yeah Yeah. Which you... Which which ideas about it? Yeah, There's several ideas, some are bad. The as an I expert do you have any issue with us registering thirty two bit plus however the decreasing is out of the a byte space. Mean, I agree with victor. It's gonna look ugly, I I I think it's a... I think you that it's it's a drop in the bucket. I'm also a relatively new expert so... And I believe my expertise comes more from the Http side than the registration side. Don't worry. None of us sign expression to what we're doing. Unless you think this is a terrible idea we're fine. I just want to say that"
  },
  {
    "startTime": "01:08:03",
    "text": "by apple Raul. I also point out that hiring Web transport, we've already decided to put multiple sessions in two a one single http, three connection which is already has the resource allocation problems and those things so I do things. Can you state your name and also can you can you get in the queue? You've been jumping the queue. Can you state your name and also use the queuing mechanism? Thank you. Sorry. My name is Ka H. And well, I just want to say that we've already we are already wanted the job of, you know, squeezing things onto the actually mapping so, I'd prefer just doing the thirty two bit. Thing and calling it today. It doesn't sound like we have any conclusive and t, I think I will wait until it was like tomorrow session resubmit data and I will reset and we'll see. Actually, Victor. I'm hearing some folks saying that that's fine and some folks saying using the entire space is not good. I didn't hear anyone object to the thirty two barrel unless I mis heard. Okay. That's yeah. That's true. That's one per... That sounds like the default course. Yeah. So let's... Before we move on, like, does anyone strongly re object we have reason to that option. You know, lock the queues you can. But... Alright. Let's let's go with that then Victor. Okay. We'll confirm the classes on the list as usual. Next. Slide. Oh that's a... Does a fun one. Flow control. So currently, we always to have trim level flow control because this provided for quid by quick streams. There is no way in web transport over a pre free if you're running multiple web transport sessions on the same connection,"
  },
  {
    "startTime": "01:10:02",
    "text": "there is no way to limit total data that are stand by an individual session And there is no way to limit the total number of streams that is used by an individual session as a peer. And since this awkward because Technically, like, as I said, if you on web transport of do you get all of this limitations. If you on web over dedicated h reconnect you also basically get all of those. So the question is do we want to actually adds capabilities for this. And if we do should they be optional for only when you do pulling instead of running one. That's sort session and say your quick connection. Do people have opinions on this matter. Okay. Lucas father. I can imagine why this might be needed. Like, I can imagine use cases, but I don't have any that exists personally, like... But it feels like the kind of thing if you wanted to trying and do. You you probably wanna try and do it in quick not in web transport. If that make sense. As another extension. I know that seems like even harder problem than the the reliable resets, but, like, Could we could we live without this for now? And Maybe try and solve it later? Alright. So you... So lucas you're proposing just keeping at as is for now when in a future document if it's a problem, we can change it. That's my proposal based on not hearing any concrete use cases but that that I kind of... If people presented really good reasons, you know Yeah. That's fine as all. That makes sense. Thank you."
  },
  {
    "startTime": "01:12:02",
    "text": "Alan don't. I feel like we've talked about this and that most people don't feel like the complexity required to support two and three and h three are worth it. Right now. Are we reopening that? Or did I misunderstand what I think we had an issues that sounded like it was very well close it because the can sense like, the conclusion on that issue was that we need free, but not two, but I am very... I was very confused that's how go cord because he took not her remember us coming to the conclusion. I think our conclusion was we thought three I think we... Okay. My personal opinion is that we ought to have all of these. It's part of the quick Api my view of Web transport is that taking an Http. Connect... Like, crew connection and giving you the quick Api on it. So when we're not exposing all of it, then we're kind of breaking things a little bit. I also think there we're fewer cases for pooled sessions when we were only thinking about kinda browser use cases, but the way I see people looking at using web transport as a building block in Mock. Where a browser may not be involved in either side of the connection. May want to take advantage of these things. That said it doesn't necessarily change that implementing two and three is hard. And I don't think there's a lot of appetite. And Alan just as a technical point if let's say Mark needed us, but this needed to be added as a separate document as an extension here that then mock would require would that address your concerns? Do you mean, like, if quick somehow provided this, like, sub dividing of the flow control space as an as a quick extension. As a quicker extension or if Mark builds over transport as a web transport extension. I mean, we... Those things are probably fine. I I I don't know that I see us actually solving this problem right now. Okay. Thanks. So... Yeah."
  },
  {
    "startTime": "01:14:00",
    "text": "Eric just really quickly, I think the reason that we had said that we wanted three and not two was because three was not super hard and two was harder. That being that it's not massively hard to limit counts of streams. Modular. Telling if they're gone or not, and that you got stream level for free. And so that that was why we said let's let's bite off the piece that seems useful and helpful to be able to partition things between web transport sessions. But within that session, you're kinda on your own. Do Do people hear believe that implementing free but that would be generally simple and useful. So and Victor, remind me. So in the draft, we have one right now, but not two and three. Is that right? Well, yeah. One comes from quick. It's Yeah sir. Yeah. We don't have anything for two or three. K. And I'm hearing folks saying like, it would be nice to have it, but it sounds hard. And I'm not hearing anyone jumping up and down saying like, oh, let's do this. And here's great solution. So can anyone not live with us just sticking with one and moving on. Okay. Works. Does this work? So. That's fine. Go ahead. Yeah. We can hear you. Hunt it took me too long get in the queue. Just because no one's tried, I think for two and three. That's the thing that concerns me that Maybe Victor you tried? I in your not actually try to implement to free. I I don't need an implementation. I just need someone to give you an idea of what it would look like I can imagine several things that would potentially give us an approximation of a sub subdivision of the protocol that wouldn't be very complicated, but would potentially allow you to fit for instance, reserve a few streams for another session. Something like that. So"
  },
  {
    "startTime": "01:16:04",
    "text": "I would at least like to see an attempt made before we go ours too hard. Alright. I'm getting thumbs up for that, that makes sense, Martin. Because I think I think we can actually do two in in the same way as we can do three, if we're willing to make some So compromises. Yeah... We would need to have a definition of for what's data we're counting. Right. Right. And and that would... I think we possible. We may have a system that is somewhat inflexible We may not have a system that is not as dynamically adaptable as the current flight control limits that are endlessly flexible in in different ways, but we we might be able to do for instance. We might have a system that says you have this many streams every time you use one of those you use up a upstream identifier, or I'll like to give you another one later. Very much like that quick manage managers streams. And the same could be said for with that and just have extra flow control ports that you pull from. Now they have some very interesting properties from and interaction perspective, when you now have connection level Fire control, stream mobile flow control, this new level flow control. So I think it would require a little bit of thought to avoid some of the more obvious sort of deadlock conditions and... Service also have to deal with, like issue with interaction with between reset some flow control limit. Exactly. You you would have to have some understand is some access to what going on on a quick connection in order to effectively do it. We'd have have very clear definitions of of what it means to to have a stream and and yes the the reliable reset question that we had again. All of those sorts of things. But I I think it's"
  },
  {
    "startTime": "01:18:01",
    "text": "we we could give it a try. Yeah. Lucas? Lucas party. I've got against people giving at go, and I like read like a sketch of the design or something, but this tearing oh, we want multiple layers of flow control, sounds like the h two how we have today that everyone hates and that we enjoy because we have quite with one. Hence why. I think it's something that really belongs in quick. So it's consistent and it's implemented by the people who do that bit. Like I know I want all the web transport. People that have to implement bore and flow control stuff just because there might be one app in the world that might need to use this thing and doesn't even know how to use it because it's so complicated and possible to debug like all of the h two flow control problems we have So I'm I'm willing to be proven wrong totally late. I'm that discussion. So I I tend to think probably it should be quick, but I'm not a hundred percent sure without, like, actually thinking through an entire design, but Personally, I would actually rather... Instead of saying, like, we wanna try to have a design before we move forward on this. I kinda of would rather just, like, move forward on this, and then if someone comes out with the design that, like, people like then we should like consider it and change it, like There's something to be said for, like, saying we're gonna close this... We're gonna say this is done for now and unless someone actually comes out for a better proposal. Let's see for the folks who thumbs up and t's idea, how do they feel about that because I do like the idea of putting the burden on proof of people who would like to see it to actually do the work. So who's volunteering to do this work. Yep. Eric, I just wanted to say, we're not like, doing lots of fun h two flow control as we thought that was just a good time. We're doing that because, like, otherwise, stuff"
  },
  {
    "startTime": "01:20:00",
    "text": "breaks then to run out of memory and people are sad. So it'd be nice if we Transport didn't just like, break and run out of memory all the time. Two. And it'd be nice if. It didn't break and run out of memory all the time because of something that somebody else was doing that you didn't even cause. So Right. Anyway, yeah. I think it it... I very much support what Martin was saying of, like, let's at least try. And if we say this is too hard after we've actually looked at it, That's nice. But, like, we're on the wrong timezone zone and it's hard to, like, think about it is probably not the right answer. Okay. That's fair since you're both editors of various documents let's discuss this at at an editor meeting and figure this out. And maybe we invite Lucas for this one. Anyone else wants to? Or maybe we don't. I think I think ian suggestion is a great one, by the way. Burden proof of they want the the feature. I think this is something that could be added. As an extension. So if if Victor wants to be able to proceed with the documents and publish them, That's a great call to have. And if we can do that by agreeing that this problem is not in the documents right now. Unless someone can Rhino draft commits the working group that it should be in graded into the documents before they get it too much further. That would be how I would suggest we do this. And because I think it can be done as an extension. There's some details to work out there. Anything could be done as an extension If you're willing to work hard enough. I think we can probably just not block on the resolution of this issue. What I wanted to sort of object who was the notion that we were gonna bury this forever. Because I think there is some value in here as Alan pointed out, but like it's it's useful. Particularly, if you wanna do the the connection polling stuff, but I think given that the predominant web transport deployments right now don't do any sort of pooling is not a problem so much for them. So we"
  },
  {
    "startTime": "01:22:00",
    "text": "of have a little bit of breathing room on this one. I do one to sort of comment about that at some point as well. I think that An un tested unused feature is a great thing to have in a protocol if you like things breaking badly in the future. So we should make sure that gets tested. Yep So here's a proposal then. We keep the issue open until the next and that gives us what is it? Three, four months to if anyone is interested in proposing a solution for solving this, we'll give them agenda time at the next Or acquire to be written down. And if no one has done the work, and we close this. So that won't delay us. But if someone wants to do the work, we will absolutely we give you time to present it, and we can discuss whether we think it's Oh, wow, that was easy. Let's put it in or, oh, this is way too complicated. Let's not. Does anyone object to that point? Alright I see a shrug. I'll take it. Okay. Drew one to the next line? Sure. Let's go to the next slide, which is the last slide then presenting and the most fun, it twice, it's support for a redirects. We have to roughly decide between three options allows requires them to be always bullet requires them to be not followed. And possibly we could allow browsers to be them to be followed but Api wise do not follow them automatically, but it's form to users sorts as they're being redirected. So why this is is is complicated. Normally, redirects are north particular like hard? Here in web transport, we allow you to send some stream data before we ever received the Http p two hundred from the server."
  },
  {
    "startTime": "01:24:03",
    "text": "This, of course puts us in a tricky positions so if we get issues two hundred and we get redirected Well, what happens to this trip now that's do we require application to this. Do we attempt to automatically res serial, all of those io operations that's the client. If we attempt to res them, what do we do if flow controlling the changes between the two sessions doing the website that but redirected from to do have said got redirected too. So those are are things that make it's complicated. It's. What opinions do people have. Patrick Noted an opinion. A clarifying question because I don't think I understood you correctly. What I I heard you say was you had a two hundred k and some data received before the two hundred okay, and then you gotta redirect. Is that... No. No. Two hundred, okay, it's not. I know I know. I concern. You sent data you send value see three hundred something. Okay. So so somebody sends me some stream data Yeah. And a three zero two telling me to go away. Let let me jump into clear the confusion. The client is allowed to send data with the request before receives our stocks. Oh, okay. So thank you for for washing away the sand my misunderstand. Who's Yes. So I just want to disagree with one thing you said. That redirects the simple redirects are not simple, redirects are incredibly complicated and dangerous things. So Sorry. I was just gonna say one. What makes the same. What makes them complicated"
  },
  {
    "startTime": "01:26:00",
    "text": "we want to hear it. I forgot to sign my name. Adam rice Google. So firstly, Oh, Do you want to focus on what makes them complicated? Well, okay. So there were, like, in the fetch spec, there were twenty steps to the redirect algorithm. It has interactions with calls it has interactions with authentication. It has interactions with timing, although that's not issue for web transport. It's frequently been a source of privacy and security problems in the past. Now outside a browser environment, I think redirects a much less problematic although a library may find itself somewhere where it didn't expect to be and maybe that's not so great either. So I wanted to mention the security issue with redirects and browsers that if a web page takes a web transport endpoint, as a argument and verifies that on the correct origin. It can then be redirected by an open redirect on the same origin to a Hostile site. So this is essentially across cross site attack where the Attacker passes in Url with with transport endpoint, which appears to be safe. Actually isn't because of an open redirect on the same site. Now open redirects are surprisingly common even in this day age, Google has one. So it's... The open redirect thing is not theoretical."
  },
  {
    "startTime": "01:28:00",
    "text": "Whether you would actually write a page that takes the web transport endpoint as an argument in the Url is debatable. But that's the security issue. That's the reason why websocket hits do not support redirects. Reason two would be that the the fetch standards has twenty steps and it's direct algorithm. But we can't use those twenty steps directly because at the the final step is calls into main fetch. We don't use main fetch because it's for fetching and we're not doing fetching. We only use the obtain connection part of the Fetch it. Then that's that's more about the w. Than the Id jeff end But in the browser, we have to implement the equivalent of those twenty steps for web transport and we can't reuse the implementation and the standard we'll also need the equipment of those twenty steps. How much of those steps go in the and how steps go in W three not clear. The third item is I'm concerned about creeping features. Websocket suffered a lot from this where it adopted some Http features and then people expect it to have all eighty two three features So how we've implemented this in chromium is we use the Http stack to make a web websocket connection. But this is extremely ugly because we need at the end of the handshake, we then need to steal the socket out from the ace to be start to use for websocket. So if for web transport so far, we've not had to use the X to be stack because where not a full X to be client. But I'm concerned if we start supporting more and more H features we will ultimately be in a position where we're re implementing much of H that it doesn't make sense to use a separate implementation anymore."
  },
  {
    "startTime": "01:30:01",
    "text": "So My preference is for dis allow. However, If one option is to do what happened in websocket land. Where the Said V were okay and the... What W said you shouldn't be redirect. What would you think about letting Api surface three contract messages, but not do anything. About it. That's fine. As long as the browser can just stop them Okay I think I am concerned about the risk of of non web clients. Getting tricked into by redirects into doing things they went intending to do. But since I'm working on a browser that's for someone else to worry about, I think. Thank you. Alex. I'm actually currently surprised that I don't think ran into this problem from ask? I don't remember us talking about Because we have sort of similar issue where you can opportunistic send data. David, do you happen to remember us having this conversation and what we decided to hear because it feels like there should have been prior. So part of this we didn't discuss because a lot of these complexities are in a web context and, like, you don't do connection actually p in a web context from a browser, so those don't. But in terms of use let's say you send a diagram opportunistic for a quick and then you get redirected I guess you gotta send it again. We forgot to mention that in the document, but I'm pretty sure someone will"
  },
  {
    "startTime": "01:32:02",
    "text": "I saw you know what... I I I what I said because I forgot to account for that in my presentation. So... You no. We definitely did talk about the fact that the opportunistic might be lost in the context of extension. So I think that's actually covered in mask. So I guess that means my opinion is somewhere between dis allow and clients can handle it. But I think allow is probably not a great idea. Alright. Support. Speak closer to the microphone, please? I what I don't see brings much complexity from the prospects of Api. So I'm wondering what the benefit to are. So probably just was this cost in the previous meetings. So... Yeah. One that fact the benefit. Of supporting. So from memory, from the previous meeting, we we got consensus of some saying either we... Everyone must support them or must not do them, like, that we would whatever we did, it would be a forced thing to avoid and drop issues. But we didn't get consensus on whether we would fully do it fully not. From memory, I think Martin Thompson who's behind you was the one arguing for it. So I'll let him answer that question. But now you okay with martin jump in the queue for that? Yeah what do the other first then. So yeah. If you if you don't have the huge benefits, so I tracked yeah went. Okay. Thanks. That makes sense. Bernard? Yeah. So that I had a similar question in that you know, we have Go away. We have drain So I'm wondering what"
  },
  {
    "startTime": "01:34:03",
    "text": "what absolutely you get from this redirects that you couldn't get you know, seventy five percent of the same thing out of what we already got. And I to go back to Adam's point. We've had a bunch of people who are under the mistaken impression that they need to have a full Http stack to implement web and that is definitely not true, but it has actually thin the ranks of implement because that that mis impression is out there. So I don't wanna feed it anymore by adding more It stuff we don't really need. Makes sense? Remember to walk up to the microphone. Oh, right. Thank you for taking that. Air apple. So I just wanted to support something that Adam said, which is that we also support Websocket and we also use a full Hd sockets because people also came to us and said, hey, it's A. Why doesn't do all things. I wanna do this ugly gross thing over here. And then I want websocket. And you say, okay. You can have two hundred million things. So it's a little bit less painful for us to try to rip out than not socket that we have underneath. But it's it's still a very similar thing of of now you've got this h Gp thing happening at a layer that didn't really originally think that it was fully. So it would be nice to have a very clear reason why we think we want to be a full H stack? On the other hand. Is there really a chance that people aren't going to ask us for that a couple of years anyway. So thompson. Right? Think that I've been convinced by the arguments that I've heard here. I really wanted this to be clear one my my way the other? Now I didn't want the the answer to be sort of Well, we thought about it for thirty seconds and said no. It seems like we thought about it for more than thirty seconds. And this"
  },
  {
    "startTime": "01:36:03",
    "text": "even just the fact that the client can send something and then we have think about how this integrates with with flow control on the new connection and whatnot is enough of a headache for me that I think but the approach that mask has taken here which I have run in front of me, which is if it's not two x x, it's busted is probably acceptable in this case. I would like to see us as long as we fit with the rules calls and all those sorts of other things expose the information from the response from the server and those error cases back up to the client they can make sensible decisions based on that. That means potentially handling redirects, but it means that we would not being a in a situation where we have to deal with all of the complexity of saving all of that data that's come from the client in the stack. And then replaying it across potentially multiple connections with all the I'm mean, I'm I'm sure we can work out all the security things. I'm sure we can you know, work through fetch spec stuff. Maybe we just don't have to, and that's much easier for everyone. And I'm okay with that outcome. Thanks. Arc. Very briefly. Eric, Can you again, I think there is a chance that we can go and work through all of those things, but actually, another example to extra support what Adam was saying. We recently had an interesting privacy issue. Around cookies and socket requests. Because, of course, it's actually, and it has all of those problems. And so I think that is there are several very clear examples of actual real world harm. That is is happening from what our bugs in implementations. But even if we sort through a bunch of these issues, and even if we solve a lot of these, it keeps coming up year after year as oh, we've closed some hole in this area. Oops c Daisy did you forget to do it for web transport too because it's suddenly different."
  },
  {
    "startTime": "01:38:01",
    "text": "In ways that are really hard to to get right. So I I think there's it's not just a, hey this would be nice I think we need to have a very clear need for this. Because it is actively harming people with Websocket right now. Awesome. Thank you. I'm hearing violent agreement we shouldn't allow this because there'll be dragons. My hearing is we should not follow them automatically and then, like, two in free is possible to double f. As to whether we want to specify Api. So I just this. Yep. Sorry. I meant well the loud agreement was not the first option. Yep. But Alright. So does anyone object to us saying we're definitely not doing the first stop ten and the w can decide between second and third because what that means is if we get a three x x would be like a forex x from the F web trans perspective, it just gets sent sent up to to the Javascript. Alright. Success. I think think that's lost of my flight and the next slide is on. Alright. Thank you, Victor. Thanks everyone. It's great to see progress and resolution on such things. Take it away on. Okay. So I I opened this issue. I can look on here. About how priorities work when you pooled multiple transport sessions together. And it came up a little bit in mask this morning. So So the imagine of an Hd three connection, you have different web transport sessions on that connection. And each of those sessions has m different streams, and the application has diligently tried to prioritize the data on those streams. But then how does the quick library managed to merge the prioritization between the different"
  },
  {
    "startTime": "01:40:01",
    "text": "transport sessions. So so next slide has a picture. Maybe it'll be clear. Okay. So you've got two web transport sessions. In one, it has two streams, which I'm just using the shorthand for each Gdp priorities here. There's no requirement that streams work like that. But I think people are familiar with the language. So got two streams one a priority one and when it priority four, not incremental. Another session that has two streams that have the same priority in this incremental So there's different ways that the sending side could could send that data out. So one is you just merge all those streams together and serve them. So that means the party one goes out. All that itself first, the priority three incremental go around Robin until they're done and then the prior before goes out The media that wasn't really what you wanted, maybe you wanted, like, while I have these two sessions and they should be round robin. And so Now I'm alternating sending packets from one session. And the other session, in within those sessions, they're applying the priority. So when web transport session. One gets its turn. It's it's sending All string one and then all string two and when the other session gets its turn it's alternating between It's stream one and three two. You're following this. Okay. Makes sense. Mid sense wanna do the picture. Okay. Next slide. Then there's also a question about what if the web transport connector quest had a priority header in it, then maybe you could do something like, say, well, web transport session one is actually strictly more important than what transport session two. In which case I would want all of its streams first and then the streams from the other one after that. And I'll note that most quick libraries I don't think have an implementation that's written enough right now to do any of these other than look, it's a stream and it has its priority and I will send all the streams according to sort of merge them all together and and it's on global. But I'm not sure that's the right"
  },
  {
    "startTime": "01:42:02",
    "text": "solution. And if you look at what Http web transport H two does, you would sort of get this option, I think. Okay. Next slide. Okay. So what do we wanna say in the draft? I you're a few different options here. Option one. Don't say anything. You get what you get. Like, things will just happen. Don't worry about it. For we are now. Option two we say streams get prioritized globally. Please to be careful what you're you're pulling together and how you set your priorities because if one session thinks priority two is the highest thing and another session thinks priority. One is the highest thing, then you're never gonna get the data that you expect? Three... And if you look at what's in the data in then, Priorities Rf c, it sorta of says, oh, when you connect, you should round Robin in your connect. But it So maybe that is what this is like, okay. All the web transport sessions they you round Robin amongst them, but then stream prioritization is relatively scoped within each session. Although that sort asks like, what how do you prioritize those sessions versus H request streams, which may also be going on the same connection. Or option for is that... We just we use the priority header for everything, web transport connect stream or otherwise, and then the Http or the web transport streams are scoped within only their session. So Some other priority enthusiasts have joined the queue B bernard. Parrot But probably will come up in that and every, which is there's a privacy concern here. About being able to learn about what's going on in another session. That I think we should we should try to Thanks drew It it seems... I mean, I think either way for what you described, you could find that you could understand some things about what's going on on the other session. I think the round robin may be a little bit less leak"
  },
  {
    "startTime": "01:44:02",
    "text": "Just something to think through. Can I just clarify you're saying what you can learn is that don't know that if you ask for some data and then you didn't get it that you then there must be some other session that is using a higher priority stream data? I'm I'm not sorry. Yeah. That's the thing is that I would it looks like I could know that somebody else was using higher priority than the as opposed to the round Robin or I kinda get it it it's just a relative priority of what they do versus me. As opposed to the absolute number. So I think I get a little bit more of a leak in that in the case are you prioritizing your cross sessions? As the w three have an opinion about that with respect to Http request I don't wanna pretend to speak for the W, but I know that the privacy people are to your us on a regular basis. So I I see. Because I I mean, essentially, it's sort of the same time I have two browser tabs open one sent a bunch of requests. Another one sent a bunch of requests and are getting prioritized, but they got... They used the same underlying H three connection then one can think it's the same problem. Okay. Oh, Mac he's getting longer and the meeting getting shorter. Victor I'm service was in front of you Bernard happy actually you fast. Okay. Luke here, I kept the a cut line. Sorry, Victor. So from an application point of view, if I'm writing a web app and I'm in Javascript and I established web transport. I'm unaware of pooling actually happened under of the hood. And I think it is almost like a gun if It could mis prioritize depending on if it was a separate quick connection or not. As in... I think option three by round robin, kinda wanna emulate what would happen if it was a separate quick connection. Otherwise, the application's is gonna have to behave separately if it's pulled or not."
  },
  {
    "startTime": "01:46:00",
    "text": "Can you you clarify how how the application would behave differently if it was pooled versus not pulled Yeah. So if if I assume that There's two separate quick connections. I'm gonna get round robin over the network. Right? Regardless of whatever priorities I set, But if I'm gonna get pulled, transparent, And now my priorities will change depending on what other applications this side to pool with me. So you do need to be aware that pooling is enabled and what I'm pulling with so I can change my behavior. Or web transport needs to just pretend like, we don't know about each other at all. Okay. Thanks. Oh, and the queue is closed. Okay. Now it's Victor. Picture huge priority this was just writing priority right now. Yes. Yeah. I think so for light atf perspective, we generate in quick don't talk about priorities since it's an Api concern, the only way we talked about them as in relation to a ct priorities communicated on the wire. My opinion is the only thing we should say here. Is that The prior Http priority of the connect strain applies to all of this rates within the session? Okay. I I think which sounds that sounds to me like option four. And I think I agree with you. I'm not just like thinking about how the local client is prior deciding how to prioritize data, it is sending but How is a server that has received pooled connections with various priority headers and various streams hanging off of each of those streams We do not allow priorities what is it expected to do? individual data back like, we recently advanced the way you could do set forth by directional strains. This is like entirely an Api concerned currently. But we should clarify such like the priority the connect stream is applies to entire session."
  },
  {
    "startTime": "01:48:00",
    "text": "Okay. I think that means four Understand. I also believe in three could clarify I also believe in free, but that's like not the layers, which way it should. Five. Okay. Okay. We already covered luke so in. Jens what. Tend to favorite option for as it feels like the correct option quote unquote. That being said, I'm not necessarily convinced people end up implementing it. I think you're gonna end up look like I should or something and hope that implementations do it, but it it feels like the most correction option, but the reality is a assuming did three suspect the performance would be fine. I'm not sure I understand the concern about pooling and how you would actually have to change your application if you're pull with someone else I mean, on multiple layers of the network like people can cause congestion and, like, overuse resources and then like star you. So this seems like just another case of, like, you know that's a terrible place and, like, you have to adapt to a wide variety of conditions. But Okay. Thanks, Martin. Know, I think I think look pretty sensible argument therefore and ensuring that the prioritization of this streams within the session scoped somehow. I I tend to sort of think that maybe four is the right answer there. Probably with a recommendation that white transport sessions get the incremental flag. Implicitly attached to them even when that they are explicitly marked as such. Otherwise, we end up in a situation where we might have strict the rules as as defined the ninety two eighteen would basically say that you Just do one session. And then you do the next session. And that's not going work out great for those people in the second session. So That would be a naive for to implement it, but that's kind of what's implied by"
  },
  {
    "startTime": "01:50:00",
    "text": "follow on that part because look how correct know. How is Why are the why these sessions different from Http requests? Well, in in so many ways, And and obviously, we've respect this question. Yeah. I I I think you're probably right in the sense that we probably don't need to say very much, but the... Because as always, the signal is just input to a prioritize patient algorithm that occurs on on whichever end is applying it. So from that perspective, it's not not a huge deal, but I think some advice along those lines would be sensible. That we don't end up in a situation where people apply the same sort of strict ordering somatic would do for ordinary resources to these sessions. And then sort of end up in this weird state otherwise, I think four is just a refinement of three. To allow for expressions of relative priority between sessions. And from that perspective, there's two things One, privacy the privacy thing doesn't matter because putting them all the same connection. And the server the other end sees all of them at the same place anyway. So there's there's no inherent privacy advantage that the other one is it also sees all of these things as well. And has the ability to make decisions about the relative priority of things even at things that kind of a little bit crazy. So if we have ones and eights in the same the same connection. You know, that's their fault. And that maybe they intended for that to happen and if they prioritize accordingly then, that might be a good thing. So I I think is probably ultimately the right answer to. Okay. And just to... And maybe Lucas is also gonna say this, but the the priorities are c does not have a norma of statement. It just says you're using Connect, you it uses a can. You can set the incremental flag. So"
  },
  {
    "startTime": "01:52:00",
    "text": "You can also not the incremental line. Right. I think that's... We could do whatever you want, and we might well ignore it. Because yeah. I think that's that's property. Yeah. Guess. Lucas Part. What I drove the queue I had a clearer idea, but the discussions are completely cloud at all. About... I agree with luke. you really explain about That's why we're here. I I like, it's sucks if you don't know what Cool going on, like, things like all work properly. I think I probably just need to grab bear with Alan and like, discuss this, try come up with something. To hear in my own head and and so yeah. I'll help you out or you help me out. Okay. Cool. Thanks, ed. That's. Have Alright Bernard, any thoughts or should we close out? I think we're pretty close to closing it out, but I'm wondering if there any things in particular that we discussions we need to have for things to prioritize for design teams or something for the next meeting. One thing we had earlier in the group was the discussion about settings versus headers for negotiating the flow site. For... Yeah. The limits in h two. And we decided to not not spend too much time. We could spend another seven minutes on it our do people feel done. Now. I'm getting a thumbs up. Alright. Can you go Perhaps since you're gonna controlling the slides. Can you go back to an Eric you wanna come back up here. Ag Yep. Thanks. And this was slide one while somewhere between twenty and twenty four Okay. I'm gonna go through tell me to stop. So"
  },
  {
    "startTime": "01:54:03",
    "text": "for for folks who walked into the room between them and now, consensus is the room was not to have fixed defaults and to instead have something negotiated. We didn't decide on whether it would be a setting or a header. Do folks have opinions on this, if we can resolve it today that would make life easier. Just say Martin, if you also want to. Articulate this having filed the issue. You may do a better job than I did in these slides. But so the the the question here, I think we said that we we were all not super thrilled about saying that if you want to support web transport, you have to be willing to sign up for some amount. Of data and the question was are we okay to try to communicate that in settings and sort out what that means across different web transport does that make us barf a little bit, and we'd like to find a better answer? Lucas Part. I I put a question in the chat. I don't think I got response sorry missed. But like, it was it settings the only frame based communication of this number that we are considering. Do you want some web transport frame? But has I already been considered and discounted. I could understand why. But I just wanted to no. So where would you put that? Capsule because you haven't opened the connector street. Right. So you'd actually put it as an issue frame. I mean, you got a wait for settings. You could you could send that frame before the settings, you know, like Okay. Well, maybe not actually in h two. But you could send it immediately. It would arrive in the same flight. I'm not Like I don't... saying that is the best option. It's just another in case you didn't wanna the settings thing was weird for some other reason that we. I mean, I was assuming I would be a catch it's a little hard to send that after on your connection before you're. So this is much nicer. Okay. So this at least breaks you out of your single web transport session. Yes."
  },
  {
    "startTime": "01:56:00",
    "text": "Just somewhere other than settings. Thank you. So I think the primary challenge here is from a performance perspective for the client And it's first flight of data on each one of connect requests, so. You you make a connect request and you would like to send some stuff immediately afterwards and you can't anything because without anything here your stream limit is zero. Your flow control limit is zero. You put a stream flow control limit is zero. You've got nothing. Right? You can send a few sort of management things and Open up the flow control for the server. So the service nodding in any sort of bind but the client years. And so having a way to for the server to express maybe just the minimum stream limit and the minimum flow control limit across all of the services that it it might serve would would allow the the clients to start doing real work. Before they got the status code back again. And so I think there's there's value to having that, but that only works. If you have that before, the client even makes the request in the first place. And so I think the only solution that really works me here is is settings. I don't really see much point in having additional frames because everything that we want exists in settings anyway. And we're gonna send it. We have a way to change them in H two unlike an Hd three, which would be a reason to use a frame. And relatively straightforward to understand how they would apply in this context as well. So if we had... What have we got? Three different stream limits and corresponding amount of flow control limits. I think we're your numbers fine session total data stream within session. Give, you these notes strings of different types as well each of your three friends. Off goes. Right? So this two different flow control limits for the streams one for the session. And different types of strengths that"
  },
  {
    "startTime": "01:58:03",
    "text": "the matter. Right? So you have all those five settings that you can potentially pack in there. It's not a big deal. It's like What is it? Six bytes each or something in Http two, it's it's pretty efficient. And would allow you to get out of that. And of course, the service can go. Well, I don't know what the back ends I'm serving a capable of, and I'm not able to even get them to commit. To allowing middle to open one stream, so I'll just not send anything and the defaults can be zero. So I think that works. Which essentially makes this optional and if you'd like people to not have to wait for you to send frames with your response then you can send it in settings. And if you say, o I serve I'm an intermediary that serves so many different back. I just cannot handle it. Then you don't send anything and you wait take I get the george recording like. Yeah. Yeah. First as an individual contributor that's sounds great to me. Now is chair I'm feeling a sense of the room that this seems find good enough. Like, that was... That we put put this in settings and for cases where you can't, you don't for places where you care you do. Does anyone object to this resolution? Give a thumbs up. You get a cookie. Lucas gets a cookie. Martin gets a cookie Alright. Good job. The cookies were outside. Oh. There's there's a bonus that comes with And if they didn't bring them, it's totally eric's fault. you get to write a pull request. Thank you Lucas. Alright. Awesome. And this brings up set time. Thanks You Eric. And thanks everyone for like while resolving a rat hole in seven minutes. We should have these deadlines more often. Thanks everyone for coming and for a very nice web transport session that was both productive and fun. And we shall see you on the mailing list."
  },
  {
    "startTime": "02:00:00",
    "text": "And see based on things progress whether we have an interim between now and July. But if if not, Hope to see a lot of you in person. In San Francisco. Okay. Take care everybody."
  }
]
