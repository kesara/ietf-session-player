[
  {
    "startTime": "00:00:05",
    "text": "adam your driving slides right that's right trying to remember all the icons are here for getting the slide started second from the left adam will get you the repo from the data tracker the second left for me is gallery view uh under beneath your name in the upper left-hand corner ah thank you very much so it's my first time sharing with meat echo and i'm not sure exactly what i'm supposed to be seeing the uh icon is lit up but i'm not actually seeing any slides myself and um we can see the slides so i think you're good to go if you want to turn on video you can adam you probably need to click on the interview and uh the the icon on the on the left on the upper right stuff ah thank you there's the problem okay just a little bit of a learning curve here all right so we're a couple minutes past the top of the hour here so i think it's time to go ahead and get started you're in the pro ball which should come as no surprise"
  },
  {
    "startTime": "00:02:01",
    "text": "right there there we go so we're going to start off with the traditional note well if this is a surprise to you it shouldn't be this is basically saying if you know of any uh patent rights associated with anything that you discuss there are obligations that are described in bcp 79 please go read it and also behave yourself that's basically the the general just here so we have uh peterson andre has volunteered to take notes initially he was going to take it for half of the session but he just very kindly stepped up for the entire session so we're good there um traditionally we have a jabber scribe for those folks who are not on audio but want to ask questions do we have someone who can just monitor the chat in case there's someone who for whatever technical reason can't speak needs to have a question taken to the microphone okay i think i see chris saying yes we agreed thank you very much all right so overall agenda we have um we're gonna start out with uh 25 minutes from ecker talking about ppm and how it's different from what we've done before we're going to go through use cases from a handful of folks and then talk about how this relates to some work going on in the cfrg research group after that we're going to have a bit more discussion around the charter and expressions of interest does anyone want to suggest changes to the agenda we have here all right hearing no objection um just a few notes on this this is a working group forming buff the goal is to make sure that everyone understands what the"
  },
  {
    "startTime": "00:04:00",
    "text": "problem is that we're trying to solve and we're going to talk about uh potential solutions here but we're not going to try to like improve those solutions or anything along those lines we'll be taking questions for clarification after the first presentation that explains the problem and then we're going to have four presentations on associated use cases we're going to ask that the questions on the use cases are held until after the four speakers are done because they're fairly short slots and we want to get through them quickly and those are going to be for clarification and then we're going to talk about as i mentioned previously how this relates to some work going on in the cfrg and then we have almost an hour at the end here to talk about the charter for the working group and general discussion to provide input for the security ads and for the iasg all right so we're going to go ahead and hop on to echo's presentation here so i group you could grab the slides slides but it does not seem to be having any impact so i think you have to do something um see there we go yeah fantastic i actually um despite what it says on the uh on the agenda i'm going to merge the discussion of the things we're trying to accomplish with the discussion of technology um because i think it's easier to understand the motivating cases so i'll just fold them right in rather having a separate use cases uh presentation um so uh um i'll say i sent a link i've written up quite a bunch of interactive material on this so if people want to read that i sent a link in the java chat um um and obviously something is unclear please stop me and and you know ask um so okay let's move on um so let's just start so here's just like the traditional like this is what i'm going to talk about kind of in order um first i want to talk about like the kinds of"
  },
  {
    "startTime": "00:06:00",
    "text": "things like one actually wants to measure in these typical settings then i'll talk about um something called anonymous measurement which is like one approach you might take to it and then i want to talk about um the kind of multi-party cryptographic techniques that are the focus of this um of this buff um and then i'll sort of talk about the technical architecture for the protocol that we sort of developed that are hoping to you know pull in in this work um that's what all aside posting i'm planning to do so there's a lot of situations where like one would like to learn about people right um you know we have like the the census um or you know there's a lot of public research um you know and you learn things like demographics and you know people's income um you know maybe they have medical issues um you know companies want to do um product development so see what features that um you know people use and don't use um how much they use them like are the products not working in some way um and then you also want to take like behavioral measurements like like so um you know say you want to discover like new websites that no one knows about or what information people care about so you can tune your product to be like more like what people actually want um uh so i mean there was a good example of this on the other day um uh um when brave did some posting about like their research engine and how they like want to discover websites for the search engine um so um um so all these problems involve like collecting data um um the information of course is like very useful but it also um is you know can be very very sensitive um you know people often don't want to and shouldn't have to disclose you know their medical issues in order in order we want to learn how many people have some medical condition um you know that's that's good because we want to know you know where where funding should be targeted for instance but we don't want to know what individuals have like you know medical conditions that's exactly this right um same thing is true for your income official orientation all those things um uh someone is unmuted amazed peter um if you could um i know it's not me because i'm not typing um um and um you know it turns out that like not only the things you naturally think of sensitive sensitive but even like much less sensitive data can be very appealing and of course this is like how ad targeting works um and it"
  },
  {
    "startTime": "00:08:02",
    "text": "a lot of like evidence that um you can put like less sensitive data together um this is often called like um uh um high dimensional sparse data sets and figure things out that people would be surprised and so i had this this is you know from an article a few years ago um about you know target inferring um you know a girl was pregnant um by like looking at her other other person behavior um so um there's been a lot of uh um you know uh research on this um and one would hope to do better um and so like the historical way that um you know one does these things is that you just gather all the data and then you promise not to disclose it um and you know this is like um not working out super well um you know data breaches um there's this the famous case of the census information we use for targeting um japanese americans during world war ii um and so generally you better have a system that does not involve just like trusting someone to like handle the data appropriately um so the good news is that actually the data that you want to know is not necessarily sensitive um the data you want to know is usually what's called aggregates so say you wanted the distribution of people's income maybe in a particular region or maybe you want to look at their relationship between like income and height which by the way there is one um or you want to know what the most popular websites are but i don't care like what lists any individual goes to what i care about is like what says people aggregate go to and in fact it's often not useful to learn the websites that an individual goes to because maybe a lot of them are like cardinality i don't like popular um often you need to like slice the data in multiple ways so you say um look i just want to look at a given region or when i compare two variables i want to regress them against each other so in these cases um you know it's as i say it's actually not useful to have um to have the individual data um beyond what lets you compute these aggregate metrics and um and of course it's very harmful to digital data if you misuse it and from the perspective of a researcher not only um you know it's not just a matter of um"
  },
  {
    "startTime": "00:10:00",
    "text": "it's not just a matter of uh uh of harmful but it's a matter of dangerous because now you have to have all sorts of controls and procedures around hand on the data that make it very hard to work with um and of course also it makes it people unwilling to you know share the data with you if they know if they think you're not gonna handle responsibly so um you know while there are situations in which it is necessary to gather full data and then just say look you have to trust me um those are these are ones we should look forward rather than foster um so um i want to tell you a little bit about the kinds of kind of output measurements um you might want um uh there's a number of common measurement tasks um that um that we're hoping to achieve in this working group um so the first is what's often called a simple aggregates this is the stuff you would like learn in an interest s class of you know um you know uh uh single figures group statistics that capture some data so mean median sum histograms i think um then there's sort of like um you know uh relationships between values um correlation coefficients um ordinary these squares um that kind of stuff um and um um you know people talk like federally machine learning that's kind of out of scope for this but um simple stuff is in scope um and um and then there's um this specific problem that's often called heavy hitters which is collecting uh common strings that a lot of people have that turns out to be a very useful technique for a number of number settings um so like i said you can notice it these are all like the aggregate things i don't depend on any anybody's individual data um so we just like to find some way to gather those these aggregates without having to be infected by people's individual data which is like basically toxic waste at some level um so like let me give you like one motivating use case um it's very useful to know what kind of sites users visit um because then um if you're like a web browser so that our backlight use case if you're like web browser you like to know what kinds of sites people visit so you can make your web browser work well on those sites and so you can spend and spend time saying okay but people like really watch a lot of videos like making"
  },
  {
    "startTime": "00:12:01",
    "text": "video where they work is important um now we have like some data on this because we collect like mechanical data uh but like for obvious reasons um it's unattractive to know like what topics any individual is interested in because on this topic some of those topics are you know implicate um information they don't want us to have um and you know it's it's very difficult to obviously know exactly what these people go to as problematic but even knowing the interest people have is problematic basically further things about them and um and so there's been a lot of work and and trying to figure out like exactly what information is sensitive and what information is not extremely difficult because in some cases like some interests are sensitive and in some cases people think they're just not sensitive and so um there's been a lot of talk about this um in the ads context of like what topics are safe um but like in ideal world if you didn't care about privacy what you want to do is like bucket the sites by topic and then count the number of like minutes but on each topic um but like i said we can't just do that but so this is another problem statement is to collect the distribution of time spent on each type of site without actually seeing the um individualized people on um so that's like one motive in this case another one of the use case again for a browser um is to see which websites are having problems of one kind or another um in some cases these problems are um i guess innocuous um so like web compatibility is a big problem some sites just don't render properly and like mozilla operates like a thing which you can press in the upper right hand corner of your browser somewhere that says like this site is broken from me um uh but like but we depend pretty heavily on people volunteering information because we obviously don't want to collect what url everybody's going to um this is like a bigger problem for surprisers with smaller market shares because things will often work on like one engine or not another um so in many cases we can detect breakers on the client we know that something's wrong like they try to use a property we know it doesn't exist or the user is saying reload constantly like rage clicking but you can't do anything about it because like the browser knows but it can't tell us um and so that's one example um another example is um that we know there's a lot of like what's called fingerprinting going on so a lot of web"
  },
  {
    "startTime": "00:14:00",
    "text": "tracking happens with um cookies but there's a lot of which happens without cookies and um and so what you do is you like have a bunch of javascript apis and you can measure how the browser behaves under the job sheet apis um so an example people often talk about is what's called canvas fingerprinting where you like render some fonts and then you read back from the canvas and that gives you information about like the gpu that machine uses machine so you can use this to build up a like single value which you can use use like follow the user on the internet um so this is like a big front tracking that is not addressed by the kinds of you know third-party cookie blocking the browsers like firefox and chrome do um this is again off detectable on the client because you're like why is this person like doing a lot of canvas read back so when like they're not actually displaying anything meaningful and or like they're actually loading some some some like script which you know is a tracking script um but you just can't like report it back for exactly the same privacy reason so we're stuck in the situation where where if you have like the sort of accurate information about which sites are having problems you could just think about it could you go to the site and you could like download the script and find it yourself um uh but um but doing that would entail collection browsers are still very problematic um one thing i would say is people often say like why don't you just do a scraper um and you certainly can do that sometimes um but there are two problems one is building a scraper that collects that much information is very expensive and the second is that it's very easy to detect when someone has a scraper and if so especially in these um in these uh uh fingerprinting cases they could just send you different different data there's a lot of fingerprints um so again the problem statement is to collect on the sites where the client is seeing some issue but only to see the hot ones and only to see and not see individually um you know what you just said you're going to but just to pick the sites that are most problematic in one way or another um and i'm just going to preview on the the rest of the people here were talking about um a number of use cases i guess i i hate this first one so i should have like um remove that my slides got changed um but there's a bunch of talk i think other use cases involving but some advertising work and also some work on um covenant exposure notification measurement um so um you'll see those"
  },
  {
    "startTime": "00:16:01",
    "text": "later but i mean this should give you a sense of breath the kind of problem we have here which is all kinds of sensitive measurements you want to collect um that uh unfortunately are difficult to collect with pride over privacy with like without fancy technology off the countertop develop here um so um it's important to recognize there actually are two kinds of privacy threats with these kinds of data collections the first is when you collect sensitive data and it's directly tied to identifying information so you say look i like you know did a survey and like i called people on the phone and they told me you know they tell me this sensitive stuff and now like i have the phone number and like you know they told me like you know whether they have a particular medical condition um so that's that's like a direct tie right um inside um and then there's but there's also this thing um what's called um uh basically this goes back to um this high dimensional data sets is often if you collect enough non-sensitive information um and then you collect one piece of intensive information you can use the nonsense information to figure out who the person is and so there's this like famous observation due to latin sweeney that like if you just have five zip code gender and date of birth you can identify like most population united states um and so um generally and there's like was a bunch of bunch of work also but um by um uh our vernerian um about this netflix data set where they're able to find like people's uh you know who people were by the netflix netflix browsing history or viewing history so um we have to solve both these problems um so the naive thing that people often talk about doing is basically what's called anonymized data collection and this is absolutely a viable technique and there's a bunch of work going on about it um in ohio working group and the basic idea is to strip off all the identifying that information and um and so what the client does the client like and it encrypts um the data to the collector and then you have some proxy in the middle that removes all the metadata like the ip address um and so this this avoids the collector like seeing that meta information but still gets and because that is encrypted the proxy never sees the report so that these things are split up um and we've"
  },
  {
    "startTime": "00:18:00",
    "text": "talked about the trust model for that so i won't go into that in much detail um and so the number of ways to do this you can do this with liquidational proxies like mask or ipsec or 20 or you can do an application proxy like oh hi um so like this is a very good technique for a number of cases it's really good for like sort of boosting the privacy of semi-sensitive data like data you collect anyway you say well like i wish it had the ip address you can get rid of it and so it's very common now for like browsers to collect telemetry and we have the ip address which you just throw away and we agree not to have that involved um there are also a bunch of cases where um you want to collect individual values um and these free form data blobs that you want to really dig into um it's also like the only way to do things that need an answer so if you like actually want to have not just like data collection data collection but you also want to have um data reports like you can't do that with that you can you'll hide for that um and the techniques we're talking about don't do that um but there's a bunch of cases where it doesn't work well um and and the most common cases are um you can't it's not good for like these high dimensionality data sets where you need to like take multiple values and put them together and um and the problem is that um goes back to what i was saying before if you give me like even the innocuous demographic information i use identify the person and so if you give me this data set that conversions consists of you know of like your zip code and your birth date and your income well i can map that back to i know your income so i can't do it so i can't use this if i want to do those kind of correlation because unfortunate demographic information itself is fine um and and the same thing is true if you want to do like cross tabs inside and sub groups um it's also not good for collecting um this kind of heavy hitter stuff and the reason is because the the even though it is anonymized you get all the values even the low cardinality values and a lot of the cardinality values are problematic so they might be for instance you know um you know google capability or else um so the good news is over the past 10 years we've done cryptographic because of all these problems um and um there's a bunch of fancy crypto which i'm gonna really only only vaguely sketch um but the basic idea is a multi-party"
  },
  {
    "startTime": "00:20:00",
    "text": "computation so um you uh the the what the client does is the client um wants to report some value and it takes the value and it splits it up into two two shares with a secret sharing technique of some kind um and the way the shares are constructed is that knowing only one share doesn't give you any information about the other about the and that that splitting is what's called information theoretically secure maybe it doesn't depend on computational assumptions um um but um when you put the shares together they of course represent the entire value so what you do each client sends like one share to one server another share another server and then the servers take the shares themselves and they aggregate them they compute the aggregate value but again you're just working on the partial data so you're not learning anything and then you could take the aggregate so like say for instance you want the sum you've got a partial sum and then you take each partial sum and you put them together and you get the final output and so the key point here is that you can do all this work um you know without ever having anybody see any anybody's individual value um so let me just pause for a second before i talk at the crypto which is the trust model um because this is like really important i know this comes up a lot we talk about these systems so um the client's requirement is the two servers do not collude um if it's a service clue they can be digital values and it's game over right um and um so um this is very hard to operate between people obviously and and the client has to trust exactly one of them um the client is great the client trusts both but as long as one of them doesn't cheat it's fine and you could do n servers but two is the most common number obviously the servers also have to for various reasons do a little bit of enforcement about like minimum batch sizes and query limits and stuff like that to avoid some attacks that we will talk about here um for the collector's requirement both servers have to actually be executed protocol correctly because either server can like distort the results that they don't but again this is only a correctness requirement um that only one server is required to behave correctly from the client's perspective um so i just want to recognize right up front it's like difficult to verify from the client's perspective that the servers aren't colliding um that"
  },
  {
    "startTime": "00:22:01",
    "text": "conclusion could happen through side channels um uh depending on the architecture sometimes the sideshows are small signs are big you can do point in time audits to verify that someone is behaving correctly but you can't uh but it's like not possible for them you're not colliding um and i just want to like so i want to highlight that um but i also want to say that like this is like a very very common scenario on the internet where you where people have data and you have to trust and behave correctly i mean if you think about like you know your data is in gmail like you know google has like your entire email record right and you're pressing them behave correctly um and you know even you know even if the software's running on your machine like uh you know um generally people think of that as behaving correctly but like your ability to verify the software running on your device extraordinarily limited and so like while like it would be great to have a situation in which you never had to trust anybody that's simply not the situation most of us find ourselves in so we're talking about here is trying to like alleviate the situation of trust people and make the number of people have the trust smaller or the number of people have to cheat larger i guess um we're not talking about a limiting trust entirely it's not possible the state and our technological development and so we're trying to improve the situation but we're not trying to like boil the ocean um so um i want to talk like very briefly about like one cryptographic protocol to give you a sense of like the situation um this is sort of the one that started it off it's called prio um and it's useful for computing like numeric aggregates like sum and mean that kind of thing and like this is like the one that's going to most apprehensible um and like we're going to punt the crypto to cfrg but this one is like understandable like normal humans so we assume each client has some value like the numeric value and like called x of i right and so the client does is the client splits up that value in the following way it generates a random value um sorry about the fancy math but basically a random value is smaller than a prime and then it basically does it sends server one like the value minus the random value module the prime and it sends server to the random value it's like you see it's quite easy to convince yourself that if you know that oh no the random value is not enough and knowing and knowing um that the subtraction is"
  },
  {
    "startTime": "00:24:00",
    "text": "not enough and that is sufficient so now each server takes all the shares i get from everybody and they add them up right um and again like because because because because these are like information theoretically they're not only anything and then they and then they basically exchange the exchange the sums or really like one sentence on the other probably and then if you take the sums and you add them up um and you like do a bunch of like much like relatively simple like you know middle school math you could just convince yourself because addition is commutative that when you add the sums up you actually get some of the initial values um so mission accomplished with all this all this computation zero knowledge and we have the output at the end um so this is like this seems like really boring and like kind of obvious um and but it's actually fantastically powerful and the reason is because there's a lot of things you can actually compute with just the sum thing as long as you encode the data properly you can compute all kinds of things as sums so like arithmetic mean is obvious that sum divided by count product you compute product by doing some of the logs geometric means coming from product you can do variance and standard deviation by computing some some of the values of the squares there's a but there's a bunch of fancy stuff for doing like or and min max and even ordinary obviously squares so the trigger is just finding the writing coding and there's like papers now about how to do all this stuff um so one problem that people really say is like what about bogus data right and there are actually two kinds of focus data there's plausible data but it's actually false so like i'm like about 175 centimeters tall but maybe i save 180 right um this is like a problem with any surveying technique unless you qualify yourself and you just live with it you have to live with the noisy data right because people don't lie consistently um and then there's like consistent completely ridiculous data or i like say like i'm a kilometer tall or worse i say like i'm negative kilometer tall right and ordinarily what you do is you just like have some filter mechanism where you said um you know uh you just said like well i i i just reject anything this is a kilometer tall right and but like with the pre of it is encrypted you can't do that right and so um uh um and so instead what you do is like"
  },
  {
    "startTime": "00:26:01",
    "text": "and this is the fancy math part each submission comes with the zero knowledge proof of validity and the proof says something like this height report is like between like 100 and 200 centimeters right the servers work together to value the proof and you only aggregate the submissions that have valid proofs right so like you have to trust me this part works but like um this part works um um but it's important remember that this part believing this part works does not we're not necessarily required for bullying privacy if the privacy claim caused the previous things um uh the youtube with zero knowledge please don't pick the data um so okay so like going back to my use cases right um uh say i want to collect these user interests right so basically what you call user interest thing like preo is that every user interest is a bucket and you have like i don't know 100 200 500 buckets right and the client individually reports time spent in each bucket you have to report the ones that are zero too by the way otherwise you can just look at which buckets are reported and then you use prior somewhat and you end up with a bunch of sums one for each bucket and now you know exactly how much your time was spent on each bucket for each bucket but you don't anybody's individual time spent um and as i know you can oh you can also report t squared you could be standard deviation as well so um this is like a pretty straightforward application or something like preamp um but there's like a whole pile of use cases that basically come into this um okay um so um even more fancy is a protocol um called heavy hitters um well actually they didn't name it so like we've been calling it hits um and the idea so the idea is that like each client submits a string um like a url and you want to output the most frequent strings and i'm gonna like very very aggressively handle this but the basic idea is that the servers can join the compute the number of strings at any given prefix p and so what you do is you start with like basically you just do binary search so you start with like strings start with zero and you say how many strings are zero or start with one and then okay fine now i'll keep going down the tree until i've gotten down to whatever threshold i want to and so you can just just find all the values and effects all the important values and basically log n tries um and i will not even"
  },
  {
    "startTime": "00:28:00",
    "text": "remotely attempt to explain how this works um i you know i understood it for about the amount of time i understood quantum physics when i was in college um um so but like um again this is like walnut math at this point um so going back to my use cases again this you can just have to like this for conducting broken sites right each client creates one report for each type who's broken or which is fingerprinting and they use his determine on top sites and it's just and the servers just spit out the list of all the important sites but you don't learn who reported them and you don't know not any sites that aren't important because the servers just won't just in the tree once the cardinality is below a certain value so um so again this is like an example how you can use this for like a real world application like we actually really care about um so um i i do one thing i want to like flag is um that there um is a there's an issue course called subset query um which is um uh that so submissions could i just giving you examples or something we're just giving you submissions that like have one piece of information but you can also tag the submissions of the demographic data because like birthdayers of code results and these are talking about right and those get passed on all the way to the aggregators right and this is like this is notionally safe because you say that the non nonsense information is safe or you you only click nonsense information and then they owed it as encrypted but then you can say okay computing aggregate over the subsets um so that's like a very powerful today that's one reason why this is like a powerful technique and ways that like ohio is not um but of course it means that repeated queries can be used to reduce values by like querying for the subset like includes them and then excludes them um there's defenses against this on having minimum batch sizes anti-replay um randomization for differential privacy um this is a piece of work the working group's gonna have to work on that's part of why this is completely done uh but i think we will understand how to do pieces of this um for for some some measurements and then other ones that you need some more um so what is the state of the play here um a number of us some of the people you're representing have basically developed a generic protocol um that is designed for"
  },
  {
    "startTime": "00:30:01",
    "text": "doing privacy pure measurement what i mean by generic is that it's a framework protocol that then you can plug in um uh you know individual cryptographic technologies um so it's compatible with the basic what these things called verifiable should be application functions which we'll be talking about um but um initially it's tuned to work with uh prio and heavy hitters um it's built on top of https it's going to smell a lot like you know any rest kind of protocol like acme or whatever you've seen before and so it's easy to implement with physical services infrastructure and and other people working on this like um you know having work from regular infrastructure to design and work well with this um so the um um so let me just give you like system architecture picture um that's what it looks like you know there's a bunch of clients they send their shares and there's like a leader and and i'm the leader like basically takes the shares and parcels about the helpers and orchestrates the whole computation and then the data is like sent to a collector and so the clutch because i look give me like the output for this this subset of the system um and then it splits the results out right and so one way to think about this um i think there's a number of different playing modes it's compatible with um one of which is that you know the collector and the leader are the same person and they're trying to do it the data collection and they outsource the helper job to one other person so that they can make guarantees about the privacy of the system um another possibility is the whole is a whole like leader collector helper box is like a service that's provided people um the trust model here you'd be assuming is that the clients have to know what the helpers leader are and so the clients know who the who who the data is being encrypted for and so they can make their own assessment of whether or not they trust one of those people um though of course in a real world scenario what's most likely to happen is that the clients are deployed by some by the collector effectively and the collector hardwires the leader and helper and so the the user of the servers can inspect who those people are but it's not like they actually choose them in any meaningful way um so um you know people are actually going"
  },
  {
    "startTime": "00:32:00",
    "text": "to ask like what like the situation oh hi is um because oh hi as i mentioned is useful for many of these kinds of settings um these are complements and not substitutes so um you know good cases for ohi are like as i say sort of standing sensitive data this kind of rich freeform data that like you couldn't really you know aggregate this way um um anything of course it needs any kind of responsible right because like none of this technology gives you a response that just measures um good cases for ppm are like really sensitive data um hopefully simpler data because like as you sort of find any idea it's not like really easy to like do really complicated stuff um and settings where you just kind of do drill down you do regression or you do do subset correspondence like kinds of things um um so like these are like you know these are two great hits that tastes great together right you can use ohio talk to ppm server that's like boosting the privacy of the system so like you say okay well i do want to collect this data but i actually store an ip address so you can remove that as well um and so um in fact i think you'll see you'll see some of the um uh some of that like later talks um you can also use um uh sort of a front-end proxy server um to do a bunch of like um you know kind of misuse detection of like you know spamming attacks and stuff like that so um so i think these are a complimentary techniques not but not not competitive techniques which is why you see some of the same people working on them um i don't know why oh yeah right um i was like why do i have two more slides so i'm now done i think i hit my target time target quite well um i have a little time for questions which i'd be happy to take oh i did want to say one more thing um but while i'm waiting to see if anybody will say anything which is our assumption is the vdf functions we standardized or are defined in cfrg not an itf and so the idea is we'll build the framework here and we'll defer the work by defining the v dash to cfrg and that work is already probably going on or is being is being presented as safety yes boss thanks uh good presentation i enjoyed um"
  },
  {
    "startTime": "00:34:01",
    "text": "you laid it out really well the one thing that i'd like to hear more about is sort of the deployment scenario for example on slide 14 don't go back you you specifically said the client wants to report some value right so would your expectation be that uh application authors and and servers would sort of make use of the same way that sort of ohio is considering being deployed by you know various organizations to get into stuff or that you know doe is being used for if for everything you know dns i mean how do you where would this be deployed because the client actually if if the client is a user right they're clueless so yeah absolutely yeah i think so i think i think the most likely settings for this initially will be the kind of like um kind of measurements that like people are already taking via the software disseminate so um you know things like browser telemetry um uh you know um uh i think um the the case of the case of the the um uh tim what we talk about next involves um measurement of covert exposures um and yeah those things are all being done like sort of sort of like um you know automatically um you know potentially by asking these are automatically by the software i think there's some possibility that in the future um you know you'd see like this used for surveying for direct surveying where you say okay you know are you willing to participate in surveys and then we'll we'll write down the client and you can do it but i mean like you know the the uh i mean i think yeah you know these are pull these are pull techniques not push techniques fundamentally um um and i think you know obviously cryptographic is so complicated that the user has to involve some pieces of software to do it do you have a use case today that that should not be used with ohio that this would be a better for that's like already in your head for you know i want this now yeah yeah um so i think so so i think uh um the uh cert so certainly the um uh the use case of measuring sort of misbehaving websites um it basically cannot be cannot be contained with ohio because you um because you learn the uh um"
  },
  {
    "startTime": "00:36:00",
    "text": "you learn a bunch of the uh you don't want to learn any of the little cardinality sites those are very dangerous right um because if you like collect like every url somebody goes to if this is behaving um then there's a real probability that you learn a bunch of like say google docs um you know capability urls or you learn you know you know some document quality this is like my plan to buy company a you know um um so yeah so it's basically like anything involves that that kind of like string data collection is very problematic without high um also even like the cert this sort of bucket eyes um you know uh um you know give me counters that would be 80 80 counters like really problematic with the high um because basically if you decide to get every single value and report it separately otherwise you end up a situation where you can do these high dimensional data sets and use them to do to do production so like those are both cases where ohio does not perform particularly well though the second like you could maybe get away with but it's probably problematic andrew just my way of clarity since ohio has come up quite a lot uh um in the chat with the new presentation um uh where you said complimentary there's no dependency on yeah not at all thank you okay chairs i i i'll hang around but like i think then we're done right yes thank you very much um so just for avoidance of confusion the the mozilla use cases were bolted into accuracy that's correct yes the case i mean i didn't cover everything we're just doing but those two here's what we're doing yes right so we're going to roll into uh tim now if you go ahead and grab the slides excellent oh the song slides oh i thought excuse me i thought you were going to put the slides up i can i can run them if you want me to or would you i'm not sure how to upload them in miner nice and simple anyway thank you um okay i'll dive in and release oh i've got it there we go"
  },
  {
    "startTime": "00:38:04",
    "text": "all right thank you um okay so let's get started uh my name is tim and i'm an engineer at the internet security research group uh we're the non-profit that operates the let's encrypt certificate authority um not to be confused with the irs g incidentally uh the dilemma that uh mr riskorla just introduced in which the essential function of gathering telemetry from the field introduces significant privacy risks for users is of great interest to the isrg given our mission of reducing barriers to secure and private communications on the internet um next slide please so echo covered that you know the ways in which telemetry is a privacy risk for users and the you know the implied benefits to users of um using these new technologies but i think it's worth noting that uh many data collectors also want to do the right thing and respect the privacy of their users besides that being a decent thing to do the large amount of personal identifying information stored by conventional telemetry systems is a significant liability for the data collector there's new privacy regulations emerging in various jurisdictions all the time which require expensive and complicated controls around user data and all that pii makes for a very very tempting target for attackers um so we have these new tools like prio and heavy hitters but uh even those organizations with pretty formidable engineering departments who have the wherewithal to explore these techniques are in kind of a jam because you need an external trusted partner to execute these multi-party protocols um so isrg is interested in providing private measurement aggregation as a public service to the internet for a lot of the same reasons that we built let's encrypt we want to make it easy to do the right thing uh next slide please so we envision running a standards compliant aggregator as a service with the same focus on automation and ease of integration that drive let's encrypt we expect that uh some customers will want to run their own aggregator and have it"
  },
  {
    "startTime": "00:40:00",
    "text": "work with ours but others will want to avoid running any servers at all and will instead choose two existing aggregators say one run by isrg and one run by some other organization or a company that chooses to participate um so in support of that we are hoping to provide an open source implementation of a ppm aggregator with the aim of making it easy for some data collector to interoperate with isrges aggregator or anybody else's so hopefully uh it ends up being a matter of grabbing a container image from some public registry you deploy it into your existing you know kubernetes cluster or whatever you have and you can start gathering private telemetry cheaply and easily um somewhat akin to how the eff cert bot can be easily deployed alongside your existing web server um to have it manage let's encrypt tls certificates we are also aiming to provide open source client libraries targeting like a variety of languages and frameworks uh chosen to you know facilitate adoption for the most likely interested parties so you know you can imagine like a swift sdk for ios apps uh javascript for web app for a single page application on the web and so on um so right so with this goal in mind an open standard through the ietf is acutely valuable because a proprietary single vendor solution wouldn't be terribly useful since the privacy guarantees are contingent upon independent and non-colluding aggregators um next slide please thank you so we talked a lot about you know the benefits of these new technologies but uh there are some trade-offs some drawbacks to these systems so for one thing there are more servers involved so it's more likely more likely to fail uh somewhat necessarily second the verification of the proofs that uh introduced earlier do introduce some computational network overhead and in particular we'll introduce depending on which which uh protocol which vdaf is in use will introduce uh potentially multiple rounds of communication between the aggregating servers uh finally"
  },
  {
    "startTime": "00:42:00",
    "text": "um metrics gathered under these schemes are these skins are necessarily less flexible than conventional telemetry systems uh you can't make arbitrary post-hoc queries sorry you can't make arbitrary queries post-hoc against your corpus of data you have to know up front before you begin collecting any data what are the aggregations you're interested in computing this has to do with the construction of the proofs as well as enforcing some of the privacy guarantees of the system um fortunately you know in spite of all these challenges we do have some evidence that this stuff actually works and at scale so in december of 2020 a collaboration between apple google uh we at the isrg the linux foundation public health initiative the mitre corporation and the national cancer institute at the national institutes of health in the united states uh launched the exposure notifications private analytics system and this is the back end to apple and google's exposure notifications express um which is a system of course for covert exposure notifications so uh so en explorer verifications of course is the system by which mobile devices can sort of anonymously exchange with each other to a cove exposure enpa allows back anonymously and privately back hauling that data to your regional public health authority so that they can get information on how many people are getting the notifications how many people are on their mobile devices how many people are interacting with them uh and all sorts of interesting metrics about the spread of code itself as well as the effectiveness of the of the en system so this is currently deployed in 13 u.s states and the district of columbia and at the moment it's gathering 2.1 million measurements per hour uh we also heard last night if there was one more interesting number that uh sometime over the night we gathered uh 12 billion individual metrics that have been aggregated since the system launched um we're also about to deploy this internationally so uh beyond the united states so we're hoping"
  },
  {
    "startTime": "00:44:01",
    "text": "to soon turn this on in uh four states in mexico okay i'm already well over time so i will see the floor thank you thank you very much tim all right martin you're up next do you want to run your slides you want me to that was kind of broken up i am sorry is that coming through better now or we i can hear you now yes thanks yeah it takes some time uh where are we i have to thank the people who provided the preview on selecting their slides because all of these slides have the same titles okay so um i'm going to talk very briefly about some of the use cases in advertising specifically the conversion measurement one i think charlie's going to follow up with some more details on on this one so um conversion measurement is something that happens on the web quite a bit when when someone shows an advertisement it's kind of nice to know if that advertisement is having the intended effect and so the goal here is to measure how many people buy the widget when they saw the ad and how many by how many people buy the widget without seeing the ad how many people see the ad without buying the widget all those sorts of things and combinations of of those things for different uh campaigns and and whatnot uh this information flows to both the advertisers the ad tech companies and those people who who actually sell the inventory or put the advertisements on"
  },
  {
    "startTime": "00:46:01",
    "text": "their websites so key question here is is the money i'm spending on all this advertising actually having an effect on me making more money so the way this works today is pretty simple we assign a user an identifier so everyone gets their own unique identifier and every time they visit a website and an advertisement is shown or they do something then we just create a little little log record that records all the details from the context and the user identifier time stamps all those sorts of other things and then you look at the log and you can answer all sorts of questions about what people have done it's um it's really great for getting the information that you need it's all quite precise and leaving aside all of the complications of anti-fraud and all those sorts of other things you can answer all the questions that you have fairly precisely however that doesn't really respect people's privacy and so the idea behind some of the efforts uh that we're talking about here is to produce aggregate statistics about conversions uh without relying on user-specific logs that means you get counts rather than individual records uh the current status of this work is that there's lots and lots of requirements that are coming through this is obviously a lot more complicated when it comes into practice there's lots of ideas people have all sorts of wonderful proposals and some competing requirements but a lot of the really promising ideas include something like what ekka described earlier"
  },
  {
    "startTime": "00:48:00",
    "text": "and the simple example here is probably the one that is easiest to understand you have an event that occurs someone views an ad you have a another event that occurs they buy an item as maybe that's connected to that particular ad and those are two of independent events and you can imagine those events going into some sort of opaque box that's operated by say their browser or something like that and you add up some numbers and maybe you add up a one if there was an ad shown and the person bought the thing and if there was no ad shown you add up a zero you get those reports from thousands and thousands of people and feed them through a system like brio or one of those other things and you get back a count of the number of people who saw the ad and bought the product and that allows you to make some uh conclusions about how you how your business has been operating and the advertising campaigns you're running obviously this gets a lot more complicated but uh we'll let charlie explain some of that any questions on this part okay so is that is that the end of your presentation that is it okay let's go ahead and bring uh charlie on then charlie do you want me to do your slides you do want to run them uh i think i can run them let me see ask to share slides and let's see if this works great you guys can see it"
  },
  {
    "startTime": "00:50:00",
    "text": "yep okay great hello everyone uh my name is charlie harrison i'm a software engineer working at google uh looking at ppm like solutions for doing ads measurement on the web uh to satisfy similar use cases that martin was just talking about um so i'll kind of breeze through some of the background just because there's a lot of overlaps with martin's slide but i i think the gist here is that here at google we think third-party cookies are not great for users uh users privacy um but they're kind of right now like critical infrastructure that powers online ads um for exactly the reason martin mentioned uh and the the problem that we're trying to kind of grapple with is whether we can build something like a third-party cookie alternative that gives users good privacy while still kind of supporting this like critical infrastructure to some extent um and i think the key insight here is that um at least as it relates to ppm is that like many ads use cases are actually like totally fine with aggregate data and they don't actually need to track you kind of around the web and learn all the data exactly um we we could be fine with aggregate data in many cases uh so i i want to go over just in a little bit more detail how attribution measurement which is also called conversion measurement um happens today with cookies essentially uh and i'm sure this is going to be familiar for many people uh what happens is that you typically have an ad tech site that is like running on a website and that drops cookies on uh sets cookies that are readable basically from anywhere from any website um this allows you basically to keep track of like a mega identifier sort of"
  },
  {
    "startTime": "00:52:01",
    "text": "which is this third party cookie which is uh tags the user or the user's device with an identifier which is readable from multiple different contexts so you'll read this cookie when an ad is placed and you'll also read this cookie when there's a conversion like when you buy something later on down the road after you've seen an ad for it and so right now this cookie id is used as the join key to join these two cross site events right um and but they they they join arbitrary events so like all of your browsing can be linked up to this one cookie in theory uh how could it be improved uh we could internally join this data in the browser um so when you see an ad we could register something in like custom new browser storage uh and when you buy something that was like pointed to by that ad that would join up with that with that event in the internal browser storage and you could uh have a communication path from the browser to something like ppm where we we you know we have this data we want to compute something like a histogram maybe we want to learn something like you know what are the counts of conversions like purchases per ad campaign so the x-axis is ad campaign the y-axis is the number of conversions we can we can use ppm to kind of uh generate data uh data share split that encodes that histogram contribution send that up to ppm and the ad tech could uh uh learn aggregate statistics like just a histogram um of of the uh of the counts and this doesn't reveal any uh user data directly it only reveals aggregates um so there are a whole bunch of like cool use cases to think about um i know there's a lot of talk in the chat about um differential privacy and this is a"
  },
  {
    "startTime": "00:54:01",
    "text": "something some formal privacy that we could add within the ppm system to ensure that the output of ppm is private um so that's something we're looking into um there's a lot of interesting research that's related to kind of the heavy hitter stuff about how do we report these histograms when they're like really really really big like you are running millions of campaigns or you want to do like all sorts of different crosses um we're uh really interested in uh systems that can help train machine learning models in some cases there's there are results that show that even just aggregate histograms could be used to train like logistic models but like are like we're looking into more sophisticated mechanisms um there's uh uh you know in rather than conversion measurement we could look at reach measurement which is asking like how many distinct users saw my ad across many different websites so this is kind of like removing a uh like remove duplicates operation um and you know we we're we're interested in exploring like you know we have this conversion measurement thing but maybe there's something that's more generic um that we could use for like a more basic browser primitive uh and i think i think that's my time but uh yeah happy to answer questions after the uh everyone else is done thanks very much charlie so now that's the end of the uh the use case uh presentations so we're going to open the floor briefly to anyone who wants to ask about any of those use cases okay so i think we're going to go ahead and move on to richard now thank you"
  },
  {
    "startTime": "00:56:10",
    "text": "so i sent a request to share slides ah there we go yeah there we go sure all right so yeah this is this is pretty different so uh ecker talked about protocol we had a couple talks about use cases um i'm here to talk about kind of the substrate and the the label we are using for this is verifiable distributed aggregation functions thanks to chris patton for that that name context here is a like many things in the in the ietf we need some complicated crypto for this and so we're doing some parallel work uh in cfrg and it goes alongside the crypto the protocol work in the ietf to kind of uh to specify the the complicated crypto bits in a place where we can get cryptographers eyes on them as opposed to just protocol nerds um just highlighting some some parallels here uh tls depending on cryptographic primitives that cfrg defines mls relies on hpke which you did in cfrg and so for this ppm work we're defining this vdf uh or vdaf depending on how you want to pronounce it uh that defining this as the kind of cryptographic abstraction that um that ppm relies on in cfrg now if you look at the draft it's kind of in two parts we define an api that is the abstraction ppm is supposed to rely on so the idea is that we have multiple instantiations that do various flavors of this private aggregated measurement dance that all behave in a close enough way that we can define a common api over them and build protocol around that api and this provides a way you know that we can build a protocol without having to"
  },
  {
    "startTime": "00:58:01",
    "text": "care about the details of the cryptography and provides a target that cryptographers can look at and design new schemes and if they plug into this api then they can presumably be used inside of ppm with uh you know more quickly and more more easily than if they'd been um designed kind of completely from scratch using a new not not reusing that construct so what i'm going to talk about here is mostly the api and i'll talk a little bit about the instantiations um at the end um so is ecker outline uh so just briefly to define like what we're actually doing here so the idea of this vdaf is to kind of capture the critical things we need the cryptography to do inside of ppm so kind of working back to front the aggregation bit is the idea that you know the inputs to this process are individual measurements and what comes out of it at the end is an aggregate measurement over those individual measurements distributed in the sense that it's distributed among a bunch of aggregators you know that the computation of that aggregate is distributed over those aggregators and the privacy properties of the individual measurements are assured by non-collusion among those aggregators and finally it's verifiable in the sense that the aggregators can check that the inputs have meet some properties um as ecker pointed out you know there's risk in this sort of uh distributed scenario that the measurement results that get reported in by uh the measurement points could be corrupt it could be invalid um and they that could flow through and lead to a garbled aggregate so we will have some checks in here um that um allow the aggregators to verify that the measurements they're getting are meet some definition of correctness now which exactly which definitions you can apply varies a little bit by the instantiation um so looking at how this kind of plays out you know this is the kind of classic measurement scenario this is how a lot of telemetry works today where you have a client out there um you know"
  },
  {
    "startTime": "01:00:01",
    "text": "actually many clients but i'm only going to portray one on this slide you have many clients out there collecting uh measurements of something you know where people clicked in an application or what what not and reporting that back to a collector and the as you know you've seen various takes on this on the last few but the idea here is that we're introducing this aggregator tier in between the client and the collector and the idea of these aggregators is that the client shards their each measurement out to individual shards of the aggregators such that you can only make sense of that measurement if you have all the shares that's how we get that non-collusion guarantee the aggregators then go through this process of what i've called prepare the term we're using the draft is preparing the measurements so that's that includes um verifying that the measurements are correct are acceptable that they're correct um and you know transforming them into an aggregatable encoding perhaps that's necessary in some realizations um so once you've kind of taken the individual measurements and prepared them you aggregate them um this is kind of where you do that step that um echoers over you add up the shares into a share of the sum and then finally the aggregators send their aggregate shares over to the collector who unshards them to get the final results so this is kind of the data flow view of the same thing um i think the interesting thing here from a kind of protocol point of view is you notice there's a kind of back and forth thing at the preparation stage there is a need for some some chattiness some interaction between the aggregators in that verification process to enable um a distributed verification of the correctness of the input shares um but otherwise hopefully this kind of explains the overall process also this um the kind of little gray squares that uh parallel to the output shares are other and meant to indicate other output shares the idea that an aggregate share"
  },
  {
    "startTime": "01:02:01",
    "text": "represents the aggregation of the output shares over a batch of measurements and as as was pointed out before it's up to the aggregators to do things like enforce minimum batch sizes define what a batch is so that's kind of the data flow um this is how kind of how we've described that api in the draft um again just kind of using notations here um so basically you know what the vdaf draft defines is this api and how a couple of instantiations fulfill that api and then ppm's job is to do the plumbing to get the inputs and outputs of of these uh local functions to the right places at the right time so that at the end of the day you can unshard into a measurement that's meaningful to the collector in the draft we define a couple of constructions i think ecker probably described described these in a little bit more detail um prio is is the one where we that i think uh enpa is is based on uh the diversion in the draft is is i think has a few more changes on top of what's in the enpa but that's one where we have a fair bit of deployment experience and then the hits protocol is the one that lets you get a distribution on strings and there's been some discussion in chat about whether you know possibly we could fit something like star in here um that's i mean it's kind of the idea of the draft is that we can look at not just these two but also fitting other things in under this umbrella so that we can have ppm as a generic construct into which we can fit multiple different cryptographic realizations there's a few implementations so far um this is kind of uh well we have the one that supports enpa uh with prio v2 we've got some early implementations of uh distributed point functions are what supports uh kind of the inner loop of hits um and there's some work uh on various previous"
  },
  {
    "startTime": "01:04:00",
    "text": "instantiations um i think one of the things interesting that comes out of this is that we can start to do some measurements of the um the cost of doing these algorithms um if you look at the papers in which they're published there's you know it might look intimidating in terms of the amount of data that's getting sent around the amount of computations being done but with these implementations we have some early data about the expense of in terms of computation and communications overhead of uh doing these private schemes and you know your mileage may vary but it seems seems tolerable to first order let's say that um so yeah i think that's all i had yeah a couple references much that puts us uh right on schedule again uh much appreciated so any any questions for richard about any of that eric hey um so in one of your slides you had the aggregators with lines between them and in the um ppm spec i mostly see sort of the helpers not connected are we thinking about a protocol where we might imagine enabling a topology that could have helpers sort of communicating with each other um like some npc protocols uh allow for yeah this slide yeah i i i i admit that i'm not an expert on kind of what the ppm protocol is doing right now i've mainly been stuck down at this layer but i think my understanding is that you can emulate you know this kind of looks like a broadcast channel and if you have a leader that um is connected to multiple helpers you could have basically a star topology that over which you can emulate a broadcast channel so if if a helper needs to it"
  },
  {
    "startTime": "01:06:00",
    "text": "for us for instance in the in the um in the pro in the api we've sketched out uh we structure the preparation process in rounds where the input to each round is uh the output of the previous run from all uh aggregators so i think in the ppm uh content kind of communications model you can envision all helpers submitting their outputs from the previous round and then the leader distributing those outputs out to all of the helpers for the next round kind of yeah just add on to that yeah i think basically the assumption is that the communication is probably mediated by the leader um but that that's the assumption it is embedded in ppm not embedded here and certainly that turned out to be not a workable function so figure something out yeah i mean i think it might be useful for the leader to be able to you know coordinate helpers directly connecting to each other just to minimize communication overhead but as long as that's sort of within scope or or whatnot absolutely considering i'm fine yeah we have a this is this is like a straw man thing um it's not uh like a first of all any change radically i imagine thank you chris patton i saw you briefly flash up in the speaker list your geocomment so i don't need to reiterate it okay seeing no one else in the queue thank you again uh very much richard we're going to go ahead and move on to um well first we're going to call and bring the slides up here so we're going to move on to a call for expression of interest from folks in this um just just to make sure that there is"
  },
  {
    "startTime": "01:08:00",
    "text": "enough critical mass behind this aside from the people who have presented here uh we'd like to ask who in attendance is interested in working on this technology trying to figure out we probably don't need people to speak to this necessarily although we're very happy to hear what you have to say on the topic i'm also going to throw up a quick show of hands there we go so go ahead tommy hello all right um i i did the show i just wanted to speak up a little bit to mention from my perspective you know uh we definitely have already using stuff like this and seeing something like this be standardized and taken on by the word group would be something that would be a positive thing so uh we didn't do any of the presentations here but we're definitely interested thank you does anyone else want to stand up and speak specifically to it that we did get um uh is two things one that uh uh i think everybody who is beak is interested in working on this just so we're all clear and second um uh uh if you look at the um there's a bunch of people in the job or chat that are saying nurses including facebook and whatever tuesday oh excellent thank you all right uh robin is the audio okay"
  },
  {
    "startTime": "01:10:00",
    "text": "yes great thanks adam um yeah this is uh so this is a personal comment rather than anything on ice behalf uh i would be interested in um at least following the work and um i would also have a personal interest in seeing to what extent it can be used to test the the um growing and increasing the mature body of work on um what's sometimes called values value based design so in in other words um what systematic approach can you um adopt for the design and innovation process that ensures that ethical factors and values are considered at each relevant step and i think that would that would help clarify a lot of the comments in the chat about along the lines of yes but this happens already or well this only works if you can assume there's no collusion and so on because what it does is it flushes those assumptions out and lets you determine whether you want to design in a way that is compatible with those behaviors or actually perhaps even conceivably inhibits those behaviors okay thanks uh yari yeah i just wanted to briefly mention that i do think this is exciting technology and we should work on that i can see other types of applications we mostly talked about the uh sort of application type uh browser type of things but also many types of networks we could probably use this this technology to do a better job at uh collecting information that means we collected for various uh debugging and uh other other reasons"
  },
  {
    "startTime": "01:12:03",
    "text": "the only piece that i was sort of a little bit concerned about was was this advertising piece and i sort of just uh as a personal opinion i had some reservations about browsers working with advertisers even though i do recognize that i need to do something better than we do today but you know maybe there's also other paths for the advertisement problems trying to try to prevent information flow rather than collaboration but uh i don't work for the advertisement in the industry so anyway i i like this they should go ahead you wendy so i i we can't hear you unfortunately there may be um an incorrect input device selected or something along those lines okay um i think we probably need to let you try to work that out and get back in the queue thanks uh ted uh thanks uh i basically got in the queue to to to answer one of the bob questions that's do i think the problem statement is clear well-scoped solvable and useful to solve and i think one of the things that kind of worries me about the way it was presented today is that it's talking about what seems to me to be a set of problems and aggregating them into something which seems to be arguing that a single solution is the best solution for each of the ones in this set"
  },
  {
    "startTime": "01:14:01",
    "text": "and i would be very interested in seeing the work go forward but i would suggest that in scoping the work that you do it in a way that allows you to say the best answer for this member of that set may be different from the best answer for that member of the set uh in particular the the set of problems you're dealing with with something like identifying bad urls etc um doesn't really involve tying two actions together in the same way that an ad conversion does so it may be that there are simpler um mechanisms or only a partial use of this system that would still satisfy those where invoking the full thing might be more difficult and not necessarily buying you everything uh that it would need to buy you when when you had the conversion case so i think as a set of problems it's interesting to work on but i would prefer that we take it as a set uh when we take the work in thank you ted uh phillip yeah i i like this stuff uh i don't think that we understand the problem but i want to do it anyway um i am a bit worried that we seem to have gone straight for the real high falutin cryptography rather than measures like let's just encrypt log files as they're produced rather than having them sit in plain text ready to be stolen so there's a bunch of real bread and butter issues i think that we need to do as ietf before we do the stuff that really excites us the other thing that i point to is this has been presented in terms of a network realization i think that the more immediate and more useful use for this would be to assist work like the stuff that my wife does"
  },
  {
    "startTime": "01:16:00",
    "text": "analyzing workers compensation claims in that there you've got enormous amount of really privacy sensitive data that is aggregated together and then she analyzes it and so it that's not a network uh application but that transition from i have this large amount of data i would like to pre-process it into a form that it can't then leak in a dangerous way and analyze it in that form i think that that technology would be very useful disconnected from the whole network case and it might make for some uh starting problems that are rather simpler than trying to think about how do we secure the web right thank you um chris so there we go so chris oh chris appears to have disappeared he's showing us offline let's move on to wes thanks so you know in general i think this is a an interesting problem to look at and it's certainly something that we could consider doing it's uh as ted said it it meets all of the buff criteria except for you know one oddity right that that this solution is really designed to help us protect ourselves or protect end users"
  },
  {
    "startTime": "01:18:01",
    "text": "from good people right because it it has no ability to prevent all of the evil sites and the the evil of you know mechanisms and evil ad trackers that don't want to use aggregation because they're act deliberately trying to track individual people and you know i i think it would be sort of fair to say that this cool new technology really helps you perfect protect you from companies that may already be behaving reasonably in the first place and that doesn't mean that it's not worth doing but you know there's a security consideration sections i'd be considering writing looks like chris is connected against let's go ahead and take him next thanks yeah sorry about that um i i just wanted to echo something that richard was saying in the chat um uh and and perhaps uh ask us to take a step back from the advertising use cases um and recognize that uh this this pattern that we're seeing across the industry across different problem domains uh for collecting these aggregates be it in you know telemetry for browsers or exposure notification for coven or even you know bandwidth measurements in the case of tor is very very common um and i think it's certainly true that we have uh confidence in um the general shape of the problem that is to say we we want to we have we need to collect these aggregate statistics to answer certain questions um and it's uh whether or not you know this is harmful helpful for the purposes of uh web advertisements and conversion measurements in a privacy provisional way um i think is a good question to ask um but that does not i don't think that takes away from the the very valid use cases that were also presented here that could certainly be improved by a more privacy preserving protocol so"
  },
  {
    "startTime": "01:20:00",
    "text": "um i i'm very strongly supportive of this work um it cuts across so many different problem domains and use cases and um it's kind of inevitable um that you know it'd be standardized somewhere given how important it is so that's what i want to say thank you thanks um so we've we've closed the queue in the interest of making certain we have enough time to discuss the proposed charter uh although uh let's go ahead and move on decker here yeah so i would make a number of points um so so first i think um you know uh ted's point um yeah i mean this is a toolbox um that's got that it has a variety of tools and i think that that you know that the one way to think about this is that the ppm protocol is the toolbox and the vdf's the tools um and so the idea is not it's not not to define any particular um you know measurement measurement um you know thing is to define is to define a set of mechanisms which can be used for taking different kinds of measurement and then you know individual applications they layer on top of those tools um so that would be true both in terms of the you know which vdas exist and it'll also be true in terms of like how um how you build applications on top of the top of the vdf because like you know like i mean one thing about this is like you got a thing that basically says i can collect counts right and but like what do you do with those counts is an important question um and one that we don't pretend to define um the um you know it's uh um uh i i i do see some over indexing on the on the ad use case here as well um you know uh um you know there are a lot of these cases they're not ads um and in particular like there's nothing ad specific to any of this um all the ad specific work that's which exists or somewhere else in pat cg um so i mean this is about building a set of fulfillment measurements um and and as a really one application measurement with that said i do think that um you know this question about like you know um about this is for letting good people do do good things i think there's two points about that one is that there are a lot of applications where i as wes says good people want to take measurements and want to be able to like bind themselves so they can't cheat you"
  },
  {
    "startTime": "01:22:01",
    "text": "um and so it can't be collected the day they don't want to have so that's application case one i think application case two is um that there's been a lot of like attempts to look at what it would take to like you know reproduce partially out ecosystem with better privacy because like i think we'll agree as they're not going to go anytime soon and one of the pushbacks that one gets once once it's doing that is it will have a negative impact on the ecosystem as a whole and like you know i'm not um you know um i think you know firefox has already played a bunch of anti-tracking technologies so like i'm not like all in on that but i think there's a real concern people have and so the idea is not that when we simply offer you know these alternative technologies and then people would use them and third-party cookies the idea is that the idea is that zulu offers alternative and that would eventually have to deprecate the existing privacy-based technologies right thanks ecker um charlie yeah i also wanted to uh respond to wes and some of the comments in the chat just mostly the plus one what ecker said but yeah i'm uh on the on the google end we're definitely looking at uh like helping helping this this uh tracking problem on the web with kind of a two-pronged approach like one is having a well-lit path where we can recover use cases in a way that we know is is privacy preserving uh something like ppm could fit into this story and once we have kind of a uh a foundation where some of the use cases that we think are are important to maintain the ecosystem are there then we can go ahead and uh remove kind of the bad stuff that we think is bad for user privacy so this question of like oh only the good guys will do it and the bad guys will use this other stuff like here i think the platform can really mediate this and if we think that the platform provides a good enough foundation then we can we can remove the stuff that we think is is"
  },
  {
    "startTime": "01:24:01",
    "text": "bad for user privacy and this is like our our strategy that we're trying to do uh on chrome where you know we're investing all this time trying to come up with new browser primitives um and also we have like a timeline where we want to uh disable third-party cookies um and deprecate them uh so yeah we're we're i i think i think this is like a reasonable point but it's you know it's something that we're we're we're looking into as like a long-term strategy thank you very much charlie um i do want to give wendy an opportunity to to come back in if we think we've solved the audio issues um all right okay um so let's go ahead and move on to the charter then i have it uh up here in front of us we've broken into two sections um it's probably not terribly useful for me to try to read through it directly as it's on your screens but we want to go ahead and take some comments here i see our area director has stepped up sorted out and told us to move on i'm i'm sorry we missed the first part of what you said there roman ah wendy said that she is not going to be able to get her audio working and told us to move on but she appreciates the opportunity for the slot all right thank you um so uh this is the current proposed charter or at least the first half of it i want to go ahead and open the floor to comments on the formulation that we have here and recognizing that people are still reading i'm going to give it a bit of"
  },
  {
    "startTime": "01:26:00",
    "text": "time okay i'm going to go ahead and skip on to the second half of the charter which is where a lot of the media's specifically the second paragraph talks about exactly what the working group will be doing florence um thank you um so thanks for all the presentations it's been really interesting um so most presentations today were about use cases um but there's nothing about that in the charter i don't think although i can only see the second side now or i don't think currently in the draft i think that would be a useful thing to kind of document somewhere um all right thanks ecker i think we're up for that i guess um you know i think one thing we're trying to do is with the use cases motivational um because i think like the these are generic techniques but i think it'd be great i think i would be more than happy to think to like write down like what like enough motivation is to understand why you want to collect the naked statistic and just to say something um siobhan should ask in the chat is there's room for like for like multiple um for like"
  },
  {
    "startTime": "01:28:00",
    "text": "for like multiple instantiations of the same basic task i think absolutely so i think for instance if there's a there's a better mechanism for for collecting heavy hitters than hits which is relatively expensive that would like something like that room for um i think you know um you know uh uh like i'd certainly be interested like like i think i want to build something where like as new cryptography gets adults we can add it and i'm gonna build something where we have multiple mechanisms that like support the same basic thing um i think you know the only the only question i i i want to be sure is like you know if we have something radically different in some like some lightweight it doesn't match up then we have to ask would be better to make a new a new protocol or better to fit it in but like i'm i'm like definitely pro fitting they do things thanks siobhan yeah i just wanted to see that um that's that's encouraging i just want to make sure that like currently the charter i think would kind of rule out some of the ideas um alternatives to achieve the same goals so i think the charter would need some tweaking just specifically which aspect of it um i think if you go to the previous one previously sorry um i think like splitting measurements between multiple non-including servers like i think um that is like a class of for techniques for achieving this goal of privacy preserving measurements but i think you could do it in other ways as well and i see there's some support to rewrite text and i'm happy to send a pr thank you for the clarification okay um jim i think the charter needs to say something about use cases and threat models i don't have any formal words for that at this point but i do think that's the important thing to capture here because"
  },
  {
    "startTime": "01:30:01",
    "text": "if we're going to be developing multiple solutions as ted was alluding to edward there might be different threat models that apply to them rather than a single overarching one and i think it would be good to make sure that information is clearly captured when things are developed and obviously that needs to fit into the charter cheers thank you uh eric come on on a similar line actually uh there was uh comments in the chat earlier about sort of different scene attacks and simple attacks and comments that differential privacy would be required and that the um uh the vdfs have um you know sort of had the scope for that is it meaningful to like say that these will support privacy respecting incorporation values um without having some definition of what we mean by a a private value or should that be here or should that be at the at the vdf level all right thanks uh charlie yeah i just wanted to uh i i guess drill in a little deeper in the charter of like what aggregation actually means and it might be good to have maybe like a definition there i see like with the server can't learn the value of individual measurements which maybe that suffices to say as long as that's the case you can't uh you can't have aggregate measurements but some of the use cases that we're considering like you know are not are don't very neatly fall into this realm of like you're learning in aggregate like you know you're if you learn like a private ml model that you can verify is you know differentially private or something like that would we consider that an aggregate"
  },
  {
    "startTime": "01:32:01",
    "text": "like maybe maybe not um but it would be it would be great if there's you know at least from our perspective i think it would be great if things like ml models fit within this scope of the charter all right thanks andrew yeah just adding to the the early point uh i completely agree with the idea of adding in the use cases but uh just to echo a point that's come up in the chat um i think also consider adding uh abuse cases and ideally mitigations as well which might help alleviate some of the concerns about some of the dark practices in this uh general area thanks thank you watson watson i'm relaying a message from wendy in the chat uh she said she wrote i'm just gonna first voice support from w3c for this work as there's working incubation that could use it ah thank you steven yeah just on the abuse cases use cases thing i think given that this i think is probably pretty good technology that could be used well i'm not too worried about documenting use cases and that could be a bit of a time sync but given that this good technology could be abused i think effort spent in that direction would be much more valuable particularly if we find ways of mitigating abuses that we see okay thanks nick nick 30 cdt and thanks for presenting all this everyone on the charter i had uh two questions or concerns for now um"
  },
  {
    "startTime": "01:34:00",
    "text": "one i think we're talking about abuse and things like that but i also think there's a lot of the privacy that depends on um assumptions about non-collusion or about how the client is going to find the different servers or or configure them and i'm a little bit worried that that's all getting marked out of scope in in the hope that like uh maybe we can just ignore that problem or someone else will fix it i i'm i guess i think we should be discussing it if we're going to put a lot of effort into um into the work of the protocol that depends on those things um the other concern is about the name i think maybe it's come up a little bit already in the chat um i i think priv is is both very confusing does not describe the work that's happening about uh privacy preserving measurement and actively misleading that this is the group doing everything about privacy or incorporation of values sounds like a very general concept that i would support but that this charter is not related to so i think it's uh actively misleading we should change it all right thank you uh so uh nick i'm going to ask that you send some concrete suggestions for your first bullet point to the list if you have some some time to do so thanks sure uh chris this plus one to that like i think it priv does seem overly broad um but also i wanted to address okay i want to talk about the abuse use cases people are talking about in chat so uh one of the things that was brought up is like what about clients like trying to like corrupt the computation by sending bogus inputs or something like that so that form of abuse is something that we're explicitly trying to rule out so um in the in the so like a vdf is is"
  },
  {
    "startTime": "01:36:00",
    "text": "verifiable in the sense that um invalid inputs can be detected and removed from the um output of of the computation all right thanks so just for clarification uh and i this is addressed at stephen um when we're talking about abuse cases is that the kind of abuse you're talking about are you talking about like abuse of the actual data being collected so again pardon my ignorance it's vast but uh so for example if this tech this this technology could be used to to measure things in two small in aggregates that are too small and that become exposing or if if some application had some kind of opt-in mechanism and then it was possible to change what gets measured in ways that are damaging for users those i think would be kind of abuses that are a bit you know a little bit less concrete than the example given um you know all right it's i was just i was hearing what i thought were two senses of abuse and that appears to be the case so thank you very much yeah but i'm not i'm explicitly not asking that we try and fix everything because it might not might not be fixed fixable but if we can think about it in detail and propose what mitigations are possible then i think that that would be good thank you very much yuri thank you um so plus one on the name confusion issue uh plus one on talking about abuse cases and i i think that's mostly by the you know whoever is doing the measurements not not so much about the abuse by by the user users should still be able to produce the data that they want to produce as long as it's sort of within bounds um the other thing is that i i think um maybe there's some room for a discussion of opt-in opt-out type of um solutions and if we learn from some other protocol cases like first in quick there's the spin bit where there's an arrangement that some"
  },
  {
    "startTime": "01:38:00",
    "text": "fraction of the users are automatically always excluded from from spinning um in order to create this set of users that don't do a particular action and similar techniques might actually apply here that you automatically exclude some set of users um and then you sort of create this opportunity to uh not do this and users can opt out without being targeted as opt-outers and so i think that that kind of thing would be important um i don't have the specific language for the charter or maybe something about um you know enabling opt-in and opt-out or realistic opt-in and opt-out possibilities thank you thanks hacker generally uh just the opt-in thing of generally these are these are configuration points of some kind and clients um um you know uh um you know the question of exactly how how users decide whether that got actually engaged is like it's like not generally saying we were too much itf um the um uh the points even making are absolutely correct um like um you know uh there's like a whole pile of material that has to be done about ensuring that this system the box the system is in does not allow for abuse by by the collector or by or by the various aggregators um and that's like a that's a big topic that um that like absolutely have to make sure we hit properly um there's some work already in the document about it but like it's it's insufficient so i think i would um uh i'd love to see somebody suggest some text but i think absolutely having um uh uh um uh like some like i think having zion texas if we had to work on you know um actually like addressing those topics would be really important okay um all right martin just to just a yari's point a lot of the systems that um are being described here allow people to"
  },
  {
    "startTime": "01:40:02",
    "text": "to opt out of the system without apparently opting out they can generate the inputs to the system that appear to all intents and purposes as valid inputs to the system but they're not actually contributing any any values to the system and that's actually a useful property that is exploited in various ways so um i think that that capability exists i'm not sure that we need to put those sorts of things into a charter i do think that we need to include some of the i think we need to include something generic about abuse there is something very specific about abuse in here about proofs of validity but um i've spent quite a lot of time thinking about things like civil attacks on various manifestations of these sorts of protocols and and those would be interesting things to to have at least in the auspices of the group uh so a slightly different thread that i meant to ask a while ago um has there been any thought put on what type of ecosystem we really want to develop with this technology let's consider the fact that collectors need to take data from helpers and leaders essentially and are we expecting a small number of helpers and leaders that will be accepted by particular collectors or you know can i and steven stand up our own helpers and leaders and expect everything to accept data that we submit to it because the two of us agree that we trust each other but uh but the collectors you know are the collector's only going to accept data from certain places and so we end up with either a very small very centralized ecosystem or uh or you know a much more flexible framework [Music] thanks yuri yes so just a quick response to martin"
  },
  {
    "startTime": "01:42:01",
    "text": "so that's that's really good news um and i wasn't looking for documenting any of that uh detail in in the charter perhaps the high level bit about being effective opt-out mechanisms could could be in the charter thank you thanks ecker the answer is question um i think there's a bunch of like compatible models um i think the important thing to remember is that the person who i'm going back to the right model i had i i put up earlier right that um the the collector has to trust um the to help the aggregators but the clients also trust the aggregators because because aggro is responsible for starting the client's privacy and so as a practical matter i would expect i would anticipate that you know um that that there that you know there'll be a a a non like you know a non-gigantic number of collectors of different aggregators because fundamentally you're you're trusting them to behave correctly right so what you just you know what you and stephen probably has worked up except for like maybe you know maybe i people intimidated that um um but um because people probably rely on relying on the rotations of the collectors oh sorry the aggregators but i mean this is compatible with like an arbitrarily large number um i would expect to see really two models that we talked about um one of which is um you know people who operate just as just as aggregators and are basically their job is to ensure the safety system and then people who operate sort of like a more global system where it's like you know i like if you see amplitude but it's like you know look i am like data collection of the service and like you know i i i i contract with several several different aggregators and that's how i provide the system in a box so i put this to those two models but um there's nothing really centralized here there's something there's like no there's no reason you know like every you know like plenty of like trustworthy entities in the world and um and so there's nothing that basically requires like that like a good the collector a work with it same set of aggregators as quantum b uh chris"
  },
  {
    "startTime": "01:44:03",
    "text": "to bump what eckerd just said but also uh also kind of point out that something that we've been working on in the protocol is lowering the bar of entry to running this this system as much as we can so there's a sort of asymmetry between leader and helper so the leader is uh you know and and this could change um uh depending on what p what people's needs turn out to be but what we have been thinking about so far is the helper should be very very cheap to run and operate and a leader is inherently more expensive because it's getting measurements directly from clients it has to store them for some amount of time before they can begin processing them um how much you have to store and how long you have to store it depends on the vda if you're running so but i think this is one of our goals and i think this is um important thing to keep in mind regardless of like how the ecosystem pans out we want it to make we want to make the bar bar to entry as low as possible i think all right thank you very much um so at this point we'd like to go ahead and move on to some questions to inform the the area directors and the iasg about community interest for this i'm going to go ahead and bring up polls for each of these but also at certain points we're going to ask that if you are not in support of forming the working group that you go ahead and put yourself in the queue so for the first question i'm going to ask who supports a working group with this charter and this is modulo any changes that we've discussed today or come up in the mailing list shortly afterwards"
  },
  {
    "startTime": "01:46:08",
    "text": "and again if you if you don't support the charter and have not yet spoken we urge you to join the microphone queue okay we seem to have slowed down so thank you very much for the input on that the next question we're going to ask is uh do we think uh um do you have something to say alyssa well maybe we should take them one at a time so we can hear from uh from the folks who did not raise their hand or explicitly left their hand down yes i i don't see anyone in the queue though okay i just want to make sure it was clear to people that now is the time to to do that if you have something to say about it yes thanks okay we can move to the next one thanks so for our second poll do we think the problem statement is clear well-scoped solvable and useful to solve ted go ahead is this also a module of the discussion in the chat because there was a good bit of it that really did help uh i would imagine yes i have been following the chat myself um but i would hope that any ceiling points for that would also be brought to the mailing list"
  },
  {
    "startTime": "01:48:00",
    "text": "sorry i'm looking at the ted i mean the answer to that is yes i mean the work that the charter itself uh is going to require some edits we've had some good kind of feedback here so consider the chat and the presentations so again if you have you have concerns about the the scoping here and have not yet spoken on them uh please add yourself to the microphone queue so we can understand better your position nick go ahead please um just uh okay probably yeah okay doing polls like this is is tricky because we're uh chatting about possible variations i i agree but um i think we have this like very open question about are use cases and abuse cases going to be put into the charter or are they going to be a work item um and and until until some of that is settled until we actually are going to define them then i think that"
  },
  {
    "startTime": "01:50:01",
    "text": "it's it's harder to conclude that the problem statement is is uh completely clear if we're not actually agreed on the use and abuses thank you okay robin and again a personal statement not on behalf of isoc um uh i think the discussion in the chat about even the name of the group and most of the proposed names did not have the word value in values uh raises questions about the the scope and intent to the group that are fundamental enough that question two is very difficult to answer as nick said ah okay thank you all right so uh ecker go ahead people seem to be like a little worked up about the name i just want to say like like what happened with the name is like we're calling this ppm privacy return measurement people was too close to ippm i like got punchy and like trying to figure out how to say priv so like like really like there's like like that's what's going on here so like if you want to like that like suggest something else because like like the the like every every letter in there was like constructed to make it say priv not for any actual reason other than that thank you okay and then the final two questions that we're going to have um typically we would have raised of hands of this in a physical room because we do like to keep track of who exactly answered yes but to do that effectively here we're going to ask people in the chat if you are willing to review documents associated with this working group please respond with review"
  },
  {
    "startTime": "01:52:04",
    "text": "okay so i've seen uh on the order of 20 people respond so far this is a very strong signal um and secondly uh who here plans to be an editor for related documents please respond with edit in the chat all right great well thank you very much um that takes us to the end of the things we wanted to discuss for this buff i think this has been very useful and i want to give a few moments over to our area director to speak hi everyone i'd like hi everyone i'd like to repeat uh i think we've had a successful buff and i want to thank everyone for kind of all their input i think the chat was just as lively uh as as the mic line if not more i mean generally what i heard was there is a critical mass of interest in working in this kind of particular problem there was repeated the tension i heard around this idea of you know are we okay working on ad tech and the ad use cases you know at the same time recognizing that that is the reality on the ground and that there would be wider applicability of this tech much larger these protocols much larger than that particular kind of use case i think in the charter discussion we're not ready to go with this particular charter there were a number of really kind of helpful suggestions about how to polish that and we'll double check and kind of confirm that so specific things i heard reading off my list is things like let's generalize the charter text to make sure we're sufficiently flexible so we can swap approaches so it's not just pre-owned the heavy hitters we need to make sure that how we define aggregation doesn't restrict uh you know"
  },
  {
    "startTime": "01:54:00",
    "text": "other kind of alternatives we talked about the need for work items to document the abuse cases and talk about the threat models against all the portions of of the architecture and then we had quite a lot of conversations that we need to tune the working group name i don't know what the solution is uh but that we would like a change there and so i think the proponents seem open to making kind of those changes and that will those changes will be made and then we'll bring that back for confirmation but otherwise i think we do have a critical mass and we should we should kind of polish this uh to consensus and so we can move forward thank you thank you very much for everyone attending this this was a very well attended buff have a good rest of your day thanks for taking notes oh yes thank you very much peter and thank you to the chairs for bringing us all together and leading us through a good conversation all right thanks all peter did you want to say something oh just thanks okay"
  },
  {
    "startTime": "01:56:31",
    "text": "you"
  }
]
