[
  {
    "startTime": "00:00:25",
    "text": "I don't Alright. So Greg, top of the hour. Go ahead, Lanny. Greg, do you have a, seeing a whole lot of stuff for managing the queue I can manage the queue. Yes. I can manage the queue. You got that one. And I can carry chat over to the queue as well. However, what we do need is a note taker, someone taking minutes. Look at all the volunteers. I see eye contact. You think I'm making taking notes? Oh, that's brilliant. Anybody. How about the next person that walks through the door, gets assigned the note taker. Like you were gonna go back in and cut past the door for us, but, you quit, This is where I I heard people and assigned them. You know what? I bought your fries last night. You're taking notes. Kenn, mighty convenient. You're coming as well. Yes. It's a Simone's coming to you. You're coming to his bed. It doesn't matter, but he didn't have any tools. See if you put that game. Someone, anyone, please. We can't move without this. It's a great way to kind of keep you awake."
  },
  {
    "startTime": "00:02:00",
    "text": "Keep me awake. Stig I'll do Tim for you next time. It was a bit sneaky at lunchtime. And Tim's harder. This is an easy day. Okay. And I owe you a Pam. I promise. Oh, everyone's head goes down, you know, like, no eye contact. That's when you get eye contact, like, I'm leaving. You can't assign me. Jeez. Also slide about that as Thank you, Stig. I owe you. Actually, but the nice part about Tim's gonna pay you back for me because I bought him this ride. Did you really Who's counting? Alright, Lenny. You're gonna start. You want me to head out. 1. Why don't you start and cover the the the meat well and the Alright. I okay. And you got slight control. So welcome to MOD. You guys in the right place? Right time. I love all the engagement great. You know, being the 1st session in the morning as early as this is. I understand why still asleep. Hey, I was at back at 2 AM again. This is the morning. And I'm hammered. Alright. No. Well, Noelle. The next noted more. Next line, note really well. Really, really well. Alright. And again, tips. That's kinda interesting. You can get the tips if you can open the but you can't open the deck unless you know the tips. Okay. That's for remote participation. How to put your hand in the queue, how to mute yourself, all these important things."
  },
  {
    "startTime": "00:04:05",
    "text": "Alright. I'll I'll also also just to add, even the folks in the room, please join. You you can because we're gonna manage the queue using this, using the tool That way, there's a fairness between remote participants and participants in the room. Good point. Thanks. We've we broke that rule in Pam yesterday and just started storming the Sorry. That's true too. With the with the blue sheet kinda goes across your lap. It's the easy thing to just squiggle and move on. But, this requires you to actually bust up your phone and take a picture log in. Oh, wait. Anyway. Alright. So here is I would say time. At Glouces Shirts I am on the front one on the bank. And then so that's cool. So we've got the agenda for today posted. It's a light agenda. We do have a little bit of room, if anyone, for for last minute if anybody has anything else they'd like to add, but, any bashers to this agenda? Okay? Moving right along? Alright. In terms of, active working group documents, there is the telemetry document that since 117, that did pass working group last call. Successfully. At this point, we are seeking a document shepherd for that. So, if you would like to participate in the process. Who has never charged a shepherd a document. No. Take it back. Who has shepherd a document?"
  },
  {
    "startTime": "00:06:00",
    "text": "Who has actually shepherded a document? Any group. Doesn't matter. The process. And because what I how I try to sell it is in your relatively new. Shepherding is a good way to kind of figure out the whole process, what the expectations are, you know, the queue structure and the people involved. So I recommend it all the time to jump in. Like a bunch of you haven't, So, you know, you can't assign for people to do necessarily, we do need participation. And if you're on list and you're attending groups you wanna see this work move forward, that's an easy way to to help out and we need it. So please Shepherd volunteer. It was more coming up too all the time. Like it's to the bunk chairs, and ensure that that sign Nice. Yes. Exactly. So if, if you're thinking about it and a little nervous, feel free to reach out to, Greg or me, offline, and we can explain to you what, shepherding actually means and what level of commitment it is. It's not too bad. And it's fun. Alright. The Yang models, draft, any updates to this is, is Sandy or any of the authors of that draft, on and want to, mention any, updates to that to the multicastia model, we just received some common from Tom. So we will, up data on your version according to his comments And about the redundant ingress failover, we think that it's the right day for working group plus that k? And the yang model. I forget, Sandy, are you in"
  },
  {
    "startTime": "00:08:00",
    "text": "Are you a coauthor on that one? Any of the co authors on the Yang model or, AMT Yang models draft. 70 updates. I mean, Sonu, I'm the author of the IMT model. And, last last I get here for, the the draft has, tested and the We need more, review and the comments for those for their chapter now. Thank you. K. Alright. Other drafts, the Jake had, the his, the multicast to the browser drafts, and max, do you wanna share an update. With, for what's going on with these drafts. Sure. Next Max Frank at you, Berlin. So we talked to Jake. And the idea is to move the 1st 3 of those. So MVC back and dorms at least forward. Kyle will take over MB, and I will take over feedback and dorms. The thing with Dom's is thank you. The thing with Dom's is We had, or Jake, at least a quote for working group last call a couple of times, but there was no really bonds too much to this. I'm not sure what to do with that now. But, yeah, I I think Dom, at least, is in a state that could go to working group last call. Because we can sort out the the handing over first and then ask again. Get an extensive, but, yeah, that's that's finished. So I would be happy to shepherd the telemetry draft as well. Thank you. Thank you very much, Max. Okay. Next is so far Our favorite member of this working group right now. Alright. So, Max, the"
  },
  {
    "startTime": "00:10:01",
    "text": "come on up. Your, your next, to cover, give us an update on the multicast quick draft. Alright. Do you drive this line? So I can. Would you like me to? But the camera, is that supposed to be automatically turning or the turn that to look it down. It's staring at me. It's fine. It looks like it's modernized, but there you go. So it's probably gonna make me and and higher. This is just a little twisted here. I think you just You could pull the shaft. Yeah. It goes for sure. Perfect. Yeah. Looks good. Okay. Yeah. So hi, it's mexican. So this is an update for the the multicast extension too quick that we introduce at, at that last year, Jake, unfortunately, as with this draft, has no more, not much time anymore to work on this, but, Louie from you. Was gonna talk a bit about FIC in a moment. Has an implementation, and we are trying to work on it this way and move it forward with with Lucas and, kind as well. So, Nick said, So, Jim, I'm just gonna give, a really quick recap again of recap again of of what the draft is about because that might have gotten since then, And then what has changed since the last time we presented the current state and what implementations and some open questions we have and how to proceed from here. Okay. Next slide. So to really and to recap quickly the idea is to combine a normal quick connection. So unicast connection with multicast and first, you established the the non quick connection where you have your cryptography and everything to secure it. And then after that in the same vein as would you have quick in a way you you open, and that's a"
  },
  {
    "startTime": "00:12:00",
    "text": "have or you you can use another pair of the server sense the client information about some available multicast channels, they could join to to receive data of a of a multicast the point here is that it's more or less transparent to the client, so to the application layer. Whether or not after it has enabled. Multicast support a set of flag that it allow us multicast, transmission, that it's transparent to the client, whether or not the the pack came over the multicast channel or the unicast connection And that is to to an, enable you know, fallback in case the multicast doesn't work, you it will just be sent over a unicast those applications don't have to worry about, Making some alternative arrangements. Okay. So after it joined, and there's you can have multiple channels. So you can combine channels. Can have, different channels with different qualities. In fact, I'm shipping about video streaming. You could have one channel that supplies to the video on 720p and 11080p and then depending on the capabilities. The the client has the client negotiates capabilities to the server. It can Did the server instructed to to claim to join the appropriate channels to get you know, the most of out of its available this multicast traffic is secured thanks to the unicast connection against injections. From third parties by you know, having integrity to to order packets on that I sent over multicast. The multicast channel is also encrypted. Of course, since it's multicast, all receivers have the same keys. So as if one of them leaks, it's it's no longer really that secure, but The idea here is that's mostly for traffic anyway. It's not that not that important to be encrypted Right. Yeah. Sure. Yeah. Yeah. Yeah. Kino, please. Yeah. Yep. Yep."
  },
  {
    "startTime": "00:14:03",
    "text": "So so a bunch of quick questions. If you wanna rekey, could you rekey over the before using it on the multicast channel, you just tell everybody, unicast, what the new key is? Is it how it's done? Okay. Yes. So the the ideas for the keys is so, of course, you have different, crypto context for unicast and for each channel as well. Okay. And you can either and you you you can change the keys over multicast, but you should, of course, do it periodically over unicast as well to to make sure that, you know, somebody was no longer connected to unicast at all gets kicked out at some point. Okay. And if, 50% of the recipients don't act, you retransmit as multicast or unicast. That's okay. So we we have open questions about how to handle and retransmissions and so on. We we now, had a added effect to to, to to it, and there's different approaches on handling it. Okay. But basic ideas that you don't specify it in the draft or exactly to do retransmission because in the end, that's up to the server if it detects, you know, there's a general issue with multicast somewhere, then it's probably better to transmit over unicast even if it's more costly, but it's you know, if this you multicast syndrome seems to be fine. It's actually lost packet somewhere, then it's probably better to fit on the Because we've been trying to do reliable multicast for 20 years. And I think this is the best combination that has ever come up. Right. Right. Even though they're we can argue that there might be act implosion, But would you consider this proposal a yet another reliable transport multicast Yes. There's a shaking, have you? Yeah. Okay. Yeah. Okay. Thanks. Yeah. Thank you. Toreless. No more question. And and just a just a reminder for everyone to tourless and Luis of, have been, entered the queue, anybody else who wants to join, please do join and enter the queue. With the meet echo tool. Don't answer the question whether this is RMT or not. It's a rat hole. Just do the technology."
  },
  {
    "startTime": "00:16:01",
    "text": "The did you did you think about neck I mean, maybe it's not really a neck, but something like I think I'm hitting congestion windows slow down to identify the slowest receiver or so something, but to to get more scalability in there. Maybe coming not 1st round and maybe an extension, but obviously that that would make it, I think, a lot more attractive, at scale. Yes. Singu is that's a really good That's actually Louis. That's actually exactly one of the points that are gonna come up is that we are debating in to the neck on x. Another queue. Yes, Louis. 1 Yes. So, Luinvar, from you Silva, also in the draft. So just to add some comments, but know if it's better that I do the comments now or when I Sure. No worries. So that's something that we're discussing, positive or negative act acknowledgements. And the usual FEC. So for example, now I've been working a bit on the implementation. And, basically, the idea was to use FDC, until, until some point. And when, for example, you have a client that's really a bottleneck in lost a lot of buckets, you can just say, okay. Now we we transmit the the data through unicast for these clients. And if it's if the crane stays really like, bottleneck, we can kick it keep your number. It's out of the multicast channel, and we'll continue using unicast. So Yeah. Yeah. So that's the idea of using multi pass, but we'll explain a bit later, but that's the idea of using multi pass because now the client can just switch to unicast very easily. Sounds good. Lucas. Next queue. Lucas. Hey, Luke. Padu, Clyde Flack, coach out of the quick work and group normally named on the draft, but been able to pay much attention to it for ages. So able to meet up with Max and Lewis this, like, to solve ETF and kind of refresh myself with this stuff. It is reliable, for various reasons, but the the acting of each packet doesn't necessarily mean you're sending an act per packet. So in the quick working group, we're looking at various extensions that the draft itself says act every other pocket,"
  },
  {
    "startTime": "00:18:02",
    "text": "but that you can do things like frequency negotiate or tune these for different endpoints. So I think there's a lot of flexibility to help avoid some scalability challenges and that is the thing that quick is good at is providing extensibility. The real power become comes from effectively, the connection is independent of the path server which data is being delivered. There are there are real practical considerations you need to make but this design compared to something I worked on years ago was a lot cleaner because it it handles this stuff as a transport service rather than trying to tack on other things or stitch other protocols together. Thanks. Next. Mighteen is just a editorial question. This is all about SSM, right, and not ASM. So It syncs the draft title should reflect us. It's going to be talking about multicast over quick Yeah. But it's only so specific, multicastor work quick Yeah. In the draft, you say, right, I did not write it down. Should also make it really good. Oh, okay. Okay. Okay. Yeah. Thanks. Right? I mean, it gets a lot harder if you also allow ASM, right? If you allow ASM, it gets way harder to do. So From the transport meeting? Yes. Well, I mean, Yeah. They waste the sauce. Right? It's because you have the connections between all the clients and the servers. So now if you have multiple source, you have to for all the grants to have a connection with all the sources. So you're gonna have you have have this mapping between the source, each source, and all the clients. You have to live like, a lot of connections everywhere if you want to support multiple the same thing. Top multicast you're saying, squared unicast connections is a bad thing. Also, for the use cases we had in mind, and since ASM is depreciated for inter domain. Right? For use case we had in mind initially It What if I wanna do whiteboard? Sure. But the it's it's it's very complicated and I think there's a lot of"
  },
  {
    "startTime": "00:20:00",
    "text": "things to consider right now. So we initially for the ASM having ASM maybe as an extension later and what you need to change but for at the beginning saying is MS like out of scope for now. Is is better. Of course, we can talk about including it. Maybe it's not that much work, but with you. I'm sticking myself in Yeah. Go ahead, Toshi. I'm like, I I'm in behind Toshi. Oh, so, I like to just ask you about the Glander. Does this act includes, some mechanism for the late control or something like a RT because it reminds me the RTCP. So it's a different. Yeah. No. It's normally quick. It's 7 I know this is not a RTC, but, just, so Aqui is just indicate to package the reservoir. Yes. Is there any, faction to change the late and so on. During on such kind of a function. Not for now. Okay. Yeah. We can consider it. Yeah. Okay. There's there's chep. Chep. Chep. Chep. Chep. Chep. Me. Just wanna comment about that, Dino. The truth is multicast has always had to know what's going on in the network. Kind of been that anomaly. And the assumption that ASM means that you have ASM phone or the network, they aren't necessarily correlated. And that I can have a many, many application that has out of bounds so source discovery, and I don't have to care about it within the infrastructure. And they're right away. My application has to know what the network is or isn't doing. So, What we're doing when when when you requesting ASM within the protocol what you're really requesting is not many to many, you're requesting network based source discovery. And doesn't have to happen in the network to still get ASM functionality and application. Okay. There off my soapbox. But I got a queue of them back here, so careful. Okay. Next slide. Right. Okay. So we've been over this, I think, enough"
  },
  {
    "startTime": "00:22:00",
    "text": "the the things itself. Right. So, yeah, you have a you have still have some unicast traffic. So you have some scalability issues, but you off load the data to to the multicast and get, like, we calculated, like, a 35 with the the current approach we have integrity claims. You can 35. Times increase in in you know, fruple capacity. Okay. Next slide. Right. So, since the last time we presented, we have we made a lot of, improvements additions to the draft, again, slowed down a bit when when and we'll just check. Unfortunately, That's alright. We got a good and long review from Martin Duke Thanks for that. And have incorporated most of the changes Right. So Also, since multiple have quick is kind of getting finalized a quick working group, we now want to make more clear how it interacts with that and how exactly we use the second path and so on. So able to carry as much from their over and to here as possible. To sheep to sheep to sheep to sheep does this? Draft. Includes the consideration about the media over quick because, moke has been discussing a lot. So, Yes. Is there any relation on the mark? We we talked to Lucas. Right? Look curly. Sorry. Last year as well. And, yes, we we have we have mock in we we we we see what Mark is doing, and we we, take that into considerations. That ended up It's a bit different, I would say. Right? Yes. Because I think Look as you can just you know, ends it in maybe to say very quickly. Yeah. In fact, we need to think of quick as providing services to applications. So the only things that applications care about is reliable streams or unreliable data grounds right now."
  },
  {
    "startTime": "00:24:03",
    "text": "And that's a big discussion in mock. Do do you they wanna transfer their video or the media or whatever. Over these different kinds of application, vessels. That subtracts it away. Know, if if The the layering here is correct. Which is it means people can just do stuff. Obviously, it comes to configuration and how you would set the service to do this stuff, there's a lot more discussion, but I see that as kind of outbound things that we'll come later on. So ice, artists, but so I've not read the draft So maybe this is a stupid question. Second Alright. I didn't quit the draft. So apologize for that, but would have thought maybe put it into your introduction, but do understand correctly that the replication of the multiclass packet is only at the server the replication. No. It's on on path. Right? It's not regular with the customer. I'll just replicate, the the ticket. But then the router's in support quick. No. It's the it's UDP. It's not the UDP, quickest using it's normally UDP packets for for the routers. Yes. Yeah. Transferred Lucas, again, just a clarification here. We detect last packets, but we only transmit, transmit, transmit, transmit, data. We don't retransmit packets. We retransmit the data that was lost, which could be reframed or repacketized or whatever. So this is what allows this you could detect that a an unreliable data gram was lost from the sun decide, but you wouldn't re transmit is it something that's different from other protocols like TCP or I think RTP World but Dana, do you tended, put your name in queue. You explain more about yes. So The in okay. So the current approach in the in the draft that we had so far with Jake Bost that we have integrity frame. So if you know, maybe so, basically, you have"
  },
  {
    "startTime": "00:26:02",
    "text": "you have checksums for each packet you transmit over multicast. And these checksums verify that the packets you get are actually sent by the server, not rejected by a third party. These integrity frames, the first one is sent over unicast. And then from there, you can include in the multicast packets, right? Because then you have, like, a mercury and you each were discussed packet received the one with the cost guarantees, the integrity of the next one, or or you can do whatever you want. Right? Like, there's it up to the server to, you know, bundle some packets. And if you have, like, video segments, then you have, like, integrity depending on each other or something. But but that's the point you have in the frame in the integrity frame, it has a bunch of checks stamps for packets. And these checksums, the client verifies that they match the the packet, and then you you you are sure that it's not injected and you can safe to use it. Does send technology. And the other approach is to have, and we're just gonna talk about this bit, actually, maybe you can do it now then. So just Next. Yes. He's gonna do it. For the experience now, we'll just wait for this late to show up after I think now now if he asks, it's fine. You can explain. Okay. So the the other approach was like 2 more, actually. When he's got the sludge. Next one, Lenny. Yeah. Yeah. Yeah, we're in another one, please. Yeah. So the the other approach was a bit like, Alta. So the other draft So it's to have the digital digitizing letters, either at the end of each quick buckets or per object, if you could say. So for example, in quick, you have the this idea of streams, when you can send a single object in a stream. So for example, in conference, you could say on, like, send an iframe per stream because you have you have a lot of streams. And the idea would be to authenticate a stream as a whole because again, in this use case, you will send a night frame, at the same time. So in multiple quick packets, and then you can authenticate the the the iframe as a whole. And so you you will have, like, digital signatures happened at the end of this Oh, the"
  },
  {
    "startTime": "00:28:03",
    "text": "all these streams, and the the clients will have just to to authenticate once for stream of multiple cook packets. But Yeah. It's costly to to do the deal notifications. I asked her to share the treat shared, key or where you're gonna use our same for signatures. For for currently, we use eGSA. But, yes, all the algorithms could be used, of course. Thanks Francois, Yes. That's actually exactly one of the points where we're looking for feedback is on how to do integrity. You guys have more experience deployment and, like, on landscape, what what makes more sense then? So just, like, a clar clarifying question about integrity. So we were discussing about many to many connection, that what might be hard, does the asymmetric signature enables a way to more easily do the many to many because my you might not need to have, like, unique as channels in that case. Like, my my maybe a simplified version of multicast with no unicast channel, only asymmetric signatures that might be heavy, but that might allow many to many more easily. Right? What do you think? I'm not sure, but I think this is outside the scope of this one, but because for us, it's just one too many for for quick. We have one server that still unicast connection with clients. We don't consider any many to many, many thing. Okay. Thank you. Yeah. Julia, crop track. If you if you're going to say it later, that's fine. But, how do you do key agreement, group key agreement, over the unicast quick connection. Yeah. But using cross algorithm. That can be negotiated, like, the rest of the the quick crypto. Is there one year recommend? It depends. That depends a bit on what you want Right? We had the idea to even use algorithms that are weaker than the ones quick allows because they just save"
  },
  {
    "startTime": "00:30:00",
    "text": "you know, but and and and you know, to video fans, maybe don't want don't need that strong encryption, but So are you going to put in an empty TI, I'll direct them for key agreements. They're probably just gonna rely on what what quick is doing. We're not I think, But, of course, your welcome feedback welcome if you have better ideas then. We can talk. No. That's why I'm asking. Okay. Yeah. Lenny's next. Legends in the queue. Quad Lenny. Yeah. I I would just, just a a plus one for SSM. Lane, I I would really caution against, you know, adding lots and lots of City for to to address many to many use cases. I I think that the the real, you know, value and need here is is one to many and, know, I'd I'd I'd hate to see us kinda repeat some of the maybe mistakes we've made in the past where we tried to do too many different things and and, you know, created so much complexity and difficulty that things never got deployed. You know, there's there's a reason ASM has been deprecated for inter domain and even for intra domain, It is strongly recommended. So I I just wanna, plus one on the SSM only approach. Alright, Daniel. Was it clear to me, did were you gonna do Diffie Hellman over the unicast channels? To to do key negotiations? Yeah. Yeah. Versus just advertising the public key, since you're doing ECDSA, already have the key pair set up. So think you just have to send the public key over And then the other side could just verify the signature that way. Have you thought about that? Yeah. Sorry. I can commencing that. So you we have 2, actually, 2 layer layers of of security, we have the encryption key that is used to"
  },
  {
    "startTime": "00:32:02",
    "text": "to protect the the packets like it's done with quick and then we have the authentication. So you have already to to to share the key with the clients using the unicast channel, the key that is used to encrypt the data, like, normal quick, quick does. And then, for example, if you use asymmetric signatures, you are also to advertise the the public key that the client will use to authenticate the buckets. So you have 2 layers. But if you Yeah. Okay. But for the automated, for the the MB side, the the hashes that you're sent over the unicast channel. So you don't have to advertise new key materials because you already have these unicast connections where you can send the to digest digest to do They don't there's no problem. You you negotiate the the the the hash algorithm, and then you just send the hedges. For the biggest Hold a second. So what what service do you actually want to offer real time or kind of best effort, you know, we make it as slow as the slowest guy can receive something as long as we save traffic, right, because I think that design impacts a lot of what we wanna do here. The the ideas that you have different channels. Right? So you can have ideas, you have different channels. So you don't the the server can like, offer different frame, like, frame rates or whatever you want, like, different bit rates. For if you have the if you talk about video, for example, is that if I got your question correct, is sent If the slows clients slows down everybody else, is that? Yeah. So Yeah. No. No. No. Okay. So you have different channels. Right? So you you can even do things like layout video eventually, maybe. Where you can have multiple multicast channels and the client subscribes to whatever fits best for him. And if there's, like, a lot of slow clients, you just offer a new, like, the server can just create a new channel ad hoc. And then publish that channel and the clients that the slow clients can subscribe to that channel. Where the faster clients can watch whatever they want to watch. So, the if if you looked into SVC in terms of, you know, subscribing to multiple layers, in SVC."
  },
  {
    "startTime": "00:34:02",
    "text": "Because that's a subset question. Right? So I think if that's the goal, well, we'll have to be very careful in whether this is actually something that Quix would do. Or some, you know, media over quick in terms of people who have an idea with that session layer management of of things like these these video codecs. Right? I would I would see a lot of value in having a simple version that is really the stupid. The slowest guy is fine. And if he gets too slow, we him out or something like that, which is I think an an an interesting class of simple application that to me would still be a transport protocol and go in quick. What you're saying here now to me would even mean likely a different working group. Right? So and both of them are good. Right? So the the idea is that we we in the in the draft itself, we keep it as unspecified as possible. Right? So we don't say we just say there's the possibility to have motive channels, But what the the server eventually wants to do with that, we don't say it has to do it. Like, we proposed, like, for media specific, as we just say, There can be more than one channel clients can subscribe to more than one channel. What happens that way or who decides what channel to subscribe whatever we we want to the the application choice. Yeah. Yeah. I I think you wanna have at least if it's informational, the validation draft going in parallel that he gets an example. Of kind of the maybe most complicated stuff you really wanna do with it, even if that doesn't get standardized initially, but we need something to verify the the multichannel again. Yeah. I think that's a good point. Yeah. Luis. One more in the queue, Luis. 33 Actually, I'm just requesting the document. To be able to exchange the slides, but And so that Ah, okay. Yeah. Yeah, Danny. If we go back to slides now, please. And we can talk about the Thank you. I think they're done with the current Yeah. Okay. Yes. So for FVC, so that's what changed that we we added in the in the draft is to use FEC to recover from, lost packets"
  },
  {
    "startTime": "00:36:02",
    "text": "so that we can now send wiper frames on the multicast channel so that now you can have different clients that have distant losses, and now with a single reaper frame, you can just recover all the the losses on clients. So for this, we could rely on the on the draft from a for submission. Also in the room. And, actually, in the one of the implementation already uses that and showed, good results. So the idea would be, like, for example, if if you want to protect the the tree, the two frames that you send on the multicast channel, you can just encapsulate these frames into a new frame that is called a a source symbol frame. And now the source, can decide to send fec, packets FVC frames. So in the draft, it's called, repair frames. And you compute, FEC based on the the source symbol we protect. The 2 strategy would be either to send them proactively or, based on feedback from the from the clients. So either the client or some clients lost specific packets, and we see that several clients lost multiple packets. The the source can decide to send, a reaper frame on the multicast channel or it can be, based on other things. It's not clear, yet. So if you have any ideas, It could be interesting. Of course, we have also to manage because it's at the transport layer. We have to manage how we we deal that with the condition control. The result we're doing RFC about that, about how we deal with, with FEC and the congestion control. So that's something that we should include also in the discussion. But that's maybe more related to to quick and that's for this group. I wanna ask you a question with this, Craig, Craig, me. If you looked at the FECT framework, took place. Gosh. 10 plus years ago now. The the effect framework, you see? Yeah. I think the working group effect frame. Yeah. So so it's not my expertise domain that Francois worked a lot. That's an and so I think the draft correctly relates about the this, like,"
  },
  {
    "startTime": "00:38:03",
    "text": "this spectrum work. So I wouldn't worry about that too much. I don't know if you have any comments on that, Francois. But but Francois Michel. So, yeah, I raised my hand in the queue so you can put it down. So, basically, the quick, if you see document is what was done with the authors of the fake frame documents. It's like relying on the same principles. We just change a bit to the fields to make it more like looking like the other, quick frames that we do. Basically, we use variants so that can stuff, but it's still, like, really generic quick FEC design, allowing to use a lot of, error correcting codes and that kind of stuff. So, Yeah. It it's basically relying on on that previous design. Thanks. Jules? Julie Scrubberschuck, just a clarifying question. Does can you do FEC for unreliable data 2, or is it only for reliable data? Oh, you you could do that So, of course, you just think you just add a source symbol frame in front of the frame that you want to protect. But then Just to mention that FEC with unreliable data for certain application worse. Beautifully well by reducing loss rates to manageable. Yes. Of course. That's something that is really easy to to do. Of course. Thank you. Gory? Gory first. The congestion control bit in quick fetch is very terse. And What what what's the standard stakes of the thing we're talking about? I didn't check that. About this RFC. The current document we're talking about is proposed standard. Exp not the the The the FECT thing is EXP."
  },
  {
    "startTime": "00:40:00",
    "text": "And it relies on something which is informational through its congestion control, which seems like terrible done ref. What on this And is this I mean, where are we gonna say the congestion control? Because this seems like the tricky to get right. Which document just if it's easier, just it was an ID, to run on this document because it's a it's an informational document just to to clarify that we have to be careful when using FEC for the condition control, but I don't think the current drafts in, uses this this document, but it's just because it's we we know that it's had some importance to to include conditional control debate when you use FEC because for example, you cannot just FEC was also in the congestion controller. And, also, if you hide the losses, from the congested control, these losses are maybe cogissuing induced So you cannot just say, okay. We recovered those losses. So it's not induced to congestion, and we can Prazo condition window. I think you got the hint of my comment. Okay. Figure out how to do it and put some musts in rather than shots where you need musts. And let's let's review that properly when we get to sorry. Hitoshi. Toreless. No. Hitoshi. Yeah. Up to the side. So you consider the, differentiating the the encoding late because, may have a lot of concern, you receivers and, who has a different condition, of course, So, someone want to in Codell. Strongly, and someone want to have a lighter encoding. So how you can distinguish, such kind of encoding created for the multiple users. That's a good question. We haven't thought about that. So maybe this is a very important point for the fund. Yeah. Currently, we just use a single FEC layer per per channel. So maybe we could also, like, cluster the clients based on their, capacity to encode and decode data. But"
  },
  {
    "startTime": "00:42:00",
    "text": "currently, it's not something that we have considered. So we'll have to dig into that. Okay. So I I do recommend to consider that kind of certification. And, also, we consider such kind of a situation, we also need to this thing is made channel itself. So this kind of, creating such conclusions also. That one. But anyway, it's an interesting thing. Okay. Thank you. So I think we can move to the next slide, please. Yes, just a quick recap, but Max told about it a lot. So we'll use we had also something about multi pass. Quick. And Max already told, about the benefits using multi pads because now you can just client, you can just sim seamlessly receive data over unicast instead of multi pass. So we we don't break the the the reception from for the clients using So that's really something that that is interesting. And also, that's easier the the implementation but I think you covered you covered it also. We can move to the next slide. Different crypto. Oh, yes. Of course. Yes. Sorry. Previous slide. I'm sorry. Yes. The last comment, which is really important indeed, that currently multi pass uses the same crypto for all the passes, inside the connection. But here, if you want to do that, we have to have a different crypto context for each of the past we use. So it shouldn't be that complex to implement, but it's something that changes from the current, version of the draft to book multi pass. Yes. So next slide, please. So that's the question. The final question we had, which is also a very important is what do we do with intregates integrity. So, we've discussed that already. But you have if you have any other comments, It could be very interesting. Currently, we so we'll speak about that a bit later, but we have, 2 visions for integrity"
  },
  {
    "startTime": "00:44:02",
    "text": "either the MB style or using signatures either per packet or per stream, of course, using signatures is very costly. And currently, the authentication per stream can bring, DOA attacks it's also something that is a really important to to to consider. And that's why we also wanted to to discuss maybe a threat model that we have to to find, if we have spoof packets, for example, And Yes. So you have a question. Sorry. So in terms of the cost of signatures, were you planning on doing the signature over the entire packet. Because you could do it on a very short part small part, like, 64 bits or whatever. That would be sufficient. Do you still think it's expensive? I remember correctly, for example, if you do, EDDSA, you you will hash the packets you will have a digest. You don't have to You could just the signature data could be whatever the protocol chooses. So it could be made cheap. I don't know if it's real cheap, but it doesn't have to go through. It doesn't you don't have the signature data doesn't have to be a 1500 rate packet. Okay. I'm not an expert, but in my in my head, it was like you hush the packet and then you do the signature, but you already have, like, a 64 digest, and you have to use these digest to do the signature. And if you if you have a packet of 100 bytes or of, a thousand bytes, it doesn't change much because the hash will is not very costly compared to the signature. So But if we can find some signature algorithms that are cheaper, that could be very interesting, to consider and so the question we had with Max was should we just focus on single method and then see how it works, or, Should we provide both approaches in the in the draft and in the implementations and then have some kind of, of negotiation with the clients. So It has complexity, but for different"
  },
  {
    "startTime": "00:46:00",
    "text": "cases, it can be very interesting to to have both approach. So if any any comments on that, Chileus Krawalchuk, less options, please. Okay. Okay. Okay. So at some point, we left you. That's a good question. I don't know if I can I continue Okay? So we have 2 reference implementations. Max and Jake gave up on the Chromium, because it was a bit messy. Sorry. So now they're working with, and they have, somewhat working implementation. Was it all the frames at the transport parameters? So that's way something that we can that they can continue and to do, some demonstration at some point And we also, a key cloud for implementation, which is not a purchase, currently. And it does FVC and the acknowledgemanagement. So we also have this we can do this comparison in the future. But it doesn't have a flow flow and control condition. Mechanism. Don't interoperate? No. No. So to be honest, I integrated the the draft very recently. And, the closed verification implementation doesn't follow the drafts, very much. So we have to do some work to make it it interpretable. So that's something we have to do for the future. Next slide, please. So the next step, we already discussed them a lot. So I don't think we have to go further than that. We hope to be able to present this work, at the quick working group, in a an IETF or 2, and if something that works to show, how we can, Yes. How it can work? And just also final comments, since Max and me, we are PG students we heard the comments saying that we don't have clear use cases. Of course, we don't have to clearly use it because we're PhD students. So If you if you have use cases that could, direct us in a good direction,"
  },
  {
    "startTime": "00:48:00",
    "text": "Of course, that could be a great idea. So due to this due to this date to to reach out, for this. And I think it's the last slide That's it. Yep. Thank you. Lucas Alright. Just very quickly as a quick chair. This was presented by Jake Bach inland, Philadelphia last summer. And this is all quick stuff. Right. We the quick tensure the right venue for that. Obviously, I can't speak for the working group and who wants to adopt that, but the the quick features themselves probably belong there, but the discussions of multicast specific things we know nothing about. So we really value the discussion here, and use cases too. You know, the quick working group would definitely need those use cases to, make the work worth doing So I just if people wanna speak to these chats or us, like, please do because this will all be good. It's cool technology, but doing stuff because it's cool as like like, like, not good. Us because gold up limited time. We can do that at BGstudents. Okay. Cool. Thanks. Okay. Maybe one quick comment for. The congestion con. Right? That's one of the reasons why we want to take the the feedback forward because the congestion control for multicast in general is really really complicated. And with the fact draft, we had we are still debating if we exactly what you mentioned. If you want to, Just say fix should be done. And then just That's it. And then however you do fact, then a congestion control effect that's in a separate draft, or if you want to go into details how to do the fact. In this case, we've Worcester, the draft from point 1. But, yeah, Seabek, that's that's one of the reasons I want to take Seabek quote. We think feedback is gonna be very relevant to you, which is condition circuit breaker. System condition Yep. Yep. Corey, Corey. Hi, Greg. Trying to come back on that and say something helpful. I think there is a problem with the number of brains you might require. So a separate draft could be"
  },
  {
    "startTime": "00:50:01",
    "text": "at least something that maybe some congestion control people can get their heads around. If you know the intersection of congesting control, people on customer word people. They're probably 1 or 2 of the people in this room and nobody else. So be be aware of that because we we need to have good congestion control, and I think that's absolutely vital to get that right. But expressing that in a document way explains what it is you're trying to do and then pushing that to a group that understands congestion control might need more help Just a just a suggestion. Go whichever way you think is most useful. Sure. Is your expectation that the circuit breaking, congestion control, excuse me, would be in quick or or the application using quick. Even that would be a good thing to write Joan, and how you deal with receiving multicast and unicast at the same time is switching between the 2, but but interesting problems. And try and describe them. Just historically, is that the application layer? And is it just multicast just UDP? Right? So we've the applications did the things like layered video, you know, and and responses like this. So that still could could take place using quick. Right? Except quick is doing something underneath as well. So that's well, could Right. If you enable that and want want to use that, sure. But I was just wondering what your expectations were to see see bill what we have in place would work, or you're expecting something more. A both cookies and coffee to be able to think about that. Was motivating with with a simple document that that just even just try make a problem statement error. Maybe even you don't publish it because it goes it goes into another document, but something so simple that you might be able to get the right feedback Toilers will probably tell you the answer, though. Yeah. For for talking point, it'd be fantastic. Alright. So, no, I I I think the problem is that we may have less on the basic congestion control business documented in a way that it can help here, then we"
  },
  {
    "startTime": "00:52:03",
    "text": "wish to. Right? I mean, we have a congestion control working group now. That slowly even trying to ramp up to where we were 20 years ago. And multicast had already, you know, with RMT and others going beyond that, but which of the RMT stuff can we really use and refer to. FEC frame did fec frames and tell us how to do congestion control with FEC loss, but not data loss. And so, I mean, that that that seems like a big block by itself, much of which, for example, the FEC stuff would even be valuable just for unicast itself hasn't been done in FEC frame. Right? So it this this already, I mean, splitting up the work into multiple things if you know, beneficials should certainly be on the table before we finalize the adoptions. Thanks. I remember at CCWG there was even the question from Magnus and Lars, right, about if multicast should be at all relevant CCWG or not and maybe the answer is then yes. It should be. Korean? Yes. I can I can ask that one directly? Not in the document that CCW are producing currently. That doesn't mean that there's latent expertise that could be applied to this document. It's just that guidance document isn't going to unwrap this kind of worms and try and tell you, but if you present the kind of worms, they might be able to give you a beauties judgment Great. Thanks. Just a clarification about your comments, about con control with the with unicast fallback. So in MPCwik, If I remember correctly, you have different congestions stale, purpose. So you will have you will have this congestion control for the unicaspads between the server and the clients And then the question now is just how do we find a good criticism control for the multicast bus, but it doesn't it doesn't influence the the, how the the fallback, the unique as fallback will be handled for the clients. Maybe. I think there could be some correlation, right, potentially. Right? Because if you some some needs to send a lot of traffic over unicorns, because you fall back from multicast"
  },
  {
    "startTime": "00:54:01",
    "text": "later. Then, yeah, yeah. Yeah. I think they're entirely unrelated, okay, scoring. Rati town. Yes. Okay. We will. Thank you. I just thought of another potential problem. You you never reach transmit over multicast. We do you do. Is there is there a race condition where you have to do duplicate packet detection on the receiver. If it goes over unicast then multicast, Okay. So I wasn't sure from Yeah. So I would decide depending on how many packets are lost the package should be retransmitted of a UADCAS or multicast. So it would never I guess, transmit both at the same time. So if a lot of clients lose the same packet, then he would probably try even over a different unique and multicast channels to retransmit if it's a single client that does the packet, the retrans retransmitted over unicorns. It's one sequence number space so it could the receiver could detect duplicates, right? Yes. Yes. Okay. And you I think that's a rare event, especially if you throw FEC in there. Right? Okay. Anyone else? Thanks, everyone. Great discussion. Thank you very much. Picking this up. Yes. Been all the funds beginning. Where are you going? Exactly. Oh, yeah. Me your eyes. Sorry. Click Hello, Emboindi Group. Would like to talk the efficiency of beer, podcast, and large networks. Next slide, please. Yeah. Oh, that doesn't do it. Yeah. So, might be it. Oh, and why do we, consider the performance about a fish himself. Be at last multi call in in large networks. Well well well with with IP multicast. What what is the what is the motivation for beer first?"
  },
  {
    "startTime": "00:56:02",
    "text": "In IP multicast networks, we have stayed in core notes from multicast groups and also have signalling overhead 12 multicolor promalika's group, And, there is also severe signal load in case linear node failures due to reconvergence. And, if we And ITS answer to these problems is a bit index explicit replication. Here, we define a model, the peer domain, and, ingress notes, so called, bit forwarding ingress routers at a beer header with bit string on the ingress. On on the packets. And this bit string encodes potential receivers. Those receive as a called bit forwarding egress routers. Each position corresponds to 1 bit forwarding equals router. So one receiver The packet is to live deliver to a specific receiver if that bit in the bit string is set to 1. And, thereby, the core remains free of any states and, also signaling is not needed for call routers. That means forward and replicate the packets only according to the you're headed to the big screen and according to the routing underlay. So this this is how PO works. Next slide, please. What is if there are more bit forwarding egress routers, then we have bits in the bit string. So for instance, we have 1024 potential receivers, but the bid stream is only 256 it's long. Then we need not only a single bit string. We need 4 different bit strings. And each of these bit strings needs to have a number and this is the set identifier."
  },
  {
    "startTime": "00:58:02",
    "text": "So, basically, we, divide down. The, potential receivers within a beer domain into sets and each of these sets requires an own copy of the of the monthly cost. Packet. Yeah. If there is a protect if there is a receiver in one of these sets, then we need one a specific copy of the multicast packet towards that set and, if you make four Monday cost packets out of 1, this is, of course, overhead. Now we ask ourselves how many packets are required from the sender? Well, if you do IP multicast, then we need only a single packet. And if it and if we do Unicast for application, unique, multicast then we need the packet for every receiver, So that's how it scales. And what about beer? Well, It depends with the the number quickly approaches to the number of set identifiers or sets that we have in the peer domain. With an increasing number of receivers. Here on this, figure, you see on the x access the number of receivers and on the y axis. You see the number of be it packets required. And, this depends on the number of sets that you have in the beer domain. So, if we have one if we we have only one set, we need only a single packet. If we have 2 sets and we send, like, 5, and we have, like, 5 receivers we quickly need, 2 packet, 2 packets from the sender. On, got this, statistically, And, if we have 4 or 8, sets, then"
  },
  {
    "startTime": "01:00:03",
    "text": "when we send 10 packets or 15 packets, then we already also need 4 or 8 packets. So very quickly, we really need the number of packets, that is also the number of sets in the peer domain Next slide, please. Good. I'm trying to raise my hand. I can't get into for some reason. I I just wanna point out that, This is describing a worst case scenario. No. This is a statistical scenario. No. It's clearly, we stop at 56 bit mask, and we can do bigger that in beer. Beer defines larger than that as well. And then your assumption is that the membership immediately is spread across the multiple sets, which, again, is the worst case scenario at all. Exactly. So we that we we assume that this is purely the random and in reality probably, we do not have, this randomness. So, This is this is correct. Sure. So but what about the sets? Links, may carry multiple copics copies of multicast packets and, this also depends on the way we choose these sets. When the left side of the figure, you you see set, like, 2 sets that are not so well chosen. And if you count the number of packets that are needed to, reach all the receivers, it's 24 hops. And on the right side, you see well chosen, sets. And if you can't hear the number of, packets needed. It's only sixteen hops. So this one challenge, choose the right sets. An appeared domain. Next slide, please. To do that, we came up with a very simplistic traffic model, namely every node within that view domain sends 1 multicast packet to all other nodes."
  },
  {
    "startTime": "01:02:03",
    "text": "Okay. Nothing realistic, but it's just an assumption to to some calculations. And the performance metric is we count the overall number of, the whole packet transmissions and that is what we want to reduce by a good clustering. So we came up with 3 different mod methods to create the sets in the petitioning basically in the Nabir domain. The first is random assignment of nodes to set and this is obviously not such a good idea. The second is an integer linear program to obtain the best theoretic solution. Actually, it's a great idea. But it does not scale here. With this approach, you can you can optimize maybe, networks up to 128 notes, but, any larger networks that's more than difficult. And, finally, we also developed a fast heuristic algorithm that covers large networks up to at 10,000 notes here is a comparison And on small problem instances so that we get an idea whether these, fast heuristic algorithm is it's good results or not so good results. So we consider different topologies. Mesh topologies with a no degree of 2 on average. With 4, with 6, and with 8. And the networks the network sizes are 64 and, 128, notes The number of sets that we have is 2 or for, respectively, And, 100% is the load that we get with the intercellinior program. As a message to, clustercluster the that the notes within the pure domain And with the heuristic, we got a load of, around,"
  },
  {
    "startTime": "01:04:03",
    "text": "101%. So the heuristic algorithm was only 1% worse than the optimal algorithm. When you look at it at the random assignment, then we easily get, additional loads of 30 up to 80% more, which is obviously Not so good. So comparing the heuristic and the random, algorithm shows us that the random algorithm works pretty well. And, why do I tell you that? In the remainder of this work, we need to cluster the that the field domains. And for that, we utilize the heuristic algorithm because we look at large networks and the optimal algorithm does not scale towards these large net Wix. But to I got this one. It's pretty good to go Next slide, please. Yes. This one. So, we consider the following experiment. We use, bit string size of 256 bits. And we look at networks with different topologies and different number notes. And the receiver, and, the traffic model is that every node sends one packet to all other notes. We clustered the network into optimal sets so that we have, Got it. The best performance week. Could get from And the results are the following. If you look at the figures on the x axis, you have the number of nodes. So that is the network size. And on the y axis, you have the relative overall traffic. So on the left side, that is beer versus ipmc, That means if you have, a value of 2, that means in a peer network, you will, you will see it. Twice as much overall. Network load as as with, ipMC."
  },
  {
    "startTime": "01:06:04",
    "text": "And we see now very different curves. We the orange and the green curve, those are line and ring network. XL, those are very special. And on lines and drinks, beer has bad performance compared to IPMC. When we look at, at binary trees or mesh networks with a node degree of 2, then this is the bottom line. So their beer and, and IPMC have a very similar overall network load. And in between, we see mesh networks with a no degree of 4, 6, and 8. So this is a line, a rising line somewhere between 1 and 2. And on the right axis, on the right figure, is a comparison between peer and unicast. And here, received. Also, beer has a clearly worse performance than IPMC. Beer can drastically reduce the the overall network load, with regards to when when we look at line and ring networks, so although beer is much worse than IPMC, is so much better then then then application, they are unicast. And, for ring, and for binary trees and for mesh networks with a no degree of 2 We also see a large reduction in load like, 90% or so. Next slide, please. Money, Lenny? Okay. Maybe he fell asleep. Nope. We got the page. There we go. Let's do the question. Yeah, please. You'll So, generally, it's part of tech. If you could could you please come back to the previous slide So,"
  },
  {
    "startTime": "01:08:01",
    "text": "mean, I agree with your conclusion that I think I could be wrong. That line is not actually the worst case. On the left hand side that the worst case would be aligned that branches at the end all the receivers at the very end, and then the value would be twice that. I'm just commenting that this isn't that this gives the impression that line is the worst case, but the worst case actually twice that, but it doesn't come for the trip. Could be even worse, but we considered our, have we limited our, consideration of the performance to where we general structures. Not to special structures. So so We looked at lines, links, and, meshed, networks with configurable. Notes degree, And this is also a question. What what topologies, really typical for Monday cost networks, but we can discussed that later. Next slide, please. What about smaller multicast groups? Because before, I just considered, full mal a full multicast group every node to send the packet to every the note, And here in this, experiment, we considered a bit strings of 256 bits and a network size of 1024 notes. And, when you look at the figure, on the x axis, you see the number of receivers. On the y axis, you see the relative overall. No, overall traffic when we compare ipmc versus Unicast So than more receivers we have, the better is the compression rate or the the the overall load reduction of IPMC versus unicast is of course, we see the same behavior for beer versus unicast, but what's interesting is a comparison of the load generated by beer. And the load generated by IP multicast. And we see here okay, for line and ring,"
  },
  {
    "startTime": "01:10:02",
    "text": "peer is clearly worse than, I can see it. And for the more hierarchical network structures like, a binary tree and espas mish. The relative overall traffic first rises, then it decreases again. So that means especially small trees, really bad for beer because, there is this large overhead as soon as we have multiple sets, it is quite likely that you need to send several packets name maybe one to each of the sets And that that generates a lot, a relatively lots of overhead compared what you could have. It's raise my hand a bit. Again, going back to the descriptors. I mean, great information. I appreciate this. But I when you have, comments as effective as a beer suffers, especially for small multicast groups, you're missing a lot of information in which you've actually tested there because Yep. Small group could be One said. And that's optimum efficiency. So really when you have a small group, over a large network with many sets randomly distributed. We need to be careful with that. So that even people won't come away thinking beer can't do something. Like, no, you've misconfigured your network to make beer work as poorly as possible. So from from from this investigation, I wouldn't, I I wouldn't, conclude so much about, the performance of with small multicast trees because when when you look at real networks or real multicastry there is some correlation about the receivers, and there's principally probably, it's more likely that multiple receivers reside in the same set if you put the clustering right. But, but we see a tenancy at here at least, Yeah. What we this is some So"
  },
  {
    "startTime": "01:12:01",
    "text": "work never got published with I played with a long time ago. Like, gosh, right when we started rolling out beer, this idea of, dynamic bid assignment getting some awareness of your edges, getting aware of of your distribution for example, you've got a national backbone and the sun comes up in the East Coast and receivers over there first. And as stuff moves, you can kind of reassign bits or sets assignments so that you can keep your clusters as tight as possible and optimize your flow. Yeah. We really need to look at, more realistic, receive a distribution to make this, Yes. This work more realistically. Realistic You know, Okay. I just had a quick question to show. If the network was really sparse, on the order of less than 1 set Why would you have multiple sets? He's because it's misconfigured. Eligible. Potentially. Yeah. But what I was saying when you say It says a small multicast group. Doesn't define the actual condition that they have the problem in Mhmm. Right? Small group. One set's not a problem. Small group over a large network with many sets that are poorly distributed That's bad. Why you you could have an applicant. It's not would you have many sets? Because it's greater than the set size. Well, most networks run more than one application. Yeah. Alright. So some applications maybe need all that stuff. Some don't. In in on that network, an application of the small number of receivers However, spread across multiple sets for large apology, that's what they're showing here. Okay. I'll take it offline. Alright. So next slide, please. So this is another comparison. Namely, it's the load on central links. In in this experiment, we have, again, considered a peer string, a bit string of 2 to 6 bits and a network size size of 1024 notes. And now Again, every note sends a package to all other notes. The metric is the number of over then the overall number of packets"
  },
  {
    "startTime": "01:14:01",
    "text": "on all the links. And, we consider now complementary, cumulative distribution function of the link loads. What is this? You look at the figures you see on the x axis, the link load, in in terms of number of packets. That we see on these links. And, on the y axis, you see the percent of links that carry a link load larger than n. So the further we go to the right, the more is the load on individual links. And, On the unicast figure, we see that many of these lines end in the range between 2 to the power 14 and up to to to the power of 18. That is the number of packets we see on the most loaded links. And when we look at IPMC and carry out the same experiment Then we see on on those links only allowed between 2 to the power of 8 up to 2 to the power of 10 packets. And we do the and if we do the same experiment with beer, It's about 2 to the power of 9. Up to to to the power of 11. So it's about doubled the load. Also, we have 4 until we have 4. Sets instead instead of just a single set. But it's not that the number of sets multiplies the traffic on the on the most loaded links. So to numb the the the the load on the most loaded links, does not scale with the number of sets in the network if you do the clustering. Right? That means both IPMC and be effectively reduced to load the most loaded links. Next slide, please."
  },
  {
    "startTime": "01:16:00",
    "text": "Another question is, what is the best bit string size? What what is this question in of of interest? Well, there's an obvious trade off if we have a large bit string, then we need to send only a very few packets, but year header is not sure. And dispute and this bit string needs to be carried with every packet. And if if we use small bit strings, then we need many subsets and but the beer header is smaller. And the question is where is the where is the speed spot? Is there a speed spot and where is it? What's the optimal bit string size? In this in this experiment, we, considered a very large network more than 8000 nodes and, packet size with 500 bytes payload, an IPMC header and a beer header. And again, every note sends a packet to all other notes. Now the metric is not the number of overall patterns. It's the overall. Traffic because we also take the that the header size into account And what we see here now is On the on the on the x axis. That is the big string size for you. And on the y axis, it's the absolute overall traffic that we, that we evaluated. After you, result. When we consider line and, ring networks. Then the overall traffic decreases with an increasing its drink size, and there is some sweet spot, around, 4000 k. If you have 4000 k as the size of a bitstream, and make it even larger than the overall. Traffic increases again a little bit, but, not drastically. Yeah. So but if we consider, other than"
  },
  {
    "startTime": "01:18:02",
    "text": "network topologies, like, measured networks Mesh 2 or measure for a binary trees, Then we see a relatively flat curve And, we can conclude that, a bit string size of 256 bits is quite good for all these topologies. Yeah. Next slide, please. What about forwarding and failure cases? Why is this question interesting. Well, when you remember how we clustered the view domain, we took the we we took the full network, without any failures, and, we optimized the clusters for that routing. And in failure cases, the traffic may be deviated to somewhere else, not as preconfigured it. And the interesting question is what happens then? Is the clustering then, a disaster, so that you end up with double the traffic or four times the traffic and the did did did did The answer is it depends. So we We co conducted the following experiment. We considered a network size of 1024 nodes, a bit string of 156 bits. And every node sends a packet to all other nodes. And we checked the routing for all single link failures. The metric of interest is the maximum load to increase on any link in in in any failure case. And, we considered only the only resilient topologies that by that's why you do not see any line here. Line is not a resilient apology. What do we see here on the figure on the"
  },
  {
    "startTime": "01:20:01",
    "text": "x axis, you see the maximum load increase. In terms of edits. On the link in a failure case, compared to, the load that a link carries in a non failure case, in the non non failure case, And on the y axis, you see the percentage of links that have a maximum load increase larger than I. Not sure than the number of tickets. Did. Solid line, is the load increase that you get with beer and the dotted line the load increase that you get with IPMC because this is not a beer problem. This is a general problem. If, if you have a fader in the network traffic is rerouted takes a longer pass or at least a method or at least the the the backup path is on another link that has to carry additional traffic. So this is both this is a general problem and not be specific. And in fact, we see that the load increase for IPMC and for beer is very, very similar if we look at the mesh networks. And only in the very say, pathologic pathologic, scenario of a ring there, we see a large use a large, load increase for PM. Why do we see that? Well, as you remember, we have 1024 notes, bit string of 256. So we have 4 subsets for tickets are sent. And if the is a failure until Ring. Then you need to send all the four packets to the left because on the right, there's the there is the failure. And this doubles basically the load many links. But Well, I would say the ring here is that a path pathologic, case, you shouldn't now you probably shouldn't use beer on rings."
  },
  {
    "startTime": "01:22:02",
    "text": "Next slide, please. And, we could wrap up because we're running out of time here. So so open issues and next steps. I'd I'd really appreciate, input for more realistic evaluations. What topologies are realistic and what are typical multi group multicast group sizes and also correlations of multi molecules groups and, topologies. What about the small multicast groups in large networks? How do how do they typically look like or how how large are those, multicast groups. And, This is particular of interest because Malibu beer packs are needed for receivers in different sets. And there are also new kits on the block, namely explicit tree structures in packet in packet headers that combine beer and segment routing ideas that we will talk about tomorrow in the peer sec in in the peer working group. Next slide, please. So this is the conclusion. Be required sets up for scaling to large networks. And that also means that we need to 1 packet per set that has a receiver. And, we developed for this problem of fast heuristics so that we can cluster the build domains into such sets. In our performance comparison, we compare beer with Ip and C and unicast and considered the number of packets from a sender, the overall network load, the load on most load links and, potential load increase in failure cases. Bit strings with 256 are good enough for most network topologies, and exceptions are linked and and and giants and rings"
  },
  {
    "startTime": "01:24:00",
    "text": "And there may be also some issues with small multicast groups in very large network. If you wanna read a paper, this has been accepted for it to be transaction network and service management. And it's available on on the excess or here, it's also a pre print on my website. Thank you. Any more questions? Alright. Great. Thanks. Let me take over. Got 6 minutes left with a 5 minute presentation. So think we can do it. Great. Thanks. So this is just a a very quick update on the treaty n draft, which is progressing in the Mops working group so in terms of what is you know, the the the problem that treaty in is trying to solve, and it's it's really that audience sizes, are now in the tens of millions, somewhat commonly for for, live streaming events, combine that with bit rates suggests that maybe we're at an inflection point for network resources consumed by live streaming. Last year, it was Thursday night football and Amazon, was the first the first time that NFL Football was, American Football was, streamed exclusively, over the internet and they saw audience sizes of about 10,000,000 10 to 15,000,000 viewers every Thursday night. 16 nights a week. Here. This year. And if I'll send a ticket, is, that's that's pretty much all out of market. American football games are, exclusively streamed for the first time ever. And, earlier, early this summer, for the Cricket IPL final, they announced 32,000,000 concurrent, viewers, concurrent streams. I believe that was that's believed to have been the largest"
  },
  {
    "startTime": "01:26:01",
    "text": "number of concurrent streams. So, the other thing to keep in mind is live streaming is not the same as on demand. Primarily for the latency budget for to match, traditional broadcast television it should be about 10 seconds. Compared to 10 seconds, from actual live occurrence. Otherwise, you you run the risk of caring. You you can't have 1 in 2 minutes playoff offers because you you run the risk of, you know, getting a text message from a friend about the game winning score. Also joint rates are vastly different. They're not smooth in predictable, like normal, on demand traffic it's it's more of like a step function as everybody tunes in just as the game starting and everybody's tuning out once it's over. The other thing is the the goal of, you know, I've I've I've mentioned some of these events the goal has generally been can it be from a from a viewing standpoint, a viewing experience standpoint, the goal has generally been, to be almost as good as what television has delivered for the last, you know, 70 years in terms of reliability. Wouldn't it be better to, you know, Amazon Prime with with all their resources, with all the CDNs that they've thrown at it, and I I believe they they announced something like 5 different CDNs they were using. Still are not they're not offering the games in 4 k. For example. Now here's a question. What what if you wanted to have an immersive experience, you know, that AR experience where you could throw on goggles. And feel what it's like to sit at a game look to your left of you, look to your right of you, feel what it's like to to be in, you know, the crowd as opposed to just a, you know, a traditional screen of high def or or maybe even 4 k. Question is, could this experience"
  },
  {
    "startTime": "01:28:01",
    "text": "is it possible over the internet, over today's internet with, say, you know, a million viewers, of that kind of high bandwidth experience. So, with median. So network based replication, it's been fairly successful in some places, incredibly successful in some like financials and video distribution networks less so over the end bone. Right? And that's that's why we're all here. So You know, we we've we've talked about the problems of internet multicast kind of beaten. Think you know, beaten, that that dead horse has been beaten many times in this working group, but you know, kind of the 3 biggest issues are, you know, they all are nothing problem. The the complexity problem in chicken and egg. There are now tools and technologies that, solve and address all three of these issues. And that's what treaty in is. It is nothing new. There's no new protocols. It's actually the synthesis of existing, well known, well understood protocols. It's really 2 components. There's the native component. And that is supported by SSM. Generally speaking, PIMSSM, but could be anything that delivers SSM, MLDP, GTM beer. BGPM VPN tree said, you name it. We are not, discriminating, as long as it's, supports SSM, that that can be used And then, combined with overlay technologies, AMT is, you know, the typical solution there. But it could be anything else like, like, say, lisp and this combination of, you know, AMT on the overlay, it delivers a service. It is, not the goal is not, you know, purity technology technological purity of, you know, pure native multicast it's let's get packets to a bunch of receivers. And we don't care how that happens. As I mentioned,"
  },
  {
    "startTime": "01:30:01",
    "text": "this is a draft in, the mops working group. It is, we've requested, working group last call. It's been progressing well, and quickly. And input and feedback. I share this with this working group because, this this working group, along with pen, has a lot of expertise, in deployment and protocols that, would be likely very valuable. So this is just kind of a a pictorial view. You know, you have the big eye internet. That is mostly unicast only. You have a treaty and provider that is native. And for off net receivers, those are receivers that are on a unicast only network. You use overlay technologies like AMT to deliver contact This is not intended to replace CDS, but, actually, this is a new CDN model. So, you know, if you think about how CDNs work without multicast, unicast traditional CBNs. Send traffic to CDN boxes. Are usually racks of X86 servers, and they they, they send it to each other, and then they handle each, unicast replication. Tradian is CDNs, you know, tree based CDNs. And they're leveraging multicast. And the nice part about this We renamed those CDN boxes. We call them AMT relays. And if those AMT relays are our, existing routers that support AMT essentially, you know, you just configure a couple lines of config in the router as opposed to racking and backing and powering and cooling and, a bunch of X86 servers that need to plugged into routers. So it's arguably 0 CapEx. Use cases, anything multi destination. I've talked mostly about streaming video. But OS updates are are another, less sexy, but really important use case. And the benefits are, you know, more efficient network utilization, gets more out of the network. Not only does it support existing content like the the"
  },
  {
    "startTime": "01:32:02",
    "text": "the things I mentioned earlier, but also, makes possible new content, things like live streaming AR that I would argue, you know, I I would wonder aloud if if Amazon with all of her resources can't even do 4 k, you know, think about technology from new con new content from new sources. Much greener and more sustainable solution. Because, again, you don't need racks and stacks. You know, you can essentially do a CDM on a chip. It this enables service providers, to offer replication as a service. Again, at@atat far less cost in order of magnitude cheaper to deliver the service because it's a much simpler model And it also is a democratizing and decentralized solution gets back to the decentralized nature of the internet. Because is it healthy for the internet and society on this small handful of companies control. All just about all distribution of content. So, again, crossing supply and demand curves, demand is, is exploding for multi destination traffic with with you know, these audience sizes of the tens of millions combined with high bit rates. And on the on the supply side, it's never been cheaper and easier to deliver multicast services. So that's what Treaty is. This is the draft, and I would, request folks, check out the draft, provide input there will probably be, hopefully, be a working group last call at Mops very soon. And, you know, if if if you're on that working group, list, please please do comment. If you think there's value there. I've I've asked them to actually copy include, Pam and and and bone d because we've I think, in, in the working group last call, but, we'll see. Hopefully, that happens. But, Anyway, just a just a quick update. And, you know, input from this working group would be valued. It for me. I'll take any questions."
  },
  {
    "startTime": "01:34:00",
    "text": "If there are any, I see none. Okay. So everyone use the app, log yourself in so we had a head count important part if I get a room size next time. Nods, We're just so engaged goodness. You had lunch and now it's nap time. Right? I understand. Stig fire those off when you get them. Appreciate it, and I do owe you I promise. Unless something tragic happens to me before I can pay you back, because it wouldn't be my fault. Don't hold it against me. Alright. Thanks, everybody. Last of our content, have a great last 2 days in Prague if you're sticking around. And hopefully, I'll see you guys in beer tomorrow. From there Wendy, you saw him on the call? What's discussed on the phone? I am. I just wanna point out something else with the comparing CDN. Is that it's you have to maximize your expenses and for potential capacity, whether the content, you know, receivers are there or not. And your your ceiling is your CDM capacity, whereas multicast Your bandwidth's already there. You don't have to preposition content or preposition resources to be prepared. Because you already have a network and your ceiling is your network capacity and not much CDN capacity would have? So I always don't like it. So it's Yeah. Sorry. You know, it's like worst case scenario. Let me let me vial this stuff and and provision it all in that somebody's gonna use and you still have a cap. Rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights rights and then in the deployment too, you've got more router port cause you have all this equipment that has to be on the network as opposed to just your cross link cross connects and off to your age. Right. Yeah. So It all but I these are like, I've I've fire these buttons off, but, oh, he told us not to do that. We were told not to touch the camera."
  },
  {
    "startTime": "01:36:00",
    "text": "No. And he told us not to put it back. No. No. No. Leave a look. He says they have to they have to precisely align it so they can get stuff right now. Exactly. I mean, I I asked for help. I think, you know, Houston,"
  }
]
