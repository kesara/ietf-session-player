[
  {
    "startTime": "00:00:50",
    "text": "sure it's quiet in here i really wish we had like elevator music before the meetings started so you don't feel like your audio is not working i did that in one of the web transport remote sessions during the pandemic and people were very annoyed at me you gotta choose better songs i mean is there a rating system for elevator music songs yeah no but it's more fitting for a working group business all right we'll we'll give folks a few more minutes and then we'll get started yeah slow mini hell is probably a good summary for what we do"
  },
  {
    "startTime": "00:02:02",
    "text": "right oh all right i i think we can get started uh welcome to the web transport working group by tf-114 it's really nice to see so many of you in person uh more than last time and way more than for two years when we you know had the first session of this working group in the pandemic um next slide please bernard so for everyone in person as a reminder we've been doing cue management using meet echo so first off just make sure to join meet echo to get yourself on the blue sheets we're not doing paper blue sheets anymore"
  },
  {
    "startTime": "00:04:02",
    "text": "there's a qr code to simplify stuff otherwise it's accessible from the itf agenda page the full meet echo allows you to have access to the chat and otherwise the meat echo light which this this qr code gives you the opportunity to join the queue and to join the blue sheets but without the rest of the interface it's uh it works better than the other one on phones for example um all right next slide please uh so here's a reminder on the buttons um you know please stay muted unless you want us to hear your keyboard and there's a hand icon for joining the queue um yeah if if your remote having the video on when you're speaking is pretty nice helps us understand you better but it's not required um then yep here are some links especially if you're following along remotely next slide the notewell so some of you may know this pretty well but it's worth taking a minute to discuss this the what the ietf does is covered by our notewell and if you're here it means you actually said you read it on the page but everyone is used to clicking things without actually reading them so let me take a minute one of the parts of it is that anything you say at an itf meeting or on the github issues or on the mailing list is considered an itf contribution and that triggers the itf um policy on intellectual property patents and all that so if you don't know what that is you should take a look because if you're aware of a patent it means you have to disclose it and that could become complicated if your"
  },
  {
    "startTime": "00:06:00",
    "text": "lawyers at your company could be upset at you if you don't do it right so just make sure you read all this and next slide please the notewell also covers the itf code of conduct and anti-harassment policy i want to take a minute to underscore that we've never had a problem in this working group everyone has always been working nicely so let's just keep doing that because everything's more fun when everyone's nice and if you see anything that you think is not great we have procedures in place for reporting it come talk to me or to the ombuds team uh we'll make sure it gets handled the best way we can uh next slide please so as a quick reminder the itf has a strict mask policy for all working group sessions if you're attending in person you have to wear a mask unless you are presenting or at the chair table which is far away from everyone and currently speaking also note that masks need to be certified with these certifications or something equivalent which means that most cloth masks and surgical masks actually don't qualify those don't really do a good job of preventing transmission of the latest variants so just fyi we have free kn 95 mass at the front desks if you need one and you've or you forgot yours there come in all sorts of cool colors too okay um and if you're talking at the microphone you don't need to take your mask off just get close to the microphone it works really well uh next slide please all right here are some more links um we're going to need a jabra scribe and a note taker can we have a volunteer for jabber scribe all right thank you jake can we have a volunteer for notetaker"
  },
  {
    "startTime": "00:08:02",
    "text": "uh the fun part now i get to awkwardly stare at people in the room and remotely and we're not going to start the session until we have a volunteer because notes are very important it's best if i don't have to pick someone i see everyone's intently staring at their keyboard oh oh thank you jake that is amazing all right can someone do jabra scribe that's a very limited bit now ah thank you alan all right thank you both oh yes we are now using zulip um you so you can either join through the meet echo chat or through the zoolop client and both of them seem to work and are bridged together and if you want alan to say something because for example let's say you're remote and you don't have audio just say mick colon something and alan will jump in the mic queue and say what you said at the microphone awesome thanks both of you next slide this is our current agenda so first is me blabbering around for way too long then a an update on the w3c process for janivar uh then discussions on capsules and on the capsule design team 4h2 and h3 led by eric kinnear and then going through some issues uh for web transfer h3 from victor and then we'll wrap up does anyone want to bash this agenda all right [Music] next slide please uh johnnyvar are you we see you and we can't oh say something again"
  },
  {
    "startTime": "00:10:02",
    "text": "yes we can go ahead all right thank you so uh i'm janavar um um co-chair with uh will law for the w3c uh specification for web transport so i'm here to give you a progress update on changes since march 24th so we published another working draft the latest version is june 23rd of this year and we have a charter extension underway for an additional year because the current charter expires september 22nd so if you have input on that it's still not too late to provide and then uh the next bullet has a type bow uh it should say more realistic time table for the year because we had an earlier we presented an earlier optimistic timetable that was not anywhere near realistic so currently we're aiming for this is what we're aiming for uh september end of september for canada recommendation which requires stability in api and then by end of the year we're hoping for that's our goal post for a proposed recommendation at the moment which would require two independent implementations for our charter which would put us in line for a call for review in february and ideally everything works we have publication by recommendation by the next ac meeting in april ish all right so um we've defined some milestones we have the original milestone for the initial implementation that shipped has three remaining issues we've been a bit slow to close those but none of them are major we have a new milestone created which aligns with the w3c release process of candidate recommendation where we have 10 open issues where about six are ready for pr next slide and so here are some decisions since um our last presentation in march we have added per stream stats"
  },
  {
    "startTime": "00:12:01",
    "text": "that means per outgoing and then going in duplex stream not not datagrams and these are uh not these are bytes written by sent and by technology bytes acknowledged which are not uh total network byte counters however they're they're mostly concerned with the bytes application bytes that are written to the stream and how much of that has been sent and how much of that has been acknowledged so uh bytes acknowledged will always be less or equal to by sent which will always be less or equal to bytes written and then for datagrams we reduced the uh priority algorithm to normative guidance because we found some mistakes in it um we still haven't gotten any further on specifying uh specifying that algorithm in detail so that's left to implementation we put some datagram stats in a subdirectionally of getstats so we have a couple of additional datagram stats that are dropped too big dropped incoming and lost and for the web transfer control web transport constructor where we now support a re require unreliable through boolean which defaults to false this is so applications can in the future specify whether they want to uh require udp and by default they will get fall back to http uh too and we added another read-only property for that so that you can tell what what you're looking at um another issue was is connection pulling off the right default and yes so a lot pulling still defaults to false next slide all right so current issues of debate so we have three uh the remaining issues we've been circling around the same remaining issues and and this is one of them which is"
  },
  {
    "startTime": "00:14:00",
    "text": "people want to send media and that doesn't always work so great with the default congestion controlling quick so we have agreement to provide some constructor level configuration api surface that would allow an application to specify its preference for the type of condition control to be used now we know that that's not necessarily available anywhere yet however we hope that we can get the api ready and have bashed out all the api decisions here to get us to candidate recommendation and we can then subsequently mark this as a feature at risk if implementations fail to materialize prior to proposed recommendation so discussions around shape uh remain so we have two proposals with two directions basically one is a highly abstract input that you specify basically the type of congestion control the type of problem you want congestion control to solve where the default will be throughput which we have today and then load latency would be a sort of application hint that i want to send something that is more real-time than that and with different trade-offs and the alternative is to have basically a getter that exposes the names of available congestion control algorithms and then you can then specify more specifically you can learn what the browser has but you can also specify a specific one next slide a second issue is uh datagrams versus streams and relative prioritization having a prioritization api discussion seems to center here on ordering instead of bandwidth allocation that's an observation from the shares ordering requires strict and not weighted levels um the the desire here seems to be to support use cases around uh you know one"
  },
  {
    "startTime": "00:16:02",
    "text": "frame of media per stream as well as a control channel a high level control channel that you can't uh starve as well as real-time adb audio video streams and long-term background downloads so some schemes may need as many levels as we have objects in flight current proposals boil down to again there's disagreement so on one end i think at a minimum we think we need to to expose at least eight resettable levels to match what browsers are doing in most cases and this would allow javascript to down prioritize ongoing streams by basically saying on this stream that i've sent before set its priority now to a lower level which should give enough granularity to solve most problems with some effort um and this the assumption there is that you are going to have javascript in involved in the send loop that is going to be very active and responding to changing conditions in your connection so this is a low-level version now the alternative would be to provide something more upfront where you can specify fixed levels and that has been specifically requested for a warp and chrome has volunteered to investigate if that is practical having in 32 number of levels would provide javascript with some more ability to just hear all the priorities levels i want send it to that order and and i don't need to change it later next slide what's up jake uh is it is there a should i ask questions now or i have a question about the yeah but for next time go in the queue from the button but yeah go ahead okay sorry uh hi i'm jake holland i wanted to ask"
  },
  {
    "startTime": "00:18:01",
    "text": "about the congestion control api are you also going to uh set the server side congestion control with this api no i should uh no i should have clarified this is this would only be for we assume servers will take care of their own congestion control and clients in that case will just receive what they receive so uh so apologies so this would only be for uh ingestion and basically for clients sending media to servers that's the that's the missing gap that we're trying to specify thanks for that question thanks and uh just for for everyone um i think it's really important to get some itf involvement in the w in these issues from the w3c so i just pasted a link to the github in the chat here and i think the congestion control one is a perfect example um so there might be some people in the room who have opinions on latency if latency or throughput matters more and there might be more in this room than they are at the w3c so please go and comment on those issues that's what we're asking for here because that that's the kind of thing where the w3c has to figure out an api for it but we have the congestion control experience that ietf so this is the kind of cross-pollination that we left to see and i see bernard in the queue yeah i just wanted to mention something yanovar uh which has come up at this meeting which is uh the idea some of the i4s uh stuff and in that situation you can have algorithms that are really about both latency and throughput like prague um so anyway just just a weird little uh uh yes uh there was a detail in the api didn't show which is that uh you can still for the second proposal when you expose the name you don't you can also expose other attributes of each congestion"
  },
  {
    "startTime": "00:20:01",
    "text": "controller such as what the aim of it is and you can have enums for for several of these properties if you will so but but uh thanks david that's a good question that uh good it's good to highlight that these aren't set in stone in any way this is just early discussion and if they provoke you to uh participate that is excellent so so we're definitely uh maybe a bit we we could definitely use some more input from more people and that would probably help move this discussion along and i see uh omar in the queue by the way if you get in line in the queue don't hesitate to like walk up to the microphone as well so you're ready to talk when we call on you can you hear me through the mask uh speak a bit close as close to the man to the mic as you can please owner shapira uh apple uh as david hinted there are many people with opinions about what matters more uh servatory latency they spent four wonderful days discussing this last year uh the opinion that i'm going to voice is that it may be counter-produ it may be not useful to allow the uh application to set the name of the construction control because by careful tweaking of the parameters one can cause neurina behave like cubic and cubic behave like tahoe and what's not may be much more productive to have the application uh express its um its goal do i want to be uh do i do i need the real damage latency how sensitive mi2 delays"
  },
  {
    "startTime": "00:22:02",
    "text": "how sensitive am i to so good spikes etc that's what i have to say thank you omar can i ask you to uh like kind of take what you said and put it in that issue i think that's really good feedback uh i'll post a link to that specific issue um on in the chat thank you so much stewart thank you yeah i think that's good input thanks i saw david was talking about throughput and latency and looking meaningfully in my direction so i felt compelled to say something um uh i worry here that there's a tendency to overcomplicate things um as we've seen this week at the hackathon with the almost work done on l4s uh and other things that have been going on in the industry this year it's possible to have low latency anti-throughput at the same time it's not an either or choice priorities become very problematic because somebody's got to decide what the relative priorities are and and if you have enough bandwidth for everything then it doesn't matter every flow gets what it needs and if you don't have enough bandwidth for what you need then it becomes extremely tricky to figure out uh what is the right way to resolve that do you have a strict priority where you have you have total starvation for the lower priority things or do you have some relative priority um uh this is all very complicated but the good news is with l4s uh and similar technologies uh the whole problem goes away you you open multiple streams and they each get a nominal fair share of the capacity when it's scarce when when bandwidth is abundant"
  },
  {
    "startTime": "00:24:02",
    "text": "then everything gets what it needs so um i guess the summary is that let's not over complicate this with mechanism that is is really hard to understand and even the people at the ietf who are congestion control experts find this hard to understand so the average web developer is probably just going to twiddle knobs randomly without even understanding the implications of what they're doing [Music] thank you stuart can i ask the same thing and just ah and also to everyone to also add that on the github issue for the w3c thank you alex um hi everyone i'm alex schneichowski i work at google and one of the things i wanted to mention is that i was a little bit surprised when i saw this slide because i remember when we were deploying bbr on the youtube cdn and one of the concerns that we had was that we actually saw people complaining about bbr's initial lack of fairness with all the other congestion controllers and one of the things that i worry about here is that even if you do something nice like saying you know aim low latency versus aim throughput if you give people the ability to choose these things they might actually end up with pathological behavior on the broader internet even though they've set their aims and i think that my gut feeling is probably better to focus on high quality congestion controllers which do well most of the time and not give apis which might actually result in poor performance unless you are very careful and know how to hold them i think the experts who work on congestion control know very well how to do this well and the rest of us should benefit from their knowledge thank you luke hi uh luke from twitch here um so first thing the congestion control uh it's something that is i think the low latency hint is pretty important um one of the things with warp that we struggle with is uh cue management and just trying to have this buffer in the socket that needs to be sent and buffer bloat is an issue like if"
  },
  {
    "startTime": "00:26:02",
    "text": "like there's no point prioritizing anything like you just everything's gonna be ordered over the wire um so just a way of you know saying congestion control like keep the rtt down is important and for the next slide just um i think there's two little things that come down to it one is um like you said mentioned ordering is important um it's not clear if the eight levels the ordering is mainly like uh is priority two always uh lower than priority three um and exactly like you mentioned as well you need at least enough levels as there are active streams and eight is kind of low but um for warp it would be fine honestly but um if you start doing stuff like per frame priorities then eight is just gonna be artificially low it's almost like a flow control limit of eight hard-coded there's just not much you can do but all end of the day it all comes down to buffer management just a way of saying uh we want this data to be sent over the wire first and uh nothing else can get in front of it thank you thank you jonathan uh yeah donald linux i mean i get the impression that the what is actually meant by throughput versus low latency is cubic versus gcc which i mean personally i'd be fine with but um i mean obviously short term longer term um personally congestion control people will come up with something more clever but in the short term those are the two algorithms that are actually deployed in chrome and i suspect the idea is to switch out the one for the other uh just to add as someone who used to work on chrome chrome supports bbr as well okay well i mean but yeah i mean i think the the goal of the people who want the low latency is to basically to get something like gcc you know for the interactive media"
  },
  {
    "startTime": "00:28:00",
    "text": "thanks yeah i know to uh relay that conversation that happened away from the mic you mentioned gcc is google's congestion control not everyone's second favorite compiler uh victor you're next oh sure oh i just wanted to say get closer to the mic please okay uh i just wanted to say that there is practical tradeoff between throughput and latency in the sense that there is some level of fundamental uncertainty of what your benefits and any attempt to probe it would result in building up secure so uh that is one of the fundamental tuning properties that pretty much every congestion control scheme has to overcome so from that perspective setting latency targets make sense thank you victor ian ian sweat uh google yeah i would also prefer a objective-based approach whether it's latency or throughput i mean even two levels is vastly preferable so like there are times or we actually have deployments where we're using bbr b1 but we have it tuned to be much lower latency and it's not as good as like uh you know a real-time congestion control but it does prevent buffer bloat um and so for a given dash controller as did before you can commonly tune parameters to like provide output that's much more similar to one of the other i don't really know what we're going to do with cubic in this situation cubic seems like always the wrong option as a concession controller but um it's becoming a proposed standard and it's what we got so i don't let's let's hope that no one actually ships cubic by default here but thanks colin"
  },
  {
    "startTime": "00:30:00",
    "text": "fascinating to hear the only thing we're standardizing sucks um the uh i wanted to actually jump back a bunch too there are some comments about users of this at the api level will just be confused with this and not how know how to set these things and that's that's unquestionably true in some cases with all these things i'm not arguing against that but i think that is the wrong thing to design for that the thing is we have to realize that whatever levels of controls here we give limit what the applications that literally billions of users use like zoom webex these other things that are using huge numbers of minutes they do know how to set this stuff okay they have some very good people at all of those companies are doing broad webrtc products and if you don't give them the controls to be able to set things up the way they need whether it's twitch or somebody else they just can't use this and they will will just abandon the web stuff and go use um thick apps which is it was the problem so we have to design for the use cases that represent large numbers of users of end users on the internet not designed for you know an average web developer who may not understand this stuff um so i think that we should design for giving lots of control of what's going on at this api level and i think that's a different direction than we have traditionally gone on javascript level apis but i think it's necessary if we want this to be successful thanks thank you and i've cut the cue after eric on this specific topic cool tommy pauly apple um so to colin's point i'm sympathetic that you want to be able to have fine grain control particularly like if you're doing something like option b where you want to give like here's the specific name of my congestion control algorithm and i i'm sure the people who know what they're doing will want to take advantage of that um my comment though on the other style if we have something like a that's more like here's just i'm describing what i like is there a reason"
  },
  {
    "startTime": "00:32:01",
    "text": "that and maybe there's something else in the api already but is there a reason that we're specifying the properties of the congestion controller we want as opposed to specifying the properties of the traffic we're doing to say you know i i am doing real-time latency sensitive interactive audio or i'm doing just streaming of video or i'm doing more bulk data transfer and that way the system can choose the right congestion controller but also potentially other things and that's the model that we've seen um in other apis and like in taps and stuff like when they expose it it's like this is your category of traffic and then you can make potentially other decisions rather than having the application try to describe the congestion controller that they want with these other names like low latency and throughput um so like if we're not going to give it a specific name can we describe the traffic instead of the congestion controller properties that make sense thank you tommy mo moza daddy cisco um regarding collins point uh i think if you look at the webrtc example where we started off um you know thinking this stuff is way too complex for the average you know javascript person don't give them control list the browser has to do the rtp the browser just do the codex the browser has to do all of these things and then we started unwrapping that and now we give more control and now you have web codec so you don't have to do rtp and now you have wasm that can do the codec directly in javascript a little so i think we should realize that the application innovators are faster than the browser vendors and we need to bias some of our designs to that um one specific thing on the prioritization though um when you start talking about abstract levels you know one two three four five six whatever like someone said those numbers don't mean much unless you know what the actual prioritization method is whether it's you know strict priority or what the queuing discipline is and all that"
  },
  {
    "startTime": "00:34:01",
    "text": "one of the things um people may want to consider is in rmcat there was a proposal called nada um it's actually an rfc now but uh it's an experimental congestion control and one of the interesting things about it is it has weighted fairness and so rather than expressing priority in terms of um you know abstract numbers they are weights and they are weights relative to what a default unprioritized stream would be so you have an atom almost you know as if you have an atom stream that if you don't do anything that's what you get but if you want to have a priority you you specify a weight so if you want to be a three three times heavy stream or a half heavy stream so that weight is in is in terms of an absolute thing it's in terms of it's relative to an absolute thing which is the default stream that you would get if you didn't do any prioritization so i think that may be a useful concept to look at when you look at doing the prioritization apis thank you that makes sense eric eric kinnear apple um if we go back to the congestion control stuff and continue to talk about it um one of the challenges that we've seen in trying to express something like low latency and throughput and i'm usually one of the first people to jump up and say no no describe what you want the properties of what you're looking for more than you know hard-coding cubic and assuming that's just going to get you what you want but i think we've alluded a bit in this discussion to the fact that like you might have something that gives you both low latency and throughput and so we've almost we've had real trouble trying to specify something that actually makes sense in real life for people uh it's almost like you want the inverse of that of let me tell you the thing i am most willing to compromise on because we haven't talked about like power here but that's another consideration um that you might be taking into account especially if you're you know trying to upload something to do ingestion of media in the background"
  },
  {
    "startTime": "00:36:00",
    "text": "while the user goes off and does something else um and so once you start saying oh well i'm most willing to compromise on latency because i'm interested in everything else being better that starts to get really messy and kind of gross so i would almost support what tommy was saying of either let's go all the way up to the top and like describe what we're doing rather than some intermediate property that we think will accomplish that goal or and maybe both also give people a direct ability to just say nope like i'm advanced i know what i'm doing i'm working on developing conjunction controls and like i know i want bbr v2 let's do it that's going to be exactly what i want awesome thanks eric all right i i had a feeling that just poking the congestion control there would be very successful in an itf meeting and it was so thanks everyone for the really good discussion i'll repeat my point about please adding that on the w3c github this is really good input for them and uh that's something that they can act on so thank you all right yanivar keep going oh yes thank you can you hear me so yes uh thanks again and uh yes i think we should say the w3c will be probably perfectly happy to specify um whatever you guys come up with and we're very open to your input so thank you uh the last slide the current the third issue under debate is to expose some stats to enable javascript to build more rtp like real-time protocols for client to server audio video so the previous discussion was all about giving javascript control knobs for what the browser can do about it and there's some some who are trying to hand off this wholesale to javascript as well and you know somewhere in the middle you want to control all of this so"
  },
  {
    "startTime": "00:38:00",
    "text": "uh this it's a separate issue that we're tracking it's assumed to be about datagrams only or at least at the connection level only um so uh this is again open for discussion uh current data ground stats only detailed loss through we have expired outgoing dropped incoming and lost outgoing and um so we've struggled with what kind of stats so we've asked the question what kind of what kind of stats would javascript need in order to build its own uh congestion control algorithm for example here uh and so there's an rfc 8888 that suggested uh latest rtt packet departure package or packet arrival which i assume is arrival on the server right and then ecn maybe an ack info would be sufficient and we've also reached out to um david balderson i hope i got your name right for some experimental data over implementing rtp over web transport with bbr 2 or maybe bbr2 plus screen so this is still we still need more information here to understand um what stats would be needed so if anyone else is doing experiments like this we would be very interested in your input and the reason why this is only for datagrams or only at the connection level is that the javascript api for outgoing incoming streams do not operate at the packet level so the questions we still have are our packets and datagrams sufficiently analogous for an rtp like implementation and you know this is again an exploratory issue so uh questions of welcome or input welcome i should say"
  },
  {
    "startTime": "00:40:00",
    "text": "i think that's my last slide all right thank you genevar um the only bar sorry um anyone have any questions on this before we move on to the next topic all right well thank you very much and now it's external come on up and take the mic off the stand probably and there's a pink x again it's almost as good as a box [Music] how's that gonna do yes no good bad sweet all right cool i'm eric kinnear from apple and if we can get the next slide please we're going to talk a bit about the capsule design team that we started in ihf 113 and the main question was what the heck should we do about capsules so like should we use them should we not use them um we had an existing h2 spec for how we do web transport over h2 and we were defining all of these new h2 frames that we wanted to use um to make a kind of a baby quick that you run over an h2 stream and we said some of these could also look very very similar to what we're using in h3 where we defined a couple of different capsules as well and if we go to the next slide we can see like we had a datagram capsule which is coming from hp datagrams and we want to use that to send it on an h2 stream just as much as we want to send that in h3 uh there's also a closed web transport session capsule in h3 and if we go to the next slide we can see we have a whole pile of them for h2 so the obvious crossover here is something like datagram we also had padding reset stream stop sending"
  },
  {
    "startTime": "00:42:00",
    "text": "actual stream capsules um and then flow control which we've stuck onto one line here but is some combination of max data max streams max stream data and then blocked variants for all of those things this is our new compact representation of that all right next slide please we had looked at this slide in 113 as kind of the precursor to um spinning up this conversation so i wanted to just look at it again so this is the full list of all the different things that we defined for h2 and if you go to the next slide we had kind of tentatively talked about hey there's this datagram one and it's shared with h3 and so that would be cool if these things shared and we didn't just define two of the same thing and that's kind of what got us talking about should we be sharing everything else how does the rest of this work what is the role if i can i send a wt stream capsule on an h3 stream and is that cool does that give us awesome version independence does that destroy everything and make things go down in flames so next slide please we also opened the can of worms that is flow control that's what we started with in terms of problems for everything and like i said on the slide here we'll talk a little bit more about flow control and some of that stuff later but the opportunity arose to say hey we have all of these different like stream max stream data blocked capsule should we send that on h3 and what does that mean so that's kind of what we set out to solve uh where we are right now is we have a pull request against h2 and a pull request against h3 that we will send the links out to on the mailing list and ask for a bunch of input and review i'm going to summarize very quickly in slide form what those do because that's often a lot more grockable than reading a bunch of diff from what it used to look like so if we go to the next slide the first thing that we talked about and came to a proposal on was what do we do"
  },
  {
    "startTime": "00:44:01",
    "text": "with this splitting a datagram out into a native h3 datagram that is actually truly unreliable and quick whereas obviously when you're sending a datagram over h2 it's going to get retransmitted whether you like it or not and so we've been referring to that kind of as you're exploding something out into a native feature or six other terms for it but i chose to use explode here so we're going to continue with that one and we looked at some pros of why we would want that it's really attractive from a symmetry perspective to have this single conceptual model that looks kind of like a miniature version of quick that you can run on any http exchange that you have anywhere you could potentially get h1 support out of this for free you just you know doesn't matter it's completely transport agnostic this is just how i send web transport streams there's a web transport stream capsule and it just goes where it's going to go and it gets there how it's going to get there this is also kind of fun because if we reuse all this stuff things that we do in the future for different extensions if we add new capsules those automatically work for h3 they automatically work for h2 and if there's a butt coming it's on the next slide there's a way longer list of cons which are mainly and primarily that we care the most about h3 for web transport and h3 is the one where you have the most native feature usage already right so like datagrams actually go in h3 datagrams um now you'd have to be able to handle all of those capsules arriving on the same stream so when you had different web transport streams instead of those being different h3 streams you'd have to cope with multiplexing them onto a single h3 stream and as much as that's great and gives you all this great protocol you know kind of transport independence now the common case you have to be like how's it coming in what do i do are you allowed to switch part way through like what if some go over a single h3 stream but others you choose to split out into its own h3 stream and like can i restrict that if i'm not willing to"
  },
  {
    "startTime": "00:46:00",
    "text": "give you some of those resources and what does that do to our stream limits for flow control which we're going to talk about in a second so that's kind of painful there's also one of our only two capsules so far for h3 which is closing the web transport session doesn't really make sense with the like how would you explode that into a native feature that's not a native feature that's just a signaling about the h3 session and then similarly one of our other main pros that we were excited about doing this with which was that you know if we define it all in one place when you improve it everybody gets those improvements for free and we don't have to keep having a parallel document for every possible version we want to send this on gets more complicated because if we decide that on the down the line some extension needs us to have used the native feature or needs us to not have um we're kind of screwed so the proposal here is that you have a per capsule requirement for how to send it which is kind of already inherent in capsules and is is already a thing but essentially that we're going to require that anything that can be sent as a native feature is always sent as a native feature so you are not sending a datagram capsule on your h3 stream it is an actual datagram and that persists throughout all of h3 everything that h3 can split out which is most everything um looks just like it does today there's no debate over oh but it came in this other way am i supposed to handle it some weird different way and can i signal to the other person about it so no weirdness there just if it can use a native feature it must use the native feature if you're in h2 and the native feature isn't available then it is sent as a capsule on the existing h2 string next slide please this is a nice easy one capsule protocol is a header field in the http datagram document we cheerfully say that you can either"
  },
  {
    "startTime": "00:48:00",
    "text": "use the capsule protocol header field or you can simply say that this upgrade token implies that capsule support exists and we're going to say great the upgrade token web transport means capsules are a thing and you have to be okay with that mike does that also require that the underlying quick quick connection must support datagrams whoo that's a good question because you've made an assumption there that it must be used but you can't assume that's true for all age three unless you're making requirements on the transport stack underneath you that is a good point we require capsule support and we say that if you have that's an interesting one if you have datagrams available you have to use them but you have a button push the button no join the queue and walk up to the mic victor you know how this works i was going to ask that question in the previous slide but it took me that long to find the button again i'm right there with you how do we normally signal if datagrams aren't there because we i mean we just say like i support datagrams i'm good to go so i guess if they don't do that then are you actually doing web transport right um so tommy paulie um i mean the datagram support is indicated via transport parameter in quick so you need that and that might not be there right and you have the h3 level setting so i mean either you're going to say you don't like web transfers just going to break in those cases or you need to tweak the language where you say that you must use datagram in h3 to say you must use datagram if you have the transport parameter but if for some reason the other side didn't do the transfer parameter you know already that it can't do it so then you must use the"
  },
  {
    "startTime": "00:50:01",
    "text": "capsule version of it but i think saying that you have to support capsules is fine because you always can do that well we say that if i can jump in and we say that web transport uses the capsule protocol so therefore if you support web transport you support capsules that's that's a given yeah that one i think is straightforward the only the only question is if our main goal here was to not have was to not force you to write code that could handle them coming in both places are we willing to say if you don't see both quick datagrams and http datagrams present that you're just going to take your toys and go home and know web transport for you yeah so we've talked about this in the past and i i think the same set of concerns apply here you can you can very much on a sort of connection by connection basis have a stack of things that need to be prerequisites for the for the next one so you need quick in order to get datagrams you need datagrams in order to get h3 datagrams and you need h3 datagrams in order to get web transport to work is a reasonable thing if we think about it on a per connection basis and i think that's still okay in this context it does mean that if someone's going to try to deploy this and they're using intermediaries in their in their deployment they're going to need to ensure that when the the front end receives one of these things and says yes it's okay the connection onward is dealt with somehow whether that means translation or whether it means uh full-on support for the same sort of feature set i think that's just something we can write down and and explain i know that's going to make some people unhappy but those people were probably unhappy with the set of design choices we've made here anyway so i think this is fine i do kind of want to caution that when when you have"
  },
  {
    "startTime": "00:52:01",
    "text": "something like the upgrade token that sits on the top of a stack of prerequisites i don't think you want to just say oh because the the upgrade token is there you can not provide the indicators for all of the prerequisites i think you want to have all all the prerequisites in also signaled in that process and if they don't appear then something's broken and you fail uh so that you you can build software that's rational with with all of these things so you don't have to okay so i've got a layer that's dealing with capsules i don't want to have that layer suddenly need to be taught about web transport upgrade tokens just so that it continues to function so um this is fine um i would note that we broke that principle in one place i think for extended connect victor did that yes indeed is it encompassing we yeah um i understand that to be a problem for some people but i think this whole idea of implicit signaling that's tied to other things is com is problematic when it crosses layers it may be appropriate at the layer in which it was done for that one because it was all tied into the same negotiation i don't like that but that's where we ended up and yeah i guess something awaits to jump in on that specific one that's there's an issue open for that that victor will be discussing later okay yeah martin no not martin martin i would like to point out that there is a use case of web transport that doesn't require um datagrams a somewhat primitive use case but you can imagine just doing the stuff you did on on websocket via web transport now and benefit from streams and never sent a single datagram so i don't see why datagram support needs to be a requirement that that was going to be my next question was is there anybody who's"
  },
  {
    "startTime": "00:54:01",
    "text": "planning on deploying this that doesn't have datagrams and doesn't want them and would rather have code that handles datagram capsules coming in on an h3 stream because that's kind of your alternative right so you're still going to have to write code that has the letters data and gram in them it's just now you have to have an if statement and deal with it in multiple places or do i not like that there's no requirement that i i ever send or receive a data right i could just say like this is this feature is not available as we have in http completely fair martin i noticed the queue has just gotten long um so i think what we're looking for here is interoperability and if we have people that want to use the protocol without with a sort of i'd like to pick and choose the the pieces that i'd like we end up in a situation where we don't have interoperability in those cases if you have a deployment that wants to use something that looks a little bit like web transport but doesn't have datagrams in it that is possible as a proprietary protocol but building something that doesn't have datagrams in it and specifically designing to allow for that possibility does complicate how we build this thing and i think it's a complication that we don't necessarily want here implementing datagrams and or implementing the possibility of receiving datagrams from someone is relatively simple to do and um even if you don't plan to use them and and all you do is throw them away then that's probably something that that you could possibly do in that context and then you would get interoperability however building something that says well datagrams are optional makes it very much more difficult for those of us who are building to this sort of thing and have to talk to arbitrary servers and then we have to deal with the possibility that maybe datagrams aren't present we have to think about how to move things on capsules and all sorts of"
  },
  {
    "startTime": "00:56:01",
    "text": "other things so i think that's nice but i don't want to go there i would tend to argue that you know aborting when you see a datagram is about as hard as dropping the datagram and just pretending you never saw it but yeah uh david schnazzy uh no hats um well mask enthusiast hat uh so just uh to add in the http datagrams document we say that like you you must support receiving datagrams on inside capsules uh so that's kind of a requirement here uh i mean at the end of the day if you already have a capsule parser which you need because of the closed web transport session capsule like having that call the i received a datagram frame function is pretty trivial so i wouldn't worry about that too much i think this boils down to do we want to say uh you must send them over datagrams if they're available or you must support datagrams like at the end of the day i cannot imagine anyone like deploying web transport like without datagrams like all quick stacks that are in the space like it is the easiest thing to add to a quick stack by far so i don't think we should like spend too much time on this and now with my cheer hat i'm gonna lock the queue on this one after luke hi it's luke uh so i've deployed a quick stack without datagram support um you're right it's really easy uh and it's it's kind of trivial to just throw them away i think the only concern is maybe capabilities on the w3c side i think there was a slide there saying datagrams are reliable or unreliable and it's kind of hard to tell if a server actually supports these unreliable datagrams if it just lies it just says i support them"
  },
  {
    "startTime": "00:58:01",
    "text": "i need to say this to get web transport but then you actually try to send them and it doesn't work um so there might still be a use case there to say that the um i yeah i don't know i can't really think of any reason why you can't just lie about it but uh i definitely would like to avoid having to implement anything complicated with datagrams there's just no reason to use them in most use cases i think so to summarize our options which i think david said very nicely we kind of have the the choice of saying either you can optionally have datagrams and everybody has to be able to cope with them showing up in multiple ways in multiple places in different forms or we say you don't get web transport if you don't have datagrams and it's easier to implement datagrams and throw them on the floor than it is to uh consider both and i think that second one is the thing that we're proposing right now but that would be a great thing to chime in with uh on the actual pull request for the stuff or we can also make an issue if we want to have a continued back and forth but like if you have a a implementation where it really would be a burden um it would be good to talk that through all right capsule protocol stuff is nice and easy that's why i said this was a nice easy low drama slide next one please let's talk about flow control this is not low drama the first one is uh fairly easy uh so we've talked about having a setting to limit the number of sessions that you can have and if we go to the next slide this ends up being fairly ergonomic it's pretty straightforward we say instead of sending a flag that says settings enable web transport you just send settings web transport max sessions and if you set it to xero no web transport for you today but if you set it to one you have web transport you have no pooling you get one session"
  },
  {
    "startTime": "01:00:00",
    "text": "and if you set it to more than one now you have however many you asked for so completely reasonable sweet next slide please so we've talked about that part the next piece of flow control that adds some complexity is limiting the number of streams within a session with max streams so kind of the problem here is especially in h3 where you have stream limits for flow control but if you have multiple web transport sessions and each of those are you using native h3 streams it would be very very easy for my first session to use the entire budget and my next session says i'd like to open a new web transport stream and the answer is haha nice try so this is a way for within a web transport session that in a context that understands that as opposed to h3 which just sees lots and lots of very equal streams um you can just say mac streams and use that same capsule the same way we do in h2 and it's all fairly straightforward and you just say yep this web transport session gets 10 streams and this one gets 100 and now i'm happy and i can limit things and i see motions as if we're going towards a queue all right just go ahead so in h2 you have an ordering guarantee between streams in h2 you have an ordering guarantee between streams in h3 you don't how do we deal with that yes so there's a fun caveat which is that if essentially uh i think to paraphrase in h3 because things can come in out of order if you've potentially uh closed a stream before the stream is considered to be opened you lose that you essentially leak that credit um and our answer is essentially don't do that and we don't think it actually"
  },
  {
    "startTime": "01:02:01",
    "text": "is going to kill anything there's more on a slide in that in a second can we go to the next slide look at that oh we got some formatting fun here so yes so street streams that are closed before they're opened are an issue you essentially lose one from that session each time that happens the and the reason for that is because you can't necessarily tell that that was associated with that session because the stream is gone and you're not gonna when the the frames arrive um you're having a bad day and so i think we we discussed wordsmithing some text around how uh the capsule kind of has to be paired um to try to make that less possible but i don't know that we ever got it to a zero percent chance possibility um i have a silly question how does the session level max streams interact with the connection level max streams connection on the h3 connection yes because i'm wondering like do you have to have a guarantee that like all of the sessions uh some max streams must be less than or equal to the connection level one or is it possible in a valid deployment to have the sum exceed and like yeah have a higher limit and like does that mean that a particular session does not have the guarantee that it can get all of its allowable maximum streams the the the latter so it's it's very similar to what happens for for you know connection level data limit versus stream level data limit is if you wanted to say hey um you know i'm willing to use i'm willing to give anybody 10 streams i could say um you know i'm only allowing the whole connection to have 10 more streams but any of you could take those 10 or i could say no no you know the first some of you only get five right two things so i don't understand the streams that are closed before being opened because the stream internally has ordering control has byte ordering so if if the con if the fin bit arrives"
  },
  {
    "startTime": "01:04:03",
    "text": "before the actual stream data the quick stack will deal with that the the problem i see is that there's a race condition here like let's say i you give me 10 streams i close i have this 10 streams open i close five of them and open five new streams and now the fin bits for the closed streams get reordered then you will think that i open 15 streams and you will give me a protocol violation i would assume got it the uh closing before being opened is essentially like so if a stream gets reset uh before you knew it existed so if i say hey i'm resetting the stream and you go excuse me what stream um very often in at least several implementations you basically say okay cool this stream is dead and when things then show up for it later you basically just completely discard everything to do with it which without careful wording it means that you're also discarding the information that told you which web transport session you should have built for that stream allen from dell meta um so yeah i think what martin said about uh the streams that are closed gracefully with a fin bit don't have this problem but the ones that reset yeah could and uh i think there may be a separate issue that we'll talk about in the h3 section but i think the leaking it is bad and that we probably need a reset capsule which would be reliable to make sure that that doesn't happen so you would in h3 you would have you would reset the stream and also um send the application level message uh which is sort of like the way things work in qpack and that would go on the control stream that would go on that would go on the yeah the h3 stream dude basically hey hey you needed to bill me for this is essentially what you'd be saying"
  },
  {
    "startTime": "01:06:01",
    "text": "speak in the mic please so i'm not even sure that we need this capability honestly um there is always the possibility that you can have the the bad session completely overwhelmed the capacity of the connection um for instance if i as a as a bad website um or just one that didn't know what they were doing um were to create multiple web transport sessions and use lots and lots of streams on them it's possible that you could exceed the available streams that are there for that connection maybe you forgot to close them right um and it could be very simple like that um but that's the sort of thing that we'll have to deal with anyway because the number of streams within a session times the number of sessions could well exceed the number of streams the entire connection could could have anyway at which point you have a connection that is entirely consumed by all of the web transport stuff and you have no means of doing other things on that connection like making a simple http request for instance or making a new session or what have you so i think that possibility exists anyway and maybe just throwing this in the well we can do this later bucket is entirely plausible here i'm not sure well and there is a line to be drawn there so uh sneak peek the next slide is going to be taking all of the data limits and throwing them in the we'll deal with this later if we decide we actually need it it's a real problem bucket so we could choose to do that for this the thing you say about you know hey i want to send non-web transport requests on this h3 connection if you don't have some way to limit this here you essentially cannot guarantee that that will ever be possible right because the web transport session can just whether you have one session or 10 can walk right up to the limit of how"
  },
  {
    "startTime": "01:08:00",
    "text": "many streams you can have on your h3 connection and you're done so it it's always going to be the case that you can exceed it unless you have reserved a few streams for the purposes of making other requests which a browser is quite capable of doing if that's what we want to do but that's a lot of that's going to depend on what the server is willing to allow for so if the server only gives us a budget of three streams then we don't have a lot of options available to us so um some of this is going to come down to just having sensible practices on on servers and conveniently the people who are writing the code to consume the streams also have some degree of control over how the server is going to be operating here so i don't think this is going to be as much of a problem as we're making out we're not in so much of an adversarial situation as much as we are in a situation of what do we do on the browser side to manage the resources so that it is easier for those people writing the server applications to to avoid harming themselves essentially so it may just be that doing nothing and is still a viable option here all right victor and then let's go on to our next slide where we propose doing exactly that yeah one observation i wanted to make is partially that because this is some of this is limiting so the situation is like when the client opens too many streams so the browser can send a http request well browser can control the number of open streams on top of what's imposed by http connections it is to say the browser might decide that you only get 32 streams from this connection and there is no need to support this in protocol because it's all local to the"
  },
  {
    "startTime": "01:10:00",
    "text": "browser the reason you would need to support that in the protocol is if you needed to have them explicitly communicate about it and especially if you wanted to let the uh application have input onto into whether or not that's happening uh i'm pointing out that samosa's resource problems are not necessarily the server imposing resources and limits on client but are purely client concerned and could be dealt locally alan frindell i think the concern that i have with letting the browsers just decide like we're going to reserve some streams and it doesn't need to be communicated is that then servers have to deal with browsers that have different limits or maybe you decide that you're not going to have the limits or some browsers do or don't so that it's sort of inconvenient just better to be able to like have some guarantees but i think also to victor's point i seem to remember maybe something like this along with websockets where chrome has some limit for like how many websocket streams you can have in an h2 session which is kind of similar so i don't know it would having some explicit way to communicate with what the limits are i think would be good right if if i if i'm offering a server and there's three different browsers and they preserve a different number of streams for extra requests and my application needed six and half of them reserved five and half of them reserved ten i'm now awkwardly screwed martin going back in time five years google quick had a configuration option for maximum concurrent streams it seems like we have the exact same situation here and quick resolve this by having stream ids and allowing a maximum stream id instead of a number of streams"
  },
  {
    "startTime": "01:12:01",
    "text": "i like that next slide please all right this is the part where we uh declare bankruptcy for bytes and we say if the conversation we just had seems kind of twisted and a little bit complicated when you start having the same type of conversation but for byte limits it gets way worse so we're going to say that at least for those bytes you have the ability for any h3 stream and obviously a lot of this also applies to h2 but specifically for h3 for any h3 stream you already have flow control you already can use it you already screw it up sometimes let's not make it any more complicated um you can reserve practically pretty much what you would actually need and so if we discover a need for some additional signaling beyond what you can already do in h3 um across you know connection and then stream limits um and to the point of you know can the sum of the stream limits be larger than the connection like absolutely yes and that can lead to all sorts of interesting strategies um we're not going to add any additional complexity there if we end up needing some kind of thing that we'd actually want to signal about that we can certainly add it later um it's not super hard to add capsules and extend things by making that work uh but we're gonna propose not doing that so the number of streams we said was this is a thing that you cannot necessarily do otherwise um and it'd be interesting if we can chime in with some some clear text on how we would explain that browsers should you know do a sensible default there or do concurrent stream count or pick some other strategy that would be excellent but we said it was worth biting off a little bit of this complexity for streams because that's something that you don't necessarily have good control over otherwise but for bytes you have lots of knobs"
  },
  {
    "startTime": "01:14:00",
    "text": "we have yet to prove that we can use those knobs successfully in every case so let's do that first and then we'll add more knobs later excellent next slide please intermediaries make the entire conversation we just had a lot more complicated that is also potentially a reason to have um a little bit more explicit signaling if we need to distribute some of that so this is a place where we have a split between a way to conceptualize what's going on and the thing you actually need to do when you write your code so conceptually the proposal here is that and it's less of a proposal and more of a reality uh flow control is terminated at an intermediary so when i have my h3 connection that is terminated by somebody who's then going to talk upstream of that via h3 or h2 um a lot of those flow control limits are actually terminated especially if they're translating between h3 and h2 but even if they're just sending h3 to h3 or h2 to h2 um that intermediary could choose to allow someone to send it more than it was then it's allowed to send upstream and vice versa so if it's willing to pay the cost to put the memory into buffering a bunch of that stuff and potentially having a sad day you're perfectly allowed to have a sad day but in practice you can usually just for the limits onwards so if we go to our uh butchered next diagram yeah my apologies to eric and everyone for resizing the slides right before the meeting and kind of nuking some of the diagrams in the process so these lovely fast forward symbols that you see are actually uh double-ended arrows between these uh different boxes and if we skip to the next slide since i think building this in in segments is not necessarily going to help much we got more numbers here um that refers to a thing that's on the left somewhere conceptually what this is saying is that"
  },
  {
    "startTime": "01:16:01",
    "text": "if you're an intermediary and somebody's saying hey you can send me 100 bytes you probably want to be very careful before you tell the person sending you stuff that they can send you more than that 100 bytes which should be pretty straightforward um did you just join the queue yeah let's do it how does that work um because the client establishes the connection to the intermediary first and during the quick handshake you communicate these limits the initial limits yes yes right so you need some sensible set of initial limits but essentially as you're going to increase those limits um you need to be careful but yes you you could be stuck in a situation where uh when you establish your upstream connection it says my initial limit is 50 and you'd already advertised 100 um and you're gonna have to deal with that so let's try to be a little bit more pragmatic about this sort of thing this is going to be a gateway sitting in front of a bunch of servers and a lot of cases the gateway's going to know something about those servers now whether that's based on the fact that it's already talked to those servers in the past or because they're actually operated by the same people and they run off the same configuration as largely a material the intermediary can advertise initial flow control windows that match those of the servers below behind them that's i think relatively straightforward if there are multiple servers on the back end they might do things like take the minimum of the servers if they have different configurations and then as the backend server provides per stream flow control credits the intermediary can just forward those credits onwards and then you then you have essentially an end-to-end flow control and the intermediary is in an interesting position there um i was just thinking there's interesting complications here when you talk about having quick on both sides of the intermediary and when you have quick quick and tcp on on different sides"
  },
  {
    "startTime": "01:18:00",
    "text": "with quick if you get an out of order piece of stream information you just forward it on and sort of say oh this is just stuff that you'll need to deal with in the future that's easy with ccp with headline blocking you have to wait for everything you have to buffer things up so ultimately the intermediary can't sort of blindly forward those things on in the in the tcp context because it does need to have all of the space that that it advertises available for buffering otherwise it could end up in a situation where it it has data that it said it could take but it couldn't right yeah and i i think that's a really good point and kind of underscores the the idea that conceptually you are terminating that flow control you you are responsible for whatever you choose to advertise and the fact that in many cases it is fairly straightforward to send that through is okay but the underlying reality you like you can't just completely ignore that hi uh luke from twitch um so flow control is usually based on like i have limited ram uh i think the assumption here the intermediary is like we've just got big beefy servers and they can have as much ram as the client of the server but i mean exactly like it sounds like it's a poor decision to just forward flow control if you're running a raspberry pi or something like there's going to be congestion all of a sudden you advertised you know a gigabyte of ram available but you didn't have that right so i'm not sure it's actually a good idea to ever forward flow control um and i don't think it's an end-to-end thing i think it's literally just i just have this much ram availability each hop yes well and martin had also made a good point around if you're translating between h3 and h2 like h2 to h2 is pretty straightforward h3 to h3 is pretty straightforward plus some extra ordering fun um"
  },
  {
    "startTime": "01:20:01",
    "text": "but at the end of the day we're not defining a new signaling mechanism for this in the spec so if you've got a bunch of big beefy servers that's awesome other people may not have a bunch of big beefy servers that's cool too i think what we're trying to do is provide enough guidance that we're giving a heads up as to some of the pitfalls and the things you need to be careful with as you choose to do this but what what your intermediary chooses to do with web transport is not something like we're not defining additional signaling about it um and we're not really putting any requirements on it either so if you've got a raspberry pi and you want to be super careful and you want to manage it completely on your own and not have any signal from upstream like go downstream that's totally cool yeah i think it's just sometimes with max streams it's conflated like it sounds like we want max stream to kind of be end to end like it's meant to be an application level decision but for data it's definitely not i think it's kind of my point got it yeah thank you yeah yeah ian sweat google uh i would agree that yeah thinking about this as end-to-end is is probably just not gonna work but the good part is that like intermediate areas that terminate like h2 and h3 already deal with this problem and so like i'm not really sure you really need to say anything at all i will call it one note for your example um the intermediate to the server could have like an incredibly small rtt like in the order of a millisecond or less it is not uncommon the client would have an rtt that is like two orders of magnitude larger as a result the bdp between the client and the intermediary is fairly often going to be probably at least an order of magnitude larger than the server to the intermediary so unless you're going to give a bunch of information to the server about the client and that bdp even trying to do end to end is going to hose you because like you're going to be sending far too little flow control from server to intermediary so like there's a drag in there maybe maybe it's even worth calling out like this isn't end end you're probably going to get yourself in troubles i think that is that is why the"
  },
  {
    "startTime": "01:22:00",
    "text": "fundamental underlying reality is it is not end to end it terminates at each intermediary it's essentially hot by hop as it were and like what you've committed to you've committed to and you might choose because you know that there's gonna be way higher bdp like you might choose to advertise something that's way higher and if everything goes wrong and even though you have a super low latency link between you and your upstream like if that gets locked for whatever reason like yeah now you have to hold up like you are left holding the bag but in practice like that's fine so what i'm getting from this conversation is that building an intermediary could be hard but people do it anyway and have done so successfully for some amount of time and it might be the case that trying to find the guidance that you're looking to put in here is subtle and difficult enough that maybe we shouldn't even bother maybe we should simply say intermediaries exist and that's it something very very simple and anodyne basically i don't think there's much so that we benefit from from trying to explore all the various ways in which you might implement an intermediary under the varying conditions that ian's talking about because yeah that's that's why people building intermediaries still continue to have job security i think so the the just for for clarity that the current proposal um is we're saying this is essentially hot by hop if you commit to it like you're the one left holding that bag that's up to you and any other text we choose to put on top of which we have proposed very little right now um if we if we want to describe something that helps people and lays out some of the here are common pitfalls and things you might want to think about that's totally cool but the um in terms of our like actual pull request for this stuff the only hard line statement that we're"
  },
  {
    "startTime": "01:24:01",
    "text": "this is not an end to end concept like if you advertise something that's higher than what your upstream could do like you got to deal with that that's on you colin jason i i mean i i anytime i was just sort of reacting a little bit to martin's uh you know like any time we're trying wish intermarry's intermediaries away we ten years later deeply regret having done that right but i think your statement that you have what i read in the draft of you're not wishing them away at all you're saying very hardcore you know you have to fully be you know whatever you advertise you have to provide and that means you're a full sbc in the sip sense or a full you know i think that's a great way to d in fact i think it's the only practical way to deal with intermediate problems but i think you should claim you are dealing with interiors and this is the answer not we're sort of you figure it out yourself because the figure it out yourself it leads to bad results later thanks beautiful all right next slide please all right so if we summarize what we've talked about we are proposing that h2 should use capsules we are saying that h3 should use capsules and share with h2 where appropriate which is actually a reasonably small list our main reason for using capsules is because the frames look exactly the same but now they're in a shared list and we can reuse them between protocols we're saying that capsules will always use native features if possible uh and i think we may want to split out a specific github issue even just so we can write down some of our conversation around uh what happens if datagrams aren't there and and how we are going to maybe have text that that makes a takes a strong stance on that if that's what we want to do uh eric can you take the action item of filing that issue yep thank you h3 and h2 are getting a setting for max sessions which replaces the enable web transport uh setting so instead of it being you know zero or one you can now have zero"
  },
  {
    "startTime": "01:26:01",
    "text": "one or more than one um and the last one is we're proposing that h3 gets a stream count limit within a session uh but i will actually split out a similar issue for that where we can make sure that we've fully written down everything we need to for that stuff and if we get to the end of that issue and we say you know what flow control is not the thing we were trying to solve when we talked about capsules that is totally okay nobody will be sad with less of that all right um so just process wise uh we have two pull requests for this um they're gonna move around a bit in github and stuff so i will send them out with links to the list so keep an eye out for that and if you can come in and read a lot of that and especially if your reading of them does not give you the same impression as the words that we all just said uh that'd be really cool to call out um but yeah please please give them a look the diff for h3 is quite small the differ h2 is quite large it's almost all packet formats and fun figures and stuff like that so it's not super onerous to take a look at so i will send a link out to that and thank you all right thank you very much eric uh any last questions for before we move on uh okay so process wise eric will send out this email and then the chairs will turn that into a formal consensus call on those pr's since this is a non-trivial change to how h2 works and uh and then while assuming that goes through we'll have a set design going forward all right victor uh hello everyone i'm victor editor for the h free spec uh each space track is hopefully approaching the stage where it's almost"
  },
  {
    "startTime": "01:28:01",
    "text": "so today we're going to go over the some of the remaining issues uh next slide so update since last minuting first of all for the overview draft uh we've merged the pr that defines the common operations that any web transport should provide this is meant to be as a layer of abstraction on top of web transport over h3 or transport over h2 and whatever else uh and uh uh this is mostly useful for people who edit w3c spec but everyone is encouraged to read the updated version next slide for web transport over it freeze up this has been mostly minor we've uh notable one is since last meeting as we decided we clarified what happens once again away frame is sent on stage free connection and added some missing details about how exactly you turn down the transport session so on the next slide we have some of we have about 10 remaining issues if the roughly five of those are either editorial or just near the pr so the issues are still not discussing are each free is though we currently do not define what we do with http redirects uh the rfc 9205 says that we have to provide explicit guidance on what to do with this uh our current behavior in the web browser is that we explicitly do not handle them as in there is now automatic redirect support but we need some normative tax since the"
  },
  {
    "startTime": "01:30:02",
    "text": "draft so do people have opinions on what should be there oh david so speaking as an individual contributor i would say just must follow redirects it's not a hard thing to implement for browsers and some folks could find this useful oh as an individual an implementer uh i err on should not uh we've definitely uh from what i understand have ran in a bunch of implementation issues when we with redirects in websockets and there are some rough edges around the fact that those aren't really http requests oh and what does it mean what is the difference between every director i would moderately prefer should not as in uh you could follow but we will not normally follow uh and so question for the room as chair here like either do browser implementers have thought on how hard this or annoying or risky this is to implement or do users of web transport have thoughts on whether they would want this feature or they don't care so this this advice that we've got is"
  },
  {
    "startTime": "01:32:03",
    "text": "not actually very helpful advice i'm afraid and so um when when you think about using something like um fetch you will normally you would normally expect to have the redirects followed until the point that you get something that requires action on the part of the the thing following the redirect here i think because browsers work following redirects generally i would i would hope that we can follow redirects here as well simply be mainly because that's just how everything else works but also because there is value in having redirects in terms of being able to put resources on different servers for deployment reasons or being able to move things around when people are given a url for something and they find that that that needs to move somewhere else so i would be on the must end anything in the should may space here is awful uh because it means that you have no determinism um you you don't know who's who's following and who's who's not uh if we can find a set of reasons why you might not follow a redirect that would be interesting but um i would probably er toward the the must end on this one mike bishop speaking from an http perspective i'm not entirely clear what it means to follow a redirect on a connect request to begin with well you if you get a 3dx response yeah i mean i know how mechanically you would do it you you get the 3xx response you go to a different url you reissue your connect request i guess but like"
  },
  {
    "startTime": "01:34:00",
    "text": "when i it makes more sense if i'm trying to fetch a resource if i'm asking you to perform an action if i'm trying to talk to you connect and getting a redirect back seems a little weird semantically yeah there is a similar question like do cache connect redirects how does that work hi luke here so just like martin said it should be a must or must not as a user i if i'm going to use a redirect feature on my server i need to know if the browser is going to do it otherwise i could just do it through some other mechanism so if it does support redirects that's one more tool to my toolbox if it doesn't i can just do redirects via like some other endpoint um so i think either way just one of the musts [Music] i was getting uh colin jones i was getting up to say what luke said it's got to be muster must not absolutely mandatory has to be one of those two uh but i totally assumed it was a must it never occurred to me in any way whatsoever that it wouldn't be and i think that that's probably what most implementers using this are going to assume yeah so um to to mike's point i think part of the problem we're having here is that the the model that we're using for connect here is somewhat different than the model that you might imagine for a classical http proxy connect where there's a target that isn't really a target because there's no resource involved in any of any of the connect stuff classically here we have a resource we're making an http request to a particular resource and the effect of that request is to establish a web transport session to that resource and so having a redirect here makes a a great deal of sense because it does fit much more within the http model of resources and redirects and all those sorts of other things so i think um that's why i lean"
  },
  {
    "startTime": "01:36:01",
    "text": "toward the must here more than anything else it doesn't make a lot of sense to have a redirect for a connect you're right because there's that that's bizarre but connect is weird in its native form and this is what we're building here is much less weird it's still a little bit weird but it's much much less weird so um i i think must donald max i agree muster must not and i also wanted to ask you said there was weirdness that happened with web sockets and redirects and i'm curious if you could expand on that so i cannot i would have to talk to people at w3c who oh okay we've drained the cue i'm hearing that majority of the folks speaking want a must or must not and i'm hearing preference towards must instead of must not er is going with must something that everyone can live with uh i would prefer to continue discussion on the issue because i believe some of the people who might have objection are not in zero okay so victor said that he believes that some people who would object are not in the room and he wants to continue discussion so that makes sense we don't have consensus here we'll keep discussing on the list and i'm going to give victor an action item to get those folks to chime in because you i suppose you know who to contact there yeah all right thank you next slide"
  },
  {
    "startTime": "01:38:01",
    "text": "stream frame ordering so there is so the way we do unidirectional streams as we define a stream type which is just uh okay the way we do unidirectional streams is we define a specific stream type uh so there is a stream type and then there is the web transport session id and then there is payload for bi-directional streams we do not have stream types so we define a special frame that once you send that frame the rest of the stream becomes just web transport data stream and there is a question of whether we want to allow to send any frames on the stream before and [Music] do we want to be consistent between unidirectional and bi-directional streams how if we are not consistent in life do people have opinions because i know that people have expressed opinions on the issue so if you if you allow other stuff on one of these streams before you mark it as being part of a web transport session someone's going to get confused so there's always a possibility that you have code that essentially opens the bi-directional stream looks for the session frame and then switches but it has a state machine that says when the first frame comes in that will determine what i what what i do for for subsequent frames on the stream that will determine the status of the stream"
  },
  {
    "startTime": "01:40:01",
    "text": "if you allow other frames on that stream you mess with that logic pretty badly is there any reason you would want something on one of these streams that isn't web transport session as far as we can tell there is no particularly compelling proposals that's fine because if if there was say a headers frame there there's a very good chance that most servers will look at that and go oh that's a request and then start treating it like one at which point things will go poorly for everyone involved yeah so i guess the status quo is for uni-directional it's already effectively forbidden because there's no provision in the wireframing and the question is do we ban it for bi-directional requests and so far what i hear is suggestions that we should yeah i think we very much should ban it unless we've got a very strong reason otherwise it really would complicate the implementation at endpoints if we if we allowed for other stuff to appear all right thank you clarification question for you martin are you saying for all frames including future extension frames or do we or do we punt that question for later i'm going to say everything i think i think you want to have a dis disposition frame this basically establishes what the stream is and will ultimately determine what extensions are are available or not now we may regret that at which point we can revise this specification but i'm um fairly confident that when you have a disposition thing that's definitive and you want to have that first all right alan frindle um so my first opinion is they should we"
  },
  {
    "startTime": "01:42:00",
    "text": "should they should be the same like whatever we do for bi-directional we should do for unidirectional um i think the point that martin was making about well if we did this then people might assume that it's different or have buggy logic i'm not sure i totally buy that like if the specification says that web transport is a series of frames followed by a frame which begins the unframed part then people will write parsers that handle that and if they don't they have they're not following specifications and i don't know we can't make people follow specifications i guess but um in terms of i don't have a super compelling use case either so i'm i'm not going to lie down in the road here but uh you know that i think just the issue mentions either greece or potentially priority i think um the ability to have extensions in the future is easier if we say if there's a series of frames followed by the beginning of unframed data uh otherwise you would have to have a different way of you'd have a different kind of web transport session frame in the future to support that um so anyway but i'm i'm i'm not super passionate uh luke here quick question do we have similar wording for the header frame because i think martin brought up a point there that if a client assumes the header frame is first but is that prohibited already or is that just left up right because we should probably just follow what hp3 does and if it leaves it open then we can leave it open yeah i see my coming and answer the question yeah so it's a little more complicated than that um because h3 does allow for the possibility of other frames to be introduced in the future so what h3 does is when the spec says that a certain frame must be first which for example the settings frame on the control stream then that exact frame must be first with"
  },
  {
    "startTime": "01:44:01",
    "text": "nothing else before it but when we're talking about the ordering of headers and data on the request stream it's all about sequence the headers must come before you see any data if you see data before headers that's an error but if you see something else before headers that's potentially okay and if you don't know what it is toss it so i think i think the challenge that um we're facing here is that http 3 assumes very strongly that the the streams that it has the the bi-directional streams that are established are for the purposes of requests it doesn't really contemplate the possibility that they could be a different type of thing and so when it doesn't say anything about the ordering it does that under the assumption that it that it's already been determined that it's a request stream and and that's okay and so if request stream is the default and the assumption that h3 makes if we do anything other than then force this to be the first thing on the stream we're messing with that assumption because um that um an endpoint that's implementing this under the same assumption will look at things and see oh this is an arbitrary frame that i don't understand i'll this must be a request stream um because of course you could add new frame types that are extensions without any prior negotiation so um we're basically punt if we if we allow other things we're essentially punting it across to the h3 assumption so i i think we need the disposition to be up up front and unambiguous and and very clear otherwise we'll end up in that complicated place oh oh did you want to say something i just want to say that i mean there's a way to have what martin is suggesting and which is you have a frame up front the first thing that says this is a web transport session but it's a frame which"
  },
  {
    "startTime": "01:46:01",
    "text": "have potentially more frames and you have another frame which says okay but now we're starting the unframed part of the stream so you could do it that way but i think you could also that was what it would look like if we punted this to some later version when we have a compelling use case so i think that's probably what we should do yeah i i think we don't yeah i think the key point is we don't actually have a compelling use case for frames and web transport data streams so uh and i believe that if we require web transport session to be in front that actually would simplify implementations including the one in our code so there are some practical advantages to that uh do you want me to jump in victor so i'm hearing from the room that folks want to do the same thing for united directional and bi-directional for consistency and i'm hearing that folks are now leaning towards disallowing any frames but this one to start does anyone object to that resolution amazing all right i'll write that up in the issue thank you oh all right next slide this one is a more interesting so there is this problem where we open a web transport data stream and we can reset it before the peer knows it's a web transport data stream and can associate it with it uh so the problem is here is that once that happens is uh let's say we do that as a client on the server there is a stream that is half open and [Music] it's now in a state where it's not clear what's supposed to happen to it because the client has reset it"
  },
  {
    "startTime": "01:48:01",
    "text": "uh and the issue is what happens to the other side of the stream and i think my answer is this is not really a web transportation person because that could happen to anything else in http free and uh so the question i don't feel like we need to specify anything i feel like implementations of http 3 already have a way of dealing with that i'm not sure if they're explicitly specified uh mike bishop so h3 has an error that basically boils down to i didn't see enough of your request to act on it go away so i mean in this case it's very similar um [Music] i if http has provision for that i feel like we don't even need to spell it out the stream terminated before a complete request was read i believe is the way it's phrased uh eric can your apple so this also could be solved by the thing that i think alan was talking about if we wanted to have an explicit message that goes on the control stream that says like hey you need to bill me for this one this is essentially the same problem right you're saying hey i've got this stream it's hanging out and i don't i don't even know if it was web transport if it is i don't know which web transport session it was what the heck right so that there is an opportunity to have a way to kind of catch up with that to to kind of have it resolve those lingering inconsistencies and leaks of things it's not not trivial i think to write text that's good for that but it would solve this and the other problem so maybe we're approaching the point where it's worth doing yeah i think i could actually solve that"
  },
  {
    "startTime": "01:50:00",
    "text": "because you cannot actually reference the string numbers inside your recent all right um mt do you have a reason to jump the queue here yeah yeah because victor said something that's wrong okay go ahead [Laughter] um so so once you create the string you'll know what the stream identifier is and so you can use it it's true but remember if you're translating each phrase to those are not consistent we're talking about h3 though h3 has this problem h2 doesn't okay alan fendell so i'm curious about what mike said it's sort of if i'm a server and like the only thing i receive is a reset stream and a stream number first of all i'm not sure everybody's quick api would even notify the application that that happened i don't know maybe they would um yeah so i'm i'm a little bit concerned if that's the only thing the server says it'll have a chance to send the like oh you sent me this stream and i don't know what it was um also what if that was a unidirectional stream how would you even well this is only for bi-direction well this is for both it has to be bi-directional this is the bi-directional case okay so you have a way to send the answer there okay um okay yeah and so i think i mean i'll admit so yeah like i said this is we mentioned earlier this is how qpac does stuff which is like just it puts the stream number on the other stream and it's like hey this thing got reset so the accounting can be taken care of it's a little bit not wonderful that it's this actual quick stream id floating around in qpack space but that is you know with just we did it that way"
  },
  {
    "startTime": "01:52:04",
    "text": "so i and and correct me if i'm wrong uh martin seaman uh since you filed the issue like this doesn't sound specific to web transport for me um it's a bi-directional stream so in h3 already if the client opens a bi-directional stream and resets it the server could in theory keep it open forever but that would be silly uh i'm pretty sure i mean i should check what our implementation does but the correct thing to do is go whatever kill this thing um otherwise that sounds like a resource exhaustion attack um therefore if this isn't specific to web transfer i don't think anything needs to be done in the web transport spec and this can just be handled by h3 [Music] so i i think this is a web transporter specific problem because when the client creates the web transport stream it creates it under the understanding that it's creating it within that particular session now we're not doing if we decide not to do any stream limiting and whatever else we don't need to deal with that particular problem but it does create on the server side a stream that the server can send on and it needs to know it needs to know what it needs to do with that that thing it might want to send on it because i don't know um maybe this is the protocol that you develop you you create a stream and reset your end and then expect the other end to do something with it i don't know um so i think eric's suggestion was perfectly good here we need we could we can resolve the problem here um whether or not we want to do the stream level accounting stuff we can still solve it in in that way and it's probably still worth doing is is to do exactly what um allen was"
  },
  {
    "startTime": "01:54:01",
    "text": "suggesting as we did with qpack just say oh by the way i reset this thing reset's fairly uncommon um we'll still need to send the resets because that's how quick expects us to behave but um having a having a message saying oh by the way this the stream was created and reset now you can connect it up with your session is probably something that's worthwhile having just so that everything runs neatly and everything can be accounted for properly yeah we have about six more minutes in five slides so we might want to limit uh the cube from here on out i think i agree thanks for putting that up bernard all right alan if you could make it quick yeah i'll be quick i just i think what martin said or someone just said reminded me that like what would happen if all he received is a reset and then the server was like sent to an http error page for example onto a web transport stream on the other side that might be very unexpected or do very weird things so that's probably not what we want okay so i'm hearing a proposal of biting that bullet and having that capsule that makes the the web transport session attached to a reset reliable does that resol does anyone object to that resolution sweet awesome uh okay so i think this is the last of the actual issues is we do not actually describe how we expect the web transport section to be closed uh there have been a couple of proposals floating around one of them is you sends a closed capsule you send the fin and then some points appear responds with a fin uh and other is like you send capsules and venus and stop sending [Music]"
  },
  {
    "startTime": "01:56:02",
    "text": "the people have preference between those two i have a mild preference for one which one do you prefer victor uh i prefer one i think you mean stop sending by the way oh which one there's no stop waiting oh i said that's a typo uh yes uh either of those would require you to have a timer to eventually tear the entire thing down if the peer does not respond so martin seaman says off the mic proposal to not need a timer i don't think why do you need a timer oh you need a timer so i'm trying to remember all right we definitely need a timer in case we will have only one session on connection because if you only have one session on connection you need to tear down that connection that's why i have a timer oh i suspect when you have multiple sessions uh you can wait and you just lead to resource exhaustion if you never send the finn in the response alan fendell i guess my question is do web transport sessions not normally have a timer is there not a concept of an idle timeout it's just completely open until the other side closes uh yes like because we have no idea what the application wants maybe application wants to push notifications once every five minutes okay okay so do you have a propos resolution you prefer that you want to propose to the working group victor uh i"
  },
  {
    "startTime": "01:58:00",
    "text": "would just write in proposal one explicitly proposal one unless people have strong reasons unless people have to prefer reasons to prefer proposal to to purpose a lot let's do this you go first and reminder to keep it true we're almost out of time can you explain why you prefer one over two oh i believe that stops there is a value in waiting for the server to acknowledge on the application level that it has received the capsule and that is how it acknowledges is by sending the fan in response what does the client do with that information that acknowledgement if it's the only thing it can immediately close a connection that sounds like an implementation issue on the client or is that just detail right so not really yep since i would like to point out that proposal 2 is closer to what we do in quick you send the connection close and you walk away well in quick you send the connection close it's not that simple okay yeah so um i think there's i think there is value to having the ability to do it either way here in fact so i think i think that there's value in sending a capsule saying why it is the connection closed and i think either either side should be able to send that in this circumstance so i think this is neither one or two um it is either side can simply send a capsule and a pin explaining what's going on independent of each other now the question is how you how you"
  },
  {
    "startTime": "02:00:00",
    "text": "respond to seeing that and um i think either one or two is is an option here because in the case where i want to walk away and i just say i'm not going to pay any attention to what it is that you send from this point onwards stop sending is perfectly acceptable in that in that scenario but you may be interested in knowing what the other side has to say as well um or at least you can't stop them from sending something with a capsule saying that they wanted to go away as well so i i think all of the all the possible options are available to anyone and i don't see why we have to pick one i i hear a proposal to maybe mention that both of these are possible and like maybe just a sentence to clarify what the semantics are i mean they're pretty clear if you're sending swap sending it means you're no longer listening how do you feel about that victor sure does anyone object to that resolution all right phew and we are at time so we're not going to be able to go through the last few pr's or if you want to say something specific about a very specific thing is there are free pr's everyone please read them uh i guess that's all i want to say uh all right so let's wrap up thanks everyone we got a lot of stuff done just to verify i didn't hear any actions for for me from this right i'm looking after this working group while francesca's away you're all good thanks mary um so we got folks to be like everyone seems okay with the output of the capsule design team that's great that was blocking quite a few other issues that we're going to be able to make progress on we got a resolution quite a few other issues on h3"
  },
  {
    "startTime": "02:02:03",
    "text": "we do have a few other one issues that we haven't gone through in prs that need to be reviewed so please take some time to do that um the editors will spend some time like we'll be discussing the capsule design tprs on the list once those are emerged we'll have more discussions on the issues that were blocked on that resolution and we'll see maybe maybe we'll have an interim before london go ahead bernard yeah that was my my question um i guess we'll bring it up on the list david right yeah yeah let's um my gut feeling is let's get the output of the design team merged assuming there's on this concessions for that and see if we can progress the other issues on github and if we feel like a face-to-face you know probably virtual conversation would help we'll organize an interim so thanks everyone for coming and see you all soon on the list thank you well you"
  }
]
