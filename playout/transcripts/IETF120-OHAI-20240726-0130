[
  {
    "startTime": "00:00:03",
    "text": "Oh, hi according to the National Research Council of Canada's official Canadian time, it is now 630 p.m. Welcome to the Ohio Working Group I'm richard barnes. On the screen you can see my co-chair, shivan sahib and I think Shavan will kick us off with a few administrivia. Thanks Richard Welcome all You have probably seen the slide several times during this week but it is really important. So please do note it well it covers several topics such as patent and code of conduct. And please do you during this week, but it is really important. So please do note it well. It covers several topics such as patents and code of conduct. And please do feel free to reach out to Omit ombud's team if you experience behavior that you're on uncomfortable with and it will not well also cover several BCPs and documents that describe participation in the IETF Just some meeting tips. The important one here is that please use Meetecho to join the mic queue so that we can keep things fair between remote parties and in person participants and if you're not using the on-site tool, please do record your participation using the QR code that's probably up there somewhere in the room and if you're joining remotely, this should already be taken care of and yeah make sure your audio and video are off, unless you're presenting if you're remote participant and use a headset we have some note-takers thanks to Laura and Chris for helping take out notes and yeah we can begin Tommy will be talking about deployment experience with OSTP OSTP, which will law into the chunked OHTC draft discussion, which is our"
  },
  {
    "startTime": "00:02:00",
    "text": "adopted draft for now And just quickly check before we launch into it, any proposed changes to this agenda? are we all good with this sounds good all right Tommy take it away okay Maybe you we all good with this? Sounds good. All right, Tommy, take it away. Okay, maybe even be quick so we can let get to their dinners or whatever else lies on the other side of Ohai. All right great I'm tommy pauly from Apple and so first i wanted to share some background on the experience that we've been having with a couple different use cases for Oblivion HTTP, just to see kind of how they're set up, how they're configured, how they perform and I had specifically asked to actually do this before our discussion on chunk chunk-o-H-DP because some of the details of how we actually deploy these things and test these things and what we've run into are relevant to a what we think we should be doing in that draft draft Okay, so I'm just going to first go over three different use cases where we are employing Oblivious. H.D and then talk generically about how we're doing configuration for OHDP and how that's being shared All right, so the first one that we for OHDP and how that's being shared. All right, so the first one that we've been doing for the longest period of time is for doing safe browsing type checks And this is an example of like iOS UI for what it would look like in Safari. So this is a protocol that previously already existed. You have cases where you have a big database of chart truncated URL hashes that indicate buckets of things that may be fishing or malware or some bad type of sites. And"
  },
  {
    "startTime": "00:04:00",
    "text": "so those are downloaded periodic And then if a client has a hit on one of these truncated hashes, they go and fetch the complete bucket of complete hashes and figure out if the site they're going to is potentially malicious, and then they will show them thing. And it's very important to have privacy on these checks because when you are A, you're fetching these things in the background quite a lot and so this would allow the server to just kind of like watch where everyone goes but also when you actually do a look up here for some lot, and so this would allow the server to just kind of watch where everyone goes, but also when you actually do a look up here for something that is potentially on the list that is revealing some information about what you may be accessing. So it's very important to have good privacy here so the overall setup that we're doing, so we have, on the left, I have two clients just to represent, there's a multitude of different clients and they are communicating generally over HP3 by default. It can fall back to HTTP 2 if UDP is blocked in your network but generally it's HDP3 and these are somewhat long-lived connections in the background from the browser client going to the relay which is operated by a third-party partner And then there is a pretty long, which is operated by a third-party partner. And then there is a pretty long-lived connection or a set of long-lived HB2 connections that are mutually authenticated with TLA between the relay and the OHP gateway which is also where the safe rising surface target is and then the clients are authenticating to the relay using a privacy pass token so the client need to kind of be able to fetch that ahead of time just to say they're are legitimate clients that are expected to use the service And for this case, there's no actual authentication to the back end And as I mentioned before, there's like a period update that happens in the background and then some more real-time updates"
  },
  {
    "startTime": "00:06:00",
    "text": "when you have a potential hit So just some of the properties here. These are normal because they're in the background and periodic it's an normally a cold start connection. So like these fetches are not reusing connections that much. They're just kind of creating a new connection to the relay. It's predominantly the background Most of the time, this latency is not actually blocking users. I think this frequency we see for people actually having a potential hit is like once a day So normally this is just like a background thing. And when we look at the times for downloading that whole data, globally over OHP we're seeing it be like on average like 350 million for the whole thing, which is okay It's a background thing so it's not super important to optimize further than that so that's one example of OHC here. There's another case which was actually discussed at the last IETF, I believe when, we were talking about some of the cases where we could have this like un- unreliable OHDP. This is for like data metrics upload In this case, you're just trying to essentially post a upload of things to the server. The responses are not used in any way, and so this is a case that could be optimized by allowing the client to just hand the message off to the relay and then not care if it actually got all the way to the gateway or it was accepted. They just want to shove the metrics up In this case, there actually are two sets of privacy passes going on here, but otherwise it's a very similar setup and it's like on the order of a day or a couple times a day that this is happening And then the newer case that we have that we're using is a for model inference So recently we announced some private cloud computer features for various genera"
  },
  {
    "startTime": "00:08:00",
    "text": "AI things. There's a blog you can read about all the details there and it explains that it's using oblivious HTTP And this is a case where you have potentially large response or small responses and they take a very amount of time to generate, and there's a lot of value to having the beginning of a response and then be able to trickle in the rest of the large generated response over time but again it's a very similar setup you have many clients with re-use and longer lived HP3 connections They're using a privacy pass to the relay. There's also a separate privacy pass that being used all the way to that inference server There's a mutually authenticated HGB2 connection in between And this is more real time based on user activity when a user would be using some of these features. Tommy, quick clarifying question The Gateway Privacy Pass here in the, I think, the prior example, is that held by the relay or is that pass through the client through the Yeah, it's an end to end. So it's within the encrypted HPK package that is decrypting by the server yeah that's right so it's saying that this client has access to that. It's also a way to provide some rate limiting where if you limit how many gateway privacy passes the client can do, that kind of controls how many AI inference operations they can do on that model So this one is more different it uses chunked OHDP and that has new features that comes along with. And in this case, previously, we had been doing the other features, and we didn't have to implement things like indeterminate length binary HTTP. We didn't have to have binary HTTP trailer support, because if you're doing single message, that cannot be decrypted partially, there's no"
  },
  {
    "startTime": "00:10:00",
    "text": "value for trailer fields since you have your header field always in the same bucket and so these were things that in deployment experience and implement experience, until you start doing chunked, you really aren't really exercising what it means to do indeterminate length binary HTTP and to have some of the other features like trailer cars aren't really exercising what it means to do indeterminate length binary HTTP and to have some of the other features like trailer parsing, etc. So just a heads up of things to test And one of the other interesting, things we'll talk about later in the chunk of OHP draft is that when we were testing this through different CDN provided relays that we're doing the chunk of OHP relaying, we found it very common that by default with post they were doing buffering of the entire message and waiting for the end of the message before they would send any bytes up to the server or back And that is kind of different the purpose of doing chunks. And we're like, well this is not giving us the performance we want. And it's like having some really huge delays in these cases and they're all relatively easy to fix, but is something that is also not obvious if you're just doing the normal oblivious HTTP with a post, that's all in one So those are the three different use cases we want to share As far as how we do the configuration and how we do that deployment, we have a configuration bag for kind of all of the oblivion HTTP servers that we know about and there are associated relays and we fetch that every like 12 to 24 hours to get updates there. And this bag includes the different OHP configs and each of those has the gateway key configuration, and that's"
  },
  {
    "startTime": "00:12:00",
    "text": "the thing that is in the normal OHDP draft or the RFC. It also then says here's the relay that you need to go through and like the path on the relay for the relay resource It indicates kind of like, what are the supported targets that I can get to behind this gateway so I can know how to wrap to go through, and like the path on the relay for the relay resource. It indicates kind of like, what are the supported targets that I can get to behind this gateway, so I can know how to route them. And then it also indicates whether it's not you are supposed to be using chunked or non-chunked for this particular rule and if you look into some of the stuff we're doing for the private cloud compute features there's a lot in there around trying to make sure that there's not non-targetability. And so these configs are also being replicated into a transparency log, and we're valid against that. And so that's another aspect of this config. So this is essentially one model of how you can get the information about OHDP and it's you know definitely kind of like the driven proprietary config that you like the client is knows about and is fetching but it is a way that does work. And I'd be curious to hear it other people have experienced with other mechanisms and if they are similar or different to this And that's all I have here questions. Any questions, comments? Yeah. I had a question, Tommy If you go to the previous slide you mentioned the transparency log Can you say more about this? So, I mean, this is essentially a long the OHP configuration there's like some proofs that you can check in a transparency log that is on that you can check in a transparency log that is all detailed on like the private cloud compute stuff I that will do a better job of explaining some of the details than"
  },
  {
    "startTime": "00:14:00",
    "text": "I can. But it's essentially just like a blob that you then verify is in the transparency law It's it would be similar to like a key transparency. Absolutely a blob that you then verify is in the transparency logs. It would be similar to like a key transparency type solution. It's just a bigger bundle of things I'm just wondering if you think this is something more generally and because this question comes up during OSGTP discussions do you feel like this would something that could be used? beyond or could be in a draft or something? Yeah, I think it could. I think it would be worth exploring and also kind of hearing from other people who may be trying to use OHDP if this is an approach that they are looking at taking or expect to take because I know we've had discussions about you know, can you do transparency? to provide the consistency? or do you do other things like a double check? And so in this case, doing the transparency log was what we went for for this architecture, but I could see other more dynamic cases being ones that double check would be easier on Other questions or comments? I'll lead to the mic Thank you for this. Now, unusual large amount of information from Apple Thank you so now we'll be talking about the chunked livius HTTP messages draft, which is on 01 01 So just as a recap, and we kind of mentioned it in the last presentation, Chunk to HTTP is about allowing you to encrypt and decrypt the request and response for OHDP in separate chunks"
  },
  {
    "startTime": "00:16:00",
    "text": "And so the layout of it is you have the existing kind of request header that you have and then on the responses, the header is just a nonce But instead of one HPK or AEAD encrypted response, you have a little length field that goes before each chunk and you have multiple chunks It's a very straightforward idea, but you know, as with everything, many complications in the details. And as I mentioned before, there's a allows use of indeterminate mode of binary HTTP. It uses the built-in support from HPKE for having multiple messages and conceptually is still just a single HTTP request and response. It's not a unlimited streaming bidirectional thing It's just a way to not have to do the entire message all in one go So things that were changed in the 01 version, we fixed one issue, which was an easy one, so we did it around how many chunks you could have HPE already does this correctly by having a map number, but the responses also needed to specify the same thing Your counter should not exceed the values that you could store in the nonce for uniqueness since that would cause nonce for use so don't do that there's some other to-does that we have of doing the test vectors and some of the protocol formal analysis that just need to get done when we have time which we don't have But the most interesting issue that has had discussion that I wanted to highlight today is the one that I was hinting at for the deployment experience where you have incremental forwarding issues on the various CDNs that are handling the post messages for oblivious HP So our deployment"
  },
  {
    "startTime": "00:18:00",
    "text": "testing found that most of the common implementations did buffering for post and would wait until you had completed your entire request or wait until you got your entire response before forwarding on any of the bytes So that as I mentioned before, does render the chunked mode pretty ineffective And so we have good discussion on the issue, Kazuho pointed out a lot of great things in the discussion, and we were going back and forth So one of the references that is used to here is from RFC 910 on HGP semantics, and it points out, that while the HCP message can be processed as a stream for incremental processing the senders and recipients cannot rely on that incremental delivery of partial messages since the implementation can buffer or delay the message forwarding So that's exactly what we're hitting. It is up to implementations to do what they want to do it is allowable is a lot equally allowable to have incremental processing as well as buffering So there are a couple different options for what to do here. One option doesn't involve anything much about the actual spec other than maybe giving some advice but you could essentially just say, do what we did so far and fix the servers that are doing this like if you know you were going to be used as a chunked OHDP relay, you should not be buffering those post messages so simply like those intermediaries that are being used for this, get updated, we could just give some advice in the document and say, if you want this to be effective, don't buffer"
  },
  {
    "startTime": "00:20:00",
    "text": "And if you have a deployment that is buffering, you just ask them to stop doing that For the cases where as what we've seen so far in the deployment experience, there's a pretty concrete setup where a client is getting a bag of like here are the known relays that you should use in this mode, and here are the gateways they work with, and they're all authenticated with tokens or mutual TLS This works OK. It's not super elegant, but it can work. The other second approach would be a much bigger design fix One of the things that was brought up on the issue is like, well, you know, post is much more common to have buff buffering, but there are things like Connect that obviously don't do the buffering and so you could like switch OHDP, or at least this mode of OHDP to stop using Post to use extended connect instead But that really changed the protocol quite a lot more It loses a lot of the advantages that you have of just say I want to use you know more normal reverse proxying capabilities that are built into CDNs or built into other existing intermediary infrastructure without having to switch all the way over to a very different type of forward proxy. So we could do this, it would be a large change and would cause more divergence between different modes of OHDP And then the third option, which was more recently suggested, was to just have a negotiation fix where the clients and servers can agree about what they're doing for buffering in posts. So this is just saying like, hey, I'm doing a post, and I would like you to not buffer this and actually be explicit about it and not just rely on the fact that this is chunked O HTTP and you hope that"
  },
  {
    "startTime": "00:22:00",
    "text": "the intermediary knows that that means they should not be buffering So Kizuo has this week come up with a proposal for a negotiation fix. It's very simple It's essentially just a new header field that the client would send in a request that would indicate that the content should be streamed instead of buffered The proposal in what he was saying I think was request streaming and it's equal to one or maybe it needs to be I guess, question mark one. Yeah, sorry But a very simple thing and so we can absolutely bike shed what the name of that should be. But I think we want to get feedback on is this of the three directions, is this the right approach? And there's also a larger question of like, where should that work be done? Because this could be a very generic thing that could be done in H2BIS because it could apply to really any application of post or many other methods to let clients indicate that they would prefer not to have their requests offered All right, we have a couple folks in the queue. Piotr, please Piotr, Cicora comments. So, hey, first on the negotiation fix, historically, reverse proxies used buffering of the post or other long long-running request as the DDoS protection or like trinity protection against generally less performance backends, right? So doing negotiations kind of like defeats this purpose second thing so this is not second thing? So this is not unique problem to, you know, oblivious H HTTP notably GRPC and streaming mode is basically one big, you know, host request that does this backend for. So I'm actually surprised that this was an issue for you in the deployment and I would suggest"
  },
  {
    "startTime": "00:24:00",
    "text": "that, you know, force your CDNs to you know, stop buffering So you go with like the first fixer, just like, just fix your thing Yeah, let's fix it. And on negotiation, fix you don't need to introduce you know separate header if you want to do that, you can just base it on content type, right? Right, and oh, if it's obvious, HTTP, just don't buffer Right. I mean, do you know for cases of like you're mentioning that already don't do the buffering like a GRPC, is that based on? content type? I use it. Okay. Yeah and just one other thing I mentioned, like, I totally get that oftentimes the buffering is for DDo prevention and other things I imagine again, like this is just a hinder request and it could trigger the logic to say, okay evaluate if this request is, you know, sufficiently trusted to do that. Like, in this case, as I mentioned before in the deployment like, we're giving you a privacy pass, that's been rateless limited and we're going to a known upstream that you have a mutual TLS with and like all of these things that should make it pass the check which of course you could just do by saying yep the deployment just has to know that this is streamed but it can be nice, maybe just give another hint of like yeah, on this particular request, I do expect it to be streamed in chunks without having to do a different content type sniffing. But I don't know I think it's an open discussion Kizu. So as Piotto said, this streaming is not specific to just chunk for HTTP, but it also existing with other application protocol such as GRPC. And for what's what the best practices for HTTP, the best practices, the best practice, document says that application protocols"
  },
  {
    "startTime": "00:26:00",
    "text": "have to be developed in a well, content type, or method, or whatever, independent way, meaning that requiring specific tuning to the proxies is not a very good way of developing protocol. So I very much prefer having this kind of discussion in the HGBWBWR group and fixing it as an extension because that pays up our way forward for having a specification that is I probably say compliant it might be a too strong word, but something compliant to the HCT mispractices that we have for HG Genaing Gar Fasley. I agree with what Peter said except that he suddenly took a left turn and went to a different conclusion. So I do agree that this is a more general thing and exactly what Kazuos said. I won't repeat it. But I have one question Does this request streaming of one, does it result in an error if it's not supported? or is it a silent failure? So that might suggest think, Zero? silent failure so that means what do you what do you think is zero silent yeah the silent failure so i think this is exactly the kind of thing that... I agree, they should go to the HTTP working group, but I think this is the right direction And I wholeheartedly support it Cool, great. And again, like in this case, with chunked oh HTTP, yeah, if you don't get the streaming you're just delayed, it's just slow, you're just having a sad day. But the request can work So it does depend on whether it deadlocks or not. It depends on the application, I mean, like in particular. So in some of the tests, we had, we would hard fail on that But generically, with chunk to HTTP, that's could work I see it but the client was"
  },
  {
    "startTime": "00:28:00",
    "text": "tolerant of it. The reason that they should be discussing the HTTP was working group is because it won't be discussed in the context of just a particular chunk to HTTP. In the general case, you had to make sure that the semantics are clear to the applications and they have a way of recovering from lack of support I think the examples of existing things like GRPC that have similar properties that are also not using extended connect, et cetera, or similar, like, and we can argue that they should all move over to using web transport, you know, like something like that but the reality you know doesn't exist that you have non-connect protocols for which you don't want full buffering I just put myself in Q to say that these options you presented are not actually mutually exclusive If even if the HTTP working group makes us to signal something like this that you would use to say request this behavior, you could also say that like regardless of that this protocol works better if you don't buffer. Right I think the deployment fix and the negotiation fix are I mean, you can always do the deployment fix. If we take a really sharp turn over to say like, ah, we're going to give up on post, that's a much bigger design change But yes, I think we could go with both It sounds like people would agree that this should be a generic HTTP thing I probably would argue that we would want to normatively reference that extension And so, you know, share our fapes across the working groups. But yeah know, jonathan hoyland Clappler So is it? permissible? I promise this does have bearing on the question. A hand Permissible, but not impossible. Is it permissible that the clients continue sending chunk to request data after receiving the first bite of the chunked response? This is something"
  },
  {
    "startTime": "00:30:00",
    "text": "that in the document as it stands today is permissible. It is an issue that is open. And I don't think we've like fully closed on it when we were originally discussing the document, I think some people were of the opinion like, oh, we wanted to be you know less streaming like less by direction From a protocol state standpoint, and I believe even from an analysis standpoint, like, it should be safe to do that. It's more about like, what do we want the world to look like So, yeah, from a security analysis standpoint, I think it should be required that the intermediary buffer the entire server response because otherwise you have, you can implement bidirectional streaming if you wanted. But if the client has completed its entire request. Sorry yeah, until the client has completed its entire request. Right, but like I could stream up my request and then you could stream down your response. Yeah, yeah, I mean... And those should not involve buffering within each of those sections. Yeah, sorry, what I meant was you have to wait until the whole request is received by the intermediate before you start sending a response So, yeah, because I don't know what imagine you are just being dumb and you decided, I'm going to implement bidirectional streaming by calling it one very long request and receive part of the response and sending part of what's effective a new request Right, but that's not what you were right? You're trying to get oblivious here, not so to clarify because the the not what you want, right? You're trying to get oblivious here, not. So to clarify, because the main problem I can think of with that, from the privacy aspect is that now you are definitely able to reveal a lot about timing and other things that could help you in for location And you're just sort of completely linking the"
  },
  {
    "startTime": "00:32:00",
    "text": "request. Right, you are linking all of those um so i i see it as a privacy regression But from a cryptographic security standpoint, it should not be regressed Like the... Really? Because I... So, from what I understand of the analysis it would be one once the client is able to start responding to things the server is doing, the guarantees all go out the window, basically the threat model of re-identifying where obviously, you'll re-identify everything that client does on that post-request. But, like, in threat model about identifying the client or potentially even pinpointing the client's location, etc., totally agree that that becomes all opened up but like the security from an attacker being able to decrypt or you know like oh yeah, that doesn't change. But like from an unlinkability standpoint, it's just the link. Yeah, right Yeah, agreed. Thank you John, I noticed you guys up, but you didn't join the Q. Did you have feedback on that? that? No, go on. Cool I think, like, the text around those implications is something that needs to be worked on based on the issue Tibbo. Timbo Yes thank you for the presentation Maybe one thing, which I saw about here, might not be totally related is OSTP does not have a standard endpoint, for instance, for retrieving configuration. So even though like, this is this probably won't fit like in a configuration endpoint but i was thinking if this is something that like could be valuable down the line if we have like multiple ways to actually like operate relays"
  },
  {
    "startTime": "00:34:00",
    "text": "to specify okay i would support for instance like required streaming, no matter what, I would support trunk which HTTP or if this should be part of a negotiation negotiation So previously we've discussed, and I think we still have some open issues around the document to describe the impact on configuration. As I mentioned before, if you have an ad hoc configuration on the side, then you're good So OHDP on the main RFC does define a configuration format and a content type for that And in the OHDP Discovery RFC that we also did, there is at least a well-known URFC that you could go to to ask for a configuration Now that configuration format does not include bits like, does this gateway support chunked? and of course it does include anything about the relays that you may go through to get there So I think the relay does discussion is I think a bit orthogonal, because that's just like generally how do you discover relays? and how they relate to gateways, which probably doesn't belong to anything about chunked, but what is definitely an interesting question is do you have have a different content type or a way to know that this gateway is supporting chunk? specifically? Does that make sense? Yep, so overall, we still have to finish some of the updates based on the other issues, which we allude to here, which we got some feedback on at 119, but we need to do more thinking on that includes are there maximum chunk sizes overall how we recommend getting the configuration of do you do chunked or not? The other main thing,"
  },
  {
    "startTime": "00:36:00",
    "text": "to do is now go forward with incremental forwarding and that's probably going to look like a new draft within HTTP BIS that then gets normatively referenced from here. And they also want to call out that it's always great to do more interrupt, more testing there are already multiple clients that that it's always great to do more interrupt, more testing. There are already multiple clients, like test client and production client, as well as really and server, Gateway Server implementations of this, but I think if anyone is interested in also doing interrupt testing, there should be plenty of implementations so we could try. Tommy, do you know if any of those implementations? are open source right now? and secondary question should we be documenting this interrupt somehow? Yeah, that could be interesting So I know Chris had it old like, go, oh, should be go repo and then it has a branch on it with a PR that is not yet merged, but it's functional for doing chunked so that one is open and just public I don't know of any others that are open up there. I've heard that martin thomson intends to update his impact implementation to also do this so there would be at least two fully open ones that should talk to each other other Okay. Anything else? Any other comments? I mean, these feedback points that would merit any discussion here? Or do you have a concrete direction on those already? I mean, if anyone has particularly stronger opinions about, like, configuration? and, yeah know, should the configuring know, like, should the configuration be extended to include, do you support chunks or not? Should it be like? an entirely different configuration content type or something else?"
  },
  {
    "startTime": "00:38:00",
    "text": "I'd love to hear it. I think there is some analysis to be done there about the privacy implications of having you know, if you did purely opportunistic attempts of doing chunks to gateways you didn't know about that could end up segmenting user populations or client populations based on like which ones ended up learning about chunk and which ones didn't And so there's all these fun aspects that could come up there which to the earlier discussion, like don't really come up when you're just like force pushing uh information to clients of like, you shall use this gateway with Chung that is the only way you shall ever use it So more dynamic cases are trickier, but I think it's also good to understand what real deployment scenarios people are thinking of that would have those types of problems because it's hard to design without that. Peter Peter Shikora, maybe this is silly question, but are you expect different chunk to be sent? via different third party relays? And like, combined in your end? No, no, um chunks are inherently on one stream. They are not oblivious to each other they are just one request and one response that just are processed incrementally Would there be any benefit? in terms of privacy or whatnot to do and one response that just are processed incrementally. Would there be any benefit in terms of privacy or whatnot to use multiple relays? I think the problem is that, you know, if you think about these as different contexts or partitions of information, the communication channel between the client and the target is kind of one connection so if you end up rerouting that, you need a way to then reestablish and say,"
  },
  {
    "startTime": "00:40:00",
    "text": "like, I am the same one Any like connection identity and then you're forced to have more identifying metadata that is visible to more partners If you want to go down that road, like, multipath quick through mask relays and like this is not that this is just like I have a request response that is getting the normal OHA benefits, but it's slow because Gen AI can be slow That's expensive. David david schinazi, Google I'm assuming you're developing this on GitHub Yeah. The link isn't in the draft if you can fix that metadata. Oh, sorry. No, no, yeah because I would have just filed an issue, but then I'll find it, I'll file an issue but I'll tell you what it is. It should be on the working, it's in the working group GitHub. It's on the working one. Okay, I can find it there. It was the point that I made after the interim about how this lacks perfect for secrecy compared to other solution for streaming protocols, which is a perfect fine thing. You've now well justified that it's worth the risk, but I think that should be pretty clear in the security consideration. So I'll follow you in an issue I think you previously mentioned wanting to explore, you know, if you can have perfect forward secrecy with the OHDP, which good on you. You can have it for free for X2551 but for all the post-quantum stuff, then life becomes a lot harder You have to spend a bunch more resources to do it And it doesn't sound like folks are particularly clamoring for it. So unless there's a use case, I don't think I'll write it up people getting wound up about this particular difference in privacy from nonchunked, which I was trying to"
  },
  {
    "startTime": "00:42:00",
    "text": "I haven't read that draft recently, but it might be worth pointing on on the draft that the, so the idea is that this basically depends on the application and what the application cares to do The solution you were describing, ultimately, you are breaking this up and then establishing context through an idea identifier which is fundamentally you're creating that linking because the linking is necessary for this to work So privacy only makes sense when you think about privacy within the scope of it actually working. If it stops with working, then privacy doesn't make sense at all at that point So that might be just something worth noting in the draft if it already hasn't already been. Yeah, I think that's an area This meta-discussion is worth more clarified text. Is it Jonathan? jonathan hui wouldn't call back My, I agree that the is, like, it's not necessarily bad The question is more we need to be sure we know what guarantees we're actually trying to provide especially if you're going to get a formal analysis we would have to like actually say that guarantees are different That, like, if you're, like, doing whole... Different from from non-junct which doesn't mean bad it just means different yeah yeah yeah if you're, like, doing whole. Different from... From non-junct. Which doesn't mean bad, it just means different. Yeah, yeah. I imagine in the case where you are not allowing interleaving of request response chunks, particularly when you just have streaming response chunks the information disclosure is really very, very similar to unchecked If you have the if you keep the restriction that the intermediary buffers until the client request is completely received, then I think it's the same Then I think it's identical, right? Yeah, no, I don't think that's always"
  },
  {
    "startTime": "00:44:00",
    "text": "desirable. Sure. So we could same. Then I think it's identical, right. Now, I don't think that's always desirable. Sure. But I think having the discussion of these are the properties you get if you do it this way and not Because the client could always choose to send a single chunk that it's complete and only receive multiple chunks of responsibility That's a perfectly acceptable use of this this All right, we've reached the end of the queue. Last call for any discussion on this draft Going once, going twice All right, I think in that case, we are adjourned and everyone can enjoy their dinner and shivan we'll see you next time. Yep, see you all Thank you Thank you I'm just going to chat with John"
  }
]
