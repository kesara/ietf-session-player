[
  {
    "startTime": "00:00:02",
    "text": "Yeah. Yeah. And but I have to back back back You can take the vouchers, and I can't Hi, everyone. It's starting to become 1 pm in Prague. Please have a seat I come to the IT of 118. Joined ops area and, ops WG meeting. I'm Heng next to me is Tiran. And so, yeah, you're team chairs and, want to welcome you to, a session that is under the note well. That I hope everybody is familiar with. This is about the content, there are a lot of links in BCPs that tell you, when to sell what and what that means. Please please please please please please read that if you haven't done that yet. There's also the note really well that is about conduct and other bot content effectively I would advise, that you be are we behaving in a way to another individual that you want them to behave towards you, So it'd be nice if that doesn't work out, there are people you can speak to, you come to the chairs, these, and there's an ombudsman. So please, talk with us about that. That's a problem with the conduct you'll experience here. On the IETF meetings. There's a tool, at the data tracker, the data tracker, enables you to, join session via media echo. If you just want to raise a hand, there's a hands race tool for your cell phone or mobile device. That's a be convenient. You can also use a full fledged meet echo client. It's a little bit new, but I think the raise hand at the bottom the screen. It's still easy to find and that's for everybody to have a fair chance to attend either locally or remotely So we have, user can"
  },
  {
    "startTime": "00:02:00",
    "text": "accommodate both kind of participants Whatever you do and find, you can, of course, on a technical side, fire an issue. Meet echo really, benefits from that, especially that we have a new client now and when I look at my client, it's pretty doing funny things. So I'm finding an issue about that. If you see funny things happening with you, heavy. Through that too. Coming to, our agenda for today. I already answered this introduced us the chairs. We are in dire need of a note taker. A note taker will click the notes icon on the data tracker to the upside WOG item in the agenda and we'll make notes on at stock. Which is a notes I TF tool. We rely on that. That is a vital part of the IETF process. Unfortunately, I have to ask one of the automated data 2 of the attendees here at the early mode to do that. Are there any volunteers? And I heard there's coffee for free. Somewhere. Thank you so much. Moving on. And this is just a this not mandatory, but, typically when we start to adopt documents, We, assign a separate now. At this point of time. We always just assign the share, but that's not new. We would like to assign them on the time of adoption. So, it's easier for, the shepherd to remember what happened, with the documents document shepherding is a, a relatively easy job, either there's not much to do just to have to take interest in the document and its progress. And in the end, when we go into working group last calls and such, the job of the shepherd is to ride up a history of this document and some other items happened. There's a list for that, like, a questionnaire to help the idea of the area. To, then do their own review."
  },
  {
    "startTime": "00:04:04",
    "text": "So, a document shepherd is like a micro chair for a single document and can help with that. And, that is a good opportunity to get some experience and hands on. So, whenever we do a working group last call, we will also do a call for a shepherd that is not the the chest can do that. If all documents are just chatted, but the chairs, there's some bias, first of all, to it. Also, it's a bottle So just wanted to announce that here. A good opportunity to get some IETF process experience by becoming a shepherd. Don't shy. Just contact us and we will, again, issue calls for Shepard on the list whenever we try to adopt adoption. Coming to the status, maybe I'm giving this over to you. and, Yeah. I think Sure. So we have, a couple new RFCs published, and we have, so congratulations or or thank you as well to the authors, contributors, and the working group on 9445and9472. We also have a ton of work, that is almost. It's on the verge of, being an RFC. So we have the segment routing ipv6, ipv6, ipv6, SRV6, We have wrong TLS TM. Those are just about ready for final publication as RFCs. And you can see we have a few other documents submitted IESG for publication. The big thing are the or is the 13 or, actually, it's more. He he Hank Hank had to do ellipsis here. We already have a a number of working group documents and we just adopted 4 more. The attachment circuit work was, was just adopted and just became working group documents as of this morning. So there's quite a bit of work, and that's why today or this time in in IETF 118,"
  },
  {
    "startTime": "00:06:04",
    "text": "we have 2 working group sessions. We today, and then we have an hour on Wednesday so so people can can share their their work. So that's status when it add anything, Chandra? I want to add that we received the 2 liaisons, in the middle east, and, we believe it's not actually of CWG work. So we dispatched then one to the content network RG ended the other dispatched the 2 the NPS working group. Sorry. I'm trying to get our, apparently, the notes have to be have to be initialized Let's do that. So if one, actually, if one of you can can deal and do the the hedge stock notes.itf.orgput something in for 118. The last thing we wanted to say before we get into our, main program today. Namcom, So we have until Friday, I think you said Henk. We we have until Friday, there are a number of people, including 3 up for ops area director in the nomcom Bucket and pool. So submit your feedback. Go to the tool, find people that you want to submit feedback, that you know, that you have experience with and definitely give us that feedback. With that, We have A number of presentations today. So we have some adopted work up front. We have 3 adopted documents. Then we have some non adopted work or unadopted work, other work, and then we will go into the ops area portion. With Rob and Warren, they'll come up. I think they just have an open mic for us today."
  },
  {
    "startTime": "00:08:02",
    "text": "But they might have other things as well. So we have time. We have some buffer in the agenda. The hope is that we actually do give ops area there to do this time. But we want obviously, we want, as always, we want good discussion, with the presentations So first up, I think we have med I'm not mistaken, yes. Med? Oh, Well, Benoit, perhaps. Good afternoon. I'm Benoit. So I'm not mad you know, the stupid game might dare you So I was discussing with Matt. So I have not seen the slides but, so I'm going to present anywhere the first draft. So this is what the the game is about. So this, IP fix 3 draft. I mean, to print the first one on the fix Next slide, please. We've got those 3 drafts and they are working with documents we started for the history with a simple fix to the registry of IPfix. And then when we look at this, we're thinking, well, why not make the registry correct? So the first one was exactly this one, the ip fix fix this. Then we realized that there were some TCP option templates, sorry, information elements that we have to, maybe not having the fix itself, but in the other documents. And the last one, the UDP IP fix also some information elements about UDP. So as I wrote in my slide, the last two documents may be merged But, somehow, we prefer to keep them separate of the dependency on the UDP options specification in TSWG."
  },
  {
    "startTime": "00:10:00",
    "text": "Next slide, please. So on the first one, the RFC C7117125 update. We passed a new blast call, an ATLAS call, the second set of draft, the IP fix, fixes is, adopted like the other one we add like 2 or at least 2 versions since the last IETF So now we're seeking for cross looking group reviews. We send a message to 6 men for the the TCP, one and, 2 t s w, tsv, double TSV working group for the UDP IP fix. No feedback so far, but we received good feedback from the IP fixed ID doctors. They've been helping a lot on the fixes draft. And they they they agree on what you want to do and receive feedback from Eric Vink. So, and this one is for humans. So I so I win Yes. So if you could focus on the, I would say, on the, basic session header. So we started with 1 information element and we ended by having 4 information limits to cover, I would say, the the, gateway session headers. So currently in the the draft, we able to, I would say to to to to report how multiple legacy extension header chains are exceeding a given chance. So is something that can be exported in the previous the cache that we had, we can also export, obviously, the latch of the accession headers, And And also whether the, what we are exporting is"
  },
  {
    "startTime": "00:12:02",
    "text": "what exactly the router or the element is observed, or this is something which is limited by say by a deceptive limitation. So this is something that we can really provided the collector. So it we say the interpretation of the information can can really be, accurate. We also export everything the order and also the the aggregate the, what the information elements there by the, when they are really observed in the in in the packet, there are some substantive in the way the extension header are, I would say, in certain analytics buckets, of them, they they are not the, I would say, consecutive. And for this one, we, because we we want really to be, as close as what's observed don't aggregate them, and we're really, just, I would say, a re report information about once. We also made, I would say, some, tweaking about the the coding of the information for the having something which is really more, optimal in term of the size of the information we are, we are exporting and we, specify, I would say, the dependency between the various information elements because we don't only have one. We have multiple ones. So there are some dependency between the information that are exported. And we also, we we added some example to, to show how the various information elements can be can be, can be used. Next slide, please. So in in the slide, we are showing, I would say, the current, extension header, which information elements, which is used by the, existing implementation to to export the, the extension headers. And then, and as you see here in this in the same code and one is really suboptimal. Because each time, for example, you will see a destination option there or an extension header, will be the one which is the the most significant one, and you you will need to to use 30 bits only for for this. And this is really not suboptimal. What we are doing here in the, in the new specification of the extension here, there is that we are reverting the way we are including the information. So the, the the the the most I would say in the less significant piece rather than most significant piece. So we will"
  },
  {
    "startTime": "00:14:00",
    "text": "be using less bits when exporting this information. Next slide, please. This is an excerpt of the new history that we are in the in the new, IP fixes fixes fixes draft. And in this one, we have we have there are, I would say, some entries that are not extension header perso, but do once we are in in hurting them, because those were already defined in the in the existing I would say, ipv6 is not either, information element and we received a comment from from Eric Vinkwin here. He's reviewing the document that would be good to know whether there is, I would say, the extension here is followed by a payload, or there is no etcetera@em just right after the, any session here in an ipv6 packet, I initially I was against that that if that that insertion because for me, this is not always aligned with the as part of the extension headers themselves. But he he convinced me that this is better for the for sake of the observability to have this information and it is really similar to the to the way the unknown extension header are inserting the packet. So the then the next issue we had is where to insert in the, I would say in the in the registry. So we decided to go for the lower values that if we encounter this kind of values in the NAP physics packet, this won't impact the size of the whole extension headers that will be that will be exported to a collector. Next slide, please. There was, a pending issue we had last time discussion with, though, we we will need to to export also the, the destination and the routing options. So we have there there's a trade of between, having something which is really comprehensive between something that is really implementable and and and useful. And our call at least with with doing what we decided is that we this kind of export will be out of scope. And if there's a need for, I would say, some specific routing here or options to be supported"
  },
  {
    "startTime": "00:16:00",
    "text": "we just, ask people to follow the same. I would say, Pat, that we followed by the, segment routing header by Petrobras. And the people can can define their their own session in there to cover that point. Next slide, please. Did you wanna get feedback on the question? Any opinions, Okay. I guess then, your So I was trying to find the Q button on the new interface. So We you're saying, are you okay with this purchase? That's the the both the left out of scope for now. And or the we do it, we do it that way. To do it that that way. That means that we we leave it out of scope. And if people interested, you can come to the working group and ask if there's a need for, right, for the specific, I would say, routing option or this national option to be exporting Netflix. Because otherwise, will be opening. I would say it's too much work for us to, I would say, just for a mentor, I started with a with a really small fix in TCP, and they it with 4 document. I did not want to last my life dealing with this document. So need to be, I would say, pragmatic here to find the root balance between something which is really useful something that can be comprehensive. Makes complete sense to me. Thank you. So this is for for TCP. In this slide, we are showing the current encoding we have for the GCP option. The the mapping there in the existing RFC is really weird. They don't know how we ended having this in in Rfc And but, anyway, what what we dis what we decided there for at least for the 2 documents here in observe working group is that we really just align the disruption in the simple fixes with the drawing to see as the drawing off will be the authoritative one. But then we will remove, I would say, that that that probably will have the description. And for the, the new, disappear of in"
  },
  {
    "startTime": "00:18:02",
    "text": "informationally element, we will use an encoding that will optimize the the number of bits that will be, required to report the observed your option in a given flow. Next slide, please. Another I would say item which which is really minor is that anticipate there is what we call the the shared TCP option. This experimentation, if you will, And then there is a specific field which is called the experimentation IDs. And in the specification, there are 2 flavors of the session headers, one which is included in two bytes and the other one which is which are included in four bytes. So if we just merge all of them and then we export them, there is an issue at the collector site to identify and to decode the, e, experiment, e, notify dim sets because we don't know whether This is, a 4 byte or a 2 byte. For the simplicity, what we went with Benoit that we we just defined 2 information elements, one for byte and one for, for the 4 bytes. And then there is this, I would say, some optimization whether we can mix I would say the 2 bytes and the 4 bytes by artificial encode into 2 bytes. 1 is a 4 byte ex experimentation ID. But we decided to, just to leave this, I would say, to be silent about this. It's not, I would say, we are not encouraging the implementation to do so, but if an implementation decided to encode an a 2 bet as a 4 bet session experiment ID. This this can be decoded with no problem, so there is no prep the issue. We are not, encouraging implementation to to do that. Next slide, please. And this is another comment we received from Eric about the of the document, we have so far about the ipv6 extension, extension here and for TCP, for the TCP, TCP matters. Hitting that is better to display this into, into 2 documents. At least my tick on this is that we don't to do that. I prefer to have one single document, no need to, to to have extra load just for"
  },
  {
    "startTime": "00:20:02",
    "text": "but this is, a question for the for the working group with all the on our our position or not. Any thoughts from the working group? As chair, I think, if this is what you wanna getting it through, if you feel that that the feedback you're getting in the single document, it makes sense to keep it that way. I I know in some other documents, there seem to be additional scope creep, and it made sense perhaps to split that out here. It doesn't sound like that is gonna be the case. Yeah. From it, it's it's just okay to to have both. Yeah. Next slide? Yeah. Yes. For for the next step, so I think that we are we have a stable content for, the 3, the 3 documents. So we'd like to working plus 4 for all of them. With a request for our shares just to, to include the TCPM, this will work in Europe, 6 man and ethics, mailing list for the, for the working class post. And for the early director reviews, we'd like to just it's up to you, chairs whether you want to run to run those in par with the working with last call or before. With our recommended, I would say, their extras for each of the 3 of 3 documents. And that's all what I have to to say for, Yeah. I actually As chair, I appreciate that you broke out who who should be included in some of these, our director reviews. I I just put a director review out for the, geolocate this document. Sorry. So I I I think I Personally, I would like to do this just at the same time as his last call and do the last call with you. Yeah, we'll see if we can get them. It's it's been kinda quiet, actually, in the director hopefully we'll we'll get some feedback. Looks like we had 2 minutes, and he other comments from the working group"
  },
  {
    "startTime": "00:22:01",
    "text": "I'm Nothing on chat. Thank you, Metzka. See. Think we've got Chifang? Hello, everyone. And this is Chiu Phang, and this presentation is about a young tech model and the various extensions of policy based network this control. Next slide, please. So, this work has been adopted after the ITF117, and we also inform the, the graduate extension, the Redex Working Group to help review, the New Redis attributes, which we call the user access group ID definition part so that this, user group ID can as a result of the the the redics authentication and can be further used for, the access control, group back group ID best access control enforcement. And generally, I I think we have received a lot of good review comments from both the ops area working group and the red x working groups and so thank you all for provide your comments and suggestions. Generally, the authors think that all of the comments should has been resolved in the latest the latest version of the draft and except for that there are still open issues that are pending now waiting to be resolved in the the next revision the first is that, should we specify any mechanism for the endpoint group string to be mapped as as ID so that this ID can be easily carried"
  },
  {
    "startTime": "00:24:02",
    "text": "in the the packet header, like MA03 header. And the second issue is about to consider at some text about what hardware ramification might exist and with operational trade offs, implementations would consider if some advanced feature of SDR is intended to be used. And of course, I think we welcome everyone to report their issues and proposed change on our GitHub repo. Are you on the queue? Yeah. Joe Clark, Cisco contributor. I wasn't clear why exactly you made the group ID string from an integer. You you touch on it here. The draft changed it from, 32 bit, I think, integer to a 64 character string, but wasn't really clear in the text why that change was was needed. And I you said to carry it in the packet header, I would think carrying an integer would be equally as easy. If it's if it's like, a integer value type length could be easily to be encapsulated as packet header, like, the whole 3 headers. So that can be info, some group best access control. Does it make sense? But you moved it to a string. If I read that correctly. You won't do that with. No. No. No. I I thought it should have been an integer but you it looks like you changed it to be a string. Now we have moved it at defined as a string and we are thinking whether we should specify any mechanism to for the student to be mapped as the ID. Okay. Okay. Yet. Yeah, raise the mic. To Okay. Next slide. So this slide gives a summary of the high level document updates since the last last IPS meeting,"
  },
  {
    "startTime": "00:26:01",
    "text": "I think the most significant update is that we now have moved the definition of schedule younger model into draft draft, which has been submitted in ops area working group. And we we use the groupings defined in that schedule young draft an in this document to in enable the debt and time best access control enforcement. I will present in the next slide, and The second update is that, we have changed the document title and add a reference to policy so that the now that I think the document title contains audit the draft actually about. And we also changed the group ID as a stream and, 6 related examples accordingly we think that, a string might be, might allow for some hierarchy then which might be useful to is the coordination of different and point groups. And we also took some, Reddick's, sanctioned session based on the red x working group feedback. And also add a restriction to the last best of their comments And at the informative reference to one of the individual drafts in that red networking group for the authentication method recommendations. And and use the derived type definition to ease the reference of note. And also added it as some ipv6 examples beside the IP reform to show how the the the the SAR policies could be used Next slide. So as I mentioned earlier, the regarding the separation of the schedule young data model from the this draft we have submitted, a 00worddraft"
  },
  {
    "startTime": "00:28:01",
    "text": "in ops area working group. It's a complete new ID, but it borrows heavily from the the 00 version of the UCR document. I think I believe it's a 95% of the contest. And there are remaining, contents with F reference to some ongoing efforts related to the scheduling. As far as and the the officers know now we have, 3 related work that has, touch some thoughts about the scheduling. And then it's the the OEM a scheduling OIM test in ops area working group, and and also this UCR document, and the 3rd effort is in TV, our working group, tap Verint, time variant routing in routing area, which is about to manage with the medical resources with time schedule changes. And in the, the schedule young draft there are currently 2 groupings have, defined in the IETF schedule, young module. And I think this is, conform us to the definition of the period and the current through formats defined in the FC 5545. This is an effort and effort of the application area. And this is a existing view. We don't really want to reinvent. So just directly translate the information model to the young data model. As this, defines a comprehensive a quite comprehensive definition, which is intended to be applyable for common, scheduling information. Such as event policy and schedule service or resources. That's only at a time. So given the common interests of different work, we have planned a side meeting at this Tuesday afternoon, 3 pm"
  },
  {
    "startTime": "00:30:00",
    "text": "from 3 pm to 4 pm. The room is according for we would like to seek some coordination among others of different documents. Both to see if the current definition it's sufficient for the various requirements and see to eliminate any potential overlap. There are some similar schedule definition. And we might also, explore whether we need a common scheduled the framework for the scheduled service. So if you are interested you can feel free to join the meeting Next slide. Tif Tifeng, did you send, my ops chairs and and and regular email gets mixed. Did you send this to the list the the side meeting no. We did not send the list to the mailing list, but we can send it after this meeting. Please. Thank you. Thank you. Next slide. So for the the next step, we will bring the side meeting achievement back to the working group and then continuously work on the common schedule, young data model, which this document normally not normally depends on. So we we don't want to, get since slowed down because we we this draft has already adopted by the working group and has normative reference. And then we will resolve the open issues of this draft. And then I I believe this should be resolved the next revision. And then we will get back to the working group for review the document updates and provide the feedback. I think that's all. Thank you. Got about a minute for questions, comments, Thank you, Chikung. Thank you. Jean, I think you're up. Hello? Hello?"
  },
  {
    "startTime": "00:32:00",
    "text": "It's okay that you Oh, you. Thank you. So this is an update on the status of the data manifest draft. So, next slide, please. So to recall quickly, the goal of the data manifest is be able to understand what is the information that collected and how we collected that information. So basically, the idea is to define some kind of data format for that that is in let's say, of the dev underlying devices and the the the the modules that this device is actually implemented. So next slide, please. And next slide, please. Do you not see slide 3? I have. Yeah. I I see. No. Sorry. Yeah. So, basically, to explain the the issue of, how to interpret let's say that at time 1, we have container that is, at the value of 42 and the status that is up. Need to understand if it's if we don't see any other value, What does it mean? Does it mean that the telemetry is not working? Does it mean that it's actually because this value didn't change and we have a one change subscription, which means that we won't receive updates as it changes, or maybe just that the period of collection is actually very large, and we didn't see the the we didn't reach the next update point yet. Basically, the the ideas to have all this information so that whenever we want to do some analysis on this data, and find stuff for under Maddie or try to close the loop we can correctly interpret the data"
  },
  {
    "startTime": "00:34:01",
    "text": "Okay. Next slide, please. So we have too many the platform manifests, which is about the entity that is producing the data. So if it's a device, it will contains the the the OS model version of the device. It also contains the module that are supported by the device. And the data collection is about, how the the correction is configured. And the the, so the the information about what's the the actual subscription. And, basically, this manifest needs to be stored with the the data. So that whenever we access the data, we can retrieve the course the manifest. And, please, So for the changes, this is the the main change is that we have added one example of data manifest, which can give us a feeling of how it can be useful. So on the left, we have the from manifest, for the sake of brevity in the young library part, I didn't put the list of modules but we should have the list of modules there and the important modules. And on the right, we have the example of, what is the data collection manifest. Actually, we have 2 subscriptions here. We have one unchanged subscription, to the status and administrative status of the interface. This is the first one. And the second one is the number of Okay. Okay. Okay. In updates for that that were received by the interface and what we can see already with this is that the first one is unchanged. So for instance, we have the answer that if we didn't receive any data, it's because"
  },
  {
    "startTime": "00:36:00",
    "text": "wasn't changed. And the second one, we can see that, for instance, it's overloaded and we requested a period of, 10 seconds, but, actually, We have 20 second So we have a we probably, the, let's say, the devices are valid and cannot accommodate the tired of tens of homes. So, yeah, that's a one example. So next slide, please. So the big, issue that we have lived draft which has spent a lot of time on the last meeting is, the Yang modeling issue. So that it's, kind of, are the 2 re include the existing modules, notably for the IETF, young library. Actually, it's okay because there is a being that's then we kind of lose the the the documentation. And, for the And for the the young, the John Push. Sorry. It was really hard to to retrieve. So we did a copy pasted version. And so we are we are working on this new draft, which is actually called fully include. I just sent a message on the meeting list, about that. Rob. You have a question? Yep. Lost one of the mics. Maybe the battery died in it. We have another one. This room is huge, so We have, like, bikes for days. I'll start seeing that. just go outside and To Rob Wilson. So can you get back the slide, please? So on your example on the right, which are now known or agreed, I'm wondering if When you send that data, does it get sent when the subscription is set up,"
  },
  {
    "startTime": "00:38:00",
    "text": "or do you send it more frequently when data's being sent? So just wondering the one on the left is clear that you sort of that's that's sent less frequently, but when does the other date on the right search. Yeah. So what we said in the draft, I think, is that the idea would be to have unchanged subscription because, basically, this change, for instance, of the actual period is the one that would be, like, not controlled, but should be kind of limited otherwise, it's only when there is a change in the subscription configuration that you need to that you need to update it and on changes. Perfect for that. Okay. Thank Robert, it might make sense to bring that mic up here. Perhaps the density of the room is more towards the front. Oh, thank you, Biller. Okay. So And, so We have another question, which is about the interaction with the, software, bill of material, for which, we are investigating with the the draft from Diego could be used to make the link with with that, with that work. Next slide, please. We are still have some open questions. So about, handling the absence of values we think that's, as a source, that's this should be, out of scope So, basically, that, this this this, this is the way we want to resolve this open question. We have, make the link with the inventory efforts. And that would be, the, to make that link, we could use the the node IDs from the from the LFC 8345."
  },
  {
    "startTime": "00:40:00",
    "text": "So this is the this is a our proposition And there's also a question, like, to the to the room about if there is anything that we we should have in this data manifest that is that is not that is not included in in the current version. Any questions or answers suggestion's comments from working group, Okay. Well, I think on the list if people have anything to, specifically, is there anything missing as John as John said from the manifest please let them know and I think on Wednesday, we have something from Olga on some read out around 8345 and and applicability in certain areas. So Thank you, Jean. Thank you. Let's see the John Evans. It's John up here Great. Oh, yeah. Thanks. Hi. My name is Sean Evans. I'm one of the co authors of this draft we checked in relatively recently. I'm not gonna talk about the detail of the draft. I'm gonna try and take you, take you through why we wrote this after why we think it's needed. Next slide, please. Okay. So we probably all agree that the basic job of a network is to transport packets. If that's the case, the the most primary signal when we're not doing our job is when we're dropping packets. So this draft really came from the operational requirement of minimizing network packet loss."
  },
  {
    "startTime": "00:42:00",
    "text": "Through automating the remediation of a detected packet loss. Next slide. So to set set the context this is, you know, simplistic view of pipeline. We've got, raw data coming in at the left. From a packet loss perspective, we've got active monitoring and passive monitoring, detecting packet loss, obviously, other data like event data. Go through detection phase, anomaly detection, uh-uh, thresholding And then That translates our time series into events, which then go into correlation and root cause analysis. And then ideally, we understand from the correlation and root cause analysis that the set of stuff we've got coming in relates to one particular problem. We know what to do about it in Syria. I'm gonna focus on the packet loss side. So, we got signals of packet loss here from active monitoring and passive monitoring. Next slide. Please. So if we wanna also mitigate packet loss events, We need accuracy of the measurement we also need to understand root cause If our options are active monitoring and passive monitoring, active monitoring is something in time and space, and doesn't give you clear indication of course. So to satisfy this problem statement, we need clear play signal from passive monitoring. And What we're trying to do is, understand we got an anomaly understand that to a low to a low level of to to a high level of accuracy. So even low level loss, understand which device and what fought question. Just real quick, could you, in in the future, use your show of hands tool to trying to queue. Ahead of time, but please give your name and a a name, please. Yes. Thank you. Greg Mursky Erickson. Quick question. So, you refer to"
  },
  {
    "startTime": "00:44:00",
    "text": "different passive. Is it in the context of RFC 7999? I'm I'm using active monitoring to mean when we're putting probing on the network to detect loss. So, IPPM or whatever it may be. But I'm using passive to me when we're getting the the data direct from the device it's Yeah. So, The RFC 7799 establishes 3 types of measurements. So active passive, as you, noted, and, let's see who heard something in between a hybrid. So the hybrid is probably can be, seen as on path telemetry. Where the data traffic can be, enhanced with their, some additional shame to, to instrument the monitoring performance. So, what I understand probably, in active, you mean injecting test packet specifically constructed, as well as using hybrid to do measurements. I mean, yeah, I mean, I'm in traffic into the network. I don't mean in this context hybrid I'm I'm really talking about passive. I think hybridism message message, message, message, for disseminating that data, but it's problem. I probably will not I would still believe that hybrid can, you, be used in different aspects of measuring performance metrics. And in some cases, including, packet to us, for example, alternate marking method can be used to do, measurements on the data traffic. Yes. I won't disagree with that. Packet was. Won't disagree with that. Focus here is on unsampled, accurate measuring measurement of all loss with cause. Okay. Okay. We we can continue it on on the list. Thank you. Thank you. Next slide, please. So Given that problem statement,"
  },
  {
    "startTime": "00:46:01",
    "text": "If we're not prior also mitigation, There are only a relatively small number of auto mitigation actions you can apply in a live network to, to mitigate packet loss. We can take a device or a link out of service or a set of links or devices can put a link or device back into service. Can roll back a change, we can move traffic, or we can escalate it. To person to do something with. And ideally, we don't want to do that last one. Wanna try and make sure that we've covered everything with the first. And so for that, we need, we need precise signal of impact. Example of where we're not precise is if, for example, we have loss on a device. We take we imagine it's due to errors. We take it out of service, but that's sued congestion, we've made the problem worse. So how do we get that signal. Next slide please. And this is an example of where we make the problem worse. So there are 2 primary metrics that are implemented during pretty much every implementation from all vendors and open source terms of reporting packet loss. I often, and I found guards and I have seen and I have out errors which obviously defined very long time ago. And are both ambiguous. IFN discards, last, sentence of this quote, One possible reason for discarding such a pack, it could be to free up package, buffer space. So it could be. It's it's not doesn't list any of the other reasons. In practice, we see a lot of variation in terms of how it gets implemented This could include intended discards, ACL discounts, it can include unintended discount. Similarly, if we look at if in errors, the number of inbound packets containing errors preventing them from being delivered to a higher layer protocol. Doesn't say here that they're discarded or not, and what we see is some implementations it counts non discarded errors in others, it accounts discarded errors because it just doesn't define"
  },
  {
    "startTime": "00:48:01",
    "text": "next slide, please. Oh, sorry. We have another question. It's great. I used the tool this time. Greg Musky Erickson, So, I had a question. When you have measurements So all measurements gets to mitigation all the, occurrences of the packet loss. Or there is some, evaluation step that decides, oh, we are intolerable for this particular, service that we monitor or we are above the threshold. Of tolerance? Yes. There's, there there's that that was that detection step in the earlier slide, and I'll cover that a little like, okay. Because, actually, I think that there is some interesting intersection between, this work and work in IPPM working group on precision availability metrics. So it will be interesting for us to discuss Okay. Thank you. Thank you. So they're the 2 primary metrics in implemented pretty much everywhere, all platforms, And there are a lot of other discard metrics implemented, but they're implemented that that inconsistently. And some of the things that we've experienced Not reporting all discards, which is actually quite common. And in which case then, even though the discard is on the box, in a in a hardware metric, in a metric. Somewhat in a metric somewhere. It's not exported. It appears like a like a gray failure. Duplicate reporting, which is, you know, really difficult because then you can't recourse. Same ID can actually mean different things on different platforms. The IFA in errors is is is classic for that. Which is the next one. And then, you know, the, reporting"
  },
  {
    "startTime": "00:50:01",
    "text": "reporting the loss associated with different entities which can be interface. It can be platform. And in some cases, it can something in between the two things. So that inconsistency doesn't help. And last point, there there are no clearly defined semantics, and that's why you went we are the fact that we don't have a a broad classification. We don't have, Symansect is why we have the inconsistencies Next slide, please. So in order to address that, we defined a classification scheme, which we implemented a cross a number of platforms from a number of vendors by essentially working backwards from the problem the actions we were trying to take then working backwards to underlying counters in the platform, mapping the underlying hardware counters, which I think in the most was up to 2 256. Into those, into those disco metrics. Into those passes. Next slide, please. This is a classification scheme. Broadly break it down into intended discounts and unintended discounts. And then on unintended discounts, break it down by type and by subtype. You can see those here. And, and they're in the document But essentially, this was this breakdown was the the minimum breakdown in order to be able to determine what actions what this, like, Next slide, please. On semantics, it's there's a longer version in the draft, but the simple version is, report or packet or packet drops once and only once where they occur and in the right class. Next slide, please. I think the question we had earlier on from, Greg was is there a, a stage where you consider whether or not the metrics are anomalous or not. And yes, there is. And and and I've there's an example here If you look at the, we"
  },
  {
    "startTime": "00:52:01",
    "text": "the first, row and look at the CTL discards, All networks have TTL discards to a level. If what we see is below that level, we may choose to do nothing. If we see a high level of TTL discounts for a short period of time, it might be that we've got convergence going on. We see a consistent high level of, TTL discards, then it may be an indication of another issue. Like a routing loop, for example, So, yes, I think for for each of the classes you know, there's a set of cause loss rate action and there's an example of that in the draft. Question. You have quite a few questions right now. Rob, I think your first Okay. Then, Thomas, your 2nd, Okay. You're gonna go at the end. Okay. And, Okay. Next slide. So, I realized we're over, so I'll try and be brief. Experience of having done this, So done this across, multiple platforms from multiple vendors. And, The number of discard classes that you choose is a compromise obviously, if you've got a a 100 discard, a 100 classes on a large number of interfaces. It's a lot of data, and it can actually add to less clarity, not more clarity. You really want the minimum that you need to be determine what actions to take. Lots confusion over null root discards and no root disc chance when you implement, when you're when you use a default null roots to implement it to to job. Drop packets with no root. So, you know, in need to look look deep to understand that. Confusion. I've, common confusion on platforms over, packet struct"
  },
  {
    "startTime": "00:54:00",
    "text": "for 22 CPU ACL discards versus transit ACL discards similarly confusion over, TTL discards where the CP has responded versus total number of CPU discard a TTL discounts that the box sees because the number responded will be small because there'll be a 2 CPU police, but the number being dropped can be very large. And the last one is that, that even if were to implement this classification scheme, there are things that you can't detect. So, configuration errors, clearly, you detect when something is wrong You can't detect that from the metrics alone. You need some other context like I undertook a change in the last x minutes, and therefore, in that context, this is anonymous. Next slide, please. Say, That's, I guess, the how this draft came about. It's we've written it as an information model, rather than, a data model. Anticipating that this could be implemented in different ways. It this is something that we have done in in practice. And the reason for writing the draft is is to try and you know, I guess ease the firstly, get feedback so that we can improve that model Secondly, and get the benefit of applying, of using that more broadly so that everybody doesn't go through the same process. And, yeah, lastly, there is a presentation with a bit more detail from then That's questions so, Roblton, so thank you for bringing this this is a great thing to, to bring the CSO. It's really interesting. I definitely think it's an anything we have to clarify what the meaning of the captors are safe as they are. And to make it very clear that even if you give an update to more specific counts, you need to also update updated generalized counter. I think that's great because I think it's confusion there. So So that's good in terms of getting better tighter definitions. I would like"
  },
  {
    "startTime": "00:56:01",
    "text": "get more towards the data models and the information models because I think that that's probably more practical that when people are reading them, they'll be reading the Yang or the mere eventually you need to get them into those. I do still wonder though, I think this will help not sure it'll fix your issues in the sense that when it comes down to populated these counters, Sometimes it's really hard because the air encounters that are in the hardware don't quite match up to the categories you have, and all the hard is a bit different. And the way that these interfaces might be modeled in the hardware might be a bit different. So there's There's always a bit of a compromise as to how you fudge them in as best you can. So I think you also maybe want to have a like an exceptions counter that's outside of this of I couldn't quite get this in the right place somehow. Yeah. You're you're you're right. You know, any classification scheme doesn't have a perfect fit to to everything. Well, a point I would add to that though is I would actually like vendors to if we if we if we if we if we agreed a classification scheme for them to expose their underlying mapping to the classes so that then, you know, you understand what's going on and you can have a discussion over actually, I think know, a layer 2 MCU, discard. Should be And the one last thing to come to that is effectively it's the case of when we had the NIBCOUNTS defined over time. It felt like the hardware as that evolves, they started to try and, make match those buckets and things effectively. So you've got some sort of standardization over a period of 10, 20 years towards the right thing. So it takes a while. Yeah, I think it's a good thing today. So Thank you. Thomas Carr from Swisscom. So first of all, I think this is really interesting work. I also Thanks a lot. That you bring that to upstate WG. I like a lot in the beginning, you, clearly outlined for an anomaly detection system such kind of an information model and semantics is really important, and especially that you're describing the the causality, the reason"
  },
  {
    "startTime": "00:58:04",
    "text": "I think that's very valuable for the operator One thing to mention here is, I'm also driving and document more for under money detection, where I believe that kind of information is actually very valuable, especially in context of outlier detection. And, have you considered, increasing the scope, not only for for dropped packets, but also bring causality and the reasons, for instance, into the control plane. So for instance, when, passes are withdrawn, why they have been with down and so on. Would that be also something of interest? Yeah. I'd say that that's another another area of work. Oh, for this, I think we're trying to keep it quite clean on clean on discards to to try and solve that problem, I guess. Sure. But you're right. The same con the same concepts you could apply in other areas. Yeah. Absolutely. Exactly. And then the last thing, I see also some relationship towards, what we have in ip fix already with the forwarding status field. And here, we have another document which is currently the forwarding status up dating. And I think, there are a common few reasons, reason codes which are currently not it's being supported there. It will be interesting to see, you work how it could update, there are more reasons goals. Yeah. Absolutely. I think the great to have that discussion. Thank you. Thanks. Thank you so much. Hi, John. This is Ben Waikla speaking. So, yeah, this is good work. We all after this type of printers, and then the the reason why I like the fact that it's an information model because whenever you do your slides about all the branches, it's going to time 1 to come from Smith from Yang, my PPM for IPSOA from all of these. So doing from"
  },
  {
    "startTime": "01:00:01",
    "text": "not a part is, like, important to the mapping that you mentioned. That's the key thing that you'll have to do right. Coming back on the falling one. We've got like, 4 different states for the IP fixed falling state's information element and we try to do exactly what you had with the different courses. So We've got unknown for a drop in consumer for shop. We've got about 12 different reason. Access is denied. Dropped GTL, battle tanks, etcetera. We don't go quite into the detail that you wanted. But it's exactly what trying to do. For entry. If it's for L2. So that's why I come back to mapping thing that we'll have to do. And, why we focus on the following status is that because you could change a key field of the flow record, and see which more with more granularity, exactly which floor records are dropped as opposed to just see drops serically. Absolutely. I didn't thank you very much. Just to just to add to that I think there's a you know, there's a valuable workflow in operations, which is I have an anomaly tected through aggregate metrics, but I don't actually know what to do with it until I can identify the flow that's causal. And then so linking those 2, yeah, absolutely. So So so so Thank you. Way over time. Sorry. Clearly, there is interest in here. Greg Thomas Benoit, Rob, on list, this would be a phenomenal discussion to bring others into. Thank you. Let's see. We've got and we do have buffer. I'm not I'm not terribly panicky at this point. Gin. Management. Incident management Here we go. Even with the technical difficulties, not panicky at this point."
  },
  {
    "startTime": "01:02:02",
    "text": "Built buffer in. This time, I, like, even then, I built buffer in because I wanted to give everyone the the shot they want to all you men Yep. Thank you for chair. And, So, I wanna present this incident management for network survey. Actually, in the previous executive talk about a nanometer detection, I think, probably related didn't have time to come in that are very interesting. And for incident management, the next Actually, this is really, you know, built on top of nanotechnology availability and, you know, really, can can be used to do this, you know, network allow me touching and, more than that, actually. So I wanna recap a little bit of why we wanna propose this kind of work? When we do this, perform this diagnosed, actually, one change we're facing is you you need to deal with, various different data sources such as alarm data, KPI data, trace data, and This, you know, data actually is huge. Actually, take a Lambda X example, you may actually connect a huge amount of alarm data in high frequency and better connection rate, actually. So traditionally use, you know, data, compressing mechanism to reduce among the alarm data. And, but the, you know, usually, these, you know, time consuming and labor intensive addition actually without, you know, interlayer nanotechnology, correlation or with, without, you know, correlation with nanotechnology data. Is hard to assess the the impact of the alarm data, KPI data, or log data on the network surveys. So these are usually without, you know, no processing efficiency or incorrect in accurable the cost of assets. In some case, it may cause, you know, troubleshooting tickets duplication, duplication, So these work are really proposed, you know,"
  },
  {
    "startTime": "01:04:00",
    "text": "network wide, you know, incident solution. Actually, this can be used to, you know, is that connection not only with the network service, but also network topology. Enriching actually, we, define the, RPC using young data model and, can say this as open API to investigate this kind of incident. So we can provide the know, narrow insight and, life cycle management. Next, I wanna, recap a little bit, you know, revisit the still this, draft, actually, this draft can be seen as a common building block for, you know, network diagnosis. Actually, it can be only using the IP network but also in the optical network therefore actually work walk away is, you know, optical team, actually, in and uh-uh we first presented in the you know, second working group in working, in working area, but also in medical working group in office area. We, one feedback we gather is, you know, how this align with, you know, Chase contacts really work, actually. And, later on, actually, we presented, you know, OPSWG working group for twice. And so this is, third time. And we get a lot of, feedback and input actually offline and, in a meeting, actually, So in the latest version, actually, what do we do, is, you know, we try to publish the current motivation and goal of this drop and, you know, introduction section. Also, we actually update a, you know, sample use case actually, we actually introduced a new use use cases based on the feedback from Luis actually, one of the use cases may be related to the recognize me, you know, he mentioned that it's a PAM in IP cam. So we also added a reference through this RBPM okay, which Okay, man. So I wanna focus on, 3 use cases update and also in the model design update, update, Next first, they use paystays, actually, we try to, you know, you know, reduce the duplicate troubleshooting thing. It's you know, take alarm management example. We connect the alarm data. Usually, we use preconfigured wide, list"
  },
  {
    "startTime": "01:06:00",
    "text": "to fill the data in in many cases, you know, these data, filter, can be said, you know, more strict or maybe a set of more, cause, granularity. So it's really, cause you know, multiple, troubleshooting tickets may be, dispatched to the same network forward that we call the duplicated troubleshooting tickets. If the, you know, data fusion, is set to restrict, you may just receive very few shooting tickets to address the issue. So we propose this kind of insight and, management can be integrated into the new, management system. So not only you can connect the alarm data, but also, KPI or performance data or log data other data. And then you can use this data correlation mechanism or another is a service impact analysis, either you can you know, preconfigure, you know, relationship between the network service and the network incident or you can use service impact analysis to, dynamically establish the, dependency relationship between the network service and network dependency. So, these depend, you know, reduce the, duplicate troubleshooting tickets Yeah. Next, in second, the use cases, actually, you can see we you know, they fixed the focus on, you know, how we can generate this network incident based on SRO violation. In this case, we gave a example, for example, you have several VPN side. You want to you know, get them set up or to talk with each other and also you can monitor the traffic that, you know, passed through this VPN traffic and, monitoring the the the the performance of these traffic, you know, go through a different VPN side for a long VPN side a to VPN side b. So, we actually introduce QA related, metric that we call the F-three VPN service availability, And, so we can use this metric to Mayor and to end that and say,"
  },
  {
    "startTime": "01:08:01",
    "text": "in terms of the level of the packet. So you can allow different level of the packet loss for from the VPN side a to VPN side b. Actually, you need to make sure the anti medicine, we're not exceed, 5 minutes, for 99% over the packet. So when this kind of, metric, you know, you can see the threshold. We will, you know, you know, automatically, generate this network incident And so this can be reported to the OSS. Next. In certain use case, it actually really to market layer for the management. So you may deal with, RP plus, optical, this kind of harbor hybrid is an area traditional, we, you know, use IP layer, on a message to manages, Avilayeralam data and use the operator managing system to manage the optical layer AlamData, but, you know, the the the the the challenge is is RP operating team, like a collaboration with optical our recent team. So so in this case, it's if the, you know, some folder happen, you know, the interlayer linker, you know, between IP layer network device and optical layer network devices. So who will take care of this. So, usually, it may involve some human, manual process of human involved to to do this kind of manual troubleshooting. So these are really cause you know, uh-uh long, locate, for locating time. So do, address the challenge, actually, we can, you know, introduce hierarchy, managing system, we we introduce market domain, operator, they can according with, you know, calculator, emerging system and optical management system so they can connect not only each layer around data, but also the the data in the enter, layer links. So and then you can use the incident, life cycle management to investigate the the the incident that you could change the incident data."
  },
  {
    "startTime": "01:10:02",
    "text": "So this can, you know, make a, you know, IT operation team to work together with the optical origin, origin team. And so this also can reduce the locating time. Next So this, the proposed model design you can see the model structure for the item incident. So you you can see actually, I'm gonna highlight this, you know, we introduce service instance, uh-uh, leave. So this can describe one or multiple network service instance. In addition, we introduce network domain or and then a source. So this describes the relationship with network topology. This, actually, related to the, I would say, 8340 5. And, so we can, you know, build on top of these nanotechnology. So, and and, we we can see how these bikoylouisnetautobodgett addition, actually, we define the RPC. So you can see it as an open API we can change the incident information. So this is a high idea for this module design. Next, So we think that this actually has been around for a while. Actually, we, think that call the basis. Actually, we actually, recent comments from Louisa and to introduce the, service on availability, monitoring use cases so we explore how the model can, can be tend to maybe introduce some new parameter to dispose these kind of use cases. And, see the p value of this network incident is, you know, reduce the troubleshooting tickets actually. Also can help improve the network and maintenance efficiency, because you can assess the impact of alarm or KPI on the network service. In reaching, actually, this incident can be used in multi layer, multi domain. So they've reached provide the multilayer multilayer handy tools to provide a the network of visibility So we would like working with consider to adopt this and comes out welcome. Yes. Thank you."
  },
  {
    "startTime": "01:12:01",
    "text": "So I'm gonna tempt fate. I put up, I think. I did. For those of you who have read this work, please respond to the poll. Is there interest in adopting this draft. Yes. No. No opinion. Yes. You think you you're interested in in dog adopting this, no, we should not or no opinion. You either haven't read it or you don't care. It's kinda stabilizing. Maybe. Stabilizing. And looks like Paolo turned on his camera. So it looks like there is, I mean, There is fifty seven people with no opinion, but it it seems like there might be enough work taking it to the list and seeing what the the list thinks. Any comments, questions? I wanna add the one more comments there. We actually I forgot mentioned, actually, we, Tuesday, we also want to organize the signing meeting to discuss this incident management to say more use cases. I want to invite, you know, John Evan to to John. I think it's more relevant also Yes. How much Yes. Same comment to you, Chen. If you haven't, I can't remember if it just sent it to shares, but make sure you publicly on the list, invite people to the site. Can drop can you go back to slide 6, please? So I'd have to have busted no opinion on this because I haven't read the dates based version of doctors. I do really know. So think this is definitely an interesting problem to be trying to solve. So that I so in terms of trying to solve this problem, I think that's great. I think the thing that, then you can lose the poll on screen. Sorry. So in terms of looking slide, it wasn't clear to me whether for 2 things. One is how you actually correlate the IP layer alarms and optical alarms together."
  },
  {
    "startTime": "01:14:00",
    "text": "Into the same instance. I think that's going to be the really tricky bit is to get those to be the same thing. So I think that's interesting. Problem. And the other thing that that occurs to me is wasn't clear to me in terms again this this sort of translation whether you are therefore saying, rather than these being reported as separate IP and optical air alarms, you now port, these are incident alarms. So you sort of generalize those. And if you do that, my question is, how do you model the extra, detailed data that previously held in those separate alarm structures into, an incident form that's more generic. So I think that's the interesting thing is you end up just putting this all into strings or description fields, gonna lose the structure, the data, if you allow it to be hierarchical extensible, then that's gonna be really interesting. So so that's the other thing I thought was an interesting problem here is to how to marry those 2 things up. Yeah. So an interesting problem, I think it's pretty worth I I I think it's interesting working on this stuff, but I'm not sure I know exactly what the solution should look like. Yeah. We always explore. Yeah. More words, try to address your your comments. Yeah. Thanks. I said one of the thoughts was, like, whether it should be a hash number. You have an incident record, and then you should be tying that back as extra metadata in the original alarms, right, or something like that, maybe Yeah. Right? Thank you. I just wanted to comment on the relationship to the topology and and the information about the layers. Can you, make the mic closer to your Yeah. All the Huddl Huawei. I just wanted to comment about the relationship to that and deleting what I believe is that because there will be different things that the policy has to connect to, like, incidents and configuration and to do it in some generic they from the topology itself so that we can work together in the way how it is connected us. We don't have different completely different solutions for incidents versus configuration versus statement. Yeah. Yeah. Okay. Yeah. That's a good suggestion. We can"
  },
  {
    "startTime": "01:16:00",
    "text": "take offline. Thank you. Thank you, Chin. Yeah. Thank you. Diego. Cruises. Sorry about my back is in a not very good Hey, man. I hear you. It was some time to to stand up. That's okay. Okay. Thank you. So, well, this is the second version, the 01 of, of the proposal that I first introduced in in San Francisco, Can you move on just to just as a reminder, it's about provenance. It's about ensuring that the the origin and integrity of, jam data sets when they are not used used or or they cannot this cannot be derived from a from a direct evidence from the, from the data flow. For example, data lakes, or what is being useful for AI, training, and to apply all the trails whatever else, is related to in fearing this provenance without, without the direct evidence from the dataflow. The means is to use cozy as a signature to ablate. For any of the serialization methods that are proposed and to use a mechanism that is extremely comfortable for doing these in COCIT as this year. Detaching the the payload from the signature. They move on? So when I presented this in San Francisco, I my impression was that there was interest in the group and, there was, some energy that well, the circumstances has made have made not so, Not so strong because I I have been, focusing in other, obligations So the changes are not as many as I would like to to to bring here."
  },
  {
    "startTime": "01:18:05",
    "text": "Has to be in summary, has been corrected. Some comments on, the idea of using of applying provenance and data pipelines. Which is very much connected. I guess it was, for example, Jane was telling before and the, idea of the recursion of how you relate it how you can nest signatures. Are, already addressing the text still some open comments that were originally, I mean, most of them come from the discussion in some Cisco regarding where to place the signature and, say, how to deal with the idea because the current proposal is to use cozy with the signed one construct that allows only one signer for the for the whole thing. There was somebody in the the during the discussion and Cisco making some notes that probably could be interesting. To consider to support multiple signatures. And that brings the point is that bring some some consideration of, in general, at station mechanisms as well for for for young, and this is something that it's still open. Can you move on? So for the regression, should be a recursion and not reposition be, it wasn't reviewing this and there was, is the problem of making this at certain time in the in the night, many rata without, with our spell checking. That's, So the idea is that, I have tried to clarify the fact that you can nest signatures with the current proposal. It is that the mechanism allows you to uh-uh the advocates for having a single signature elements that is It contains the cozy signature of all the general elements that contains it, with the exception of the signature itself. So you can have a chunk of junk like this"
  },
  {
    "startTime": "01:20:00",
    "text": "you sign the whole thing and you associate the signature with it. If inside there is another element you can simply sign it by doing the same procedure taking that, that's part, signing it, and including the signature That implies that when you're going to sign the whole thing, you would sign as well the element that would contain the signature in the in the internal in the inner, elements as well. So the idea is that you can aggregate signatures. You can aggregate data. You can you can take pieces of junk that are already already signed sign them yourselves and share it. So the hope the integrity of the aggregate will be associated with the aggregator integrity of the individual components would be aggregated with the original one. It is that this is this support as I said, this this idea of that proven and provenance verification can be done in a record safe way. Kang, David. Analyze the components of the whole, depending on who are who has provided the, this this is intended. I mean, I have dedicated some time to try to clarify this on the text. Should be clear, have a a a read of identity. It is not clear yet I'll try to improve it. Can you move on lease. With the signature placement, during the meeting, I took notes and afterwards in some discussions on the these ideas as a in a into the The the one proposing the draft is totally transparent. This is just to put put it whatever you feel like as a leaf at the same at the level of the of the component that is being signed. Whatever it is, but just one. That's that's the only condition. Was some, suggestion about using, a notation reading the the annotation draft, I was a little bit about, city the the"
  },
  {
    "startTime": "01:22:01",
    "text": "about the serialization, u neutrality because CIBOR, as far as I can tell, is contemplated there. So it's something I'm I'm I'm not sure whether it's it's the case. There was a, suggestion of using using it for gen push and, well, That's some it was not clear to me from the comments and the discussion in which part of the jam push whether it's part of a of a RPC parameter or as part of the general elements. I tend to believe that it will make the, would mean the general elements, but because it's become it will become a particular case of the of the general proposal And it's the same if we go for the for the junk based files, if we a low as well that is associated with the general structure on the in the in the same file. So The the question here is, is whether you believe I personally, I believe it, but this is something that I would like to to get some feedback. Is that I believe that in these documents, we can consider case for the gen push and the and the and the and the and the gen base files can be considered a general, particular cases of the general case put in the the signature, whatever. You see fit. And that would imply for sure, uh-uh, an update of the notification and the and the and the and the, file metadata schemas, but did we would be for for for signature and and would be enough to have this this modification on this proposal in these documents with changing or making any updates on the on the previous one. For the annotations, I don't see any problem in adding it in the annotations. Simply proposing a new kind of other notation. And asking someone that is working on about the, the the silver extension in the in the future. So this is will be the the next, my next target. Next one, please."
  },
  {
    "startTime": "01:24:01",
    "text": "have, two questions. You wanna take them now, or do you wanna Oh, I just yeah. Well, no. If people get in, it would be Alex Oh, yeah. If you don't sorry. That's okay, man. We're we're we're we're looking out for you. Alex Wang in Salian, so two comments on here, seabor. It's a young see what you are referring to, I believe. So for the c wire encoding. I believe that it would be transparent as JSON. So that's, I believe there is no issue in there. I have a comment on how to validate the message. This signature placement, I believe it's important and the way you use it breaks a bit the validation. How can from a data plane perspective know if there is a signature or there is not because you are not changing the young module. For a specific use case for yankush, I would suggest to at this quasi signature in the yankush header. Instead and not just placing whenever you you want as you propose the graph. That would easy a lot, the young validation. And, yeah, that was, you know, in my comment. Okay. So, well, we'll talk later about this. So and I'll call it. The other Alex, only only Alex is here. So Yes, Alex. So I think that one one company actually regarding also the placement issue think, actually, you don't need to even modify Yang push or auto notification. You can accommodate that just that you have separate signer, basically, 2nd, a separate validator. It basically, it generates, they're not occasions part of the messages that that that go out from there. I think I do think this can be easily accommodated or straightforward way. Okay. No. No. That that's going with the the We can talk also afterwards. what I had in mind. 2nd, it's good it's good to to to have some"
  },
  {
    "startTime": "01:26:00",
    "text": "hi. Yes. So so I haven't really liked for it in the draft, so I think I had one comment effectively on terms of yang based files and things. So the Yang packages draft is sort of languishing someone the version work is going a bit slowly at the moment, but one of the concepts we're considering there was the ability to add checksums to the yang files that you included in the package. So in that case, I'd then put charters 5, 6, or shaft I've got hashes in, but it's the same idea effectively having some guarantee of the file. You gotta look like a confused look in your face? No. No. No. I'm I'm I'm just thinking what whether or not I was about to ask you whether you have a reference on that because I I I have not spotted Okay. I detect some. Oh, yes. It was taken out the late yeah. version, I can find earlier version where we had them in. And the reason it was taking it out of the time was it's like people weren't sure whether the necessary complexity to add in that time was to get out and maybe just go back in one thing that we hit that was we were struggling with was what what is the check somewhere yang file? Is it just to check some of the text, which case that makes white space suddenly become significant because it changes or do you have to define a canonical yang structure representation to convert it to and then do the checks and things. So that was the interesting thing. And again, there's a related discussion in Net Mod about the versioning that open question about whether white space is significant or not and what that means is an open question on the version stuff. So you can you're welcome to join join that discussion. Sorry. No. This is this is always the the the joy of panic economic realization that is, yeah, yeah, that's I mean, if if you can share with me some some reference, I will have a have the check on if you think that this could go to some hardware, we could bring in a certain moment, this to NetBank, I'm I'm happy. I mean, just Yes, I wasn't so much saying it'd give me any direction of where it should go, what to do. It was more just a bit like a like a like a like a like a like a like be aware just to to let you know that exists, for you to take into account because I don't know the answer here. Okay. Wah."
  },
  {
    "startTime": "01:28:04",
    "text": "Well, I guess that we are almost done if you move to the so plans for the, next period, and I hope with more energy. It's to solve the the, these or these issues with the signature placement with the help of, of the Alexis and and the and the reference that, we would discussing with Rob. We keep, like, I keep in the day of, refining and detailing the use cases. Is, they are general, and it will be good to to have you go into more detail. Consider the implication of multiple signatures, this is something that is, I mean, when when thinking about this, my idea was to keep it as simple as possible, and that's why it is the same one. Well, considering this, it's something that it would be worse. We we we would need to think a little bit on the implications in terms of the potential use cases and the additional complications for for, verification and the generation of the signatures or think whether there should be another construct that will be separated go for the practical evaluation. I am glad to say that just before coming here, the I got to confirm that there was going to be a student working with, with us on this. I mean, and this there is a commitment that's, she will be working on this and knowing not on on anything else. So I I hope to have a practical implementation. So And I'm glad to, say as well that we have involved a cost expert you. But at least, gentlemen, that is going to help us with the with the cozy aspect. And so, well, the idea is to to continue working with this I'm bringing something probably I may might be that for the Brisbane, we can show something at the hackathon, hopefully. And that's all. I, I locked the queue. Sounds like though there is potential for discussion on list So thank you, Diego. Yeah."
  },
  {
    "startTime": "01:30:04",
    "text": "Christian, and I see you are there. Yes. Can you hear me? We can. You're a little, you're you're talking right into your mic, it sounds like. Oh, okay. So is it very loud or it's very distorted. Okay. Let me see. Is this better? That's much better. Much better. Okay. Excellent. Okay. So my name is Christian Martin. Right? And I, would like to talk to you about storing young based, telemetry. And time serious databases. Next slide, please. So here is the big picture idea. We have some telemetry collector here in the upper left corner, it subscribes to some yankpath, on a network device a router in the, bottom left corner It could also be doing polling in case you don't have subscription, if that's not available, Right? We then receive some telemetry data from the router, into the collector and uses a yang model to parse and validate this data. That we then that we then wanna insert into our time series database that we see at the center of this diagram. So that wheat from there can grow pretty graphs like we see in the bottom right corner. We also wanna enable other systems to consume this data. And if they have the AI model, they can consume the data programmatically by querying time series database. So the yang model acts as a as an index of all the data available in the time series database. For this to work in a generic fashion, we need an encoding, or sort of representation of yangulvedata in a time series database. And we want this to be model driven so that the collector can support any I model, without needing, bespoke manually written and and sort of hard"
  },
  {
    "startTime": "01:32:00",
    "text": "coded conversion rules for storing the data in the time series database. Next case. So how do we do that? Here is the the the gift of this the core of this mapping specification. So it's about converting you know, conceptually or a netconvexml or on JSON or any conceptual sort of yang model beta into something that we can store a time series database. And as an example, we have a Yang instance identifier here in the middle, this is for the IETF interfaces model. So there's first an interfaces container, right, then the interface list then the with the name of the that that key right Then we got the statistics container. And finally, the in unicast packets leaf with its value. So we can then transform this into a representation that is suitable for times your database. So the the metric name itself becomes the yang instance identifier path but with the keys removed, and the list key is instead added as a second label where the label key is the yank path and the the the value is the all the yang list evaluator, ethernet 0. So you can see that at the bottom this is sort of how we separate these out, to make the data fit into times your database. So this is it really it's a simple example, there are certainly more convoluted or complex ones, but it's the the core ideas are just of of this concepts. Next, please. Real quick. Would you mind if I ask a question here? Sure. Joe Clark Sysco, contributor. I noticed that you modeled or or you you mapped the path separator in the xpath as as an underscore, as well as you mapped dashes to be underscores. I I've seen in some other projects the path separator gets modeled as 2 underscores"
  },
  {
    "startTime": "01:34:02",
    "text": "in case. So, a parser might be able to differentiate a path a element versus part of a a of a single name. I'm wondering why you did the common why you made both here. I I I think that's actually an excellent suggestion. This is, I'm gonna touch upon this sort of in the next slide, right, but you know, what is the actual target? Right? There are multiple different time series database. And they have different capabilities. In some, maybe we can even keep slash as the path separator in others we might be able to use dot or or something that's, you know, you're not allowed to use in a yang name. But indeed using a single underscores here is is actually a very bad thing because it would collide with with other special characters. So, yes, I agree with you. And we should fix that. That's something for the 1, I I suppose. Thank you. Okay. So, you know, yeah, when we talk about a time series database, what do we actually need? Right? So in this case, we target something that I like to think of as label set centric time series databases. I mean, similar you have, you know, SQL and and relational databases that are you know, very similar, right? There are different implementations, but they're similar to call a certain standards. We have the same thing with time series databases. So at least if you squint a little bit, you you'll see that they implement this concept where a metric or time series is is is primarily identified by a set of labels. Some systems they call this tags or or dimension, it's really sort of the same thing. Right? So it's it's a it's a set of key value pairs. There can also be, like, a metric name But, again, you can think of that as just another, another labor. So I'm listed here at the at the bottom, you know, a couple of the keys to be I knew that I had at the top of my head. You know, there's certainly some variation in their capabilities. The path separated that they would support is, is, you know, potentially one."
  },
  {
    "startTime": "01:36:02",
    "text": "But I would still argue that they all have in common as well. Rough concept of the labels, etcetera. So, I mean, you could imagine that, you know, maybe you want profile for different databases or something like this to sort of support that, that very I'm not entirely sure. I mean, I I haven't worked out the details for. All of these. Next please. Right. So time share database. Super common today. Yang, I think, is also super common. It's prevalent on network devices. Right. And we're seeing lots and lots of config faults data. I think, m, operationally, many networks today, production networks, days that rely on plastic means like, SNMP, but a lot of things are moving, right, then I I just wanted to sort of highlight how some of the larger platforms that we have out there, they actually have a really large amount of company false data, and I think that's really encouraging. So I think we're in a good spot to make good use of this data. Next place. Those large, yang device models together with the scale of the typical service provider network means we end up with very, very large amount of time series and, you know, large data set overall. Right? You have thousands of p routers with thousands of interfaces. You have probably tens of counters on those, and then you have all the CPs which run-in in sort of the 100 of 1000 or a 1,000,000 of them. Right? And you're gonna end up with 1,000,000,000 of metrics or at the very least 100 of 1,000,000. This used to be a problem with early generation time serious, but I would say it's much less of an issue today, and there are systems. I think M3db, I have mentioned on the last slide, drew it. There are deployments of these that are in the billion rate trip. So we're in a good good spot there. Here using gang mugs, I think, provides sort of a unique rich index to get the overview over this just vast amount of data, right, that you just wouldn't be able to achieve otherwise."
  },
  {
    "startTime": "01:38:00",
    "text": "So I think being model driven is a necessity to work with the data at this point. And finally, I've included some examples of how we can query this data using this specification. And again, the queries here, you know, they're driven by the through of the AI models and they work because the data and the times are database, they follow this standardized mapping specification. Next, please. Right. So sort of on the goals here and the next steps obviously the the big goal is to have this deterministic model driven mapping for programmatic consumption of AI model telemetry data in stored in time series databases. So, first thing is complete this specification. And please you know, come talk to me if you're interested in this topic or I'd I'd love to, collaborate with people. We have some proof of concept load called that's, yeah, it's running, but we need to make sure for the progress on this. I would love see that go sort of hand in hand to get with specification, right, just make sure that everything actually works. I think it is a goal to fit well into and systems not to rely on some next generation something, but to actually work with production systems that are out there today. And that sort of necessitates the next point, which is self describing data. Don't wanna we don't wanna have a strict reliance on Yang for the visualization part of this. All that existing tool in your system that we wanna leverage, they are not yang aware. Today, today Right? Similarly, we wanted to be suitable for direct human consumption because if the visualization tool is not yang aware, you know, it's gonna present the data without in a form of, mangling in between, it it needs to be human consumption friendly, right? Of course, with the model, you get extra metadata you can then do more things like, with that. So for example, you could build the sort of dynamic dashboard. Right? If you crawled through all the animals, can find all the temperature sensors. You could build a dynamic dashboard out of that stuff like that. Right?"
  },
  {
    "startTime": "01:40:02",
    "text": "I also wanna mention here just an I think what I think is a non goal unlike the XML and JSON, those are coatings that we have in that comfort restaurant. Right? I think this is less than that. Any XML any data those kinds of leaves, for example, I think, are very hard or impossible to represent in a time series database. So this is probably gonna be and that's also what we call this a mapping. Finally, alignment with the fullest draft and perhaps others as well. And I'm out of time. Thank you for listening. Any questions, comments, and kara minute or 2. Rob is coming to the mic. I'd like to put Thomas on this and then ask him, like, what 3 is the overlap in terms of what's been done here and and the work that Tom's is proposing in terms of the Kafka schema and how that relates. Is there is a correlation Thomas is coming to the mic. Thomas Krausscomb. It complements very much the work we are doing. At the end, we need a time series data where we need, to ingest, young semantics and the work which we are, proposing is basically preserving not only the young young post message, but also the the schema end to end. So Yeah. Yeah. Complements nicely. Thank you. And Benoit said in chatting, Yang and time series database is necessary next step for the industry, and thanks you, Christian. 4 the work presentation. With that, ladies and gentlemen, it is 841 Eastern time. So I guess that's what? 241 here, and we are on to the ops area portion. Of our agenda."
  },
  {
    "startTime": "01:42:01",
    "text": "Our esteemed area directors, Robin Warren, are coming to the stage. I will do any notes for them and I will traditionally heads on them. It's been sitting there for, like, an hour on I'm Yeah. So hello, everybody. Wow. This is my phone sense stuff and stuff and directional, this is the ops area portion of the joint upstate WG and upsell area meeting. The reason that we have it as a joint meeting is there's usually very little that in the opposite area part of the meeting. Apparently, we have how many minutes still 15. Good lord. We we gave you Thank you. Wow. Usually, you give us 15 and take most of them back. So now I'm just sitting here bumping. I'm assuming that there are no questions for the open mic part. But while you're trying to think of of questions. Hi. I'm Warren Kamari, and this is rough. Rob. Oh, we actually have a question. Benoit, while you're walking up, I will remind people, please fill in the nomcom feedback If you do not fill in the non common feedback, the next time you might get someone like me, and that would not be good. Benoit, so, actually, it's not the question, but since we have time, there is something happening, which is interesting in this ATF is that we start to have many side meetings that are ops related. And that are actually even sometime more interesting that the working group wants. So if we have time, maybe the people doing those side meetings might want to introduce what they're doing. Just suggest him."
  },
  {
    "startTime": "01:44:02",
    "text": "That sounds like a great way to fill up extra time. If there are folk here who are running side meetings, and want to have a few minutes to remind people of their site meeting, that would be great. However, I will remind everyone's side meetings are not official IETF working group meetings, official IETF work can't happen in them. You know, they're just side meetings. So, This is purely just informational if anybody wants to sort of mention, Hey, I'm talking about Larg. He you know, come see my talk about Lara So the one I know about, there is one 6 opportune Margin I believe this is later, today. There is, another one tomorrow about Yang and Kafka, half an hour followed by the digital map POC of it. And there is a a last one thing is on, Wednesday about what's a exact topic No, it's not the one I was thinking about. It's the one about, open config. Like, issues of open conflict and network management to something like that. And the one if you want to mention that one So maybe the people here, they check the side meeting they may find some other topics that are of interest in terms of related to this area. Yeah. I was just trying to find the URL. If you type an IETF site meetings, I think that will take you to the Wiki page. And there's a list there of side meetings. But a reminder, again, you know, side meetings or just side meetings. They don't have any official standing. Something something something disclaimer, etcetera. Anyone else with a comment or a question or would everybody like to end 15 minutes early. There we go. Regarding the file, side meeting, I already, you know, make a, you know, presentation for the,"
  },
  {
    "startTime": "01:46:00",
    "text": "the the group based access control and also time schedules. So for, time schedule, we actually we have signed a meeting on Thursday afternoon And, for the, instant and the management, actually, we have a summit meeting Tuesday afternoon. So welcome to join. And if people want the list, you go to the main IETF page, and then down towards the bottom, there's a link to additional events and there's a list there saying public site meetings. So that's the easy way to find them. A suggestion to people who've put inside meetings, but including a would be super helpful. Because some of them just have, like, the name of a meeting and no other info And I was just gonna re reconfirm the putting noncon feedback in is really helpful for the non comm. So I've not been on the non comm. I've been on the receiving side of it. But effectively, I think that all the people put in is red is taken into account in terms of choosing. So and depending on who who you want to represent you as your new, ops management AD come March, that now's the time to put feedback in and help noncon make their decisions. Thank you. Yes. If you don't provide feedback, you lose any right to pitch it get somebody, you don't like Of course, if you do put in feedback and get someone you don't like, Yeah. Okay. Well, I think that this is a good amount of random vamping and side discussions, So unless anybody has a question for area and the next ten 9 8 765-4321. We're done. Thank to everybody. See you in March in Brisbane. 4. Or see you Wednesday at 1 PM. Or see you Wednesday. Thanks, all. Sec."
  },
  {
    "startTime": "01:48:11",
    "text": "What? Thank you as always. Not"
  }
]
