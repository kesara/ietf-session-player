[
  {
    "startTime": "00:00:27",
    "text": "Thank you Thank you Thank you"
  },
  {
    "startTime": "00:03:27",
    "text": "All right folks, let's get started. Happy Friday everybody, and welcome to ICCRG My name is reese enghardt and I'm Joan by my co-chair video Let's see if VD wants to switch on audio or video. Hi You're fine You're good. I'm going to pass your slides control so you can take us through your chair slides"
  },
  {
    "startTime": "00:04:00",
    "text": "awesome do you hear any echo or anything on that side? I can hear you well. Okay, great All right, then Let me just turn on my video real quick All right. Hi, everyone Today, welcome to the ice ICCRG meeting at Vancouver. I am removed remote And as we all know, this is a hybrid meeting and the session is being recorded and these are some of the useful links that you can go through through And next slide, this is all boilerplate, note well intellectual properties stuff I'm going to go to the next slide These meetings are recorded So if you, I suppose, do not want to get a photograph then follow the instructions and then these are the privacy and code of course instructions right here and just a general idea of the goals of IRTF it's meant for research and the drafts that are published are also meant for research For standards, you would go to IETF And today agenda, so we do need a note taker. Do we have a note taker yet? I do not have a note taker yet. Can I get somebody? to take notes for us in case there's any discussion, pretty please?"
  },
  {
    "startTime": "00:06:06",
    "text": "Oh, Neil volunteered to take notes. Thank you so much, Neil Thank you, Neil And I want to update that we have these things on our agenda So for the working group document I'm currently shepherding the research let bad document and this has already passed the last working group call. It's out there for, I suppose, IESG publication step So if you still haven't looked at it, it would be a good idea to take a read and if there are any comments you still feel free to send it to the author although it is a bit late but we still appreciate it for presentations today Reese wants to give a update on the hackathon, I suppose this is about the congestion control work that we were doing at the hackathon this time There is two presentations from michael jones is about slow start threshold after, you know, we have a slow start overshoot. And then he's working on another research draft or more like a draft to document how to do pace and transport protocols. This has some coverage from what is happening now and what people can consider if they want to implement pacing in their protocols in the future. And then Kuhn will present L4S and Prague condition Control as well So I think he presented some, gave some updated in CCWG and in this one it's going to be a"
  },
  {
    "startTime": "00:08:00",
    "text": "more elaborate discussion about Prague And then Matt is planning to present congestion control response when your application limited which is also I think was the discussed to some extent in CCWG So these are mostly today focusing on congestion control, pacing, and related stuff So that's what we have for the agenda. Is there anyone who has any concern or question? comments? I am not seeing anybody get up in the room so it seems like we're good Awesome. Thank you, VD And let's do matthew quick hackathon update. So this is just about what I present on Sunday following the hackathon And I'm aware we already have had congestion control testing going on at the hackathon before at the L4S table but I figured it was time given that we have a variety of different proposals in different groups, I figured it was time for a more general congestion control testing table and the idea was to hack on tooling, maybe have a test suite, maybe some sort of congestion control interrupt, even that we could write at the hackathon, and then also, of course, if we identify any issues with any particular implementation hacking on that as well. So this time I was joined by a small team to test the careful resume draft on TST TSVWG And there were a variety of different quick implementation as clients interoperating with a KISH server that implements Care for Resume and they were connected using an NS3 simulator an emulation mode, emulating a link with six hundred milliseconds delay and 10 mbps"
  },
  {
    "startTime": "00:10:00",
    "text": "because we wanted a specific bandwidth delay product and we're only able to achieve a low throughput due to NS3 limitations we kind of made it along satellite link and then there was the careful resume jump of 400 packets so here's just a quick overview of the results and Gori has presented details in TSVWG but the gist of it is that all clients exhibit their careful resume behavior, but they're subtle differences between them, so they all end up with a, slightly different sea wind but this is basically what an interrupt matrix of sorts could look like but I would like us to iterate on sort of the different things we test but yeah I feel like this was a good start, and I would like to get more people involved. So I'm planning to reach out to protect participants for the hackathon in Dublin and then bring back the congestion control testing table. So if anybody in this room or this session is interested in joining, please send an email or just table. So if anybody in this room or this session is interested in joining, please send an email or, you know, just show up to the Dublin Hackathon And thank you for this time. Thank you to Gori and his remote team for the careful resume test to get us started does anybody have any comments or questions about the hackathon? Okay, notes seeing anything I am going to bring up michael rosa slide slides slides Hello. Oh, yeah. Sorry, just very quickly on the heck of phone, I can make my phone work fast enough this late in the week I think this is a really, you know, the hackathon"
  },
  {
    "startTime": "00:12:00",
    "text": "stuff is really good. I think it's really great the group is doing it I would encourage you to sort of circulate lists of topics early enough that people can organize students to involve Yeah I think this is a really good thing. colin perkins Thank you, Colin Okay this is about SSWASH after Slow Start. I underlined the overshoot to make it clear that this is not about things like high start, high start plus plus When they work, when you get out of it, then that's not what this is about, right? It's when you do overshoot overshoot overshoot Oh, oh, oh, now many things move Yeah, sorry, now many things happen. Okay, so this is about a very basic thing of TCP that I believe people probably have misconceptions about. It's also partially hidden by modern TCP's because, for instance, cubic tends to exit with high start, BBR paces which is actually magnified the problem that I'm showing here, but then BBBB also will exit not necessarily by overshoot You know, we have also things like high start plus plus, but when these heuristics don't kick in and you do overshoot with it, then well, there is something about as a threshold value that I want to explain. So it's not uncommon to see after such an overshoot to see double loss in the sense that you have a loss you end up, I mean, you know, one consecutive round trip time of a couple of losses, you end up in recovery but after recovery, you lose a packet again And I wanted to understand when that happens So I made a model for that and it's shows that after slow start overshoot, even the very foundational, better times congestion window when that is 0.5, and I'm not getting into that flight size versus congestion window discussion. That is not the point here Even 0.5 is actually too much"
  },
  {
    "startTime": "00:14:00",
    "text": "and 0.7 is even more too much and pacing even makes this worse So when S.S. Fresh is too large, this means that after loss recovery we'll lose again. It's a double backoff and I suspect also that this has maybe to some degree contributed to the complexity of recovery that we're having. Because, you know, these people have been working on trying to solve the problem whereas maybe the target is wrong and you can't really solve it because when they come out of it, you lose anyway so it's a very basic thing right why do we have 0.5 in original TCP Reno? It's on this basis from the congestion avoidance and control paper. If you're starting, you know, that half the current window side worked. So Windows worth of packet works change with no drops. Slow start guarantees this This statement is wrong. In fact, it's the other way around when you do manage to saturate the bottleneck with Slow Start then it's actually guaranteed that this value is too much The only reason TCP, even the old TCP, works at all and doesn't always have double drops, is that sometimes, because slow start is so bursty, we have burst losses happening and then your window may be way below the with the lay product laybroder and then your window may be way below the bandwidth delay product. So here's a very simple example of this. Imagine that the BDP plus Q is 30 packets this is all in packets, right? We begin with initial window 10. We send 10 packets. Getting the X for these 10 packets, we go to 20, getting the X for these 20 In the third round, we go to 40. Now in this round where the congestion will win will definitely go to 40 on the basis of the X from the pre- go to 20, getting the X for these 20, in the third round, we go to 40. Now, in this round, where the congestion window will definitely go to 40 on the basis of the X from the previous round trip time, loss happens after after sending 30 packets right so the first 30 packets of this round also go through That means that the congestion window will grow to 70 then two packs will arrive And 70 is the current window size"
  },
  {
    "startTime": "00:16:00",
    "text": "and 70 half is 35, not 30, so you will lose again And that is, you know, it's a very basic thing, but it does happen So I wanted to understand this better, made a very simple model some maths on the Friday afternoon, what's better than that? But if you do follow along, I mean it's it's really pretty simple and you can extend it you know, for all kinds of other overshoots, maybe cubic overshoot or whatever so it's not not hard to reproduce this I begin with a lot of assumptions here. It's one TCP connection one bottleneck, a FIFOQ, there's a greedy sample reproduce this. I begin with a lot of assumptions here. It's one TCP connection, one bottleneck, a FIFOQ. There's a greedy center, there's no pacing. I will show a pacing diagram but not for the model Connections are long enough to finish slow start. There's no link noise so packet loss is only cost by congestion. X are not delayed, so there's no DELAC The X are not lost. I will also show a DELIC diagram. So, yeah know, it relates, but not part of the maths The unit is a packet. All packets have the same size We make this all very simple, many, many constraints Is this useful at all? Yeah, I mean, this is how we understand TCP. You begin with one connection. And if you find the that something is terrible, then it's not going to work well with hundreds of connections. Okay so the fundamental thing of the model, the fundamental understanding that I want you to take away, I think, is that there are two types of losses that can happen from congestion One is a burst loss. We have a burst of packets arriving faster than the router can find are two types of losses that can happen from congestion. One is a burst loss. We have a burst of packets arriving faster than the router can forward it, and this burst is also long or too long to fit in the queue so that you have packet loss and the other case is that is what I call saturation loss is when the bottom leg is really full If you look at this picture here, we have a time in, or time T that it takes to take a packet to leave the pipe on the receiving end. It's going to take a time T to get one packet from the queue, but in this example,"
  },
  {
    "startTime": "00:18:00",
    "text": "during that time interval two packets will arrive to the queue so it doesn't work and that's a saturation loss And, you know, slow start hands when one of these two losses happens right? It's simply. So basically, I can already tell you my model has a big minimum function at the end right? It's calculating will birth loss happen, will this loss happen? It happens when the minimum of these two applies Okay, so when will the end. It's calculating, will burst loss happen? Will this loss happen? It happens when the minimum of these two applies. Okay, so when those burst loss happen? Again, it's when the burst is too large, and arrives too fast too fast is about the arrival rate versus the departure rate or the other way around, just to have a number between zero and one And then there is a special case that makes the whole model a bit more complicated that maybe, you know, you know, doubling the rate in slow start, but maybe you can't For instance, if the bottleneck has 100 megabit, and the links before the bottleneck have 10, 1024, megabit, or 125, I don't know, you can't go to 200 because the capacity just isn't there So there is a ratio and if in slow start you are able to really double, and that ratio becomes 0.5 we talk about the burst B when the last packet of that burst arrives, a ratio times B minus 1 packets will already have been forwarded. So you can think of, for instance, if you have a burst of five packets when the 50 fifth packet arrives, then with R.5, two packets will already have been forwarded and then also cue packets can be queued so there's a constraint for a burst that will not have any overshoot, which is B minus R times B minus 1. I route this down because only full packets can be forwarded, minus Q right? And that's got to be smaller than one what, yeah, at most zero. So if we make this equal to zero and solve that for B, we get a very simple, nice equation that gives us the largest burst that doesn't cause any loss. So it's B"
  },
  {
    "startTime": "00:20:00",
    "text": "you know, that burst is rounded down, Q minus a divided by one minus R. And then the smallest burst that does cause loss is B plus 1 It's very simple if we relate this to slow starts bursts the x reflects the bottleneck capacity Doubling means that instead, of just having the ratio that we may know about from the Ingress links and the igres links, we work with 0.5 when we can, so it's maximum of 0.5 and R. So, you know, the ratio can mean that you can't really double This max is just to cover the case where the ingress capacity isn't at least twice. Now packets in slow start, when you do it, you know, without any pacing they're sent as twos of two with some space in between, but that really doesn't matter for the model, because all that burst altogether around faster than it arrives at normal twice the rate at which the bottleneck can forward it. So we can ignore this, the fact that it's in this microburstiness that it's in tuples So slow start begins with an initial window v0 in every round eye starting from zero, TCP sends B0 times 2 to the power of I packet right two to the power of zero means one so you begin with initial window then you have twice the initial window, then you have 4 times the initial window and so on Now, if no initial loss happens, which means the p plus 1, we had that as the last window then you have twice the initial window then you have four times the initial window and so on now if no initial loss happens which means the p plus one we had that as the largely or the burst that the smallest burst that does cause loss, if that is bigger than B.C then no initial loss will happen won't happen in the initial window. And if we call okay the round in which the loss happens, then we can make a equation here to say B plus 1 is smaller or equal to B 0 times 2 to the power of K if we call K the round in which the loss happens, then we can make an equation here to say b plus 1 is smaller or equal to b 0 times 2 to the power of k. If we turn this into a"
  },
  {
    "startTime": "00:22:00",
    "text": "equation, not a smaller or equal, and turn it around, then we can get K from that and then we don't need to round down for B anymore because actually the round is not influenced by fraction of packets. And in this calculation, k gives us the round in which the loss happens right? So for instance, it can be a number like 1.3, and 1.3 means it's larger than 1, so it's not round 1 so it's got to be round 2. That's why we have to round up the whole thing thing and getting from around to the congestion window it's not much more left from this model Now we know the number of the round. Okay, and in this round, the congestion window will definitely become B-0 times 2 to the power of K but if you remember this example again you, of PDP plus Q is 30 then I told you that the packets of this round will also go through the queue and they will also cause X and they will make the window grow more. These are exactly B packets, right? It's exactly the burst that doesn't cause loss So we get basically something like for the top of the top window at the end of slow style, something like B zero, exactly the burst that doesn't cause loss. So we get basically something like for the top congestion window at the end of slow start, something like B0 times 2 to the power of K plus B It gets a tiny bit more complex just because of this ingress capacity thing. We cannot always assume it 0.5 point five and also because we didn't uh we assumed no initial loss here right so we need to have that possible case also in the model but we can use the equation for k to test for that. So we can say with duration, being R and not, you know, not point five is K zero And if it's zero, then loss will happen in the first round so we're getting there we can now use i for the round of which the overshoot happens and define it like this. So I becomes relatively similar rounder dot block 2 k over"
  },
  {
    "startTime": "00:24:00",
    "text": "B0 plus 1. If if it's not you know if lost didn't happen in the very first round and if uh well if r is really 0.5 because we're in slow start And it's K in you know, in case R is bigger which means that the ingress capacity is limit us and zero otherwise And using I, we can define RI again to cater for that case of the English capacities That's a bit annoying, you know, makes the whole thing slightly more complicated but then we can write zero and max as a burst or the type of burst loss, is B0 times 2 to the power of I plus this equation for B here. Because simply it a little bit. It looks more complicated, but it's simpler actually because the previous equation depends on i and i depends on k so it's like a couple of equations in sequence whereas here we have a few pretty simple cases right? You have the case where loss happens in the first round, where it's the speed and i depends on k so it's like a couple of equations in sequence whereas here we have a few pretty simple cases right you have the case where loss happens in the first round where it's just b0 plus q minus i over one minus r and then you have the other case where uh few pretty simple cases, right? You have the case where loss happens in the first round, where it's just B0 plus Q minus I over 1 minus r. And then you have the other case where you are able to double and yeah, it's not the first round and then there is the third case which again depends on i it's not the first round. And then there is the third case, which again depends on I, which depends on K, but that is a very special case. That's when it's not in the first round but you are limited by the ingress capacity capacities so uh when loss is in round zero that's one thing to take away, and it depends only on the initial window, the Q length and the ratio and any later loss depends on really only on the initial window and the Q length not on the link capacity or anything We get to a really final one in Q-Link saturation loss. The maximum window in saturation is very easy that's just when you have everything full and then it takes you another full round-trip time to get all the X, to finally get the act that tells you about it and in the meantime the window has grown by another BDP"
  },
  {
    "startTime": "00:26:00",
    "text": "plus Q so it's two times in bracket BDP plus Q. And the maximum window that you have is the minimum of the saturation window and the burst window and as a thrash is better times that Now this is one final equation if we forget about this nonsense of you know the window, and as a rush is better times that. Now, this is one final equation. If we forget about this nonsense of, you know, it could happen in the first round, the ingress capacity might limit us all these corner cases to basic things you know, is just this equation at the bottom, which is better times the minimum of two times pdpdb plus q and then this thing here with some log Okay, a lot of theory Let's get real i come to diagrams and in these diagrams the q length was varied from zero to two BDPs, that's for all pictures that you'll see. As I started for every line of maths that I did, because I'm not good matt mathis, you know, I doubled and triple checked with an that you'll see. As I started, for every line of maths that I did, because I'm not good matt mathis, I doubled and triple checked with NS2 because I thought this gives me a very basic, very fonding slow stats, no tricks, nothing And you can see that this is spot on, right? The model will be the minimum of the two lines, but I'm sure showing both lines all the time so you understand if you do reach saturation or not. Now, I tested in Linux, and it's also spot on which made me very happy This is Linux without the latex. You see that the web marks in Linux, and it's also spot on, which made me very happy. This is Linux without the latex. You see that, you know, the web marks, this is where the construction also spot on which made me very happy this is Linux without the latex you see that you know the web marks this is where the world as a fresh and web marks, this is where the culture of the world as a fresh. And you see that it's always on the line, so it completely works And what is interesting is what we can see is as soon as as the thresh does reach that saturation line we get double losses so the blue cross below, they are the minimum as a thrash taking over a slightly longer time period, you know, a second or something So you see that as a thrash once goes down deeper and then TCP behaves very normal And this is really inevitable with point"
  },
  {
    "startTime": "00:28:00",
    "text": "or anything higher than 0.5 in that right range. Then there are the losses on the left side The double losses, these are unnecessary. These are cases where recovery could have done it better, you know, with some more fixing If Marco Coyo and Neil and all these people have more patients Then there's a picture with the LADEX I did, I think this could be incorporated in the model. You see that this is roughly the same thing right? It's a bit more jittery. There are a few cases where the window goes even larger. And I put vertical lines to show that the cases where it goes even above this stress, I mean, sorry, the saturation line, it actually has triple losses every time. I don't honestly know why, but the reason I did not incorporate that in the model is because delayed action slow start based on heuristics, because on the receiver, you know it's trying to guess if the sender is in slow start and I did not want to put that in the model And yeah, and with pacing, this now interesting, with pacing, it gets to saturation when the key put that in the model. Yeah, and with pacing, it is now interesting, with pacing, it gets to saturation when the queue is a bit smaller. That's a good thing, right? We want that. We are able to work with a smaller queue because we insert gaps between the packets. And so we can ramp up higher and get to such saturation earlier. But as a result, this guarantees a double loss with an even smaller queue. Using better is great equal to 0.5 This is showing both together that's a larger capacity case with a larger BDP What you can see is the yellow line on the right in the pacing case is a little shifted to the left right? So there's a slightly lower Q length that you need to go to the saturation line and you know the reward is that you have a double loss earlier This is just a diagram to show you how this depends on parameters in case you're interested. I mean, this"
  },
  {
    "startTime": "00:30:00",
    "text": "nothing, nothing very special here, just changing some parameters look at how they look if you would have a different initial window or you would have a different backup factor So, you know, this could be used for all kinds of things I mean you can directly use that model in TCP to make him more intelligent decision without any heuristics, because it's always in including the Q length, which we don't know But we can learn some things from it. First of all, the better page becomes the earlier it will reach the saturation line and the more likely it is that better is greater or equal to 0.5 after slow start is actually bad We should have a smaller value. So maybe it might be an idea to use a smaller value when pacing in general There could be heuristics, right, to check which line we're on, you could try and guess the queue length or something looking at delay. Decide about better initial window from history that many things we could do here. We could adapt this for ECM could adapt it power for us right I'm a poor academic and if you think this is cool then please don't do that without me And I was asked, what yeah okay you want to go right away, yeah, sorry, I went on and on and on you had a question before already so It's really cool to see this time written down. Some of this I did on the back of the envelope. I don't want to admit how many years ago one of the things you might consider looking into is if you save with each segment the sea wind at the time this segment was sent. Yes. That's actually a better starting place for setting SS thrush Yes, I completely agree. That implicitly gets you a factor of four in slow start The other thing is comparing sea wind SS thrash and mulch changing the multiplier to factor four. I think I've seen that code shipped someplace, but I don't recall. It wasn't recently. Okay"
  },
  {
    "startTime": "00:32:00",
    "text": "Both of those, both of those helped the second one of course, is a heuristic. The first one is yeah but in my opinion not using the congestion window at the time the packet that caused the loss was sent is almost a bug in TCP. I mean, that would be the right behavior i would agree with you that especially since the newer TCPs keep more state about each segment sent. Yeah that's an obvious fix So it's just the last slide here Just a quick one about high star plus plus because I was asked to talk about that my problem is I'm not a fan of doing maths without validation, also because I'm not good enough at it, and I didn't have any open source code for High Star Plus Plus So just to give the idea, right, the presented logic applies to overshoot in general it's just when you overshoot only one packet in Reno, it doesn't really matter but it could apply to all kinds of things High start plus plus has this conservative slow start phase which is like slow start with a smaller exponential base So this probably affects R in my model and two in the equation of C C-wind is B-0 times 2 to the power of k right? We'll probably not be two or something else And probably there's no longer 0.5 in R1 and r i mean and also in the final equation right this all based on power of power of two so it's lock two that will probably become lock of something else but that's pretty much all it is i mean small changes to the model and you can calculate that That's it Neil Yeah, this is really interesting work. And I agree, this is really good area for working on improving TCP. I wanted to follow up on the suggestion made my mat about saving the sea wind at the time each path was sent and using that to help set CUS threshold. I think it's possible to even do better So what BBRV3 does"
  },
  {
    "startTime": "00:34:00",
    "text": "for example, is at the end, of the first round trip of fast recovery you can literally just count how many packets were delivered And that as far as I can tell, basically allows you to measure exactly the BDP plus Q that was available for this flow. It could tell you how many packets, lots in the wire were available, and how many were available in the queue for this flow. And then you can use that and you can make sure that your sewind is not about that so that you don't have a double loss as far as I can tell So I'd be interested if you could be you can use that, and you can make sure that your sea wind is not above that so that you don't have a double loss, as far as I can tell. So I'd be interested if you could maybe do your nice experiments with BBRV3 as well Okay, yeah Did you want to? Well, I was hesitating. Yeah, there's an algorithm that I published here several years ago called Relentless. It was about 10 years ago. Called what? Relentless TCP. Oh, yeah, yeah. And the algorithm that Neil is proposing is similar to that. The basic issue is, is you can decrease your estimate of the pipe size by the number of losses you experience And that's very accurate in certain cases The way Relentless was done, it was too aggressive in the increased function So it was pushed everybody else off the network But it was fun to play with But I wanted to point out that there's an intrinsic trade-off between three things here. One is how fast you accelerate, the other is the intrinsic overshoot from the fact that the control signal is delayed And whatever set point you pick is a control point the earliest you can slow down is going to be one round trip later. So we go one round trip faster and the third thing is trying to estimate the link bandwidth from dispersion type method And you can, there are a bunch of algorithms now out there that are essentially dispersion metrics They measure how much the burstiness of traditional"
  },
  {
    "startTime": "00:36:00",
    "text": "slow circuit spread out the the problem is, is dispersion-type metrics get confused by things like Wi-Fi, which reclocks the air Yeah. And so the conclusion is, is that on a network where the ax are being clocked, reclocked, you've got this fundamental trade-off between how much you're going to overshould and how quickly you're going to come up to the right rate which fundamentally affects the performance of trend transaction-oriented traffic, web browsing and conflicts with things like L4S and such like that where you're going to minimize queuing Is Neil still in the queue or is that? Might be an old hand No, okay. Sorry about that You're fine. All right You're happy with that? I'm happy share that with the community Well, of course. Thank you for bringing it. And then we have one more present from you. Okay, that's a much shorter thing It's an informational draft that's meant to document PACE pacing, how people implement it and, you know, some, I don't want to say pros and cons, but cons consequences of it So the idea is to give guidance to implement us via know, some, I don't want to say pros and cons, but consequences of it. So the idea is to give guidance to implement us while discussing the general considerations and consequences what it means to pace, and give an over of how implementations do it Regarding implementations, up to now, we have Linux TCP and two implementations of quickr BBR being described Linux TCP based on this document is at this URL General considerations and consequences for now I mean, there can be more things. A couple of, you know, Matt has told me other things in the break, but for now, one thing to say is that it is more likely to saturate a bottleneck"
  },
  {
    "startTime": "00:38:00",
    "text": "And we get back to the model of the previous presentation, which is where I asked to have that before It's more likely that we have lost from saturation than burst loss and it may be a reasonable thing to do to use a smaller, better value upon slow start overshoot Good RTT estimate are important that's problematic in the beginning we hint that RFC 9040, which could be used for initializing when that is possible, at least better than having nothing and just beginning with the constant Here's an implementation example There's a very high-level description of Linux TCP Essentially, what it does is to calculate a pacing rate when an ACA arrives as a factor times MSS times congestion low, over SRT. The fact, there's a configure value. By default, this is two in slow start and 1.2 in congestion avoidance. Packets are sent at that rate and this is limited to a 1 million granularity to be able to send more packets down to TSO so basically every millisecond, one millisecond work of data at the calculated rate is sent and then there are many exceptions to this the first one is that the first 10 packets are never paced, not the initial window, but the first 10 hard-coded, which I found a bit weird, but how it is if you have your 20 packets initial window then the first 10 are not going to be past, the next 10 will be More than one millisecond worth of data can be sent when the peer is very close when the min RTT is smaller than three milliseconds and then do there is a minimum and maximum burst size where instead the gap between packets changes. Maybe should I take new? close, when the Min RTT is smaller than 3 milliseconds, and then there is a minimum and maximum burst size where instead the gap between packets changes. Maybe, should I take Neal's comment or question already? Yeah, Neil is this maximum burst size where instead the gap between packets changes. Maybe should I take Neal's comment or question, sorry. Yeah, Neil, is there's a comment for right now? Oh, I was just going to say that the way I would express it is that um there's not a limit of pacing for there's not a limit of a granularity one move"
  },
  {
    "startTime": "00:40:00",
    "text": "millisecond for the pacing. It's just that the default TSO or GSI burst size is one milliseconds worth of packets The pacing interval between the bursts can be anything in this sort of microsecond granularity. Just thought I would mention that I'm not sure I called that microsecond credibility Yeah, so the, it's not that the pacing delay is always one millisecond it's that the target bursts size is pacing rate times one million So by default, so that usually for most sort of rates between, I think, it's roughly 24 megabits and 500 megabits you usually you do de facto end up with bursts spaced one millisecond apart But as you were saying, at data center speeds and RTT is off Usually you do de facto end up with bursts spaced one millisecond apart. But as you were saying, at data center speeds and RTT is often, the pacing delay is much shorter, yeah I mean, the code is as you say, but it amounts to essentially sending a million seconds worth of data every millisecond. That's what happens as a result of this calculation. Well, that's not true at higher speeds, right? That's only true up to about 500 yeah when you're going to more than 64 kilobyte that's what i mean below so there are the pause changes when the birth size is smaller than two packets or right. I'm just saying it's not a rare case because that 500 12 hundred twelve rates above five hundred twelve megavits are pretty common these days Yeah okay. Quick BBR this typically in user space there are more challenges here regarding timing, coupling with the underlying stack and hardware For instance, it may wake up too late and a highly loaded system. There are two basic approaches that we describe here, the tokenless approach that is in the MVFST implementation So these are two open source implementations"
  },
  {
    "startTime": "00:42:00",
    "text": "that we looked at or west looked at computers at a regular time interval and batch size, oh sorry, computes a regular time interval and a batch size in number of packets to be released every inch interval to achieve the basin, the pacing rate and then as the token based approach which usually uses a leaky bucket and then there are extra tricks I would say, play to the leaky bucket, like a burst token that allows back-to-back packets after periods of quies sense or lumpy tokens Things like that already it there's an overview of that draft. Thank you, Michael. Did you have any specific plans with this draft? Do you want to add more implementations or are you looking to publish? it in any way? I would like that to be published as an informational document from ICRG, but we authors also to treat it as a slow thing. So I would like to get more implementations documented So you're looking for contributions from other people? documenting implementations. That is great to know Okay, Matt. Great work I'm glad to see this stuff written down I want to give a warning on something that you touched on, which is actually very important. I've come to realize that TSO is probably extremely important in terms of having a side effects in the fairness space. In particular, if you pace without TSA, and you run up against Q-full, you expect your packets to arrive just as a whole was created in the Q by some previous packets draining. So there's a phase effect type of effect which guarantees that you lock out other flows a very high probability of locking out other flows TSO causes the queue occupancy to jitter by the TSA size, which means that cross traffic can get in more easily. And I've become concerned that some of the simulations that don't do TSO, apparently"
  },
  {
    "startTime": "00:44:00",
    "text": "none of the simulators do TSO today, that simulation on about TSO show artificially high on unfairness. And this is a you know sort of a broad concern of the okay community. It's not specific to your talk, so we bring it up here but it does seem to be a venue to make some noise about this. I see. Thank you Okay, we have guard in the queue queue Sorry. Sorry. I'll go before you. jonathan lennox. jonathan lennox Another thing that might be interesting to look at as it's available open source, though not documented very well or at all, is the pacer in Google's WebRTC library, which is interesting because that's target a low delay bandwidth algorithm. So if you ever get congestion loss, you've done something wrong or something's gone wrong so but but there's algorithm. So if you ever get congestion loss, you've done something wrong or something's gone wrong. So but they do have a pacer in there too for the output of that and that's targeting. It's an algorithm that outputs, here's the bandwidth you should target so it makes pacing conceptually easier but it's still doing various complicated things so it might be worth okay thank you very fast it's still doing various complicated things. So it might be worth. Okay, thank you. Go ahead first. Hi, Michael. Yeah, we'll read that. That sounds fun. Obviously, we're doing page with the careful resume stuff So we can say in pico quick, there's like a show scheduler doing this at the top In the cloud flag cache, the there's the notion of kind of choosing the rate and then letting tc do all the pacing underneath which is super cool as well well We'll comment on the draft. I don't have any questions on it um yeah word of warning if you're doing the clarifflare approach you as well. We'll comment on the draft. I don't have any questions on it. Word of warning if you're doing the Clarifler approach, and you subsume your pacing to TC underneath, you do actually need to do the pacing underneath. I've seen people do simulations as involved where we run the thing. It thinks it's pacing and it's not"
  },
  {
    "startTime": "00:46:01",
    "text": "actually pacing. So pacing's a little bit tricky to check what actually happens on the wire because just because you tell the operating system to do something doesn't mean that it actually happens And you say you give it to TC in this case it's tc okay using fair queuing i suppose you need to configure tc to do that pacing consistent with what the application is to doing and then it all works beautifully but there are a little pIETFalls if you doing these measurements on real boxes to know that I actually the pacing you think is happening actually happens on the wire I mean, I guess offload also has that same application Yeah. So I looked at the native implementation of pacing in tcp and as far as i understand the FQ thing, which is in TC, that will take the time stamps that are being calculated by TCP, but the TCP machinery and it hands it over to FQ and then FQ basically implements the same VAT behavior that's how that normally works I think. That's kind of what's happening in Cloudflictations I'll let Cloudflare people talk about it in more detail if they were Yeah, happy appears we've dragged the queue. Did anybody else have thoughts for right now? Okay, thank you, Michael So next up is Prague for Fores and I will pass Koon Slides Control All right, ready to go Hi, everybody I'm going to give a short introduction about um brock what has been done Maybe it's a repetition for some, but there are already some interesting topics discussed like based introduction about Prague, what has been done, maybe it's a repetition for some, but there are already some interesting topics discussed, like pacing, like getting faster up to speed but the gives let's say a little bit the overview of what we have today, and then also I want to"
  },
  {
    "startTime": "00:48:00",
    "text": "introduce what we call UDP Prague. It's, let's say, a Prague congestion control object that you can embed in your application and could be part of Quickstack or even even applications directly controlling the source so one of the big changes let's say or new things in Prague is that it's actually capable of not building a queue and that the network doesn't need to build a queue for it to control the rate. If you look at today's congestion controls, they all kind of assume that the queue will be built up and that they can base their rate control on that so why did we came? to the situation that Prague does build a queue? It's actually part because of a dual queue, how the dual queue is working So you can recognize that the rights, I guess, the dual queue and then how it works is that the let's say, the scheduler gives a low latency priority you could say towards the galfress the Prague packets And it needs to leave some gaps in between to allow then also classic traffic to get scheduled in between. It's kind of filling up the gap kind of scheduler for the classic flows but of course that means that if there isn't enough space between those gaps, that the queue of the classic will grow"
  },
  {
    "startTime": "00:50:00",
    "text": "and that with a coupling we introduce more markus amend the goal of those marks is to just leave more space in between. This is the only way let's say to to make Prague or the dual pie square work and avoids a little bit that you need, let's say, a certain weight at Rand Robin, of course it's it's better to have one it's just in case, but in theory, it's not necessarily, if Prague would always be able to pace small or to at least a low enough bandwidth So quickly some slides why classic needs a queue we all know I guess is to cover rate variations you want to keep the link full. Also, you need to create delay for delay-based congestion controls, otherwise they are not controlled, and that's also with loss Then also, typically, and especially for low rates, the lower you create this queue the more drop you need to apply So there is also a limitation with AQMs, how low you can keep those cues. Also, depending on these delay-based congestion controls, how well they respond to loss, you might want to keep the target of the AQM above the target of the delay-based congestion control So that's also, let's say, a limitation on how low you can make those cues ECN doesn't at necessary, at least the classic ACN didn't add a lot, because the cues still needed to be large enough for the loss base because you can mark more frequent but still it was sharing queue with loss loss-based congestion controls and again it still"
  },
  {
    "startTime": "00:52:00",
    "text": "need to be large enough for the delay can mark more frequently, but still it was sharing you with loss-based congestion controls. And again, it still needs to be large enough for the delay-based congestion controls the different let's say or a change again change you could say was was data center TCP, which decided let's get away from all the legacy and start all over We have our own private queue We don't need to take care about the rest We can have frequent marking. We're not mixed with drop. That's not a problem And we can get a very smooth truth throughput. That was one of the properties and of course a separate network means you don't need to cover the rate variations because data centers PCP doesn't have a lot of rate variations Everybody supports ECN no limitations on that loss rate and also no delay-based congestion control so also no need to create detectable latencies But still, data center TCP when trying to use it on the internet it needed some let's say, a queue in the order of the base round trip time if we just apply it um One of the reasons was TSO indeed so in a in a data center with very small round trip times you could have a threshold which is bigger than your round trip time. It was still very low Also, the TSO bursts of maximum 64 on 100 gigabit link is five miles microseconds. And if you have a threshold of 100 microseconds, or whatever, it's not a problem. On a 10, here, gigabit link, it's 50 microseconds. That's also pretty low. If you look at the internet and you would send the TSO burst of 64k packet on 100 megabit link, you had a burst of 5 milliseconds"
  },
  {
    "startTime": "00:54:00",
    "text": "Also, if you would kind of like data center TCP is often doing alternating the packets between flows and they kind of group together you would also get very long rounds of one flow running and then the other flow running Also if for instance you would burn out for a full round trip time, all your packets you meet immediately get, let's say, 100 milliseconds burst in your queue And you can see that it's the way around you the round trip time is much bigger than the threshold we want to have so one of the things was to have these adaptive TSO bursts and there was just a discussion about one millisecond we target 250 microseconds. Then we can have multiple ones fixing in under that queue and that would mean for a two gigabits link that you could get your full 64 kilobytes diesel burst so I think that's reasonable below that of course, the TSO bursts need to be trimmed down to not create a QBigan and 250 15 microseconds Another change that we did was this round-trip time unfairness So if you have a big difference in in round trip time the rate is dependent on one of the round trip time So you get a big difference between one a one millisecond round trip time, base round trip time flow and 100 milliseconds one. About if you have a queue of one millisecond, in between you have about 50 times rate different Same is happening for data center TCP so it's not very nice to have a very small queue if you look how it is solved"
  },
  {
    "startTime": "00:56:00",
    "text": "without Alphyrus, it's just add a queue If you add a queue, you balance the rest latencies and the rate ratio will become more acceptable. Of course, we don't want to create a queue in Alvarez We want to avoid having a queue. So that's why it had to be fixing that congestion control. So we made Prague let's say, rate compliant or with 25 milliseconds flow. That means that the well, the rate is then, of course, the rate ratio if you're having a flow lower or around trip time lower than one millisecond you behave as a 20 millisecond flow and the rate ratio is much more acceptable That doesn't mean, of course, that your application sees a 20 millisecond latency it still sees the one millisecond latency for the data it's sending. It's only response not so uh your application sees a 25 millisecond latency. It still sees the one millisecond latency for the data it's sending. It's only responding, not so, or is not getting 25 times more throughput as it would otherwise and that also has an effect on let's say the responsiveness and what I call the inertia It's not only let's say, getting the bandwidth or it's not competing as much to get this bandwidth. It also is let's say, as unreason you could say, to grab bandwidth as a 25 millisecond flow And it's at the end not so bad because if you look at how if it wasn't like that how a flow with a one millisecond latency which is very lightweight, you could say, has a little little mass because it can respond very quickly to the marks how it would compete with a flow which has the 25 million"
  },
  {
    "startTime": "00:58:00",
    "text": "let's say, inertia or responsiveness that flow with much slower response And you see in the in the left side that the lightweight flow the one millisecond get bullied by this heavy weight and actually always adapts and the other one hardly adapts and at the end you get these sausage type of flow throughput with i guess is not what you want for your application so indeed with alpharez the inertia or the responsiveness is as a 25 millisecond, so it's compatible up to 25 milliseconds, and it gets a smooth throughput although it won't get the optimal throughput So that's, let's say, a compromise you have to make you can't get it all And that's a choice we have to make at a certain point. Luckily, we have the two systems and that's why I'm proponent of having both classic and alpha is still there so that applications can be depending on the need of their data, use both in parallel Maybe also more recently was that we paste now also below the minimum window Pacing is the being discussed, that's very good because we also have some let's say, challenge or problems with pacing One of it is also this FQ disk pacing, which will pace, but it's not always clear when the FQQ disk is sending this packet because if you update the pacing rate, the pacing rate with the last packet is used for the packets that are going to be sent so just adapting the pacing rate"
  },
  {
    "startTime": "01:00:00",
    "text": "has some problems maybe it's better to use it packet is used for the packets that are going to be sent. So just adapting the pacing rate has some problems. Maybe it's better to use the timestamps, or it would be better if you use a pacing rate at the timestamps in the TCP we would be adapted because we don't see when the packets are effectively sent and then the especially for very low latency bottlenecks it can create big deviate from the major turnaround time time. So with this spacing, below the minimum window in this stage you see what if you had a minimum or the time so with this spacing below the minimum window in this table you see what if you had a minimum or if the minimum window was two and depending on how many flows you have, and what's the round trip time? what would be with marking a hundred percent what would be the lowest rate that you could achieve in purple here you see the first row for one flow on a 10 milliseconds. You cannot you cannot control a flow below 2.5 megabits per second that might be acceptable but if you have 10 flows, it's 24. If you have 100 flows, it's 240 megabit especially if then the latency becomes lower like one milliseconds, you're already going into gigabbit per second. And if you have really really, low Podnetworks you see for one flow you cannot control the rate below 240 megabits for one flow let alone for 10 it's in gigabits So either we would have needed to create drop for loss or building let alone for 10 it's in gigabits. So either we would have needed to create drop for loss or build a queue. The alternative is to again fix it in the congestion control and here we can independent from the round trip time, we made it going down to 100 kilobits per second per flow so 100 flows is 10 megabit and that's it independent from the round trip time the question you immediately want to ask"
  },
  {
    "startTime": "01:02:00",
    "text": "but yes matt mathis did you consider scheme where you allowed the pace that below one segment for roundtrop yes yeah yeah definitely so this is doing, so we keep the minimum win of two, and then we pace below whatever rate we need to pace. So we have a fractional window which goes in microbytes actually very low can go very low and then we use that to based on the ramp trip time calculate the pacing rate yeah because i would describe the window size then to be in microbyte. Yes I find this chart very confusing so this window is is it's just two uh so this is assuming you don't have any pacing at all, and you just would have a window of two packets so we didn't show here the um and you just would have a window of two packets so we didn't show here the the window size that we calculate which depends on the Rantrip time So OK okay so what we do is we calculate is fractional window which allows smaller basing step and then we round up the window to the next Inti integer, which works well if you don't have a queue if you have a queue inter inside then the window will become blocking again and the pacing rate not So then you will go in more steps if there is a queue but if there is no queue the pace will be very smooth so below one megabit per second, we also reduce the MTU size. It's a choice so we can we go down to I think, about 156 bytes per packet which means that we still can send two packets per 25 milliseconds so we get"
  },
  {
    "startTime": "01:04:00",
    "text": "good feedback from the network and we still can have video of 80 frames per second with this So that's also an advantage um so also with this hundred kilos per second we will send a packet at least every 12 and a half milliseconds so that also reduces the latency if you would have doubled or kept the minimum packet size of 1,500, it would be under 25 milliseconds per data that you can send at that rate rate So with all of this, especially around this, this responsiveness, we see that our with all of this, especially around this, this responsiveness, we see that there are still compromises and it's good to have these two types of traffic. Like I said, here you see for instance, a flow where or let's say the to have these two types of traffic. Like I said, here you see, for instance, a flow where, or let's say, some plots where we have spectrum utilization the blue ones are Alvarez the orange one are classic flows and the green ones is a flow which is an on-off flow. What you can see is that because there is a buffer for the classic ones, they immediately can use any opportunity to that is there they can fill in all the gaps, so to say, also if around a bit after six seconds all the orange flow stop except for the dark orange one, which is a single one left over he immediately can grab all the capacity because he has a buffer you see also that is his buffered drains as well, but he can fully keep the link utilized while the Prague flow they slowly, more slowly go up and one and a half second later there are more or less fair and when the other flows communicate and a half second later, they are more or less fair. And when the other flows come in again, okay, if the network gives them the time to reduce they also don't see any latency"
  },
  {
    "startTime": "01:06:00",
    "text": "So that's again important This is an opportunity so that the networks can further improve and have other strategies than schedulers to come networks can further improve and have other strategies than schedulers to control the rate and to make and also to keep the rate low for these applications. We should think not only trying to solve everything in a congestion control if you collaborate with the network, the network can also do a lot to improve if, of course, on the other hand, a congestion control doesn't need a queue, the network can develop strategies not to build queues as well So, I already said it's good to have two types of traffic, one which prefers to keep it both of them empty because you want low latency On the other hand, it's also good for the network to have buffers to fill in the gaps And even for physical layers, it's also important to know this is traffic that wants low latency We can treat them differently. It might be a bit more costly. We can compensate it with them getting it little bit less throughput but it's clear for the network that what are the intentions of the applications and then we don't need to compromise somewhere in the middle where we have both ends probably not happy it's better to to see those two extremes in parallel and that we can optimize and give them what they primarily wants. Of course, it doesn't mean that there needs to be a very big difference in throughput but there will be a some trade-offs to be made Then next topic is UDP Prague, so we just start made this during the hackathon this repository public, we tried to build, let's say, this congestion control"
  },
  {
    "startTime": "01:08:00",
    "text": "which is interwoven in TCP, which is difficult to create We wanted to have a very concise single source Prague congestion control object with minimal dependencies. You can compile it on any platform, at least we hope so. We want if you have feedback, if you have problems compiling it at least that object then let us know so it's let's say the prior congestion control that can be used in UDP-based applications and at once the really low they want to optimize their application and how they generate data So it allows also the evolution of price without impacting the API so that's that's at least our goal there is now a first version. We will further improve that These APIs, from that perspective reasonably stable but on the other hand, there is also, let's say, how applications will use it might still have impact on the API in how we evolve it. So we will add additional functions maybe change some functions maybe how round trip time is measured or how packets are identified can change and that we can add in this APIs. So we have an example for a sender and a receiver, but the object cannot also work bi-directional There is IPERF integration ongoing by BOP McMahon. He's working on IPRF 2. There is a link to the repository You can see is a link to the repository. You can, and then also, of course, it's not not yet finished we are we are still working on it like features"
  },
  {
    "startTime": "01:10:00",
    "text": "which are already in TCP Prague but not yet in in sorry my voice I have to do the present here from the hotel room because of the COVID but okay So I was saying yeah some some things still have to be added to it but if you use it, let us know Also, if you have suggestions or on the API or on the concession going has to be added to it but if you use it let us know also if you have suggestions or on the API or on the congestion control improvements improvements So, UDP Prague so this is an example, how you can use it in for instance, a quick stack It can be used for your congestion control. So if you have a quick stack and you want to have a, let's say, ready congestion control source, you can integrate it like before but we see it also that it's possible if you really want to improve your real-time app If I go back, you will see it that it's possible if you really want to improve your your real-time app if I go back you will see that there is always this flow control between a typical stack where you have to deliver enough packets to the TCP buffer, socket buffer and tune that in a correct way that there are not too many packets in there. It's an extra layer with doing the pacing and the stack in your application. You can direct bind this congestion control algorithm for instance here to video so you can directly steer the encoder of the next frame size you get this information what is the pacing rate, what you might have a send budget in which time you want to send that frame"
  },
  {
    "startTime": "01:12:00",
    "text": "And you can even hold packets before start them encoding because you know the window and then you can mix in also real can even hold packets before starting them encoding because you know the window and then you can mix in also real-time traffic that you have cloud interact sorry, gaming interactions could be added inside the stream you have full control without it being blocked or have to wait behind a queue which is in the TCP socket or in your quick socket Of course, it's also an opportunity to open quick stack especially or even TCP stacks to give more information to the applications that's an another way to allow the interaction that your applications directly steer the generation of the data and not get blocked by a stack in the middle If you really want to make it very fancy, you can do multiple instances of this UDP Prague object even, and then have paces and different paces, maybe schedulers, you can divide the budget over the different flows but that's up to those who want to really make their application really low latency and unblocking That's it more or less from my side. I hope if you have any questions, we still can discuss a bit Great, thank you. Cool That's lovely update and Matt has a question or comment First of all, I want to thank you for this deck. This deck would be a huge help. I was away from congestion control for almost 10 years, and I have yet gotten my head back around L4S And so this will really help that There's one detail I want to ask a question about this, 250"
  },
  {
    "startTime": "01:14:00",
    "text": "microsecond recommended pacing size when was that conversation and who was involved in it and how heated was it? Yeah, it has been, let's say it's eristically based on experience and a little bit also based on the threshold that we had for one millisecond and then we got quite some pushback let's say also well there has been some on and off at the end I think it's okay to have 500 microseconds up to 500 microseconds but preferably, let's say, around 250 A good point is also, I think you made it also, that with different bursts sizes, some networks that to mark might give preference either to the one who gives the biggest TSO bursts or the one with the smallest TSO bursts so I think it's even important to try to get to a certain common reference and not deviate too much from that And these are all based on comparisons between different implementations and so on The reason I mention this is if you imagine Google going from one millisecond tSO to 250 microsecond TSO, although I'm no longer privy to any of the numbers, it would be many megawatts of power consumption and possibly a large fraction by gigawatt and so it's and the problem is that the burst size that's implemented in Linux ends up being imposed on everybody else on the internet Yeah, it's also a bit, let's say, a concern of us seeing now that in the latest releases, it's one millisecond, I think Chiyu trying to upstream had to do some patches to make it configurable again from within the congestion the congestion control so but"
  },
  {
    "startTime": "01:16:00",
    "text": "yeah maybe to be further this discussed of course if we make it one minute second in means that we can't have a one millisecond threshold if it is rate based there might be more opportunity to if the network is not blocking, let's say, and doesn't have a cue thresholds it might have less impact on the data to see if it's a bit bigger but open for discussion, right? Definitely now we have TNG Tianji. I have some situation regarding the L4S you described here Because for the LFAS is in marking and congestion path happening on the theme node. Have you air one? evaluated? The congestion happens on some node, a one or two hundred later, where the marking has to be done in the current one. So basically, this will be a mini or micro feedback process from the real concrete past to the ECN mark the current one. So basically, there will be a mini micro feedback process from the real congested path to the ECN marketing node Actually, there's a real scenario, so yeah. Yeah, yeah So, of course, if you have a bottleneck and you determine certain mark probability or marks you need to apply them as soon as possible if you have, and maybe you refer referring to 3GPP where the run is doing the marking or gives the marking probability and preferably the CU is the first hope that you do is doing the marking or gives the marking probability and preferably the CU is the first hope that should do the marking if it really it's possible to go really to the UPO but that's really, let's say, crazy It's probably difficult to get that working correctly especially if the latency between your run and the UPF is failed"
  },
  {
    "startTime": "01:18:00",
    "text": "potentially large in distance at least So it's really not recommended The recommendation is at least to do it within five to maximum, I think, 10 or 25 milliseconds That's might already start making your system unstable it means that the system unstable. It means the 10 to 25 million, that is doable Actually, you know, the scenario just described here is between the REN network and the core network here. It's 10 to 25 million definitely doable. So seems your recommendation is okay, even if we have this type of situation. Yeah but it's probably not always and then it becomes problematic more problematic probably but it's recommended anyway to do the updates in the ceo and not in the core in the UPF. It's kind of, it was a kind of a compromise I was not happy with, but nobody's happy in this But, you know, however standardization works Sometimes you have to compromise, but it's up to the way but you know how standardization works sometimes you have to compromise but it's up to the ones who want to try to do this to make it work, I would say oh yeah by the way if you have both the alpha-s flow and the non-alpha as well on the node, on the marking node, while in the same situation as I described here, like the feedback process here so will this one delayed feedback a delayed marking will affect the non-alphets? or the other way around? Normally, alpha-res and non-alpha is separated in 3GPP but it is possible to mix it if you do it right you can have it in the same barrier and use the same"
  },
  {
    "startTime": "01:20:00",
    "text": "feedback, let's say, to both draw and mark if you do it in the right way Okay, okay, sure. But you have to make the right coupling between them, let's say, and to do the right smoothing for classic and immediate marking for Alpharez Thank you It's lovely else have? any questions or comments about Prague? for now? If anyone is interested in trying this in their applications or in their quick stack please let us know, we're happy to collaborate and exchange ideas and improvements Thank you yes please All right, so that is Prague. Thank you again, Kuhn for presenting. And now we have one more presentation from Matt which we didn't get to in CC CCWG but we do have quite a bit of congestion controlled expertise in this room Did you want to? so take it away, Matt. Okay, next I guess I can do this. So I want to start out by saying I was planning on giving this in CCWG, and I was quite prepared at that point. This time we'll be winging it a little bit more than I would have been. I came a across an issue like this Do it this way. Is this better? Good. Thank you you take a step back here I'm talking about"
  },
  {
    "startTime": "01:22:00",
    "text": "situations where stalls interact badly with congestion control Everybody in the room here should know about various application limited conditions and stalls. I'm not going to go through details. Actually, the tiny detail I want to remind people of is for non-pay stacks recovery from a stall, or after a stall, you almost always have a line rate burst. And that's pretty much for everything so here's a protocol diagram of classic flask retransmit and recovery this is actually looks pretty much like Van Jacobson's original renal I've left out additive increase because it makes it complicated and in that code, classic Reno, congestion code in the C window is set to half of SS rush is set to half of the congestion window which happens to be the same as the flight size, so both classic and 25 Reno look the same Here is a similar diagram for more modern lost recovery this includes limited transmit and PR It looks a lot prettier because it This includes limited transmit and PRR. It looks a lot prettier because the data ends up spread out. It's more complicated to under a lot prettier because the data ends up spread out. It's more complicated to understand, so I'm not going to talk about it. I'm going to use a simpler model because the outcome is the same. So the problem that I was aware of actually a long time ago was that if you have a stall that happens actually after the last you end up setting SS thrash from the flight system that was reduced because of the stall not because of anything that you were actually measuring prior to the stall, prior to the loss. And that is, the outcome you want to have here is to set SS threshold from the sea wind before the loss, or the sea wind at the point that the loss happened. And the stall just caught doesn't cause anything in fact the"
  },
  {
    "startTime": "01:24:00",
    "text": "problem I was wrestling with was the right thing to do in this case is nothing because the congestion control would put you at a C-win which is above the flight size So I observed this problem actually I was working on this sort of in the wake of doing some of the other, of doing the work on 2018 the Selective Acknowledgement draft, and I was actually working on a production problem which was transferring large volumes of data from Pittsburgh to San Diego. And you got a nominally poor performance because there happens to be kind of a resonance between track size and the disks and the window side and the pipe size and the network and there were pauses that were happening that were aligned with the loss and it was abysmally bad performance and if you took a loss that AIMD cycle at that scale was more than 90 seconds So it caused a amazingly bad performance And now, especially because of streaming, applications, it's essentially most TCPs, many, many TCPs run for long, long period of time with the flight size below sea wind So, what's the history here? There was an editorial change going from 0-1 from 0-0-0-0-1 in, I forgot what it was, 2001 BIS, RFC 2001 BIS, I think, was the earlier document I actually had, I didn't put this on the slide, but Vern Paxton and I had a big fight over this this And I lost and so that's where we are today, that this, this text is put in lots of places. However, However, justification, see when my rise above flight size, is"
  },
  {
    "startTime": "01:26:00",
    "text": "text is put in lots of places. However, justification, see when my rise above flight size, isn't relevant for many of the widely deployed stacks today I actually got, I tried to poll people on what their stacks did and I got authoritative answers that disagreed for at least one of the stacks. So I decided not to place any blame in case something was wrong But the justice in 5681 is moot and we're still carrying it forward is a restriction. I don't know exactly which stacks do what and it's likely that there's some at least one vendor who might be shipping two different versions that do different things I want to point out some of the downsides of this status the current status quo first of all in different things. I want to point out some of the downsides of the status, the current status quo. First of all, and today it almost always causes bad performance for streaming applications because flight size has always below sea wind And it causes very erratic performance for bulk transfer on systems that are not absolutely smoothly scheduled. So if you have doing a bulk transfer on a time sharing system because of those CPU slicing and the time sharing, you're likely to actually have bursty transactions, and that causes this very erratic process smoothly scheduled. So if you have doing a bulk transfer on a time sharing system because of those CPU slicing and the time sharing, you're likely to actually have bursty transactions and that causes this very erratic performance. There's also, a huge mind overhead for people like Neil cardwell who are actually testing two to different versions of protocols. And so for instance, PR a lot of the PRR testing has actually been done the production Google PRR testing is being done with one set of code, but some of the testing for the standard is doing with a different set of code. It doesn't have as large a volume. It's also the case that there are a lot of people here who think they understand how the code works in the field, but in fact the code is different because of this restriction that sea wind doesn't grow when it's not there and the last point is this is a"
  },
  {
    "startTime": "01:28:00",
    "text": "big source of a lot of people my own and included sort of mistrust in the IETF process So short to do list Questions and comments Michael Michael Weitzel be, I think, as a thrash at the time when pack, when packet was sent that caused the loss Sorry, SSWesh, flight size, I mean mean But I don't agree that the statement that flight size, well, congestion window, could grow well beyond flight size, I mean, that's not wrong, right? So if I, in principle, always send 20 packets per round trip time, then I can let it grow for forever. I can go to thousands apparently there are stacks that do that the n s time, then I can let it grow forever. I can go to thousands. Apparently there are stacks that do that, that the NS code does that, and that was how people noticed is that it's sort of outside of the scope of the standards that you don't grow sea wind when you're not using sea wind The problem is there's actually a complication here because it's actually not if you're you if flight size equals sea wind at the point where you see even act it's a flight size equals sea wind at some recent point because that means you've tested it If you have an act for which flight size is equal to sea wind recently, then every act is safe to raise sea wind. And it's not a just a trivial oh yeah there's just a one sentence that's needed it's actually needs to be an algorithm. And it's actually related to both sea wind validation and the"
  },
  {
    "startTime": "01:30:00",
    "text": "all of these are related and they have to do with how assumptions you make if you're not at this instant using sea wind how recently did you have to actually use it and how do you decay that information? Yeah yes Thank you fairhurst 7661 and the rate limited increase are different pieces of the same puzzle, but they're not the same thing Right. And 7661 doesn't do congestion window validation like it yet to be. It kind of says when something goes wrong, do something smart, rather than just kind of always keep that congestion window bounded Right. So since I've been away from congestion control for nearly 10 years there was a bunch of stuff that happened that it wasn't a party to, and I don't trust my understanding of the documents or the history to be quite as blunt as I would like to have been Believe it or not, this actually held back on what I thought I could what I thought I could say, and then I realized I didn't actually trust my, my understanding as the newer documents Okay, and you probably should look at quick as well when you do we talk about it's seven, six, it's one. Correct, correct Yeah, I really like the notion of capturing Max's flight side and because that tells you what you've actually measured recently. You actually observe max flight size and you actually got it successfully delivered And so you know the pipe at that instant was at least that big So, in CCWG"
  },
  {
    "startTime": "01:32:00",
    "text": "I might have wanted to propose a work item that's not suitable for this group Anybody have anything else? Pardon? You can tell us what that one item would have been. Well, sort of the stuff on the slide with more detail. I mean, we need to understand the diversity of what's in the field. We need to understand the implementation of the tradeoffs because the people who have implemented it have done, since it's outside of the standards, the people have implemented it have done different things, and there's actually advantages and disadvantages and we have field data because it's been fielded without a standard it would be useful if the IETF documents incorporated what is already known known NIA? Yeah, I just want to enthusiastically second all of Matt's statements here in the thrust of the argument here I think this is a huge issue We've got this huge discrepancy at the core of congestion control when we look at the difference between the RFCs about TCP and then what actual TCP implementation actually do and if you actually implement the standard, you're actually going to get terrible performance on all of today's most important TCP applications where it's whether it's a webbush browser or streaming, uh, journaling web page or it's streaming video or it's RPCs or what a web browser or streaming downloading web page or it's streaming video or it's RPCs or whatever. And so I think we, as a community, we need to invent in time and fix this discrepancy Also, I wanted to mention, I'm not sure if it was clear from the discussion here that matthew quick congestion control, RFC, already, I would say, fix this They say that SS Thresh should be congestion window times"
  },
  {
    "startTime": "01:34:00",
    "text": "a multiplicative decrease factor. And so it my opinion, it's a little absurd that we have this massive discrepancy between the way TCP handles congestion and quick handles congestion and I think we should resolve it in favor of the way, you know has documented it and the way that all the major TCP stacks, to my knowledge, have implemented it, which is that us as thrushes we should resolve it in favor of the way, you know, Quick has documented it and the way that all the major TCP stacks, to my knowledge, have implemented it, which is that us as thrush is not only is way that all the major TCP stacks, to my knowledge, have implemented it, which is that Assess Thresh is not based on the current flight size, but it's sea wind. Maybe we can do something better, but we should define do something better than what we have in the DC standards. So thank you, Matt, for bringing this up Thank you sweet, the more I think about it, the more I wonder why would it not work to really take a flight size at the time? the packet was sent that caused the packet loss because it can keep track of packet numbers, right, the sequence numbers, you know from the dupeg which packet that was I think it would be the right thing to do correct But fundamentally it's going to be the act reporting that segment at some point in the past yeah and and so there's a quite there's this question about so you need to keep a history the data is always a little bit stale, how stale is okay Yeah I agree completely So that's all I have Yeah, Neil, did you want to add something? Yeah, we just have that so the BBRV2 and V3 patch series do exactly that they snap shot the pipe estimate at the time of Pack transmission. And so that's available when you're processing the act So hopefully that's coming to Mainline Linux soon. I would add though that I think my gut is the"
  },
  {
    "startTime": "01:36:00",
    "text": "is the calculating your ass is thrash based on the flight size of the time of loss is still a little bit too pessimistic. But I agree it's a it's a it's kind of an open question that doesn't have an obvious answer I just feel like if you if you're if you're mostly doing fine with the sea wind of one minute and not hitting any loss and then you happen to see two packets and one of those is lost it would be a tragedy to set your sea winds to one just because you happen to send two packets and one of those was lost I think it's, so I think there's an interesting research problem here I think We have gory again This is gory again. I think this is something interesting, but the application limited sense where sometimes you send a lot of packages and sometimes you don't send quite some much is an interesting kind of one to think about and it's not so weird as people think it is because when we actually looked at real sites when we do doing the new CWV thing, well, the stack kind of pause before they send, or it takes a bit of time to get stuff from disk before they do. So flight size actually goes, well up and down. So some of these numbers are not kind of sampleable at one point You have to have some average and that's why new CW has this notion which may be horrible of buckets where you put things in and you work out what happened over a period of time so and new cwv was a experimental at the time because people thought well at least bucket things i don't know whether that's right or wrong Maybe a few years have passed since 7661 Maybe we actually met with some wisdom now Yeah, that needs to be explored. I had no never seen that proposal before, I mean, because I was away from congestion control, and I found it very interesting They're actually two very common pathological cases Any sort of web browsing where you reuse a connection you get lumpy data and all sorts of different sizes"
  },
  {
    "startTime": "01:38:00",
    "text": "chunks. And then the other thing is, is, um, video where you get periodic chunks that are likely to be similar sizes And what's fun is if you happen to be on a path where there's a relationship between the chunk size and the pipe size, you get this resonance effect that observed in the late late 80s, I guess. I think it's certainly also late 90s, yeah. Sorry it's certainly also possible to see this for no particular reason whatsoever just because you have something providing data up here and a kernel or another entity providing the transport protocol they're trying to interface the network I mean the they don't just behave like a state machine that just clocks around that flight size just bump up and down for some implementations in quite a dramatic way if you actually look at it by logging every time it changes so um it's fun and we and we should know more and we probably do know more because as I said a lot of time has passed since 7661 has happened the way in which we design protocols has even changed since 7 7661 happened. It might be interesting to our actually look at producing a standards track version of 761. Now we know more, but um data first, please. That'll be a wonderful tab data data. Yes, data first please. But after the amazing amount of work it's taken to get PRR from experimentalist and standards track, there's a certain intimidation factor there Anybody else? I think I'll sit down Well, thank you, Matt. It looks like we've got a bit of interest in this topic So I encourage everybody to follow up with Matt and see where this topic can go go All right"
  },
  {
    "startTime": "01:40:00",
    "text": "so this was our last presentation for this slot So unless there's any close thoughts, I think everybody for your attention and participation in this wonderful session and I'm looking forward to seeing people in Dublin your job Thank you Thank you. PURGING TO WOMAYOR Yes So we're going to go We're ready Thank you"
  }
]
