[
  {
    "startTime": "00:01:00",
    "text": "okay everybody please sit down I would like to start very quickly because we have only an hour today and we actually got a couple of nice talks and we have to make sure we keep ourself and time welcome to the MapReduce session I\u0027m a equipment and unfortunately my co-chair Dave is not here today but hopefully he will be back next time we have an R IRT F session here but we also have a note well which is very similar to the IETF note well regarding property rights so please if you\u0027re not familiar with that make yourself with it but I would also like to point out the code of code of conduct and any kind of privacy related policies that we have in the IETF which of course also apply to this meeting and which is especially important when discussion get heated right everybody should maybe think about a second to use the right words to be friendly to each other which I don\u0027t think it\u0027s a problem usually have in this session but it\u0027s still good to remind yourself about it and then also another reminder that this is actually an RTF session here and not an IETF session so while it\u0027s nicely integrated and we get like a nice of exchange of people and I hope everybody is enjoying this from like in research and an engineering point of view the IRT working groups usually don\u0027t do standards right so that\u0027s important different in member G we mainly focus on I\u0027m getting researchers in here to present the measurement results and I think this is very well received in the IETF as well and but like be aware that this is not the same okay that\u0027s the usual slides you will find inside stick if you need any of the pointers here and this is like we already have the last time so Dave and I would like to add another chair because as you can see if it\u0027s not here so getting a little more coverage would be nice but it\u0027s also a matter of outreach right so Dave and I do a lot of things where we try to contact people in the research community to get them here and present their work so having more help with that will like extend our outreach and maybe also get different angles in so ideally we are looking for somebody who is like well-established in academia and community and and has contacts there and is interested in engaging with people and bringing their work to the IETF or the IR gf we put up this call last time already and we got a couple of responses and like we from our side we\u0027re actually not very active about it so we didn\u0027t we didn\u0027t make any decision yet so you can still send us an email if you\u0027re "
  },
  {
    "startTime": "00:04:01",
    "text": "interested in to help out and that\u0027s the point where we already get to our agenda and um if you have noticed or not there was like a last-minute agenda update that I did this morning so in the last couple of days I actually reached out to a couple of people and just asking them for deployment number four quick because that\u0027s a question I get very often you know how much quick is out there at the moment and I thought it this is a nice group to actually you know collect these numbers and present them to you so I have like four slides from four different companies giving me different kind of numbers and I will present them to you and and you know if you think there\u0027s something useful then maybe that\u0027s also something that like we as the chairs could could do more often in this group not only for quick but maybe also for other protocols just to get like an up-to-date number [Music] afterwards we have a presentation by a Nicholas about loss in sacrum networks and then we have three presentations from of papers which have been presented just like two weeks ago I believe at the IMC conference all really nice and good and exciting papers one author is he in person which is really great and the other two others will be joining us remotely okay so because we have four talks in only one hour time I will be very quickly here and so that\u0027s what I got in the last couple of days I got a number from YouTube which is kind of the number that we all might be aware of to some extent so quick had like within YouTube about 30 33 percent of the traffic was in December 2017 was a quick already which was published in a sukham paper and also presented in memory I think and the updated number is that there now about 50 percent of the u.s. traffic volume and this also aligns was about being 50% of the playout requests I got this information from Ian who I don\u0027t see at the moment but like if you want more information you can probably talk to him and then I talked to Facebook and they are rolling out quick on there between their own server and for their major applications so for their major application it\u0027s actually 80 to 90 percent of the traffic is using quick and that\u0027s quick ITF quick version 23 and the other 10 to 20 percent it\u0027s actually some some of them is fallback to TCP of course that some of them might be also connected to their role all strategies so like sometimes they don\u0027t use they decide to not use quick right but like that you know is basically "
  },
  {
    "startTime": "00:07:02",
    "text": "their role or plan then I wanted some Marco\u0027s from Deutsche Telekom who\u0027s actually here and forget also the operators few so like what\u0027s on if you look passively on your network what\u0027s the number of quick you see so that\u0027s the number for any traffic that is VDP on port four four three which is assumed to be quick and what they see is about 10% of the in the fixed network here 10% of the downlink traffic is quick and about 4% of the uplink traffic in terms of traffic volume is quick and so he also presented a comparable number last year in T sweet Abbott you and you can see the number here and actually the the percentage went a little bit down so it used to be something eleven point something percent and now it\u0027s around ten which is a little bit surprising but you can also talk to mark about this Marcus about this if you want to stand up so people see your face yeah that\u0027s a guy and then I get some numbers from Akamai and because they support quick for their customers for certain products so not all of their products get the the possibility to use quick but most of the products that have actually large data audience can use quick and I can use quick kind of by default of course if the other end supports quick so this is only possible for Chrome because that\u0027s like kind of the only client that supports quick and it also depends a little bit on your gear location so so basically there they offer the possibility to use quick the next time you connect to the same service to a large set of client but then that doesn\u0027t mean that like all of these lines come back to you and actually use quick the next time they connect to you so for example in China is only about 20 percent of the clients that would actually use quick because a lot of the clients that claim to be chrome are not and so they don\u0027t even speak quick in terms of traffic volume it\u0027s after all about like 1% of the total traffic volume and but that\u0027s also a lot as you see with the numbers here also interesting was that about 50% of the connections used and 0 GT so that\u0027s also Val used and very interesting do you want to add anything ego or also raise your hand so people see you and can ask your questions actually I have one more slide right so um he was a so kind to not only provide a number but also something over time and this is a tiny so you can look up the sides in the proceedings but this is kind of two minute samples about the amount of quick traffic you can see and split up by version or so that they "
  },
  {
    "startTime": "00:10:03",
    "text": "support different versions in line with basically what Google is deploying right yep okay so that\u0027s just a very quick update that I squeezed in here and I hope you find it useful thank you everybody who provided the data also in a very short notice and if there are no questions we just move on you in showed up so index the guys for YouTube if you have other questions yes okay then we go to the next presentation [Music] everyone I know the timing is tight so I will go quickly over the slides and be available for any questions if you have so this is control Queens lots of people you have the names of the slides and my name is Nicholas we are focusing on losses one of the thing is on satellite systems the wife the satellite thing is actually lost free we don\u0027t have any losses usually except for specific parts of India where you have very heavy rains in specific bands however we wonder whether we have losses somewhere when you use our in accesses and why because we usually use TCP proxies not only for acceleration and these kind of things but more for splitting the reliability management so on the picture below you have somehow a multi gateway satellite system so we see that we are there are many places where we could actually lose packets and but to have a better view on that we just made simple experiments so we have several in Isaiah that school with Pico quick enabled and so we have just been using a public satellite access and they\u0027re learning quick packets we have seen so I have more data if you want to speak and I\u0027ll have time to go through it but we have done from G but at modeling and we ended up having almost two persons packet losses so I had to chat with GUI earlier and it helped me that on Ã®le access you don\u0027t have such losses so that may be how the operator is tuning doing from traffic shaping and that may be on the wireless things we have after the satellite terminal so to have more ideas on where the losses come from we have done another experiment using another type of satellite axis so this time we have been using Akamai servers with the the HF quick "
  },
  {
    "startTime": "00:13:03",
    "text": "implementation we have used the loss intensification missiles on the draft proposed by Horan Akamai and for one we enter the rotor nests we are in our own company company Network where these very low losses and the 30 light here is satellite we operate so we have some bandwidth on it so when you were alone on the satellite at this moment on this part of the satellite you note that rich and basically we have three different points where we could make TCP dump measurements captures one is before such a gateway one is just wide after a satellite terminal and one is just before the end user again I don\u0027t have time to go through the details but what the useful tool enabled us to do is to show that basically green if end-to-end losses and purple is the Russ\u0027s we have up to these points so we know that from the pot point before the gateway we know that before we change satellite gateway we don\u0027t have losses and then if we look at the purple part here basically these are the losses we have that are happening at the gateway level of the satellite link we have from tools knowing that that doesn\u0027t come it wasn\u0027t raining that day so we know that the satellite was properly sized there wasn\u0027t losses on that part and then we have emulated 1% Wi-Fi losses and we could see again the losses between satellite terminal and the end user so to go back to our problem here the losses can actually happen before that like gateway we could we know from Gateway manufacturers increment traffic shapers so that is what we see here and then another measurement we did is what that was more from benchmark of my Wi-Fi access at home and I could measure one slightly less than one person dresses on in the on the wireless part so that is something I use at my place because satellite industry you have fun focuses on physical layer problems and why I explain to people that losing one packet out of out of thousand is a big problem for TCP I just made these measurements using the same real satellite where basically we have HTTP server and and you there we don\u0027t have any proxies and we just have these losses that are emulated and before after the ferry terminal and we can feed just if we take one point that if we do with one packet out of a minion the throughput electric divided by two so "
  },
  {
    "startTime": "00:16:03",
    "text": "the main point of my talk is that we do actually have losses on satellite links a satellite access whether it is on the satellites part or on the ground segment or after satellite a me know if we are using quick that\u0027s a problem we will have to solve so at the moment we are looking at maybe to solve this issue we know that for short files quick is actually great with because we save the connection in Tabish meantime so that is awesome we foresaw theis we do the same with of proxies and we want to achieve the same gain with quick without boxes but for large files as well and for that we need to do some tunings so at the moment we are thinking that one solution could be using coding in quick then there are discussions on between interactions between this collision control in quick and and the coding we add so for that we have started the discussion documenting and networking with each group on the interactions and base current practices on how we can actually neglect or not packet that have been recovered by opening mechanism so and also to speak a bit more wider on all these topics that we have for and issues we have in quick with over hi bgp networks we have site meeting on wednesday afternoon the room is open for anyone who wants to show results discuss and collaborate with us on this thank you yeah thank you very much I think we have a little bit time for questions if people wanted ask something otherwise it\u0027s also good to move on because of time thank you okay thank you very much so next because Blake thank you being here and should be set up perfect hey thanks so I\u0027m going to talk about some work that we\u0027ve done recently about combining endpoint and network data and what we can do with that just a level set TLS fingerprinting real quick we have the client hello we take some features like the protocol version cipher suites extensions and we ideally want to map them to some you know responses of interest like the process named the sha-256 operating system version so this is the idea that kind of where we want to go with TLS fingerprint or what people typically want to do with TLS fingerprinting we created a couple of open-source tools that generally do network fingerprinting and some light analysis they\u0027re both on github one of them is see high speed one of them is "
  },
  {
    "startTime": "00:19:04",
    "text": "Python where we do some experimental work okay so this is kind of the key component of the IMC work the network endpoint data fusion pipeline it is currently getting data from five geographically distinct enterprise networks so kind of scattered around the world so we get a good a good representation of different applications there are 24,000 users it is I guess one of the biases primarily opera mac OS and windows so these are instrumented managed endpoints there\u0027s some Linux but it\u0027s very small amount and yeah so basically we\u0027re monitoring those networks we\u0027re collecting kind of typical net flow information plus the the fingerprinting information so the TLS fingerprint there and then the managed end points are also exporting NetFlow like information through the any connect and vm VPN agent so that gives us again the network 5-tuple plus the sha-256 hash of the process that created the connection process name and some operating system information and we collect all of that data continuously and then at the end of each day we run a Big Data job that merges based on the five tuples and the timestamps and then we end up with all of these kind of super flow records that have all of the network information that we need for fingerprinting plus all of the ground truth that we can use for you know generating the labels at that point I guess that the I should say that we kind of piggybacked off the current security monitoring infrastructure so they obviously need identifying information for our uses we don\u0027t need any of it so we just strip it all to avoid some of those issues and the last thing that I\u0027ll say about the data fusion pipeline is that you know throughout this talk it will be very specific to TLS but it is much more general than that so it\u0027s actually applicable to pretty much any any protocol so as soon as we define a new fingerprint generation module and Mercury the the data fusion pipeline can automatically pick up on that and start generating daily fingerprint databases for that protocol and then just kind of how we do that I mean it\u0027s pretty simple right so we get the thing databases for each day then we you know run job to merge them so we get the master database this is very very simple but kind of it really has a lot of nice advantages so in terms of performing and "
  },
  {
    "startTime": "00:22:05",
    "text": "longitudinal measurement studies like having the TLS sessions in the form of these like daily databases it\u0027s very efficient to compute trends right so instead of looking at kind of terabytes of data it\u0027s like a megabyte per day of data that we can that we can look at and in terms of the the functional aspect of TLS fingerprinting it allows us to age out older data so data that we saw two years ago is probably not representative of you know TLS fingerprints that we would see today and the the actual fingerprint database is up on the github as well it\u0027s it\u0027s a slightly reduced database to avoid you know some issues privacy issues but it is updated I try to update it every Monday so I updated it right before they talk and I think that should continue for the foreseeable future and this is kind of the format of the database so there\u0027s a lot of information we represent the fingerprint as a string of parenthesis string we collect some metadata about the fingerprint like when is the first time we\u0027ve seen it once the last time we\u0027ve seen it we map all of the TLS parameters to the RFC\u0027s and report min and Max implementation date so we get kind of a sense of the age of the the parameters and the fingerprint some count information and then something we do a little bit differently than the previous TLS fingerprint databases we give a list of all of the processes that we\u0027ve seen associated with TLS fingerprint which is probably a more realistic representation of what information a TLS fingerprint actually gives you and for each fingerprint in a list we give like the name the sha-256 and some destination information that we saw that process using that fingerprints during going - and some quick database statistics so we break up our fingerprints into three different sets so passive is where we don\u0027t have any endpoint information and so this is the majority of the sessions we see you know we don\u0027t we don\u0027t actually have endpoint information on all sessions but we can still map those those fingerprints to a list of destinations that they went to the the endpoint is the fuse data so those fingerprints that the list of processes the malware fingerprints we basically recreate the data fusion process with the artifacts of a malware analysis sandbox so we get the the process attribution there we have in total about 70,000 fingerprints from 46 billion sessions this is growing the rate at which it\u0027s growing is increasing as we deploy to more sites so I actually haven\u0027t checked it in a couple of weeks I think these numbers are significantly larger now the last "
  },
  {
    "startTime": "00:25:05",
    "text": "time I did check we get about two billion new sessions per day and 200 million of those are in point labeled in this kind of ongoing continuous database generation is obviously really important because see less fingerprints change just real quickly I this was very surprising to me very interesting so like what does a fingerprint give you if you go and look at presentations on TLS fingerprinting it seems like this like magic silver bullet that\u0027s gonna solve all your problems in reality it\u0027s a little trickier than that so for a given fingerprint we looked at the average number of you know processes that belong in each fingerprint and the average number of fingerprints that each process generates these are the mean and standard deviations of those so it\u0027s kind of broken up into categories - so Chrome and Firefox or browsers email is like Outlook etc you know for the typical browser fingerprint we see around 22 unique processes and that\u0027s on the low end and then for other fingerprints they range from like 40 to 70 unique processes and these are processes that are using the standard libraries of the operating system or openness cell so err you know especially for those categories looking at you know just observing a fingerprint string or a TLS client hello you actually very little information about what actual process generated it so it\u0027s not this you know magic bullet and you know there\u0027s some things you can do around the incorporating the destination information that helps but you know that then you get into kind of like light machine learning okay so some of the longitudinal studies that we were able to do with the daily fingerprint databases this was actually when I first saw this this is what made me write that I am sea paper because it was really interesting to me so we had been monitoring TLS 1.3 adoption for a couple of years I think like everyone else and you know we expected that in August 2000 18 when the RFC was actually published that we would see a huge uptick right that just kind of makes sense at least to me but but actually you know the months following the publication of the RFC we actually saw a decrease in TLS 1.3 adoption which seemed kind of counterintuitive and then you know six months after that or so the support seemed to rise again and you know just looking at the raw measurement numbers it\u0027s kind of hard to explain why we saw these trends but when you\u0027re looking at the kind of the ground truth that we know which applications are using TLS both of those become obvious so first in terms of the increase that happened you know March 2019 Mac OS sir "
  },
  {
    "startTime": "00:28:05",
    "text": "clarification you go George from a Pina can you go back one please so that\u0027s a closed set isn\u0027t it that\u0027s all things seem that are TLS what is the inter type yeah yeah yes so the variance in the set is not a reflection of increase or decrease in absolute sense it\u0027s the variance in the set yes that is correct and that\u0027s important so you know March 2019 Mike OS release TLS 1.3 as the the default system library and we saw this huge uptick in all these different categories using TLS 1.3 before that pretty much the only two applications that use TLS 1.3 or Chrome and Firefox or chromium based browsers and Firefox and that was kind of it so we understand the uptick so the down tick was basically that first bullet point chrome and firefox were the only ones using TLS 1.3 for a really long time and this is a kind of a breakdown of the the percentage of applications that use TLS so each of those are different categories and what we see is that the overall percentage of applications that you see LS for browsers is actually decreasing over time and I think this is a good thing so most most other application categories about the applications instead of just calling out to their home using HTTP they\u0027re actually now using TLS which is obviously good the maybe not good thing is that they\u0027re typically using either standard OS libraries on Windows which would result in TLS 1.2 connections or they\u0027re using older versions of OpenSSL which we see a lot so you know that it\u0027s it\u0027s not that we see overall less TLS 1.3 we\u0027re just seeing a lot more TLS 1.2 you know the the percentage of TLS 1.3 is actually increasing or yeah the number of sessions for all sessions so the last thing real quick I\u0027m going to talk about some of the trends that we saw malware with TLS fingerprinting and I think so TLS fingerprinting for malware identification has been a really popular subject we looked into this and straight forward TLS fingerprinting is maybe not the best solution to identify malware so there\u0027s a kind of an open source list of ja3 it\u0027s a TLS fingerprint form fingerprints it\u0027s a hash so to get it into our format we kind of had a reverse-engineer it and we failed it sound so we were only able to identify 64 of the 67 that they listed at the time and to their credit they were very they\u0027re very explicit about saying you know we haven\u0027t evaluated false positives and there could be a lot of "
  },
  {
    "startTime": "00:31:05",
    "text": "false positives from this list so when we looked at their lists 55 of the fingerprints were heavily used by benign software so if you were to use those as like a black list to identify not where you would have an unreasonable number of false positives the the remaining nine fingerprints when we looked into the database they were associated with older versions of Windows like Windows XP so they\u0027re really old or older versions like 0.9 of OpenSSL and and those may or may not be good indications of malware but yeah they\u0027re better than the the other 55 that definitely weren\u0027t and so this last slide is looking at malware\u0027s abuse of censorship circumvention tools so on the left and again the relative versus absolute and you know numbers come into play here looking at three popular tools tor ultrasurf and and psyphon and so the the absolute numbers of all of these tools increased dramatically over the past couple of years it\u0027s just you know similar to TLS 1.3 malware is just using more TLS so it kind of looks like it\u0027s going down but consistently we\u0027ve seen malware use abused tour and then relatively recently these other tools like ultra surf and psyphon which obviously have very important uses we\u0027ve seen malware abuse those tools and then randomization is another kind of general strategy to evade censorship so I\u0027m going to randomize my TLS parameters we saw malware start doing this in June of 2018 the interesting thing the most obvious are the most common randomization strategy that they had was to randomize the cipher Suites so they don\u0027t touch anything else they just take the the cipher suites add some removed some changed the order again they did in a very naive way so with their randomization strategy they kept the same exact extension data and extension type so that was completely static and using the the fingerprint format we you know we can hold its its decomposable so we can only look at the extensions or the cipher suites with the version and so what we did was create a very simple rule where if I have if I have seen this you know extension static extension data before but I\u0027ve never seen the cipher suites before kind of logged this fingerprint because it\u0027s interesting and in doing that we saw at the end about nine different extension patterns that were very common in TLS fingerprints "
  },
  {
    "startTime": "00:34:07",
    "text": "with randomized Cyprus cipher suite strings and and they all map to the same variant of malware that\u0027s slipping my mind right now we saw this over time and and there\u0027s you know different ones that were more popular and all of the extension strings were kind of related to I think a specific version of open SSL yeah and that\u0027s it so I really think that I mean for the the measurement community having this endpoint network data and a kind of a continuous way to generate these fingerprint database is a really great way to do very efficient measurement studies over time from the TLS fingerprinting point of view having a database that isn\u0027t automatically updated provides very little value overall and to actually make a TLS fingerprinting system relying on things that are just available in the in the client hello like the cipher suites and protocol version is not really enough to give you a good solution you can\u0027t really solve the use cases that people say that they\u0027re trying to solve that\u0027s it thank you very much we actually don\u0027t have time for questions but we could still take one or two if you really wanted okay thank you I\u0027ll be around yeah talk to him okay go ahead that\u0027s a great analysis oblique tense a couple of questions on the analysis I\u0027ve seen malware\u0027s typically lying is awry right so how is your prediction when malware\u0027s are lying about this and I value well and sorry I missed malice house email is typically lying about SN idealist the server name indicator in that goes they lie about that right they go to the ballot amendment they say they go into Google for example right so how does the prediction work in those case so like through domain front chain yeah so so the the actual TLS fingerprint string doesn\u0027t it strips the connection specific information so if the fingerprint is actually a good indicator for malware then you could use that because it doesn\u0027t depend on the SN I I think the the general takeaway from our results is that TLS fingerprinting without bringing in additional contextual information is not a good way to identify malware okay I can\u0027t hold this work with let\u0027s say TLS 1 or 3 RDS and I where I said I would get it encryption tit all that you get to see is Glide hello then would that be sufficient to identify the applications essentially I mean TLS one two three adds a lot of parameters and the different applications use them slightly different way so what all the other parameters other than the client hello are encrypted right so what what parameters are encrypted in the car I mean the client hello is in clear-text the server load all the other messy handshake message settings so all of "
  },
  {
    "startTime": "00:37:08",
    "text": "this work relies strictly on the client hello okay yeah emotions Shapiro so bouzouki you combine data collected from the network and data from nos and you have a kind of engine you put it inside and you get a data base from it fingerprint database yeah and this tool is available completely so the that tool is not available right right now it\u0027s a bunch of Python smart jobs under that that\u0027s it at least yes the the datasets on Cisco / mercury and github okay thanks and career Edition thank you so now we switch over to DNS related topic and it will be more presentation so I hope kind is there yes perfect go ahead okay so this work is a collaboration with me rocky and like organ a bit check Case Western University myself at Akamai can you go to next slide okay first just a little background little quick actually on this page ECS is the studio record and the additional responses so the purpose of each two so if you look at that little bigger usually the topological information that England it\u0027s happening for cursor it always gives you is actually fix of the client itself originated so use this to tailor the response right and then the ETS response okay and you\u0027re fading in and out a little bit so if you can make sure you\u0027re close to your microphone that would be good okay I\u0027ll try to stay as close as possible the the DNS responses then from authoritative nameservers - recursive resolvers have a scope prefix length as well this is really a cache control it\u0027s it tells the recursive resolver - what prefix length they should they should honor this cache record so only clients covered by the client IP address - the scope prefix length should get the response - next slide please so there\u0027s two high-level implications the easiest one is a privacy implications right authoritative nameservers and for that matter anyone on the path between their cursor resolvers and authoritative nameservers "
  },
  {
    "startTime": "00:40:08",
    "text": "now have information about the client is ready they didn\u0027t have that before the RFC specifically recommends that recursive resolver is limit the prefix length to 24 bits in ipv4 and 56 bits in IQ 6 to kind of reduce the impact of this privacy leakage also because recursive resolver sorry also because authoritative nameservers are not expected to universally need or support ECS recursive resolvers are recommended that they probe for support in authoritative name servers and then only send the client prefix to authoritative name servers that that are actually going to use it in order to reduce the potential a lot of privacy leakage the other implication is a security one this is on the other side this is on the CDN side so the easiest option gives a relatively easy way to scan an entire CD on platform right from a single vantage point you can just iterate through all of IP space sending ECS prefixes and get the entire entire network the entire stevens network right the some authoritative name servers and the cbons that run them have implemented white lists of recursive resolvers that they trust and they only do ECS with those recursive resolvers and that this trust is why a some out-of-band means it\u0027s not actually part of the ECS protocol next slide please so these were the goals of our study then we wanted to broadly look at the current behavior and recursive resolvers around ECS because of time restrictions though I\u0027m only going to focus on the blue blue bullet points today for the other things I suggested please go read the paper as there\u0027s details on all these topics but today we\u0027re gonna talk about adherence to the ECS scope in cash four regressors Oliver\u0027s and then several deployment pitfalls that we observed along the way in our in our studies the datasets that we used for this are first logs from a major CDNs authoritative nameservers and in those logs we can see the IP addresses of worker sir resolvers that send the ECS option to the authoritative nameservers we also conducted a series of internet wide scans of all of ipv4 space and in those scans we sent DNS queries for a domain under our control - every single IP address and what we find is either open recursive resolvers that send the EECS option on to our experimental authoritative nameserver or far more likely we find open forwarders that forward that geena\u0027s query on to recursive resolver that then sends it to our experimental authoritative nameserver with e CS option I say that\u0027s far more likely because past research has actually observed that most open responding DNS IP addresses on the Internet are actually some sort of home CPE device like a home Wi-Fi router or something that then forwards the DNS queries on to "
  },
  {
    "startTime": "00:43:08",
    "text": "a recursive resolver such as the ISPs recursive resolving next slide please so our first result then and this is on honouring scope restriction on caching we we studied in the internet wide scan the caching behavior of 278 where we attempted to study the caching behavior of 278 non-google supporting ECS non-google ECS supporting recursive resolvers two things here first the notice the number 278 is pretty small the deployment of ECS today c appears to remain small based upon our data sets also we excluded Google because the number of Google IP addresses that we found is actually it its massive compared to 278 so it tends to skew our results everyone to focus on just the non Google IP addresses out of those 278 we were able to study the behavior of 202 of them using a variety of probing strategies and the behavior we\u0027re studying here is there caching behavior and the details of that are in the paper but the high-level point that I want to share on this slide is what\u0027s in red there out of those 202 we studied 102 of them actually didn\u0027t obey the scope caching restriction at all what they\u0027re doing essentially is they\u0027re sending the ECS option in their queries but they\u0027re ignoring the ECS option in response so they\u0027re not using it in their caching behavior at all this is at best misleading to the authoritative nameservers because they they get this information suggesting that they have greater control over the behavior of the server\u0027s resolver than they actually do next slide please so we also observed in the internet wide scans that some workers resolvers 33 in total when they sent the ECS option the prefix inside it was actually a nun routable IP address or IP prefix and in these cases it was the loopback address and this was a bit of a surprising observation to us and we wanted to answer the question could this be actually confusing to authoritative nameservers did you go to next slide list so to test just we ran the following experiment we sent five queries from our test machine in Cleveland Ohio to the authoritative name servers for youtube.com those five queries were all identical except that they varied in the ECS option in the first query there was no ACS option at all in the second query we sent the ucs option but the prefix was the test machines own IP address truncated to 24 bits and then in the final three cases it was various forms of unreadable prefixes in the first two cases then we got location we got servers that\u0027s review tube comm in the location Chicago now you can see from the RT that\u0027s actually pretty close that\u0027s that\u0027s nearby to Cleveland right it gives a "
  },
  {
    "startTime": "00:46:08",
    "text": "reasonable to the end-user however when we sent on routable UCS prefixes we seemingly got mapped to server\u0027s world over so obviously the the experience is going to suffer right the the Arctic is higher and your your content delivery will not be as effective so clearly they answer to our question is yes this can confuse some authoritative nameservers YouTube has one particular example here right could you go to the next slide please now the RFC has language in it that says that authoritative nameservers should at least treat unreadable addresses as equivalent to the recursive resolver zone identity that wasn\u0027t happening here one more side please so we actually suggest that maybe this language should be strengthened it should be changed from should to must but also because it\u0027s obviously you can control your own behavior more than you control control those you communicate with we also think there might be a need for language on the other side of the communication as well that recursive resolvers must send routable prefixes in the ECS option and if that\u0027s not possible then they should just not sing the ECS prefix at all or a dcs option at all next slide please next topic was the impact of the source prefix length so in the easiest option you can send a prefix of any length you want right if it\u0027s ipv4 you can send from zero bits all the way up to 32 bits if you so choose right and we set out to actually determine what the impact of varying that prefix length is on CD ends to do this we picked two different CD ends and then 800 random ripe Atlas probes we then resolved CD and accelerated host names for each of these CD ends with the probes IP address to truncate it to various prefix lengths in the ECS option then from the right file as probes themselves we actually measured the TCP handshake RTT to those to those IP addresses that were returned by the CD ends next slide please so these are two cumulative distribution functions for the two CDNs CD + 1 and CD + 2 as you can see with CD + 1 if you send anything less than 24 bits you get a longer RTT so if you send less than 24 bits your actual round-trip time increases with CD + 2 on the other hand you seem to be able to send all the way up to just 21 bits without your RTT actually increasing so the answer of how few bits can you send without impacting performance for these two CD ends appears to differ right next slide please on the upper bound on the number of bits you should send the source prefix length the RFC especially encourages for "
  },
  {
    "startTime": "00:49:08",
    "text": "privacy reasons not to send more than 24 bits for ipv4 right but we also just observed that for at least one CDN and probably more that\u0027s sending less than 24 bits actually negatively impact CDN performance so it seems like there\u0027s the the range of viable options is 124 bits right the alternative for cursor resolvers would be to keep some sort of state on a per CDN or per authoritative nameserver basis on how many bits they should send and well obviously that increases complexity and that\u0027s really up to the recursive resolver whether they were actually limited implement that but if in a general case if you want to send the same number pissed everyone the answer is 24 bits next slide please okay this is our last topic then and this is about hidden resolvers so one more observation that we had from the internet wide scan is that a number of the queries that arrived at our experimental authoritative nameserver carried prefixes in the ECS option that neither covered the recursive resolvers IP address or the forwarders IP address so they\u0027re from somewhere else something else on the path in between the forward and the recursive resolver right but ultimately the authoritative monster was going to use this ECS prefix to do its topological map to some edge server that will insert content right so the question then really becomes is this hidden resolver prefix an appropriate representative for the forwarder the actual origin of this query right next slide please you can break this down to three separate scenarios in our data set and the first one is cases where ECS actually is hurting the the mapping between the forwarder and some edge server and those are cases where the hidden resolver prefix and the easiest option is farther from the forwarder than the recursive resolver so you would have been better off actually not sending ECS in those cases and that was almost 8% of the cases in our internet scan when you exclude one major public resolver that tends to skew the results there\u0027s another case though where ECS doesn\u0027t actually help it doesn\u0027t hurt but it doesn\u0027t help and those are scenarios where the prefix in the ECS option the hidden resolver is the same distance from the floor door as the recursive resolver so you went through the complexity of adding ECS but you actually didn\u0027t gain any benefit from doing so and that was another 20% of the cases in our test site and then at the rest are of course scenarios where ECS is actually helpful and that\u0027s about 73% of the cases so that this result really highlights that nearly supporting ecs alone does not guarantee that you\u0027re going to improve the end user experience right it\u0027s very important to look at the entire path from the the edge device whatever the user is sitting in front of all the way to the authoritative "
  },
  {
    "startTime": "00:52:08",
    "text": "nameserver and make sure that the proper information is being propagated throughout the entire path next slide please so I covered four topics here today but there are many other observations encourage you to please go check out the paper for any other points that you\u0027d be interested in reading about and I have time to take some questions I think thank you very much we have actually no time to take questions but like we will run a few minutes over anyway so if you have a burning question you can try now otherwise I encourage you to read the paper in contact they also fear for other questions thank you very much Thanks so the next presentation is again remote and as I just said if people are okay with that we would probably take a few minutes go few minutes over to running a little bit late sorry for that Santiago I think basically going oh yeah are you guys yes so so today I\u0027ll be talking about characterizing data on traffic patterns on the CDN and this is a joint work with boot crush and words from Akamai as well as my advisor professor Aruna and yeah you could go on the next slide so just to get right into these this talk we observe that JSON traffic is actually growing on Akamai CDN so on the slide we see a figure that shows the ratio of JSON requests to HTML requests over a period that started about three years ago and interestingly we see that JSON is actually four times more requested than HTML and optimized CDN and this has been something that\u0027s been growing over the observation period it started at 1.5 X this ratio and it towards the end it ended up at 4 X and when we observe this and we decided to dig a little bit more deeper to figure out how JSON compares to other types of content and we actually found that this is this trend is not just happening for HTML it\u0027s also happening for other types of content so JSON is actually the leading content type on Akamai CDN except for video depending on how you count video views and video hits so next slide please so it just has a brief background on what is JSON content so JSON is just a text-based data format with key value pairs that\u0027s lightweight and structured next slide please so when we observe this we asked why why should we even look at JSON what\u0027s important about this and the truth is that little is known about JSON content delivery there\u0027s a lot of work on on "
  },
  {
    "startTime": "00:55:10",
    "text": "looking at other types of content delivery on the on the web so a lot of other works focus on optimizing for browsing and median content the media content and also a lot of work looks at security aspects of these these different types of content but when we looked at work that focuses specifically on JSON content we didn\u0027t really find a lot of work on this specific content delivery and and the question we had was is this is JSON content delivery the same as other types of content and do the same optimizations apply for this type of content so we seek to answer these questions in this talk next slide please so what we do is we collect HTTP request logs from Akamai site servers we use specifically this Akamai\u0027s CDN network as a vantage point to understand the global pattern of JSON traffic and we collect two data sets I specifically collect a short term data set that spends ten minutes of time but it covers all of Alchemy\u0027s edge servers and we collect a longer a data set which ends 24 hours of time but it only covers two of Akamai\u0027s data centers and we use the short term data set to do network wide coverage for overall characterization of JSON traffic and we use the longer term data set to do a wider temporal coverage just to analyze powder characterizations and just as a clarification these data sets every time a client makes the requests and it\u0027s an Akamai edge server if HTTP requests then the server know notes down information about the request and this is the information we use in our data set next slide please so to understand my data better we develop a taxonomy and we specifically look at three main aspects of a request and response and we specifically look at the source of the requests so we ask the question where is the request coming from we also look at the type of request and also the type of response next slide please so I guess an interest of time will specifically focus on four of these aspects so for the source we\u0027ll look at the device or the application that is generating this request and we specifically use the user agent field and the HTTP request and we use user agent databases and and other things more details are in the paper of what we do there to analyze the device and application we also ask does this request come from a human or is it machine generated and when I when they "
  },
  {
    "startTime": "00:58:10",
    "text": "speak about human versus machine generated I mean is this something that a human click the button or performed an action to generate the request or is there a shame that just due to some program is sending these requests and this is pattern based analysis that we\u0027ll go over later on in this talk we also look at the type of requests whether it\u0027s an upload or a download of requests and we look at the HTT method for this so it\u0027s a get method and we consider this a download request but it\u0027s a post put or any other sort of method then it\u0027s an upload request and lastly we look at the cache ability of the rich ants of this JSON content using CDN cash labels next slide please so getting right into it the the first question we have is what is the source of this JSON traffic and we first look at the devices that generate or that request JSON content so first we find that the majority of JSON requests come from mobile and sparklin mobile smartphones and embedded devices so we find that about 64 percent of content actually comes from these types of devices and that\u0027s shown in the figure on the screen so on the y-axis we see different classes of devices and on the x-axis we see the percentage of requests that come from these classes of devices and I\u0027d like to also mention that there is a bar that of the blue bar the dark blue bar is unknown traffic and it\u0027s unknown because it didn\u0027t contain a user agent and the HTTP request or we just couldn\u0027t classify the user agent in our in our databases we also see that 88% of this JSON is non browser traffic so it doesn\u0027t come from from a browser and this is interesting because we there\u0027s a lot of a research work on how to optimize browsing traffic but we\u0027re unsure if this applies to this type of JSON content to the majority of the JSON content next slide please so the second question we asked is can we identify if it\u0027s a human or machine with the idea that if it\u0027s a it\u0027s a human sending this request then the response is probably really sensitive in terms of human perceived performance versus if it\u0027s a machine it may not be a sensitive response it may just be data that\u0027s being ferried back and forth between machines so use autocorrelation techniques which we which are further described in the paper to identify periodic JSON traffic and we actually find that 6% of the JSON traffic we have is requested periodic so these are clients that are periodically sending a request every X amount of seconds for "
  },
  {
    "startTime": "01:01:11",
    "text": "the same JSON content and on this on the figure in the screen we have the percent or we have the histogram of frequent periods or most requested periods per JSON object and we see that there are spikes along standard periods so there are spikes at the 30-second period their spikes at the manipulate the two or three and two and three minute period and so on and when I mean by standard periods at that is that they\u0027re not random so they\u0027re not like there aren\u0027t spikes at 16 seconds or at 59 seconds or or another off number these are more or less pretty standard numbers so using this insight that these requests are requests that these objects are requested frequently periodically and they\u0027re also done requested at standard periods we we identify this as machine to machine traffic since it\u0027s very difficult for a human to very frequently make requests over prolonged periods of time and to do so in standard periods as well so when looking at this machine to machine traffic we look at well we want to understand more with but it\u0027s doing and we see find that almost 80% of it is upload traffic so these are requests that are actually sending data to origin servers and we also find that a large part of this traffic is uncatchable so the caching optimizations that CD ends and employee don\u0027t really help this type of traffic or don\u0027t help a large majority of this traffic so instead we suggest that perhaps this traffic can be prioritized since it\u0027s machine to machine traffic where a human is not awaiting response and we leave this de prioritization decision to be made up to network operators next slide please so then we ask the general question does caching help all other traffic that is not machine to machine traffic so we do find that 80 almost 85 percent of requests are download requests which makes them ideal for caching since they\u0027re not sending data to servers are actually just receiving data however when we look at this traffic we still find that a large number of these requests are uncatchable so fifty-five percent are uncatchable and we want to understand why so digging a little deeper we see that 50 percent of all of domains that do transmit JSON on the CDN don\u0027t use any caching at all for their JSON content so not one of their requests are casual so we create a heat map of domains we specifically bucket domains into the domain industry that they\u0027re in and this is seen on the y-axis and the heat map on the screen and likewise we plot the domain cache ability on the x-axis next slide please "
  },
  {
    "startTime": "01:04:15",
    "text": "so interestingly two very different types of of patterns emerge for these domain industries we have one set of domain industries that are almost completely in cacheable so almost all of them of the domains are 0% cacheable and we also have another set of domains that are almost completely casual domain industries that are most completely cashable Tom and when we look at the domain industries that are uncountable we see things like financial services business economy and extremes so a lot of these industries do carry a lot of user personalized data or for the streaming industries they\u0027re they\u0027re more real-time data so we would expect them to be uncatchable since they do carry this user sensitive data that we don\u0027t want to cache or they carry this real-time data that\u0027s constantly changing but likewise on the other side we see industries that carry more static content so things like entertainment and news and media industries often just deliver content that doesn\u0027t change so we we ask ok since there\u0027s a lot of domain industries that that don\u0027t benefit from caching optimizations are there any of their other optimizations that can help and next slide please so we look at ordering patterns we ask the question perhaps we can leverage information from ordering patterns between JSON requests and its we specifically look at JSON dependencies as an avenue for optimization and what I mean by a dependency is consider we have JSON decay and then object B is requested next and then object C\u0027s requested after that and these objects are always requested in this type of order we consider this to be a JSON dependency in HTTP requests so the question we asked is given a request for object a are we able to predict that object B and C will be predict will be requested after object a so we performed this analysis as well on clustered objects with similar urls and an example is shown on the side so if the URL has the same structure but perhaps there\u0027s specific information within the URL that it\u0027s it\u0027s user specific or specific to the request then we just cluster these types of URLs and interestingly so we use an Engram model to analyze these requests transitions and we find that when considering the top 10 predictions we do get about a 90% accuracy using these models so we conclude that actually analyzing these ordering patterns would be helpful for optimization for things like prefetching or server push and so on so yeah in conclusion we we see that "
  },
  {
    "startTime": "01:07:17",
    "text": "JSON is becoming a leading content type on the web the majority is from mobile and it\u0027s non browser non browsing traffic and we do present to a veneer optimization so one is to deprioritize machines machine traffic and to look at JSON ordering patterns so with that I\u0027d like to thank you guys and I\u0027m not sure if there\u0027s time for questions yes and yeah good thank you very much for the presentation we are already over time so please just read the paper and contact the authors directly thank you for everybody who brought data presented data especially for the remote speakers because it\u0027s not the most convenient time for them it\u0027s late here as well so thank you everybody for staying a little bit longer I know this was a little bit squeezed but we actually had not it was not easy to find any presentation so I\u0027m very happy that we got those from the presentations in finally to get a nice program but next time you will have the longer session again because we already have some stuff queued up and therefore thank you for today and see you next time [Applause] "
  }
]