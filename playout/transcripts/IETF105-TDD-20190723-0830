[
  {
    "startTime": "00:05:57",
    "text": "no he told this control that I just look at the attendance unique oh I see it\u0027s really sweet stuff doesn\u0027t work yeah oh we got mythical yeah I did already I sent them brakes link know there\u0027s some Italian participate even verify oh they are okay I already had central yesterday okay so miracles occur okay is that supposed to are we supposed to see this this works this is not one of those cool ones where you go highlight any pictures hey so people who are registered can use "
  },
  {
    "startTime": "00:09:11",
    "text": "this link if you send it to me I will okay yeah if you\u0027d send me directly I can cost you two now mister ten ten minutes it was neat eight foot eight foot five no thank you all right I\u0027ll send after a sunset should I send this well I said I\u0027m gonna send any material they\u0027ll be an investor I would say my name do you want to say your name is a chick so good morning everybody sorry for the slight delay but now we\u0027re also online that\u0027s great good morning this is the deep dive session that\u0027s kind of the second dipped ice type session we are having at ITF last meeting we had a deep type session on router architectures which people gave us a lot of positive feedback about so we thought we\u0027d do it again this time we will look into Nicks and we have some people here who usually or some of them usually come to the ITF they\u0027re from the Linux Native community we will introduce them yes and where were you happy to have them here so my name is MIA KU Leuven I\u0027m Jamal Hadi Salim and we will start it\u0027s a wrong machine the note well applies here as well so the note well applies here as well this is kind of a site meeting but we\u0027re still at the IDF so it applies and that\u0027s our today\u0027s agenda we have the user stuff we have to care about which is mostly now set up and Jamar will now do a quick introduction to scope our date or our morning meeting here and then we start the presentation and we have some time for questions at the end can we solicit a scribe somebody who\u0027s going to take minutes or come on volunteer scribe we have the record "
  },
  {
    "startTime": "00:12:14",
    "text": "alright and the blue sheet okay so when we were scoping this talk an hour and a half didn\u0027t seem to do justice to the content so we had to limit the scope we could have a half-day discussion on not a tutorial but just high level topics so they what is in scope is we will talk about basic Nick\u0027s about how basic Nick works will proceed to medium range offload from the host stack to the cut to the hardware and slightly more advanced features we\u0027re going to use Linux kernel as a reference point not necessarily the only way the only operating system that does this once out of scope is we\u0027re not going to talk about kernel bypass so not the PDK discussions we\u0027re not going to talk about small CPE devices that use the same API on Linux at least or very large Essex multi terrible Essex which may use the same api\u0027s in some vendors Essex we\u0027re not going to talk about virtualization offload technologies s RI o VV m DQ and any newer schemes are out of topic and storage is also out of topic so this could be another session in the future should this session become exciting to the attendees we could have another session in the future the relationship to the ITF if you implementing protocols this is very relevant to you you\u0027re running on the host or some middleboxes which end up using NYX for nodes that perform both host level features or forwarding functions so NYX can process a lot accelerate as well a lot of and have a lot of helpers in the hardware for TCP UDP quake TLS IPSec a lot of the nvo 3 is mostly commodity offloading at this point in time you can accelerate any of the layer 2 to layer n forwarding and filtering a lot of QoS offloading it\u0027s a very condensed session so what we\u0027ll ask is you can will only allow for clarification questions and any other questions that you may have will come at the end I\u0027m gonna introduce the presenters you have a very competent set of folks here on the left is Tom Hubbard from Intel and the Gospel Derek from Broadcom and Simon hormone from "
  },
  {
    "startTime": "00:15:15",
    "text": "metronome and I\u0027d like to acknowledge Boris where\u0027s Boris from Mellanox these are very competent folks they have they know how the implementations work they understand the hardware very well so you\u0027re in good hands having said that these slides took a lot of community effort from the native community in general and this is a list of people who in one way or another contributed shaped opinionated on what should be cut out what you became how the slides should be structured etc and with that I\u0027m gonna hand it over to the first speaker Tom is it like working the mic is can you turn it on how\u0027s that yeah no much better okay so I\u0027m going to present the fundamentals and basic offloads Nick\u0027s so a few definitions might be useful on Nick is a network interface card sometimes network interface controller this is the host interface physical interface to physical Network host ACK is the software that processes packets and does protocol processing in the host typically this is layer 2 layer 3 layer 4 processing a kernel stack is simply a host stack that runs inside a kernel and Esther mell mentioned for the most part will be referencing Linux for that offload is when we do something inside the NIC on behalf of the host so this is work that we move essentially from the host to the NIC for some purpose work that involves a networking and acceleration is offload that is done mostly for performance gains so what is a network interface card this shows a picture on the left of a card and most of you should be familiar with these whoever\u0027s had a PC for instance and that\u0027s not a plug these in so they go into the system bus I would point out this particular card very ancient actually it has a BNC connector so this is true Ethernet and ISO connectivity but nevertheless it\u0027s a NIC and modern-day NICs obviously look a little bit different but basically perform the same function so we NIC is the receiver and transmitter of packets to the network to the physical network it\u0027s the device that does that and on the right we have a stack and you can see that in "
  },
  {
    "startTime": "00:18:15",
    "text": "the protocol stack the NIC is kind of at the bottom and on one side to the outside world connects to the physical media that could be fiber cat5 radio and we use some sort of encoding or framing over that media Ethernet Wi-Fi fibre channel on the other side of the NIC it connects into the system via the system bus so typically today this is PCIe or USB in the olden days like this card it was ISO so the way this works is that Knicks have queues typically they have a transmit Q and a receive Q and these queues store the packets or indicate the package for transmit and receive the queues are composed of a set of descriptors and the descriptors describe the packet for the NIC some of the important things in the scripters are where the packet is located in hosts memory what the length of the packet is and then some ancillary information that may have involved for instance if it was received as broadcast Ethernet and other information like that so in order to transmit the host stack fills out a transmit descriptor and most importantly it writes the information in that for the packet where the packet is located in its memory and what the length of the packet is it puts the transmit descriptor onto a cue and I should mention it\u0027s producer-consumer type of cue so it puts the transmit descriptor on the cue pumps the producer pointer and then it sends an indication to the NIC usually through a pci write register right that there\u0027s work to be done so the NIC wakes up it processes the transmit cue and it looks at each of the transmitted descriptors figures out where the packet is in hosts memory performs a DMA operation direct memory access to pull the packet into its local memory and then the NIC may perform some offload processing which we\u0027ll talk about in a bit but eventually the packet has to be sent on the network so there is a by an assertive serializer inside the device that takes the packet in its memory serializes the data and sends it out to the actual network receive is somewhat similar in the receive path the host sets up a number of packet buffers where packets will be stored in its memory and it puts these into the receive queue in the receive descriptors so again in each descriptor there\u0027s a memory location and in this case maximum length of the packet when the NIC receives a packet it deserialize it puts it in its memory again may do some offload processing but eventually wants to send that to the host so the way it works is the NIC takes the next received descriptor "
  },
  {
    "startTime": "00:21:16",
    "text": "available on the queue gets the host memory location DMA is the packet into that host memory sets the length in the received descriptor increments the producer pointer or consumer it\u0027s consumer pointer in the receive queue and then it interrupts the host which is typically an actual system interrupt and the host wakes up and knows there\u0027s packets to process and there you see if queue so it actually reads the queue and then and get the packets that have been received and processes them in the stack so what I just described is kind of fundamental and that\u0027s the basics of the neck and be started in approximately the early 90s not soon after some of the basic off loads that I\u0027ll talk about in a minute came into being and we\u0027re developed and we can track the evolution of NICs since then so in the mid-2000s we have data plane accelerations so these are more advanced features inside the Nix IPSec offload for instance QoS off loads and more recently there\u0027s a general movement to make these devices programmable so at each phase of the evolution you can think of this as more advanced features more functionality more capabilities to process protocols and packets but fundamentally the operation of the NIC is the same it\u0027s the thing that transmits and receives packets to network so we\u0027ll talk a lot about offloads today I want to give a little bit of motivation one way you can think of offloads is these are just advanced features having to do with the packet processing or protocol processing that happens to be done in the neck so there\u0027s a few rationales for this one is we want to free up the host CPU cycles for application work this makes sense if the NIC can do the functions of networking in a more efficient way so since its specialized hardware that is often the case for instance we can compute a checksum more efficiently than going in the host CPU more generally one of the motivations is to save hosts resources so offloads may save not two CPU memory DMA operations memory movement a number of interrupts scaling performance is very important and offloads helped a lot there particularly in low latency and high throughput there\u0027s also some interesting use cases particularly in mobile where we might offload certain operations having to do with protocol processing to a device for the purposes of saving CPU cycles and saving power in particular on the core CPU so in short offloads makes sense as a cost-benefit trade off if the benefits of moving work into the neck you can "
  },
  {
    "startTime": "00:24:16",
    "text": "think about its coat process or exceed the cost then it makes sense in practice this can be interesting analysis and we know that CPUs for instance are always increasing their capabilities on the other hand the network and things we want to do are always getting more complex so there\u0027s always a bit of a trade-off between whether to offload or run on the host CPU but in general we found offloads to be pretty useful on probably will continue that trend in terms of developing off loads and nic development in general in the Linux community at least we kind of enshrined some of the principles in something called less is more and I want to give three components of this so first of protocol agnostic mechanisms are better than protocol specific and this is somewhat of a formulism of trying to prevent proto classification but the idea is if we can develop an offload that supports say all transport protocols equally versus one that is only only works with TCP or plain TCP IP packets generally the offload that is more general it\u0027s going to be more applicable and better for the user in a similar vein common api\u0027s are better than proprietary a peons we have a lot of OSS a lot of NICs the more common the API is across those the easier it is for users to choose different pieces of hardware this is particularly important in that we want to avoid the concept of vendor lock-in which is where a vendor whether purposely or inadvertently kind of controls the API such that it\u0027s really difficult for the user to change vendor the vendors that they\u0027re using the third point is the program program ability is good so I put this in generally in parentheses one of the aspects of program ability is if we make it completely openly programmable especially user programmable and allow users to do whatever they want users will do whatever they want that as we know leads to some interesting fracturing of the market and can be precarious so we always want to make sure that if we\u0027re going to create a open program environment how do we develop the ecosystem properly and maintain some semblance of sanity across these and portability so we can turn and look at some of the basic offloads I\u0027m gonna skip that slide so we\u0027ll talk about three basic offloads and these are kind of the oldest ones they\u0027re very common amongst Nicks most of these have been around since the 90s at least checksum offload segmentation offload and multi cure "
  },
  {
    "startTime": "00:27:16",
    "text": "check some offload is the offload of the venerable TCP UDP transport checksum so the idea is that we want to offload the computation of the checksum so the ones complement summation in particular is CPU intensive if we offload that to the NIC we get a nice performance gain as I mentioned checksum offload is particularly ubiquitous it would probably be pretty hard to find a NIC and on the market today that does not sort support some form of this an interesting twist that\u0027s a little bit recent is encapsulation so what we found is that say IP nukkie encapsulation or particular udp-based encapsulations actually can have multiple transport protocols per packet that contain their own checksum so conceptually it\u0027s possible to have two three four five or six check sums in a single packet TCP check sum and UDP checksum a GRE checksum it\u0027s all possible so we want to offload all of those check sums and we found some techniques that can leverage rudimentary checksum offload of one checksum to actually support multiple check something even in the same packet so a little bit of detail so transmit checksum also offload has two forms one is protocol specific one is protocol agnostic the protocol specific one we the host sends a packet into the device the device actually parses a packet determines if there\u0027s a transport header and the checksum and if there is it does all the operations to set the checksum so to perform the ones complement checksum over the data it will compute the suit pseudo header checksum if there\u0027s one there and it will set the checksum in the appropriate field of the transport layer the more generic method is for the host to indicate in instructions exactly how to do the checksum so it provides two pieces of information to the device one is where the checksum arts starting offset in the packet and the other one is the offset to write to checksum which would typically be the checksum field of TCP for instance and then the start would be the offset of the TCP header the device gets this and it will perform the ones complement some starting from the starting point to the end of the packet and that some whatever it gets it basically adds it in to the existing value and the checksum field and checks then sets the field as long as the host set this up and initialize a checksum field correctly the device will set that\u0027s correct checksum has no idea what kind of checked something it is doesn\u0027t know if it\u0027s UDP or TCP it doesn\u0027t care it just knows it\u0027s the standard internet package checksum for "
  },
  {
    "startTime": "00:30:19",
    "text": "receive we have an analogous situation there is a protocol generic and a protocol specific method the protocol specific method is called checksum unnecessary as packets are received the NIC parses the packet determines if there is a transport protocol that contains a checksum and performs a work to actually verify the checksum so it doesn\u0027t one\u0027s complement checksum computes the pseudo header adds them checks if the result is checksum zero if it is the checksum has been verified and sets a bit in the receive descriptor to inform the host that it\u0027s verified the checksum so again that is protocol specific it only really works with TCP and UDP packets that the device explicitly parses the more generic method is checksum complete in this case the device performs a one\u0027s complement some of the whole packet starting from the IP header through the end of the packet and it simply returns that some in the receive descriptor to the host the host can take that and actually use it through simple manipulations of checksum to verify any number of check sums in the packet so this is really efficient really a very generic and is able as I said to verify many checks on the attack so looking at segmentation offload one of the observations that we\u0027ve made is that networking stacks are more efficient when they process large packets as opposed to small packets so in particular per packet processing per packet overhead in the stack is significant more than processing the data bytes usually so we want to see if we can arrange the system so we can cross this large packets instead of small packets so there\u0027s two forms of this is one on transmit and one receive on transmit segmentation offload the idea is the host produces a large packet say a 64k TCP segment and we want to break this packet up into smaller chunks for sending out into the network which may have say 1500 byte em to you so we want to do this as low as possible so the idea is the stack process is the big packet processes one IP header one TCP header and at the lowest point possible either in the software or even in the network device there\u0027s a type of segmentation or fragmentation so we slice up the data give each packet its own IP header on TCP header and send each one so there is a software variant Hardware variant of this software variant is called GSO generic segmentation offload the hardware variant is lsah large segmentation offload you might see it also called TSO "
  },
  {
    "startTime": "00:33:21",
    "text": "TCP segmentation offload with this when this is specific to TCP received segmentation offload is the opposite so when small packets are received we try to coalesce these into larger segments and larger packets so again this is per flow similar operation and there are two variants of this one of the software one is the hardware the software is generic receive off the Jaro the hardware is LR a larger sieve offload this particular are flowed of all the checks or all the basic offload is probably the hardest one it does require the network device to be able to parse the packet and understand a lot of details of the protocol so for instance the implementation that do this really only understand TCP usually some of that encapsulation but until we have say a fully programmable environment it is hard to generalize this one one thing I\u0027d like to mention about segmentation offload this really only works in conjunction with checksum offload so this is a good example of where we develop a more complex offload but it requires some of the basic offloads in order to operate and we definitely see this with some of the more advanced offloads that we\u0027ll talk about in a little bit the third basic offload is multi cue this is done in conjunction with multiprocessor systems and the idea is that the NIC has some number of received queues and some number of transmit kids where queues usually are assigned to a cpu and we get a sort of parallelism by the CPUC to queue affinity one of the interesting properties is that once we have kids we can assign properties to them particularly on transmit each queue can have its own attribute so for instance we kind of high priority kids and low priority kids one of the important aspects when we deal with multi queue we do want to try to keep packets in order so for instance we don\u0027t want to be distributing packets in the same flow across different queues either and transliteracy so there are some techniques in the model of queuing to try to enable that in order delivery as much as possible so on transmit there are essentially two methods to do this one is the easy method which is fundamentally each CPU is assigned to a queue so when an application is sending a packet for instance the Hieu chosen is the one associated with that cpu the applications running on and the advantage of this is that we get this sort of siloing locality for instance when a packet is sent on a queue we have to lock the queue in order to manipulate the queue pointer if we do "
  },
  {
    "startTime": "00:36:23",
    "text": "this in cpu / queue then there\u0027s no contention for the lock and no contention for the structures of the queue these second method is when the driver selects the queue so as I mentioned queues can have some rich semantics such as priority what we\u0027ve done there instead of trying to expose all possible combinations of this we allow the driver to basically understand the queue layout of the topology what the different queues are and when the host Jack wants to send it basically asked the driver that has intimate detail of the device what\u0027s the best queue to send this on the driver can do that so for instance if we\u0027re sending a high priority packet where the metadata associated with packet said this high priority when this goes into the driver it looks up the queue that\u0027s appropriate for that so they may have a CPU to queue affinity priority there\u0027s also other attributes you could apply like rate limit on the receive side this is normally called packet steering so the idea is when packets come in to the NIC they need to be distributed amongst the queues it turns out this is a lot like a CMP and some of the techniques are very similar where we\u0027re trying to distribute in a CMP to multiple interfaces on the state listen a stateless ID there are two forms of this one is called received packets jaring that\u0027s a software variant RSS we see site scaling is a hardware variant they both essentially work this the same when packets come in a hash is performed over the five tuple of the packet of the transport layer is available or three Chappel if we\u0027re using the flow label but the effect is to identify the flow by a hash take that hash and map that into one of the queues and that ways we\u0027re also consistent so for this particular flow it always has the same hash therefore we can always map that to the same queue in order to facilitate in order delivery an extension of this is something called receive flow steering in this case the host itself can actually sort of program for each flow which queue to use this is very powerful mechanism so on a per flow basis the host can indicate okay for this flow use this queue there are two variants of this also there is a software variant and hardware variant the advantage of this is to get a really good isolation some people use this where they pin an application to a CPU where that application only runs on that CPU and they associate a network queue with that application and receive flow steering can actually arrange it so that packets only for that applications flows go to that queue so it\u0027s very siloed the application acts like it\u0027s the the only application on the system we get a lot of performance gains that way so with "
  },
  {
    "startTime": "00:39:26",
    "text": "that I will turn it over to Simon who will talk about some of the more advanced offloads thanks Dom so so far tom is taking us to some basic uploads and the basic functionality of the Nick itself well as the use cases the demands of the users evolve and the hardware evolves at the same time it only makes sense that more and more processing could be pushed down to the hardware and so in this section we\u0027ll look at at examples of that in terms of uploading more of the data plane or of the back of processing but before I get into some examples in that area I just like to quickly cover some of the hardware solutions that might be used in in this kind of area so it\u0027s important to note that these solutions are it\u0027s a little bit of a mix and match it depends very much on the use case which choice is appropriate and some hardware choices match some use cases more chillie than others but at the same time they\u0027re not necessarily mutually exclusive so so far the next we\u0027ve talked about a fall in the first category where you have a fixed data plane so this will become kind of a sick that implements a pipeline in hardware and we can also use more programmable technologies and this kind of fall into three sub categories we have semi specialized processes called network flow process or NP or network processing unit and so in this it\u0027s a little bit similar to a general process a purpose processor like a CPU on a in in a server you have instructions it executes the program and that program describes the pipeline they differ from a general purpose CPU is that there are a little bit more specialized so they might have instructions to do a network related functionality or or they might have much higher thread density things along these lines to make them more suited to network processing and then you have the FPGA which is probably the most programmable solution possible here we have gate level programming so essentially you can program the hardware itself find it and so you can describe at the gate level what the pipeline should be and then we have general-purpose processes so this would be putting say an ARM processor onto the neck to execute the pipeline and and you will get back to to this the programmability aspects a little later in the presentation so back to data plane acceleration here we have a diagram that represents roughly how this works so we have applications and then in the corner we have a implementation of a data path and then down in the in "
  },
  {
    "startTime": "00:42:28",
    "text": "the offload Nick we have a data plane which implements some more or maybe all of the functionality of the data path in the in the kernel and so this is able to afford for for example for packets around and so on so things advantage of this is that more more of the processing of the packets can be done in the hardware and this alleviates the hosts of this task so the CPU can be used for other things it also can lead to higher performance depending on the use case so here we\u0027re going to go to for topics in the data plane acceleration before I move on so the first one is match action so this is a foundational building block of a data path and an indeed of offloading and data path into the NIC so the first step is that we do some kind of header extraction so we pull out some fields for example the five Chuco but those we also have metadata for example the port that the packet arrived in other things can also be available then using this data we typically do a hash and then there has looking up in the hash table we try to find a match and if we do find a match then the match will supply some kind of action there should be executed or a list of actions even so this could be to forward to a different port it could be to drop it could be to move on to another table if you have multiple tables present it could be to do some kind of magnification of the packet we can also do more stateful things like do policing which I\u0027ll get to a little bit later or connection tracking so using this max action scheme we can create a forwarding pipeline and so here we have the matches and actions which we can use to for between physical ports the header extraction can operate at various levels of the protocol stack so you would begin with with l2 you can also extract the source and destination IP addresses from l3 and then you can also select for example the ports that layer for date so you can create a specific role to example do some kind of special treatment port 80 traffic possibly to a separate host it\u0027s fairly flexible in this regard oftentimes this is set up in such a way that if the offload data plane can\u0027t process a particular packet for some reason or perhaps it\u0027s for a protocol that it can\u0027t understand perhaps it\u0027s table capacity has been exceeded anyone a variety of reasons we may have you may have a mechanism in place that allows the processing of the packet to be "
  },
  {
    "startTime": "00:45:29",
    "text": "pushed back to the to the host and the host marry after that in in various ways in my process the packet it might process the packet and then also program the hardware to tell it what to do the next time it sees a packet foot say for the same flow and we can also do tunnel encapsulation and decapsulation as well as tagging at this point and this is of course optional depending on what the desires of the system are and so so in this system we can see that essentially we have a pack of processing pipeline packets can come into the machine they can be processed they can be encapsulated or des capsulated and they can be pushed back out of the machine or towards the host so building on this a little further we can also implement QoS in the neck offload the QoS and your floating so in the ingress cases as packets that are arriving on the machine the interesting thing about this use case is that there\u0027s no queue available so the actions that can be applied are fairly limited we can release the packet perhaps by dropping it or marking it we can filter it and so us is a little bit more interesting or a little bit more complex perhaps is a better way to put it because we have a queue so we have the option of doing a much larger number of different things with the packets in order to for example enforcer decide packet rate we can delay packets we go of course drop them and so on and this is an area of which is received significant research over the years and also this research is applicable there of course are challenges in implementing individual algorithms on an offload NIC as opposed to a holster it\u0027s usually a more limited execution environment but nonetheless the same principles generally apply now in this diagram we have packets coming into the machine into the neck and also exiting the neck so they\u0027re being folded from one court to another of the neck that could be a virtual port or a physical port and the NIC is applying some kind of QoS as they traverse the neck in the next slide in this slide we have a slightly different setup so here we have packets of course it\u0027s two directional but I just will only talk about one direction which is Mac it\u0027s originating from an application running on the host and heading out towards the wire are the physical port of the neck and the neck is applying some kind of QoS policy to those packets as they traverse the NIC this is so in this particular case we have different applications and by some kind of selection mechanism they are allocated to different queues and each queue has a "
  },
  {
    "startTime": "00:48:29",
    "text": "different read instance running on it and this could have could mark the packets or drop the packets if they\u0027re exceeding a certain rate and and so on and if this of course is not limited to read I just use this particular example so with the point I wanted to draw out here is that there are two fundamentally different models here one is of applying curious between ports of the neck and one is Carraway supplied two packets originating from the host and then passing through the neck so moving on to the last part of my section I talked about crypto offload a little bit so this is a little bit different to what I\u0027ve talked about so far with the data plane a processing package in the sense that what we\u0027re really focusing now is or floating from the host a very computationally expensive part of packet forwarding if you are applying crypto and the crypto itself tends to be quite complex so what we have at the moment is we have an offload of TLS and this is only dealing with TLS connections which are in the established state so the the host is still responsible for the connection establishment it\u0027s still responsible for the TLS handshake the certificate negotiation and so on and once all connection is established then it is able to pass that connection to the Kay TLS module inside the kernel which in turn so at that point it\u0027s passing the credentials of the connection into the kts module which in turn can push those same credentials and connection information down to the hardware and then when we do transmit essentially what the host will do is the format a TLS frame but it does not perform the cryptographic operation so the authorization hash this space for it but it\u0027s not filled in and the packet or the the record payload is in plain text and then the offload Nick will receive this record and perform the cryptographic operations so it turns the plaintext into some text and it tends to fills in hash on rx things are reversed however it\u0027s worth noticing noticing that Rx is significantly more complex implementation wise than TX because we one needs to deal with things like out of order packets reassembly of fragments and so on essentially we you have much less control of what\u0027s coming into the box as opposed to what\u0027s going out of the box "
  },
  {
    "startTime": "00:51:29",
    "text": "IPSec acceleration flow follows a similar principle to the TLS in the sense that some parts are floating and some parts are not and at this time we have two models for this one it\u0027s the crypto offer load which is very similar to what I described to two TLS in the sense that it is the hosts responsibility to add the IPSec headers to the packet but it does not perform the cryptographic operations which are left of the card it\u0027s worth noticing at this point that on the one hand in did this combines a number of different offloads which which we\u0027ve already discussed the LSO the segmentation offload and I check some offload so one if one is offloading the crypto one also needs to upload those operations they conversely with IPSec traffic one cannot offload the segmentation offload or the checksum offload if one does not also offload the cryptographic so there\u0027s significant benefits to being able to build this stack but in a sense it\u0027s an evolution well one could not build this particular piece of technology without the other pieces that have come earlier that were the ones that Tom spoke about the other model we have in is a full offload so by full of load what you mean here is that Hunter is responsible for adding the IPSec headers transmitted and of course removing the Munn receive this can lead to additional savings and host resources it is clearly also more complicated to implement in the hardware in which regardless of which of these two models you use the DI key the key negotiation between the endpoints rate which itself is quite complex remains in the host there is got to offload this but the way that these things tend to evolve visible that start with something simple that has a very large benefits krypter offload and then we went to a fuller offload and potentially IKEA he could also be offloaded at some point in the future so with that I\u0027d like to hand over to Andy who now addressed further evolutions in Nick technology thank you alright so you\u0027ve already heard a pretty long discussion about how this how these things all works that\u0027s good I appreciate everyone who\u0027s still awake and it\u0027s finished checking all their email no no talk about programmability so you know what Simon and Tom talk about really all these offload features that were enabled exclusively by hardware providers or hardware vendors who feel like this is something useful probably from feedback based on users "
  },
  {
    "startTime": "00:54:29",
    "text": "maybe not it sort of depends we\u0027re gonna we\u0027re gonna build on that and talk about how it\u0027s sort of the next evolution in this in this path is fully programmable mix so as time talked about those are could be good probably good but really there\u0027s a couple key features I want to highlight and think about and why programmability of a NIC would matter so right out of the gate I think one of the really important things is that it facilitates really a rapid protocol development so we\u0027re kind of in a phase right now where fixed-function offload is so powerful and so useful that if you if you want to deploy a new protocol or you think you want to help develop a new protocol and you want to rapidly iterate that one of the problems you find getting yourself into is that well are we gonna really cripple our current infrastructure are really gonna burn more cores processing packets just to support this new protocol what if we just live with the old one and deal with that so programmability gives that option to offload those operations to hardware and really still give you the efficiency you want in the new protocols so the other obvious one is to quickly fix bugs and security problems no one the Linux community likes to remember 15 years ago when anytime you you had somebody report a problem with some TCP related thing on Linux one of the first suggestions on a mailing list or on a message board post was oh did you try turning off TSO because it was sort of famously it was problematic for for some nicks or some kernels at some and you know that became that would be something that if you had a programmable Nick and you knew what the problem was because probably you wrote it you could go fix it so additionally rolling out new security fixes always a great idea there\u0027s this notion right that that if you if you run a large or small scale data center there\u0027s there is going to be some magic packet that\u0027s going to melt your network and this would give you that opportunity to snuff that out and hardware before it gets too far so so today in the programmable Nick world there\u0027s really two to sort of main types one is special purpose hardware or FPGA and P use that Simon is referenced before so this is something that we\u0027re going to program very specific hardware we\u0027re going to write code for and then the other one is really a new class of Nick\u0027s that have appeared in the last couple years that really just contain a general-purpose processor so this might be an arm and x86 a mips maybe in the future like risk 5 but really just something-something general-purpose they can run any code so and and I think really well this might seem today like something that isn\u0027t exactly what what you might want sort of looked at some of the forwarding plane realities slides from the last IETF I think there\u0027s a really interesting quote at the conclusion at the end of that that what\u0027s what\u0027s niche today can be broad tomorrow and I think that\u0027s generally speaking what we\u0027ve seen across the board in networking and then Nick\u0027s that there\u0027ll be someone that\u0027ll roll out a new feature and someone will think I don\u0027t know before long everybody\u0027s got it and "
  },
  {
    "startTime": "00:57:29",
    "text": "everybody wants it so so I think programmability is going to be that next that next piece so kind of build on the common language that we had for our pictures earlier hopefully this language resonates with people otherwise that\u0027s a bummer because we use it in the whole deck so so we really do in this case when you have an FPGA or an NP you the control plane is still gonna stay in your host kernel so it\u0027s going to do if you\u0027re running a routing daemon or something else the setting up flows all that still runs there but now we\u0027re in a case where this this offload data plane is going to run down in the FPGA or the NPU and in fact one do unique pieces with this is we\u0027ll be cases where a software data path does not exist in the kernel for whatever feature you\u0027re adding now that\u0027s it\u0027s a little bit different from what we do in the Linux community where there\u0027s Hardware offload capabilities that are there and your hardware there\u0027s sort of an insistence that there\u0027s a software fallback data path that exists and within Linux that\u0027s been extremely helpful and we\u0027re going to continue I think to push that but this is a case where that might not be the case you may just have a data path that\u0027s completely done in the kernel with no software fallback at your own risk I guess and and in fact that data plane could be expressed in a variety of languages so you know maybe p4u vpf and PL or maybe just a native instruction set for for that that NP you as Simon talked about many MP use have has something that maybe have special instructions for performing operations and the key that we talked about too is that this is this is dynamically program so in this you know this is def packet that could exist you can roll out new code quickly or if you\u0027re rapidly developing a new protocol and you start to say you know what maybe I don\u0027t need 350 bytes of header to describe this this new protocol maybe will make it a little shorter like 324 or something who knows so they ought to keep the other the other piece is a really a general-purpose processor and so this is a little bit of a unique situation a little different than we\u0027ve had in the past but it\u0027s becoming pretty popular and so this is a case where we\u0027re moving the entire host networking stack down on to the NIC so and yes I said that right so what that actually means is your NIC could actually run another copy of an operating system some people shudder at this thought because maybe it sounds a little more complex but the fact is if you have this already implemented software on your server you could actually move it down to your NIC and free up those server cores from doing that work so in this case the data plane offload is down on this general-purpose processor as I mentioned and also the control plane so now what if your routing daemon was running on the neck or what if your whatever was receiving you know open flow messages from a controller was running completely on the NIC so now you\u0027ve you found yourself consuming host resources server host not Nick host you know sort of different CPU complexes "
  },
  {
    "startTime": "01:00:29",
    "text": "there are not sort of actually different CPU complexes so now you\u0027re not consuming any of the resources of your server and you can free them up for doing useful things whatever those may be so this control plane offload is also really nice if you have what some are calling now a bare-metal deployment where you\u0027re really you\u0027re setting up servers you don\u0027t know exactly what they\u0027re going to be used for but you\u0027re responsible for networking you can feel pretty confident that there\u0027s a good chance that your server administrators are not going to ruin whatever network setup you want them to have pretty confident also in the multi tenant deployments this would be really good you can can make sure that that no one no one person has a chance to destroy too much and it really it brings a lot of the server networking administration back into the purview of the network admin I think that\u0027s a sort of a constant struggle between those two groups somewhat understandably so this this gives networking networking tentacles to get a little bit further into the server if you will so kind of in the same vein here\u0027s that picture again so now we\u0027ve got this general-purpose processor running down on our programmable NIC running whatever OS you want and then this forwarding functionality again moved completely away from the server course down to the NIC so this this would mean that obviously if you have applications that are running in your server they\u0027re still going to get the data that they need but you\u0027re not spending your time just needlessly moving packets between between different applications whatever those look like and and the reality to here and it doesn\u0027t get any more recursive than this I promise is that the programmable mix also have offload capable devices these things are all being put on the same die so you all have a control and a data plane and a fixed function device that\u0027s all embedded down but like I said I promised that that offloaded data path on the fixed function device doesn\u0027t also contain another general-purpose processor and another one on down it\u0027s just as simple I appreciate that it\u0027s just just the simple the simple fact is we\u0027re building these chips that are pretty large and have both the general purpose you know maybe you know maybe armored MIPS cores on the side with with a fixed function ASIC there are also people building building mix that in addition to that have FPGA is RMP use as well so so I think this is kind of a new world in a lot of ways I think there\u0027s not a lot of not a large number of users that are doing this but I think this is a strong case especially in a place like this where we\u0027re seeing rapid protocol development where the programmable NIC is an extremely powerful option and an extremely interesting going forward so I think really the way that we want to summarize this is that we think about the networking trends going forward there\u0027s an insatiable need for more "
  },
  {
    "startTime": "01:03:29",
    "text": "bandwidth and lower latency I think the devices that we carry around in our pockets every day that that help us consume more and more data and not only in the air but where the actual wires are in the data centers there\u0027s just people want want more all the time I\u0027m amazed how many people are walking around doing video calls or driving doing video calls I wish that was a joke but it\u0027s not and I wish it was passengers but anyway and I think there\u0027s we\u0027re seeing more and more to that there\u0027s an interest in deploying new protocols we regularly hear requests for things that you know we wonder how we can make the hardware that is fixed-function support how long it will take to maybe support that so this this gives a new option for people who want to want to do this things quickly and I think that that the Knicks are going to work together with post operating systems to make these things happen we don\u0027t see offloads going away we see offload it\u0027s becoming more powerful and becoming more flexible and and continue to be important so and I also think that the programmability and this flexibility will really spur innovation that that we haven\u0027t thought of before I think that\u0027s the magical part about about some of these devices that that are completely or not completely bare the flexible is that is you get the chance to do something that you would have never thought possible a few years ago and and who knows exactly what will come next so I think that\u0027s to me really exciting I think that\u0027s it thank you thanks let\u0027s give them a hand okay we don\u0027t have much time left so I will open the mics now for anybody who wants to ask it\u0027s a discussion please use a mic state your name and it\u0027s being recorded and who hasn\u0027t signed the blue sheets everybody if you haven\u0027t signed the blue sheets please sign david black first of all many thanks for consistently using the Linux example throughout the talks everybody\u0027s on the same page you may not be able to answer this question to ask it anyway can you say anything about significant differences in other important operating system environments I\u0027m being told to sit down here and share the - everyone wants to answer this one so I think the question was about whether or not we see consistency across other operating systems is that right the question in particular was case anything about where how you described things work in Linux differ significantly in non Linux "
  },
  {
    "startTime": "01:06:30",
    "text": "operating systems I honestly don\u0027t have a ton of visibility in that so it answered the question I would point out that some of the earlier work actually came out of windows for instance RSS was literally in Bennet I think it was endeth described that and I believe they had the early checksum offload and I think what happened is as Linux became kind of more popular and open-source we had a lot of developers that are working on that and at some point the knick vendors as the volumes go up they start to pay attention that being said we do know that FreeBSD may use some of these I know that some of the work that we did and package during was being applied and that\u0027s a good thing so like I said in my talk we do want common api is across os\u0027s but most importantly there is nothing I don\u0027t think there\u0027s anything we\u0027re doing in the Nick that would be specific to Linux or any particular OS in fact I think some of these techniques would even be applied in something like DP DK or kernel bypass so again we\u0027re just using Linux of the reference you know obviously major vendor support linux freebsd windows and DP DK and the more features we can have common across those the better obviously and it\u0027s good for us if it\u0027s the features are common because then we have common support and things like that okay but let\u0027s go independent in this sense you\u0027ve talked a lot about different features on different cards in the list of it when you\u0027re writing the code you\u0027ve got to know what the car can do that\u0027s on the machine the your code happens to be running on so I don\u0027t I don\u0027t want you to sort of explain all about you guys now but it\u0027s really the questions more about what up from what I\u0027ve seen is going on essentially it\u0027s someone writes a page to say you know what\u0027s the consensus on what people do say for offload and does that need standardization is it working at the moment just having it done ad hoc would it screw it up if it was standardized I\u0027m just thinking it seems to be all very ad hoc at the moment how the description of what\u0027s the hardware is capable of so that you can write a code to know what to use yeah so I think I don\u0027t know that it might feel a little bit ad hoc I feel like there\u0027s a fair amount of communication within the kernel development community - as if to communicate up to the upper layer stacks what\u0027s available we have lots of feature flags and feature capabilities that are enumerated but and and I think "
  },
  {
    "startTime": "01:09:30",
    "text": "part of what\u0027s done at some of the different conferences throughout the year whether it be net dev or Linux plumbers or others is to help get people together and come up with some of those and I think as we\u0027ve started with basic offloads whether it was checksum or TSO or things like that and we\u0027ve gotten beyond doing things like flow offload that\u0027s been negotiated so to speak fairly well it is it does feel a little bit ad-hoc though especially I think from the outside because what would probably burn outsides the wrong word to use it as an observer it probably might feel ad-hoc because you just see patches show up and support exists and usually what happens is one vendor will come up with it another will say yeah me too and then they\u0027ll they\u0027ll do it and maybe enhance it a little bit more but I think that\u0027s a the goal is to have you know write the code once that\u0027s your application and have it run across multiple vendors and I think we do a really good job of that right now okay so basically your answer is process isn\u0027t broken it just looks like it is from the outside that\u0027s probably fair yeah I had a clarification question for Tom I think did you mention that I hear correctly that you said that implementing any sort of receive uploads requires checksum upload and if so why or did I miss misinterpret what she said so implementation for checksum offload of course I think you said and I may be mistaken that implementing anything like jro or LRO requires checksum upload otherwise that doesn\u0027t work and just wanted to know why that is cuz I don\u0027t know well so if you think about let\u0027s look at large segmentation offload so in the NIC this is splitting a packet up into individual TCP segments each TCP header has its own checksum so I need to actually after I do the segmentation then I need to set the checksum it has to be perfect and this is actually one of the trickier things with something like segmentation offload the fewer things I have to do per packet the better if it\u0027s the case where I could just copy all of the headers to each segment that\u0027s a lot easier but each time we have to consider like IP IDs another good example in the IP header but packet lengths are always interesting and check sums the hardest one so anytime I have to set something that is unique for that packet I have to do that in the nick and checksum offload is definitely one of those so this is a case where we kind of lose the ability and make it completely generic though there are other other avenues but at some level we have to have the NIC have "
  },
  {
    "startTime": "01:12:31",
    "text": "a way to understand how to how to dice a packet and for receive you have to do it because you have to check the individual check sums otherwise you you might end up returning a corrupt bigger packet to the stack and in terms of capabilities you know for receive you also need checks on the floater to receive upload yeah okay yeah the other question I had was do is there somewhere I mean it so the earlier questions that have essentially said there\u0027s a cabal of of you know 10 people in the world who actually know how to do this is there any sort of documentation where we can point you know vendors to say hey please implement checksum upload and here\u0027s how to do it because there exist cards that don\u0027t do it today not you know PCIe cards but I think it goes back to kind of the ad hoc nature of it so that\u0027s a no then we I mean do you want it to be super formal or informal I think when we\u0027re shooting for programmability that will actually solve a lot of that such that you can add different protocols on the fly with different characteristics Thanks we quickly take a question from Java actually so there\u0027s a question for Mikael a person about path MTU he\u0027s asking when doing hardware offload doesn\u0027t matter how much the doesn\u0027t matter much to the host OS if there is no higher MTU or muwaiya and with gr o it seems it shouldn\u0027t matter so the question was about path MTU and I suppose segmentation offload so it doesn\u0027t matter and in fact when we\u0027re doing something like LSO TSO we aren\u0027t just chucking up packets per them to you we want to abide by the path m to you so the way it works is the host jack actually tells what the size is of the packets to go out so we can abide by path into you one of the interesting things that we try to do is when we\u0027re sending LS o try to keep the packets the same size except for the last one that simplifies the problem that we just talked about with lorenzo where we have to set the length for each packet easiest way to do that is to kind of infer what the lengths are so we tell the NIC this is the the length the "
  },
  {
    "startTime": "01:15:33",
    "text": "maximum length make all the packets the same size except for the last one which could be short and then that way we can accommodate path m to you so in terms of larger m to use in the data center we\u0027re seeing like 9 km to use with jumbo frames that\u0027s actually a little less pertinent to l ro and Ella and LSO in that case we\u0027re actually just using the native MTU to accomplish the larger packet size so in some circumstances that\u0027s a little less important if we have already have a large unto you or path MTU to begin with raw chicken on flaps I was wondering about the crypto offloading that was sort of in the middle of the presentation that sounds very interesting but on what I\u0027m wondering about is to what extent does that sort of repeat the risks of all of these vulnerabilities such as the padding Oracle\u0027s and all of that and repeat that in the NIC implementations is that is there any information about that these are their experiences for that all of the stuff that got solved in crypto stacks that are just on normal CPU was a question of portability and the neck now the question is I mean there are all these vulnerabilities if you do crypto implementation a climbing attacks things like writing Oracle\u0027s specific to symmetric implementations to what extent are these what is the risk of these get repeated in the NIC implementations and if they are a your NIC how do you fix that yeah so though as I understand the question is that if we look at crypto there\u0027s a wide variety of attack vectors varying complexity and any individual implementation might be suffering from any number of these so if we push adding crypto implementation down to the hardware what is what kind of problems might we see there in this area yeah so I think that that\u0027s a good point and certainly we can\u0027t pretend that there is not going to be any problems I think that as the complexity of what you\u0027re as sorry as the complexity of what\u0027s being offered it increases so for example if we use move from a crypto on the offload towards a full offload then the surface for these kind of problems must surely exist in my mind I\u0027m not really sure what the the best way to move forward on this certainly that the vendors or the surprise of the code or ideally open code we\u0027d need to move rapidly but perhaps we also need to have some kind of communications in the system so I don\u0027t know if they would be relevant to something like a timing attack but if it was say a packet of death type of attacks you may have a facility in the system to allow the user to maybe using some programmable component or something to to mitigate against that but you I "
  },
  {
    "startTime": "01:18:34",
    "text": "mean would it be a matter of flashing the the NIC and putting new software in to fix something like that or is it you can throw away your NIC if the don\u0027t already - you serious enough I didn\u0027t quite catch that but I guess the question is what would be the mechanism to fix it a bit I think the it would depend on the implementation I mean if it\u0027s a if it\u0027s a kind of a fixed device and you\u0027re receiving firmware from the vendor then I suppose the main avenue other the mitigations would be to get an updated firmware okay Nina but as we move to a more programmable world that that the user should have more flexibility to address these problems themselves and so to react more more fluidly to the situation thank you so the next session starts at 10:00 so we could have like very very quick question oh we take this offline romance it\u0027s a simple question okay quick what what we see exactly what we see is that there is a trend towards moving protocol implementations to the application space for bios reasons and we see that with quake in particular what I have seen in your presentation is that the interfaces that are shown are food camera at the system level he still walk to enable api\u0027s that can be used directly from the applications to use the offload functions I think within the Linux kernel there is a little bit of that there is actually there was a presentation done last year in Prague the net dev conference about actually offloading quick and and what could be done what kind of kernel interfaces are needed in order to to make that possible so I think that I think the move to protocol implementations like that in user space is may be a result of hardware and flexibility so you could say that but the move to user space is also the idea that it\u0027s developed independently of what the kernel does so so actually christian i have to ask you to take it off then because there could be a whole another session to talk about that right that was a simple question I think you\u0027re still here for the rest of the day all three of you right so people can approach you with more questions thank you everybody I hope you enjoyed it [Applause] "
  },
  {
    "startTime": "01:21:42",
    "text": "[Laughter] "
  }
]