[
  {
    "startTime": "00:00:08",
    "text": "nope nobody\u0027s comment about this [Music] all right it\u0027s time to get this party started hello and welcome to the TLS session number two at IETF 104 our chairs Chris Wood Joe Sal alia myself Sean Turner this is the note well you should have seen it a couple times by now you know what Oh closer okay I can do that too this is the no well super loud up here but uh anyway basically if you get to the microphone you\u0027re gonna get recorded there\u0027s video in here if you get IP are supposed to disclose it don\u0027t be a bad person you can read the rules um next requests we need a minute taker and a jabber scribe before we proceed who\u0027s willing rich rich did jabber alright great Wendy you can back them up if you want okay minute take a note-taker all you got to do is say what the outcome was we don\u0027t need minute by minute who okay I think someone\u0027s phone\u0027s gonna blow up in a second Richard\u0027s gonna do it thank you very much sir we need to the microphone please state your name let\u0027s keep it professional and let\u0027s keep it succinct to you because we have a lot of presentations today Thanks "
  },
  {
    "startTime": "00:03:10",
    "text": "that was the first day the next day so this is the administrative part and we had a whole lot of presentations the ones between the two lines our working group jobs and the ones at the end are not we did have another none\u0027ll we did I don\u0027t know another non working group draft called CTLs for compact Els but the agenda bashed it off it\u0027s out there so you can go read it but it wasn\u0027t really ready for part time and without further ado is anything else we want to talk about before turning it over which is deprecating tell us one then nope all right let\u0027s do it so I think it\u0027s Kathleen hi so we\u0027re just here together well students in the audience so we\u0027re here to give a little update on deprecating TLS 101 one we think we\u0027re pretty close to done and just want some finalizing we hope finalizing feedback and so there were a few updates made since the last meeting it\u0027s all on the list and so I\u0027m just gonna review the ones that we did resolve but are open to additional feedback if there is feedback so please provide it if necessary oh it\u0027s right in front of me let\u0027s see so a 261 has text about DTLS implementations supporting 1.0 and so a result of this conversation was should we be more explicit about DTLS 1.0 deprecation in light of this work being recent and the work in this case is that I believe it recommends D TLS 1.2 but doesn\u0027t do a hard deprecation and so the update made was in the abstract here to add mention of DTLS 1.0 so are there objections to that update do people need time to look at this no objections fantastic ha did I miss something I steamed well I just didn\u0027t understand what that means so we currently don\u0027t have an update say tea to 61 and I know it if he sure enough here yeah okay I I thought the point was actually adding the DTLS 1.0 text and that was the question from the list traffic as opposed to updates 86 21 so I mean details 1.0 is no better than TLS 1.1 um being more widely used because there\u0027s no detail us you know she jealous 1.0 he "
  },
  {
    "startTime": "00:06:11",
    "text": "is TLS 1.1 but to me more widely used is no 1.0 to match um but I think like we she\u0027s don\u0027t all people to stop using it and start using details 1.2 so add it to the updates list yeah I\u0027m it\u0027s a process that process a process issue but it sounds like it okay thank you I mean the only point I would add is that obviously the system was not one of our documents so we should you know make sure that we tell them before IETF last call that we\u0027ve done this right isn\u0027t 80 261 am I looking at the right one more details encapsulation of SCTP packets done by the transport area ad sponsored yeah so we I mean I\u0027m just saying another process thing just tell them hey we\u0027re doing this so that we have the firefight now and not later okay that\u0027s just a question there on that jumpers this thing updates like 70 or 80 different parties but that\u0027s great I love that process but isn\u0027t that last call enough because maybe it is yeah I mean I I\u0027ll send the God and them in email and we\u0027ll see all right a script email he\u0027ll do half the massive bounce yeah okay okay so the next question discussed on the list was about nntp and some should not and must not language for using i think it was 1.0 in this case and the response Stephen gave on list was that it was good to do this update because should not must not are not the same things right so we are deprecating its usage and so this makes it a little more clear by doing that update does anyone have a problem with that okay wonderful a reference for 3gpp deprecating TLS one oh and one one was updated and then so I think at the last meeting I\u0027m pretty sure Stephen already went through the 70 different documents that this updates and these are just some notes on additional updates to other documents I don\u0027t think there\u0027s anything here to really comment on but it\u0027s here in case they\u0027re something that\u0027s you feel is important to comment on before we move along and we\u0027d like to know if we think this is ready for a working group last call so to be explicit we\u0027re looking for violent objections thank you violence support perhaps Martin Thompson I dropped a link to our current stats on where things are at in terms of turning yourself for the web into the Java if anyone\u0027s interested they should take a look at that it\u0027s pretty bad there\u0027s a lot of sites out there that still don\u0027t support tell us one - I don\u0027t mind that people accept TLS one oh but not supporting TLS one - "
  },
  {
    "startTime": "00:09:12",
    "text": "is kind of bad in this situation and that\u0027s really what we\u0027re pushing for here by deprecating one one oh one one wait and what I\u0027ve been saying to people who asked is like open SSL said they would support for five more years so you do have transition time even after this is published all right so I see no violence see no violation will get that started when we get back to our respective homes and thank you for perfect timing Kathleen all right next DTLS is that Hannes all right this may be even shorter certainly more that I said so I just put a rush to 31 draft literally like um I think yesterday I was doing other things for the meeting um this has basically a bunch of a deferral improvements thanks to Martin especially date samples has sort of an annoying bug with the pizza for the original orders that the new order the new orders better but the urgent workers confusing so I fixed that and I chef I said twice 1 fix it then I fixed it so hopefully people funnies an implement now clinical example there are a small number of open issues raised most call I think maybe all my Martin the first one of these issues honey I D JLS has sort of a fuzzy kind of attitude towards amplification um so um detail so quick like there\u0027s a lot of effort and says basically like the client hello has to be a certain maximum size and when initial Latasha client alone in this case um and that you only let us send in packets in response on DTLS has been like historically kind of vague on this and I said has this on this path validation that has been nice to be called however if I request now it\u0027s called hollow retail request on but you\u0027re not required to do it you just kind of concurs to do it on there\u0027s some scenarios where this isn\u0027t any kind of issue so especially for the probably like one of the most common detail scenarios is doing where where TC and the you\u0027ve already done ice you have you have confirmation of receipt um so questions should reduce anything here um something not something I guess I\u0027m sort of like not enthusiastic doing a lot of things here on the major reason this is an issue is if you have like a big server which is like solving a lot of traffic and therefore the DRS amplifier and also has and and also has like a big response hum and also is basically doing zero RCT or has like a giant certificate and like no these things it seems like super wobble on if people wanted to have a quick type thing I\u0027m like not gonna like be completely objected to but um I think "
  },
  {
    "startTime": "00:12:12",
    "text": "probably just like say you should do HR in sensitive cases where you\u0027re like leaving amplifiers good enough on are you gonna get to this so I think the advice can be a little more than that but sorry Martin Thompson the the advice we\u0027ll be having in quick is basically don\u0027t send significantly more than what you\u0027ve received for this apparent address and so in the z-row ICT case there\u0027s there\u0027s this huge potential for amplification but if you\u0027re not careful simply say don\u0027t do that unless you\u0027ve validated that the other side would be fine I think you you probably want to have a must level requirement in here I know it gets a little fuzzy and in certain deployment scenarios but a must level with some number for amplification would be would probably be better than that right so like no more no more than 10x without I think probably the right way to thread this particular needle obviously once you have ice then you have the most most deployments won\u0027t have this problem but once we have zero ICT I think there\u0027s a genuine problem here to address so we should should always address it somehow how dishonest the only environment where we have seen DDoS attacks in the IOT context was when co-op was used without didi Ellis actually we had not seen any amplification attacks with based on the use of Detailers and specifically thinking about 1.3 there\u0027s also depreciates secret based mechanism where there\u0027s actually no need and to use the H or the Hillary fury request because you\u0027re already authenticating the client on the first backup anyway we don\u0027t know the IP address there\u0027s a song stuck in your pocket you can have not even on not you could have not not on simplification though it\u0027s a much quicker attack because if you\u0027re getting like a zillion say of like Martin\u0027s coming in is it with a zillion requests it\u0027s pretty obvious Ben Caidic well I mean you have to be able to store the address in your session ticket or whatever with your resumption state and then you can use it as a address validation yet people do that as well yeah so this is great for the handshake but there is also the addition of connection IDs to the protocol which means that you can use the protocol on new paths based on an existing handshake and we don\u0027t have any explicit pass validation mechanism in the protocol sure which makes it a little interesting that that I feel comfortable saying as an application issue there\u0027s no migration that is about either that basically condition are these give you a doesn\u0027t know you have "
  },
  {
    "startTime": "00:15:15",
    "text": "but basically there\u0027s no on like if there\u0027s no instructions if you get if you get a new IP addresses or sending another P address and do we not it wouldn\u0027t be good to because then you because then you have like you have active you have information attacks so we assume that\u0027s gonna look like when we just we need connections plus it aside to keep our Gration out of the protocol right so you would we say that on no but we couldn\u0027t if you want I can I think we should yeah I mean details even know what IP addresses really yeah Jenna anger does the I haven\u0027t does the spec say that the same connection area you must not be used on multiple parts by the same endpoints not even Roy can make some noise about it I mean given that correction ID is a construct that\u0027s India TLS is not available to the application it might be something it may be worth saying something about how some sort of a signal from the application if the path has changed might be useful for DTLS in details can then use that to change the connection ID sure I can I can you know that someone you know I can do that assuming that you know thank you you okay next slide this is me um I think this is just um like Millia to toriel the spec doesn\u0027t say what to do so in details 1.2 there was a cookie and details 1.3 that got moved into the cookie extension because we had a cookies for hvhr so there\u0027s no good reason to have a non-empty cookie and the cooking extension so um my proposal is just to forbid doing this and require the server to detect it and choke so I think this is really just a troll issue at some point okay 72 Martin at that Martin Mastiff Weeki separation with TLS first to GCLs my proposals no um and the reason is falling on the transcripts is different the headers are different because because the handshake headers are constructed so there\u0027s no chance that you can have it huh and and the record layers are as well so there\u0027s no thank you there\u0027s no so even though you even that so you you can\u0027t get the same key you can\u0027t have a TLS CG on you all have at each Ellison DTLS like proxy that would take it like the TLS messages and send the details connection and get the same keys out and even if you did uh messages wouldn\u0027t be correctly formed when you talking use them so I think this is gonna be fine so I posed in noting its if Kitty or cuff that comes and tells me is any different I will change my opinion implementation status liking a bit on on tales for my three but we now have implementations in an SS mint and embed I thought I said she knew size which had better in AB story on them so you know F story is improved slightly from this slide and that we got new out but can we mint and SS I think yesterday these are sort of these are somewhat genetically similar implementations and I did some of the work on mint but I don\u0027t work "
  },
  {
    "startTime": "00:18:16",
    "text": "with them by Chris Wood um I know I\u0027m not sure instead of classes but honest I\u0027ve been getting together to do some how many discipline tation work together and get that working as well so hopefully have I\u0027m interrupt there as well on these are both to have 30 I think but 31 is unchanged if I recall correctly on there actually they\u0027re actually displaying the released version numbers for convenience but otherwise we\u0027re the same um so on that\u0027s not on there\u0027s a few more tests I want to run but I think that looks basic okay so post next times here I want to update the issues as discussed above uh there\u0027s a few more soil issues which I just haven\u0027t dealt with but are easy to deal with ornery new draft and multiplied Nolan run-up testing and send a report to the list and I think we\u0027re ready to go guess you yeah so I mean that\u0027s the question will if they come back and do another last call and I didn\u0027t see anything that actually kind of warrants that so I think if you guys just continue dinner up and if you find anything great if you don\u0027t then we can just push it forward yeah we\u0027re also going to start to deploy this in Firefox all right thank you who\u0027s next hello I\u0027m Alejandra I\u0027m one of the authors of the certificate compression draft um there wasn\u0027t been any major update on the drop itself since the last presentation I think a year ago except that few small changes there have been some developments on the implementation and deployment side boring-ass asalaam integrated some support some time last year and chrome shifted in July I don\u0027t remember the specific version CloudFlare also integrated that boringest south support and deployed it on a last year in September there\u0027s been also some work from Apple I think also boring itself and from Facebook so we were able to collect some data from the CloudFlare point of view what we see is an average 1.5 kilobyte reduction in the certificates for both ECDSA and ever say ECDSA went from about 3.5 kilobytes to 2 kilobytes RSA from about 4.9 kilobytes to 3.5 so one of the main reasons why we did this work was to "
  },
  {
    "startTime": "00:21:16",
    "text": "reduce the potential amplification factor during the quick handshake which uses TLS 1.3 the quick specification allows for allows the server to send about three times the amount of bytes the client sent in its first light before the address verification so the client is is supposed to send at least 1200 bytes so the server can then send back something like 3.6 kilobytes um what certificate with certificate compression we get quite a good Headroom with ECDSA but for RSA there\u0027s it\u0027s not there we still need more space I forgot to point out that cloud fair use is partly brought recompression with the with the default compression level so tweaking the the either the compression level or the dca used we can probably save more bytes do you have a question just quick clarifying question the numbers you\u0027re talking about here is that for the whole certificate chain your presents things yeah a certificate Thank You Dina asada is really interesting d know offhand how much of that value you\u0027re getting is um because your Co compressing like you know the issuer in the subject sorry what so I mean do you know how much so do you know offhand if you just compressed like the EE certificate how that would compare to compressing the chain like if you get as high compression ratio most part about you\u0027re getting there\u0027s duplication within a subject in the issuer we don\u0027t actually yet those numbers about our chains are I think most of them are like three certificates for each chain right so you divide that by three I guess right now joins wondering like for instance the name like the issuer name is duplicated between as it is renamed this your name is like duplicate but in the subject of this years to forget in it as I was wondering yeah are you if part of the value you\u0027re getting that you\u0027re compressing redundancy between the chicken inside the chain or not no this is like the full chain so we don\u0027t have the the numbers of okay noise so so there\u0027s just one remaining open issue which is the attacks about presented some time last year the idea is that if the decompression function produces different results based on the timing it receives the data an attacker might be able to for example delay network packets and then cause the the receiver of the certificate to to physically process a different certificate and what the what the sender sent it was pointed out the disease did the same problem "
  },
  {
    "startTime": "00:24:17",
    "text": "basically applies to a s and one parsing is just that compression is something that we would add on top of what we already F that\u0027s so so one potential solution is to just add the decompress certificate today handshake transcript Azaz um either by adding both the compress certificate and the decompressed certificate or just the decompressed certificate so the question is do we do we want to fix it we actually care from the mailing list discussions and from talking to people in person there it didn\u0027t seem like there was a lot of interest in this um but the discussions were like pretty small anyway so you tell me I guess so yeah so so besides that issue I think that we think that the draft is ready so if we can actually solve the the issue I mentioned we should probably go to La Scala so so yeah so I guess the question is there is there anybody that like is gonna get up and fall on a sword about the open issue and the way to solve it because it\u0027s been out I mean I she\u0027s been posted for six eight months maybe a year so I\u0027m just if we want to do it let\u0027s do it otherwise let\u0027s push this document out the door go ahead honest or Victor do you want to go first oh sure so Victor vasila of Google what I wanted to notice as the issues with potential attack and compression while the issue is itself is a very theoretical in nature in nature fixing it will lead in various ways of fixing it to various forms of headaches and which are not so radical unlike illicit post is an issue which is theoretical which is why I would advocate for not fixing it on this on the open issue I wouldn\u0027t want to have the decompress certificate in the in the transcript because it makes the implementation much more complicated um because in a way how most flowery including ours works so at that time when you process the packet and create the transcript we actually don\u0027t pass the content so maybe not good a separate question did you if you look at the code size of the functionality in specifically the compression algorithm and the van requirements that would be I would be curious about this in light of many of the discussions we have currently in the idea for that TLS being too heavy weight and so on and so on right so we do add some numbers on the actual compression I think it\u0027s kind of like if I we saw some some of this "
  },
  {
    "startTime": "00:27:20",
    "text": "some incrementing in CPU usage I think it was about 1/2 percent globally I don\u0027t I don\u0027t know if that means anything to you but so this was on on clawed floors edge Network okay odd you didn\u0027t do anything on a sort of more client-side device no okay so I I don\u0027t think chrome had any date on this but someone else might correct me benkei doc is interesting the house was up before me to say that he doesn\u0027t want the decompressed certificate in handshake because I was sort of considering the case also for IOT where you might have an extreme compression function which is like a one byte index into a table certificates and in that case it\u0027s a lot easier because you might have some miss binding problems but I also don\u0027t want to suggest that we muck around with all our TLS implementations to you know have the decompressed certificate in the Angi cache just for the benefit of IOT stuff so not on Thomson I\u0027m reading this description and listening to what you said about this and I don\u0027t understand how this is possible can you give us a little bit of a I think the idea I I think the idea is that you have some kind of compression function that is either like specified badly or it has a bug in it so don\u0027t do that then yes so that would be one solution so I guess we don\u0027t really say what requirements we have on our compression function that you know it\u0027s got to be stable over time say given the same input you know you could sort of consider this this like index into a lookup table to be a compression function in a strange sort of sense but I think we don\u0027t actually want people to use that in this case we\u0027re gonna we\u0027re gonna cut the line here so I think that the main point here is that is the part where an attacker can somehow influence the process I don\u0027t know if like whatever index compression function of would a lot of that so there\u0027s a there\u0027s an important point there and and that is that if we\u0027re putting the compressed form into the transcript we need to ensure that the identity is in the transcript in somewhere other now doesn\u0027t matter if it\u0027s compressed as long as it\u0027s not it\u0027s in that form so we talked about this with cached info you\u0027re compressing it but you\u0027re providing a hash that is has a sufficient entropy that you know you\u0027re not you\u0027re not exposed to the thing that Ben was talking about with the the Miss binding attacks because well if anyone can manage a Miss binding attack on a on a hash then you\u0027re free to cash that in anytime you like so yes I think that\u0027s what the fix is is define the requirements on the compression function in such a way that you can avoid these problems that leads me to conclude however the the "
  },
  {
    "startTime": "00:30:21",
    "text": "dictionaries that I have been advocating for might be a bad idea so the thing is I guess most compressions wouldn\u0027t actually compress for example signatures or public keys but I guess like you can describe a compression function or whatever you want so oh yeah yeah I\u0027m much less concerned about this this particular flavor attack I I think that they they\u0027re relevant concern is it somehow possible to have I\u0027m not sure I\u0027m just taking my feet on a compression it\u0027s not closing the compression function where there are two things that compression the same thing but that seems like it\u0027s gonna have other problems like operationally yeah aside for us aside from like any miss binding issues so I think we have to convince ourselves that the crush pressure function is collision free so I think the the the the the point type said before was on that sounds like a bogging the implementation rather than a potential or task that\u0027s one more he had a wall see compression function you wouldn\u0027t be collision for it right right so it has to be it has to be lossless right I think that\u0027s the record ask the relevant requirement um I\u0027m thinking incorrectly that it has to be like you know that you know that has to be injected right but I\u0027m not concerned of this defect issue I think it\u0027s like I think it\u0027s like as you say it had the a bug in the deacon pressure I think it\u0027s fasting more likely that bug in the deacon pressure is like it\u0027s like an undersized memory you know a UAF for something else so I think much more likely than this if you have been much more serious so I I think we should proceed with this as it is and just talking with Rome isn\u0027t it yeah then the phone should be injected so any idea just put in the security considerations if you had something that was so badly designed that it did X you\u0027d have this problem so don\u0027t do that basically what I think we\u0027re about getting to you right okay Victor Vasily if I wanted to us answers a question about the binary size so the binary size of broadly is quite substantial because it carries a giant dictionary with it which is why this is mostly intended for clients and servers which serve web content because when you serve web content you want perfectly for other reasons and this is like this is three really both clients and conservers can trivially choose to not implement this and they will work just fine yeah I mean the the spec also specifies support for ceiling so that\u0027s better yeah it seems to me that the that B constraint we want to state on the decompression function is more or less already implicit in the word function if it has more than one possible output for the same input then it\u0027s not a function so I "
  },
  {
    "startTime": "00:33:21",
    "text": "think we got a way forward right so if you can write that up will you write that up put it in the security considerations and I\u0027ll hit a button to make a new blast call happen and we can we can note that specifically something that was neither was at it okay thank you very much all right delegated credentials Nick all right hello I\u0027m going to talk about a draft that is on draft number three it\u0027s been adopted as of very long time we haven\u0027t talked about it for a while so it\u0027s due for an update so just as a bit of background if folks remember what this is it\u0027s meant to help protect interface internet facing applications from having their long-term TLS keys in memory allowing them to instead have a shorter lead lived key in memory and keep their long-term key somewhere else there are other proposals to do this via HSM remote HSM type of applications there was I believe a buff or lurk about this but the motivation behind this change is to do so is to not introduce the latency that these changes these proposals introduce so just to kind of spell that out if you want to have your key in a very secure location in say west coast of United States and have traffic in Europe you may have to incur additional latency to do the handshake itself this latency comes here in delegated credentials the way it works is that a short-lived key is issued to the web server every X number of hours or every periodically that is signed by the certificates key so it comes with some additional parameters so quite specifically it\u0027s um the way to think about this is not as a sub certificate it\u0027s more as a time bounded key swap this is how Richard bonds described it so on the right here you can see all the the the aspects of it it\u0027s not an x.509 it doesn\u0027t have all the different details of this it has specifically a this see where it says protocol version this has been this is the part that\u0027s been changed between version 2 and version 3 almost everything else is the same so what signature schema can use how long it\u0027s valid relative to the starting time of the certificate and a key and that\u0027s basically it so the way that delegated credentials works in TLS is there is a client extension that is an empty extension that advertises support for it and then the server responds with an extension on the certificate message that contains this delegated credential and in terms of the handshake changing the certificate verify rather than using the public key of the certificate you use the public key of the delegated credential and so to validate this on the client there\u0027s a requirement for the certificate to have "
  },
  {
    "startTime": "00:36:21",
    "text": "a specific object ID that enables this feature but other than that all certificate content constraints still apply so the subject alternative names any of the key usages all everything that\u0027s that\u0027s on the certificate still applies to the validation of the connection including revocation any certificate transparency requirements they all apply to this certificate that\u0027s delegated the credential structure itself is validated validated against this public key in the delat in the end entity certificate and and then obviously the certificate verifies is checked against the key in the delegation so this has several benefits you can put the signing key far away you can centralize the control of it you can kind of split your operations for key management into short-lived fast rotating keys and a long-lived more secure key and this doesn\u0027t have the risk of expanding the scope of the certificate unlike say adding additional certificate at the end of the chain and relying on name constraints cristinaw SIA so it\u0027s a extension to the protocol so if you if you have clients that don\u0027t support the extension you still have to use the regular key that\u0027s right so it can be used in conjunction with one of these remote key mechanisms so you still have to keep the long term key in memory while you have the short term key or you know there\u0027s several options there\u0027s a if you want to keep the long term key far away you can continue to do that if you want to refuse connections from clients that don\u0027t support this extension you can choose to do that as well in and those would be the two ways to not have the long term key in the local memory it\u0027s worth knowing there couple other benefits here aside from sorry I thought I was is what other benefits here was - the security benefits adjusting so I mean basically the idea is you have these remote datacenters right and so you can remote key port O\u0027Call the way nick says you can backhaul the data all the way back to your main datacenter on but you point easier motivated you can have a lower security level than then you mean datacenter another but though this worth noting in the method is deployment algorithms so like et tu 509 is like very slowly rolling out in cab F in fact not really more young cab Beth but this lets you deploy teacher thought of a ninth day yeah good point this this does allow different signature algorithms that may not be in x.509 I mean not just ever synergy algorithms also you can play that giffy Hellman keys in to i7 is that like opt I lost alt if he Hellman if you wanted but that\u0027s correct if the word changes to tell us that were adopted would allow different types of identity keys then this would support that so madam Thompson and the case that you wanted to use a new type of signature algorithm the client would have to support both the signature algorithm on the end "
  },
  {
    "startTime": "00:39:21",
    "text": "entity cert and the signature algorithm on the delegated credential and so that that\u0027s a little limiting but it\u0027s it\u0027s certainly a lot better than it has been really sauce you\u0027re not worried about the size of the browser executable audio no no it was just a point that\u0027s one way of thinking of this is an extension of the certification path but it\u0027s not it\u0027s it\u0027s an in protocol thing which just makes it now that you have to deal with two signatures you have to deal with potentially two different signature algorithms and that was that was all as a feature but it\u0027s also means that you need to have the code for those things and assess doesn\u0027t have eighty two thousand nine at the moment but you know it\u0027s really more a feature from the server side because it means you can ASSU that only said the same ways you can have this server backhaul you can server the onliest one or you can just there\u0027s just algorithms faster so you can save server CPU on I mean generally like it\u0027s pretty much not possible it\u0027s possible for like the foreseeable future to have like uh even if he teach you five out of the nine it was like rolled out by Kevin of tomorrow it would be possible for future buzz or client that didn\u0027t do PT 56 although finding those new drsa right so on the client side the validation of the handshake and the validation of the PKI if they\u0027re separate like they are a lot of plot this allows you to rely on upgrades to the TLS stack without necessarily needing upgrades to the PKI stack and and the certificate I guess the signature algorithm extension is reused in this case so this is the same thing that you rely on so updates in oh three there was some short discussion on the list about protocol version so this is this was removed this is TLS 1.3 only and there are no non TLS 1.3 versions but in any case it is potentially useful for future versions of TLS 1.3 without changing the structure of the credential there is now server implementation of oh 3 in boring SSL there is a server deployment here at this as this website with the proposed ID which is issued by did you cert we there\u0027s a hiccup in deployment so we we\u0027re going to try to fix that by by tomorrow so this is this is optimistic in any case there\u0027s ongoing work to support this in NSS and Firefox as well and so not only have we kind of kick-started this from the implementation standpoint we\u0027ve started some initial discussions on the policy list the Mozilla\u0027s policy list about interactions with the C a browser forum and we don\u0027t necessarily see any any negative points being raised from that that would that would block the distribution of this this protocol so next steps the one thing that we wanted to have here is is some sort of formal security analysis so we\u0027ve grabbed somebody\u0027s to do that essentially the "
  },
  {
    "startTime": "00:42:21",
    "text": "properties we want to prove that this is equivalent from a security perspective of having an additional certificate in your chain as well as we want to have this brings stronger binding then you would for a PKI chain and PKI chain so you can change the intermediate and use the same key and it\u0027s still fine in this case delegated credentials are specifically bound back to the credential itself so the next the questions here are pending the formal security analysis are there any other questions as a working group think this is ready for last call should we wait for the analysis or what are the next steps and are there any questions yeah rich Sol\u0027s is the intent that the client who supports this are they expecting if they speak to multiple servers that they all have the same delegated key or is it just it\u0027s gonna still follow the same PKI rules and then this in essence an extra key exchange rule to follow it\u0027s uh it\u0027s like you\u0027re seeing a certificate that swaps out its private key for another short that private key so there\u0027s no assumption of keys being pinned or anything like that I think we should wait for the analysis like I mean we can still get house deployments going but like I think a good policy for this work and good grand for where things on the standards track meet some kind of formal analysis I was like a good idea for three so okay what I was hoping for okay let\u0027s say thanks next Chris alright hello you\u0027re here to give you enough the unencrypted at SN I for those that are unfamiliar just a quick summary of the problem as it exists today assuming you\u0027re not using an encrypted DNS or using a legacy version of TLS your network traffic leaks a lot of information to any passive observer you\u0027d buy at the DNS query itself in the equator answer or in the TLS S\u0026I or in the certificate that comes back but if you\u0027ve lived in a world where you had encrypted TNS via Doe or dot and you had TOS 1 3 wave encrypted SMI the amount of information that\u0027s actually leaked from those particular places in the packets substantially reduces still it\u0027s not a perfect solution but it\u0027s certainly a step forward in the right direction so in the latest update of the draft several changes went in probably the biggest one was the probe across from David Benjamin to improve robustness basically to add a secure fallback saying or secure retry signals who get a new yes ni key in the event that the client doesn\u0027t have the right version of the ini keys anymore the server misplaced it or whatever we also added a rule that "
  },
  {
    "startTime": "00:45:24",
    "text": "prohibits sending a cached info search so we specifically because they don\u0027t want to leak what certificate the client has previously cached and responds to a connection we add some clarifying words around HR our behavior although they\u0027re still a little bit more than needs to be done there we added also an initial simple st. design for the multi CDM problem that is the combined records approach along with a mandatory extension mechanism in a way to specify with a particular extension in the s and I record is mandatory by mandatory I mean not mandatory to implement but clients which get a mandatory extension and they don\u0027t understand it they must ignore the East and I record that comes back so I\u0027ll describe this solution a little bit more in a couple of slides and in kind of landing this initial PR we also dropped the S\u0026I prefix for the the special DNS query and moved from a text record to a new are type per advice that we got back from various stakeholders so there are also some pending changes that are probably ready to land particularly the Greece one from David management as well it just needs is it up to date I it is okay so we should merge that relatively shortly there\u0027s also APR to swap the version in check some fields in the sni keys record if you believe that the check sum adds value for version one it likely adds value for any kind of version of es ni so by swapping them you\u0027re effectively making the check summit invariant which seems okay if you buy that particular argument so assuming there\u0027s no pushback will likely merge that soon and then there\u0027s another work-in-progress pull request for a more generic solution to the multi CPM problem which is something I\u0027m gonna talk about and hopefully we\u0027ll have a longer discussion about in just a little bit so to them to kind of go into detail about the multi CDM problem there\u0027s just DNS load balancers out there that could be doing strange things such as you know redirecting or giving you different answers to different a or quad-a answers to different CDN providers based on any kind of deterministic random strategy for load balancing amongst these providers and that\u0027s I guess that happens quite often as Mike will likely point out and they can either do this by just giving you an an A or a quad a record that points to this particular provider or giving you a seening that points to a different provider so say for example if what\u0027s listed if you query for W example.com and you get back you might get back AC name to CDM one versus CDN two depending on what they load balancing strategy is so the the failure case that occurs is because you\u0027re doing your a in quality queries and your es and my queries independently from one another they\u0027re not atomically bound you can end up in a situation in which the addresses that you get back from your a or quanta "
  },
  {
    "startTime": "00:48:24",
    "text": "queries are effectively provided by one CDN or one provider and the PSNI records and says Texas should be not text the PSNI records you get back are provided by another CDN provider and that doesn\u0027t work because CDN two in this case doesn\u0027t our CDN one doesn\u0027t have the SMI keys receiving and two and vice-versa and the robustness technique that we added does not allow you to fall back from this particular scenario so either you have to hard fail in this case or you fall back to point excess and I both of which are not great and ideally we\u0027d like to avoid heart failure in order to make this work so the simple PR that landed effectively works as follows combine the PSNI keys and a in quantity records into a single simple record by adding the addresses to an es ni extension that marked as mandatory and then when you query for the es ni records and you get back one of these that has the specific a set of addresses that you can connect to you simply go ahead and use them you ignore everything you get back from the in quality records because one could argue that this is how potentially how DNS could have should have worked in the first place just being able to add more information to a particular record so this is potentially controversial and the event that urn if you view it as deprecating ain\u0027t quite a records but it seems fine and particularly if you\u0027re a you know a client and you\u0027re talking to a provider but it\u0027s able to vend these particular records and make sure that they\u0027re always in sync it seems okay so there\u0027s a couple of rows and columns here the the probe is that does not matter how often the mismatch occurs between your addresses in your DNS you sent our records because you\u0027re simply not using the a in quality records at all the you know the mismatch rate is completely irrelevant the count of course then is it requires an a provider that\u0027s able to actually make these records and make sure that the contents of the ESN I record matched those that match match the addresses that the for the hosts that they happen to service so some providers out there can do it some cannot so that\u0027s the trade-off there is a the general PR or the generalized PR that\u0027s out there right now this is what\u0027s written is not effectively what\u0027s in the text what\u0027s in the text is arguably a lot more complex but effectively what we\u0027re trying to do is allow for the addresses that are contained in the sni records to filter or to act as a filter for or ability check for the addresses you get back and the Inc wide a records so the algorithm basically works as follows you would query for your a in quantity records for yeeess and i record and assume you get everything back and you get back in a sign a record that has one of these address pointers structures which contains a netmask inside of it or one one or more net masks you use them to "
  },
  {
    "startTime": "00:51:25",
    "text": "match against the addresses that you get back from your in quasi records if they match go ahead and use those addresses and connect just fine if you\u0027re a or quad-a answers happen to resolve in a cname chain of some sorts and the cname are the canonical name ur we didn\u0027t quite not quite sure what to call it the name inside the ESN I address pointer record happens to match the cname that you got back from the a or quad a record used the address otherwise you have to take the name that\u0027s in the es and I record and resolve that to an address and in connect to that one because presumably that particular record has been constructed in such a way that the name will ultimately resolve or must resolve to a host that is serviced by the same CDN provider otherwise you\u0027ve royally screwed up and things will go badly of course now the efficacy of this particular approach depends on how often the mismatch occurs so if you have a scenario in which mismatch occurs say 50 percent of the time that means you will be doing a second sequential DNS query to resolve the name that\u0027s inside the ace and I record to an address 50 percent of the time if you don\u0027t want to run into heart failure it\u0027s also problematic because depending on how you\u0027re doing your happy eyeballs like querying for your a and your quad and you use my records you might introduce a delay of some sorts in order to get your initial yes and I responds back so the time you it actually takes to go from nothing to you know es and I keys and address some addressing information could potentially be quite long so not only you paying the hit but the time it takes to actually start connecting could substantially increase which is not ideal so also we have for the group is as follows because we have a working solution in the document right now that is doesn\u0027t have it doesn\u0027t it makes the mismatch rate irrelevant provided that you can actually bend these particular simpler es ni records that have the full addresses in them we propose moving forward with that taking and then taking the generalized version that is in 137 making that an extension potentially in a separate draft and you know working on that and I guess either bringing it back to the working group or something so I\u0027m I\u0027m hopeful that people will people who are opinion about this particular topic well come to the mic now and tell me whether or not they\u0027re okay with this or if not because the multi CDN issue was the big thing we needed to address order to move forward with the s and I so I would really like to see something happen here so just that we can get ourselves on block to move forward so Stephen so as a as a "
  },
  {
    "startTime": "00:54:31",
    "text": "kind of in the slight lean if you come and put it impacts on the moving forward part I I think expressing these as extensions is a bad idea and having any extensions and this is not a great plan my personal opinion do you want any extensions in ES and I or yeah which if anybody bought into that which I don\u0027t think they do that would rule out this up to this password on the 1:03 has I just kind of worried it will cause another shit fest with a bunch of people who have dependencies on how a and quad a records a manage that therefore wouldn\u0027t be like able to use yes and I but if because we\u0027re deploying this to an extension is assuming you by the extension thing and because we\u0027re deferring it to an extension it just not prohibit them from doing that later in extension right it simply enables the people who want to do yes and I and me the way it\u0027s currently specified to do it right if you assume that everybody that implements both not everyone might implement both so if you\u0027re a client and what is the incentive for a client to do the more general one if you\u0027re paying a potential but it\u0027s the server side who has the dependency on the a in for a management right but if if the clients are paying a substantial performance if a large fraction of time for one of the signs versus the other there\u0027s not much incentive them to do the the other one regardless of how hard or easy it is for the server yet actually if you it right so my point is that if with this plan is making an extension draft if there\u0027s some servers in the world who for whatever reason merge their a and quad a so that they can\u0027t use what\u0027s on 1003 yeah then and and if clients only implement what\u0027s in zero three they\u0027re not able to use a smart don\u0027t plan my response to that would be as a client so I\u0027m now speaking as a client as Apple we would not it\u0027s very unlikely that we would implement a solution that had a performance regression in this that PR 137 potentially provides or gives us so regardless of whether or not it\u0027s in the draft or in it in this draft or in a separate draft it doesn\u0027t change the fact whether or not we\u0027ll implement it we\u0027re looking for something to implement yeah I haven\u0027t talked to anybody on either side that\u0027s interested in the potential terminal regression case of 137 if it were a common case in pursuing that like as an acceptable cost if this was happening all the time there are different opinions on how often this happens I think that\u0027s kind of the territory we\u0027re in I\u0027m so Mike will hopefully clarify that slice but yes go yeah I\u0027ve heard whispers that\u0027s kind of scary data I you know I might ask that I be given some time rather before moving on to see if I could replicate that data or have a different data point because "
  },
  {
    "startTime": "00:57:32",
    "text": "it if it\u0027s really scary and this is like pre this is like talking about my explicit ation Tory\u0027s made it which is completely unfair you know but that\u0027s kind of what the question is asking as well you know it\u0027s not intuitive to me so I\u0027d like to understand it more deeply the current solution in the graph actually does impose a performance penalty as well right because you\u0027re essentially racing an a lookup of the es and I look up right and you\u0027re giving it you\u0027re tolerating a certain amount of time right isn\u0027t the connection delay apart yes but you\u0027re doing that regardless no I\u0027m saying so this whole solution is that\u0027s not it it it brings on a little bit of incremental overhead right you\u0027re waiting to see if you get an ESI record yes done in a record right so that\u0027s part of this but yeah to me the most important thing is and watering around the topic is that we sort of soft shoot like deprecating there in quad-a and as as a guy he\u0027s kind of dumb added in HTTP level on a couple of different ways already and this will surprise a lot a lot of people and such as the extent that we can avoid like especially at the TLS layer which I think will be even less intuitive than HTTP directions which is where I\u0027ve you know earned my Spurs in this particular topic you know I I think we really do want to avoid that because of if nothing else all the diagnostic machinery you\u0027ll see requests landed in one place and you\u0027ll see people firing up ping and they\u0027ll be like hey it\u0027s gonna be hard so I can\u0027t speak to the difficulty that will you know operator will incur there I\u0027m hopeful that operators that can do 136 spear 136 that is a simpler virtual you know comment on whether or not this is actually a problem for them so Patrick this isn\u0027t the journey the tell us later this is the application doing a lookup and just telling the TLS they were to go I mean quite serious that\u0027s how I simple emitted like these two Firefox so um I guess like try to reframe this discussion perhaps on the situation we have at hand is that we have some evidence and some work experience that will t see the ends we have a real problem and the yes and is not to playable in the current state because it causes wait because it was a hard fail on we have a number of a non-ideal approaches for like dealing with us on we had on the this approach on is something that we know a will work in the sense that it doesn\u0027t like it is safe to deploy and at the client and and that the clients will not divert in the catch of the wings over Martha and then we know that some servers can deploy and the clients could work and it has no performance regression worse than the asana forms regression itself um now it may be the case that there\u0027s some mechanism that will be easier for some people employ on the server side and will not induce a major performance regression on and I welcome that solution and that solution is available "
  },
  {
    "startTime": "01:00:32",
    "text": "today I prefer solution to this appear even if it\u0027s not more complicate for the client to play however my impression is that another situation and then available solutions like you have no performance regressions or seem likely a performance regressions and I can tell you this is a client we have no interest in having something which requires like a second dance resolution in Syria before I do a connection that\u0027s like not as a non-starter for us um yeah for like a very ain\u0027t like for any substantial fraction of a fraction of requests or all that happens like will liability for any server um like you know if it was like you know like some 1% of requests and it was I suppose across servers like including with new server was always taking regression like me but like if it\u0027s gonna be like a major regression of all time we\u0027re signers then so on that that\u0027s that the requirement for solution on the as far as like the design of the system the I think I was personal closest attention version this invention approach I think the advantage of that is that lets us get some deployment Springs with this with something we know will work for the people can play wall and then if something awesome comes along that makes us work better like that like has the properties version we\u0027re more than interested in like taking that as well but this lets us have a smooth upgrade path for those but having like basically the N squared like number of Records with all the different possibilities the same time hey so we got about eight minutes left on this topic so we\u0027re gonna keep it short and Nick you\u0027re the last person in this queue I forgot to mention one more thing also Chris didn\u0027t mention I\u0027m being nice I\u0027ll probably this extension is that um that because we have that the bit indicating it\u0027s mandatory it\u0027s quite straightforward for you advertise yes and I keys with the like some sort of peel and 3070 thing and then conform clients will should reject it if they don\u0027t know about it so like that great quite smoothly yeah thank you I think the on that topic of the they what happens when when it\u0027s mandatory I think it\u0027s some of that will be to be a function of how comfortable are we then if adding that extension is unintentional Ike one 137 is mandatory if that means that clients using that than that happened implemented 137 basically can\u0027t use yes and I anymore yeah when they just I guess it was enough being in echo Eckerson said that that was intended behavior um and I think that where that starts becoming important is that like with 136 I think the biggest challenge is going to be cases where the sheer number of a and quad a record the more that might get used in any one point of time is more than necessarily fit in I think to Patrick\u0027s comment I think there is a bit of a I also do have some anxiety of what of what are going to be the operational issues we\u0027re going to run into of having the a and quad a records that people use be something that comes from another record that\u0027s not a or quad a that\u0027s something that does have a big enough operational change and breaks a lot of assumptions that could have interesting side effects but I think until we try to deploy that we don\u0027t know what the side effects are doing that are yet so to clarify are you "
  },
  {
    "startTime": "01:03:32",
    "text": "concerned about the operational risks of a provider who is able to produce those records debugging your systems or some other operational products outside of that the operational stuff of all the complexities of how the internet works between that like the example would be like the known known case would be dns64 of dns64 relies on being able to rewrite do those rewrite we can put specify that one in the draft to cover that case if clients implement that properly but it\u0027s very likely there are other things we haven\u0027t thought of which were worked by accident and not for an undocumented manners it will start tripping over as we start trying to roll this out sure yeah thank you david benjamin most i just want to echo the like as the clients the performance in sir like for possible performance regressions for yes and i are like a significant concern for like whether we can deploy it and so the it\u0027s already kind of scary that we have to raise two of them and maybe like yes and i records are really slow and would have to measure that if we have to then additionally do this extra round trip like the we would do like at least try the first one also sort of fundamentally like the problem with this multi CDN thing is that you are not managing to correlates two things from cDNA and two things from CDN be and is sort of unfortunate that dns didn\u0027t let us do that but like that is the thing we want and 136 sticks them together and like however you spell that is sort of we can fiddle with that but it\u0027s sort of more in the right direction yep agreed Wester occur I say so I I need to go read the document in greater detail and I\u0027m glad that I was in the room so that I could see this issue propping up so I\u0027ll speak at a sort of a higher level which is that I think this is especially to the chairs if if this is gonna go forward we need significantly more review from expertise that is not in this room there\u0027s some rather large interesting ramifications of doing this type of stuff where we have to some extent your fracturing the namespace right so the dns has considered the this global naming system and and there\u0027s already some fracturing because because of things like S\u0026I and things like that where x.509 MPX certificates tend to have their belief of what the proper name is and the dns has their belief of the proper name and now we\u0027re splitting the address space too and so these danger bells are ringing off in my head but until i actually go dive deep i can\u0027t be for or against it but I do think especially to the chairs we need to go paying a wider set of people to make sure that there\u0027s not other issues here that are not thought about I will note that we learned a lesson from one draft that was involved in DNS recently as well and so obviously we will do that luckily a lot of these people that are proposing this have DNS "
  },
  {
    "startTime": "01:06:33",
    "text": "experts in shop and my assumption is that they\u0027re also consulting with them as well but obviously we will have to make sure that we don\u0027t just spring this on everybody we\u0027re also not hiding it right so Eric line I actually would like to second eric Negron\u0027s dns64 equipment that will be problematic but I had a question so please pardon my ignorance how what are the implications for an authoritative server for a CDN that doesn\u0027t use cname chasing and does do like geolocation so if I have a thousand IPS I could return to you I\u0027m going to return three that I think are in your area do I generate one yes my keys record with those three addresses in it or do I have one yes Nikes record per address and give you those three I do not know I have heard to the server operators that were going to be venting this these particular records speaking as a client it\u0027s I don\u0027t really care I recognize that\u0027s not like a great answer to your particular question I don\u0027t know I Nick Sullivan CloudFlare so to speak from the server operators point of view I spoke with the folks in our team who do this and the way that we construct es and I keys as well as a and quad A\u0027s from the same database so we see no problem with 1:36 and going forward with it if a query comes in for any one of those three types we look look it up from the same database and construct the the correct record and that seems to be pretty straightforward and so with respect to this there\u0027s no we see no operational issues on the server side so perhaps we could you know send a message to the DNS ops mailing list or something indicating that this is both potentially what we\u0027re planning to do this is what\u0027s currently in the draft and seek additional feedback for safety minutes please note that yeah I\u0027d be good to get some more feedback there certainly again I\u0027m not trying to dismiss her belittle anyone\u0027s particular comments but this is certainly a difficult problem to deal with and we\u0027re just trying to find the best way to move forward so there are a lot of other open issues right now some of them major ish some of them sort of minor the list is here I will I guess call out at least two of them in particular the first is that there\u0027s the question as to whether or not we should remove the split mode from the draft currently if the s and I works in a shared mode architecture in which the gos terminating server also is the thing that serves up the content in the split mode it there TLS determining server but the server that actually decrypt CS and I and four is a connection on to a another server that bends up the application data potentially there\u0027s valid use case to that particular architecture it does increase the complexity and actually getting the information from the fronting server to the back-end server is sort of it\u0027s not well "
  },
  {
    "startTime": "01:09:36",
    "text": "specified or it\u0027s a little bit odd currently in the draft case we don\u0027t really have a better way so if we were to remove that it would reduce a lot of the complexity occur the complexity of what you mean it\u0027s complex to have this split mode but this is a complicity does not protect itself into the protocol there was like city in terms right so sorry complexity in terms of you know number of words or the amount of words in the draft it also allow us to do potentially other things like encrypt more than just the SNI in the client hello I know there are people who desire to potentially encrypt a OPN values and potentially other extensions as well so sure yeah I\u0027m actually hoping DK cheese in the room um means so you mean you\u0027re gonna want to stand up probably oh well no I think we\u0027re back open for discussing issues are there like other tribe you sitting down I can okay I think I think in since we have a bunch of other presentations to go through I think in just in terms of managing time we should delegate and so take this to the list guys there\u0027s lots of stuff to talk about so you know please have a look at the issues on github and comment sure so I I just had a look at 1:34 there and that reflects of discussion and github but I don\u0027t think much it says it was a comment for me which I don\u0027t think much is by coming so I think we need to manage the github discussion just gonna list this Kosh and then maybe a little better on some of these okay point taken lots great thank you thank you all right so a lot of the discussion around those two proposals for yes and I it depends on whether you\u0027re actually going to see divergence between any SMI record type and the a or the quad a results I was fortunately able to tap into somebody else\u0027s research database with lots of DNS resolutions to try and find an answer to that the problem is you can\u0027t actually test something that hasn\u0027t been deployed so I can\u0027t directly measure what would happen with yes and I so instead we actually have a pretty decent proxy question we\u0027re already doing two resolutions for the same hostname in a and kwatak so I\u0027m using the database to try and figure out if a and quad a diverged from each other and if they do it\u0027s likely the des and I will diverge from Warner both of them as well so I\u0027m going to a lot of detail about what the database about how its contracted but we have lots of host names lots of resolvers and every run it takes a random selection from those resolvers does lots of resolutions I\u0027m "
  },
  {
    "startTime": "01:12:38",
    "text": "sampling one week of data which gives us about 235 million a and Guate a resolution pairs of course we throw most of it away because if what we\u0027re looking for are potentially divergent cases we\u0027re boiling that down to what CDN where we point it to so if we couldn\u0027t figure out what C D and it was doesn\u0027t help us also there are host names that use one CDN and if you always return the same CDN it\u0027s not possible for you to diverge so you\u0027re not helpful in answering this question and DNS resolutions aren\u0027t always successful I\u0027m not everybody supports ipv6 so sometimes we didn\u0027t get an A and a quad a result from the same resolution attempt for the same server so at the end of it those 243 million resolutions left us with about 2 or 24,000 of resolutions that actually meet the criteria we care about so drilling into that little slice of data we see about 23% divergence which is a lot higher than I was hoping to find I was a little surprised by that I tried to spin it in a couple of different ways to see if there was something that tied it all together if you look at it on a per host name basis there\u0027s like 40% of host names that never diverge and the other 60 do some portion of the time if you spin it by resolver there\u0027s about a third of resolvers that give you consistent results but not most of them hi it\u0027s dkg sorry I\u0027m like I\u0027m I\u0027m just not following can you describe can you just define what you mean by diverge for us please diverge means when we figure out what CDN the a record is pointing to and what CDN the quad a record according to the reference idioms okay so you\u0027ve got this set of 234,000 and my understanding was that that set was entirely divergent so what happens is so no that\u0027s the slice of things that could potentially diverge so we know who we were able to identify a CDN with quad-a and CDN for the a and we got a result for both of them is your questions about it I\u0027m not understanding what the what what your what what the pool is that you\u0027re working from here okay because if they you\u0027re looking for things that resolve to both a and quad a and to multiple CDN ease so reason that by definition divergence what am i what am I missing sorry the ones that I throw away are the ones where every resolution attempted every resolver for the entire run of the sequence even the same CDN so okay those I can ignore because I know I\u0027m guessing they always give the same result but "
  },
  {
    "startTime": "01:15:39",
    "text": "then what I\u0027m drilling into is for a particular pair of resolutions at the same time at the same resolver I got different results does that help it still sounds to me like you\u0027re saying every pair that you\u0027re looking at in this set here is divergent so I don\u0027t understand why it\u0027s 23% so I\u0027m sorry guys maybe I\u0027m the only person who\u0027s not understanding it but maybe I can ask a clarifying question here that might help okay if I if I understand correctly what you\u0027re saying is that the host names don\u0027t resolve to the same CD and at different points in time but also potential different resolvers and when you\u0027re looking at ones that are returning both a and quad-a results when you are looking in the next we know them for divergence you\u0027re looking for them to be at roughly the same time and from the same resolver but yeah so the the client is running the test virus an A and a quat a request off to the same resolver at the same time the ones that I\u0027m discarding every attempt over the course of the week from any resolver from any client always give us the same CDN so I don\u0027t expect to ever be able to observe divergence theorem because it always goes the same place you\u0027re the ad you ought to go yeah I mean this related to the point I wanted to make which is that we need to be clear about what these data are representative for and you sort of described it and that your sample space is already kind of constrained and you know yes this is a high percentage number but we may have been throwing out some other cases that yes if we don\u0027t know which CDN thing corresponds to or we don\u0027t know that it is a C and at all and by throwing those data away we\u0027re sort of increasing the percentage number that we come out with and I kind of kind of compare that on my last slide yeah and there\u0027s the idea being that you know so we\u0027ve got this set of stuff which is like a sense which is data points that are across space and time and you\u0027ve used that dataset to figure out your to try to figure out when there are multiple scenes and that\u0027s necessarily lossy and we just don\u0027t have to assume or make inferences about how reliably we can do that yeah and it\u0027s only within that space of data that we\u0027re actually looking at these percentage numbers correct and I think as John was saying when we\u0027re actually trying to get something that diverges that\u0027s only if it\u0027s diverging at a single point in space and time whereas we\u0027re using this broad set in space and time broad in space and button time to determine if there are different CD involved correct I I think probably the longer a time period I look at the more representative this would be when I tried to do a month it told me I was out of memory so I scaled it down to a week and then it\u0027s not just limited to folks with multiple CD ends it\u0027s specifically then constrained because we need an artificial sort of second record type in the test people are doing both a in Quon "
  },
  {
    "startTime": "01:18:40",
    "text": "a which i think is the first part of that multi CD n is actually that people were looking for for the problem but the second constraint is actually kind of artificial to what the working group is considering so I\u0027m actually curious how big of a filter that last step is on the multi CD and data set yeah definitely that\u0027s sort of one of the places where we can\u0027t really be sure how much we\u0027re distorting the data by you filtering it this way correct so but I\u0027m so I understand the aim the quasi percent off you know more or less in parallel and there are a lot of resolvers in the data I\u0027m trying to understand like what these resolvers are they the authoritative or these different we know they and they are open resolvers at different points on the internet we have a database that we send off request to and how many of those I mean you can move ahead newest line I think there was a lot of them right yes there are about 4,500 total that we use for any given resolution attempt we send it to a subset 500 selected from those 45 so those are how your networks they\u0027re not open open resolvers they\u0027re open I open or sellers open open open 500 room that\u0027s I\u0027m awesome okay so one of the one of the suggestions has been that this shouldn\u0027t happen that often if it\u0027s in parallel because because it\u0027s not really in parallel one of them gets there first and that a cname ought to be cached or there ought to be some back-end connection collapse in happening to make that cname get cached it would then prevent this from diverging and you\u0027re saying that\u0027s not happening or eight peers that\u0027s not that that was my supposition as well I thought that this should be fairly aware because either the see names already cashed or you collapse the resolutions and so you would maybe potentially see this when they happen to straddle the expiration of your caste name or maybe it was cold and it said both in both lookups backward I have heard that there are some pathological stub resolver \u0027z that then round-robin amongst multiple upstream resolvers which almost guarantees divergence are you controlling this stuff so I guess we just talked about what the recursive let\u0027s what is the stub in this test so that so I think Eric who is behind you than is the one who told me about that so I may let him go he\u0027s gonna make sure we cut the line to the exam like one example cases you is it is that the research the recursive resolver service interface you might talk to may actually be load balancing across the pool of recursive resolvers and each have independent caches and because there\u0027s because it\u0027s connection is less and UDP there\u0027s no necessary reason that you necessarily do determinism in terms of which of which mission of which back-end server a given request goes to so if you just doing pure round rod than that stateless across to all of them true requests may go to two different ones back-to-back those make issue out queries if those and then though yes and "
  },
  {
    "startTime": "01:21:43",
    "text": "then there\u0027s nothing to cache do we think that\u0027s the right model for client facing traffic yes so I\u0027ll just say that that\u0027s what Eric\u0027s describing is common very common on whether it\u0027s the right model or not it happens yeah we\u0027re trying to get a number right for the model isn\u0027t the right so I guess thanks for doing this Mike this is like great like like everyone else is too lazy so it\u0027s really awesome you did it I mean I think I guess what I would say like this is scary and so I think I definitely love to see like more people take in it\u0027s like I think I\u0027d love for you be wrong right if you\u0027re like wrong in this later point O or one that be like so um it\u0027s like you know Patrick like definitely like if you want to unzip it here they\u0027re like that\u0027d be sweet but I think like you know like he was he pretty wrong for like the numbers could be acceptable um so yeah so I don\u0027t know thanks thanks and like people people who are like sort of beating on this like you know like go to the mic afterwards and see if you can like you know conquer the study that makes you happy yes my thesis supposed to be looked up over TLS right would that make a difference using TCP about UDP it might very well I don\u0027t have a way to test that Jenn I\u0027m good I think this is I mean the data is obviously great and and I do they couldn\u0027t accept and I love that Ian caudate might be qualitatively different than other things I don\u0027t know exactly how do how to think about any further or how to turn that into an experiment but that\u0027s a caveat we should keep in mind I mean the only thing I can think of is if we could identify some other type of DNS record that is commonly present in X text whatever so so you have on understanding how one could replicate this experiment now with like maybe the question is even asked about you know so I\u0027m right root is never know I perhaps um one thing what you do that would be ideal what would be some yes yes you question how do you how do you distinguish whether people or what were the things on a CDN person s database yeah so basically what happens is if there\u0027s a cname chain there are fairly predictable projects is that you can run against hosts in the cname to say oh this has a KA and Akamai yeah so we don\u0027t we don\u0027t get the seen eum\u0027s unfortunately you get a street if you get a straight IP result then you look up the pointer record and those usually have the same person in patterns right right right okay so I mean it\u0027s one thing but they\u0027re two ways to look at this one way to look at this is what\u0027s the you know what\u0027s the space like how often is happen in general and I were to look at it as Holland\u0027s it happened people who like our potentially exposed right so that would be my last slide yeah awesome "
  },
  {
    "startTime": "01:24:44",
    "text": "so I guess so one thing we could do I think like my Firefox to do is if you want to give us a list some people who are exposed we can go and ask how often it happen for them because we can just do a force resolution and then compare the an equality and and those numbers and then will at least get work it ever got cross I mean that won\u0027t tell us I won\u0027t tell us how often it happens overall but if we cross car like with your numbers and I gotta get it like if it happens like you know sometimes people expose that\u0027s pretty bad okay so I hate to do this but we\u0027re gonna grab because it\u0027s presentations short because we have four more presentations so we should definitely take this negative on that slide you can go to the last slide okay so we\u0027re talking about how does this affect from the browser perspective versus the surfer operator perspective from the browser side multi CDN the hosts are a about 1% of all the hosts that we resolved and it happens about 20% of the time for them so from the browser\u0027s standpoint this is about two-tenths of a percent if you are a multi CDN site it happens about 20% of the time for you which kind of stings for your site\u0027s performance okay and then we\u0027re short on time and this is a lot of content so I\u0027ll try to get through it opaque this is this is this is a presentation about a password-based authentication system for TLS pink we\u0027ve had these before SRP is one of the first which is an Augmented password authentication authenticated key exchange it\u0027s widely implemented using a bunch of protocols but not necessarily on the web dragonfly is another one that was recently brought through it\u0027s been published as an RFC s and independent submission including integration into a TLS 1/2 and 1/3 and so there are some so if we look specifically at SRP there\u0027s a couple issues with that one is the salt is sent in the clear which is actually a problem with almost all of the previous pics and so this leads to potential pre-computation attacks if you get a the server\u0027s database of passwords you can have precomputed rainbow tables to to check password databases rather than having to do a brute force the security analysis of SRP is unsatisfying I know there\u0027s several different iterations and lots of implementation problems finite fields no elliptic curves it\u0027s kind of an awkward fit as described for TLS 1.3 which is kind of more cosmetic than anything but "
  },
  {
    "startTime": "01:27:44",
    "text": "when it comes to post handshake messages this requires a renegotiation okay so that\u0027s just kind of a background on on SRP in some of the original pigs and opaque was presented last year and this is a new paper that has it\u0027s a methodology for describing how to build a secure asymmetric pigment cake in conjunction with a authenticated key exchange like we haven\u0027t TLS 1.3 so the way that this is presented by a crotchet and others at real crypto there\u0027s a paper published there\u0027s also a proposal that\u0027s been through several iterations at CFR G so the way it works is you take an authenticated key exchange like TLS 1.3 and combine it with the primitive called a no PRF which is an oblivious pseudo-random function i\u0027ll give some intuitionist how that works and if you combine them together together in the right way opaque describes how you can you can get a secure a pic out of that as a peek description primitive it has some really nice properties such as security proof it is the first one that\u0027s provably secure against pre-computation attacks of all the pics that are published as well as it has a very efficient implementation based on elliptic curves so as I mentioned opaque this is this is a draft that we wrote for TLS 1.2 LS working group but it depends on a bunch of other primitives that are currently going through the CF RG specifically the opaque overview the OPR F which is currently an individual draft as well as a hash to curve which is a dependency of that o PRF draft which is currently an eye RTF document for CF RG okay fundamental components o PRF this opr F protocol is it allows two parties to negotiate a value based on something that the client provides something that the server provides the output of this protocol is not known by the server so you can I\u0027ll sort of explain it a little bit later but um but but essentially it\u0027s it you have a server with a private key and a client with a value like a password and you get a result that\u0027s based on both of those that involves a two two round when I guess one round-trip flow so in in the construction for opaque this OPR F it the output of the o PRF is used to encrypt an envelope which contains a set of public keys or a that contains opaque keys which is if you think in TLS when you\u0027re talking about key negotiation you have your signature keys and you have your key agreement keys this is another pair of case it could be either signature keys or key agreement keys "
  },
  {
    "startTime": "01:30:44",
    "text": "these are keys that are tied specifically to the user account on that specific server so in in terms of what\u0027s inside this envelope you have the clients TLS 1.3 compatible private key as well as the server\u0027s public key so I\u0027ll go a little bit into the math I know this is worked or tight for time but um to get a bit of fundamentals as to how this what this relies on it\u0027s the same sort of thing that TLS 1.3 relies on so you have a prime order group so for example the groups the group of points on elliptic curve such as P 256 these group elements are denoted by capital P or pet capital Q you have scalar multiplication which is adding a point to itself n times scalars will be lowercase letters and then there\u0027s an additional element here called hash to group element this is something that almost all Pakes have taken to account and it\u0027s how you get from a password into the group this is as I mentioned currently being gone over at the at the C FRG so it takes a scaler and outputs a red group element that is uniformly random the O PRF flow kind of looks like this client takes a password blinds it sends it to the server the server applies its private key operation and you get a blinded o PRF out and then you can unbind that o PRF so this password and the bonne blended o PRF are tied together by the service private key and same with the blinded password password and the blinded o PRF out in terms of Mattia is just a random value so you take your password you hash it into a curve element multiplied by the blinding value that\u0027s your o PRF one value send it to the server the server multiplies by its secret scalar sends it back then you divide out the blinding value and then you\u0027re left with the server\u0027s private scalar times the point that represents your password and this up your OPR flow is the fundamental basis for opaque and so this the final output there is what\u0027s used to encrypt the envelope that contains the extra keys okay going forward or backwards here we go okay so the user creates the envelope during password registration it runs the o PRF and puts its own private key in the server\u0027s public key into the envelope sends it back to the server and server saves it with the identity of the client and for this to work as part of a handshake the user provides knowledge of the password by being able to open the envelope and use the keys that have the private key inside and the server knows the associated public key so these private keys are used to authenticate the handshake and there\u0027s two proposed well there\u0027s two main proposals in this document of how this can be done the opaque private keys can be used in place of the PKI keys so if you think of the certificate verify you rather than using "
  },
  {
    "startTime": "01:33:45",
    "text": "a certificate and private key there you take the opaque private key and use it in the certificate verify in client authentication or option the other way is to take those keys and mix them with the key granite keys in a kind of four key based key agreement and and use that as part of the key schedule so this is sometimes called Triple D H and there\u0027s a more efficient version called HM QV which is I believe has some IPR but um Hugo said it was fine I think we\u0027re probably safe from it for now but in any case that details details are in the graph so how does it work in place of P khakis this is that the instantiation called opaque sign so you\u0027re opening keys the ones that are in the envelope our signature keys the client sends its identity in an extension as well as the first OPR F and the server sends back you know a certificate message that has the it\u0027s it\u0027s piece of the OPR F as well as a certificate request for that identity and the certificate verifies the server\u0027s opaque key that\u0027s associated with the account on the and the clients then completes the client auth flow with its certificate verify is the client so pinky so in this instantiation there\u0027s no PK at all anything that you do in terms of certificates in TLS 1.3 are replaced with these opaque keys which isn\u0027t kind of nice the other way is to take the result of these keys and throw them into the key schedule so in this case the opaque keys are TLS 1.3 key shares so the client takes the identity OB RF key and then the OPR F key is a the key share that is used for the key negotiation has to match the type of key that was in the envelope so you have essentially two client key shares and two server key shares the server sends back in encrypted extensions with the OPR f2 keys encrypted extensions is derived as the normal key key exchange flow works so just with the doing the regular diffie-hellman and so the main mess the master secret is actually derived there is a specific place where you do an H KD KD f extract of zero to get the master zero master secret and and the handshake secret from above it and what you do here is you you add in your place zero with this value K which is essentially a 3-way diffie-hellman between the ephemeral keys and the opaque keys or its mq v is the same sort of thing it\u0027s just more efficient to do so and the interesting thing about this instantiation is that you can also have a certificate and do regular PK PK I off both client or server and the reason why this is relevant is there\u0027s certain applications where you could think that you would do the pake to do individual "
  },
  {
    "startTime": "01:36:46",
    "text": "off but you would also have some sort of higher level off like all of these the devices belong to the same or we\u0027re manufactured by the same manufacturer or something like this you could do some PKI system and augment it with pig ok so these are the two in handshake modes and we talked about using pigs for in handshake modes because you know you\u0027re activating devices or things like this but when you think about potential uses on and say the web you\u0027re not going to want to do have user experience of typing in a password before any of the content loads so having a post handshake authentication option here is is is pretty interesting so because a pig son is really just replacing the pka off it actually Maps really nicely on to exported authenticators so the client can create an export Authenticator request with its identity it\u0027s au prfd the server can send back an excellent export Authenticator with it so PR f signature key and then also initiate a second export authentic airflow where the server asks the client to authenticate itself and so you get one round-trip flow and this this uses the exact same structure that opaque sign does okay so properties here a picks on is there\u0027s there\u0027s no username privacy so that be the identity itself is sent in the first first request this is a problem with both of these in handshake forms there could be some ES and I type mechanism that could be used to protect this opaque sign does not have PK off and PKI off in the handshake and opaque sign-in exporter authenticators because it happens after the handshake it requires PKI off and but it does provide username privacy and it can fit potentially nicely into things like hgp to you could think of rather than having an additional certificate that uses export authenticators you could have a opaque flow that is part of hb2 frames going forward so as a recap the strap proposes a new password based authentication mechanism for TLS 1.3 and opaque is the first secure apec protocol prove of provably secure against pre-computation attacks there\u0027s multiple constructions and this is this is I guess the main question for the group is is this interesting for the working group to pursue as as an alternative for for SSR P or for having some sort of Paik that works into u.s. 1.3 that\u0027s that\u0027s the end of the slides and again we\u0027ll have to keep it quick benkei doc so I agree that just repeats kind of lousy and it\u0027d be nice to have a better option I don\u0027t know we could really do here other than to say that people should read the draft and think about it more like that\u0027s exactly where we\u0027re "
  },
  {
    "startTime": "01:39:46",
    "text": "gonna go pinks we have a long history of needing to make sure that they\u0027ve been blessed slash looked at by the CFR G before we give him a go point in the whole nine yards so yeah yeah I was gonna say much what you just said namely this seems interesting I\u0027m obviously too much lag gaiz repetition and this has been well reviewed from I understand Cain tells me it\u0027s good I\u0027d loved I think we shouldn\u0027t take it see how far gee you know signing off I think the other thing I\u0027d want to see is some people telling me they\u0027re an implement deploy because one of the problems that the previous thing has been like people didn\u0027t want and so I think as I just want to note there are locations where this is gonna be used John it\u0027s a little hard to fit in the browser context though maybe we could do it at some point but like that\u0027s what I want to hear yeah okay jumping in again like specifically to clarify I don\u0027t think it\u0027s terribly useful to ask people you know we just looked at this slideshow make a decision right now so I think we should probably take most of this offline but I\u0027m still happy here it can he has to say Kenny Patterson and thanks for bringing this forward I think opaque is a very interesting protocol that\u0027s had a lot of formal security analysis I guess I\u0027m slightly concerned about the integration with TLS 1.3 because now you\u0027re bringing together all kinds of other Tiff\u0027s Hellman shares and signatures and so is there any plans for doing some kind of formal security analysis of this integrated protocol none officially but I\u0027m gonna be talking with you go about that after this I read Sol\u0027s it\u0027s really cool you\u0027re good it goes really good but nobody deploys srp and existing things unless they\u0027re on light poles already that have got a 20 year lifetime so I don\u0027t see a need to put this into TLS so to eccrine is Richard Barnes from Cisco head on for this one so it\u0027s a curious question about implementation the reason art and my name and no and Friel\u0027s name are on this Travis because we\u0027ve been looking for something of generally this form you\u0027ve seen me on the stage before with some earlier paper puzzles like we need some cakes and stuff for some of our low touch onboarding for devices with limited interfaces so I think we\u0027d be looking to deploy something like this and in this context to help with IOT like onboarding oh and also on the browser context I think that\u0027s I think there\u0027s some possibilities the export Authenticator flavor you could imagine some scenarios if there were interest by web developers in this in exposing a an API that could pipe through to that sort of export Authenticator mode like you there\u0027s obviously all the standard you know Web API considerations there but there\u0027s there\u0027s at least the logical connection there seems to be mum Thompson we have some use for this or something like this unfortunately my understanding of how this operates means that it isn\u0027t really suitable for the use cases that we had and that they\u0027re fairly similar to Rich\u0027s one so I\u0027m not sure that I understand it properly but I couldn\u0027t "
  },
  {
    "startTime": "01:42:47",
    "text": "work out how to make this map onto the sort of use cases that we were talking about and the similar one still I think what I think you\u0027re talking about which is you have low entropy values on devices and you want to be able to boots drive them up into into a security context one thing assuming that we can resolve that then I\u0027m pretty happy with this one but I didn\u0027t see how that would work considering that you have an enrollment stage that that exists in opaque that is a little awkward and honestly it sort of assumes that you\u0027ve got a pre-existing relationship that in this context you don\u0027t necessarily have so I\u0027d like to understand what the requirements are for bikes and how this one would fit into that context a little better so we should see and I should come and talk to you and Richard and they say what what happens there is this pre-empting the CF RG discussions on on pikes no I don\u0027t know because we never pick it before they said yes so I mean he\u0027s talking about and hey look at it I think it\u0027s yeah this is this is more about this is how you would fit in it fit it into TLS 1.3 how does the Sunnis read together with a couple of other companies have standardized the password based on syndicated key exchange in thread which has been used for device IOT device onboarding we actually brought it to the IDF and was send away it\u0027s still deployed it may also be something to look at to see whether that does meet your requirements because it was developed also by after looking at SRB which didn\u0027t which had the same issues as you found out but it\u0027s a different scheme you didn\u0027t mention it on on the slides I wasn\u0027t appointed to the list it\u0027s and there\u0027s an inspired idea draft around please do Alexei Melnikov please come tomorrow to see FRG that would be initial discussion comparing them including OPEC so bring bring popcorn folks it\u0027ll be fun well we might not have enough time for popcorn but there will be initial discussion all right so I am presenting on behalf Douglas and Shai who unfortunately could not be here due to academic and timezone reasons so because we are very short on time I\u0027m gonna have to hustle through some of this and just kind of focus on the high-level goals so this is all about hyper key exchange in TLS 1.3 as you may or may not be aware there\u0027s currently a lot of interests a lot of interest in this area in particular there\u0027s a number of drafts describing how you might do it with TLS 1.2 and 1.3 there are actual experiments going on in particular the ones by Google there are implementations by the open quantum safe project that Douglas is working with there\u0027s even some work and a similar framework of sorts for Ike as I understand that and in general people are trying to build or future-proof the "
  },
  {
    "startTime": "01:45:50",
    "text": "protocols such that if you we had a you know blessed post quantum or next generation ehh algorithm we could potentially slide it in and in the meantime just begin doing experiments alongside existing classical can exchange algorithms so of course the motivation for this is the post quantum threat but this is more so focusing on just how would we add multiple key exchange algorithms to TLS 1.3 should we desire to do that a specific non goal is to do any kind of algorithm selection or recommendation of any kind of sort specifically and especially because the this competition is still ongoing and see how hadji has not decided that they are going to take on any particular post quantum key change algorithm and just I hope that\u0027s very very clear this is simply just the framework as to how you would extend the existing protocol to incorporate multiple key exchange algorithms and so there\u0027s a couple different design parameters you could go or just axes along which you could potentially slide things in so for example you have to answer the question is how you would actually negotiate use of a classical and post quantum khi algorithm there\u0027s the question of how many algorithms and key change our energy we want to combine typically wishes to but I don\u0027t know in crazy world maybe you want like ten incomes in a particular connection that might be overkill probably you have to specify exactly how you would convey the public keys necessary for the key exchange and importantly how you would mix the result of the key exchange into the traffic secret and into the key schedule and in doing this sort of looking at these different design trade-offs it\u0027s important to keep in mind that different aspects of their implication on the protocol itself its performance and potential software implementation complexity for so for example does it introduce any extra round trips ideally no because that would be probably non-starter how much duplicate information is sent in both the client hello and the server hello or whatever and check messages are necessary and how abusive is the required changes to the TLS key schedule ideally want to keep things as simple as possible for myself for implementation perspective and from a changes to the protocol perspective such that things like formal analysis just kind of fall out at the bottom so I\u0027ll focus on two particular design parameters because the other ones are sort of related and potentially I can just kind of defer you to draft to read them the first is I would actually negotiate use of a particular classical and post quantum variant so you could negotiate them individually so for example have two separate lists one that lists all the classical algorithms one that this all the post-mortem algorithms that\u0027s probably not great because especially if you\u0027re alright sorry it might be good "
  },
  {
    "startTime": "01:48:50",
    "text": "from a reducing duplicate information if you want to you know have a design such that you can mix you know one post or one classical algorithm with potentially multiple post onto my rhythms you might only in this case send one most quantum key share in the client hello whereas if you were to have to both gone through algorithms running it in a particular experiment or in a particular connection you might have to offer up multiple key shares if you were to initiate them in a combined fashion but from an implementation perspective especially with the experiments that are ongoing right now it\u0027s simply it\u0027s much much much easier as was pointed out by the list by Martin and others to simply negotiate them together such that you offer up a new name group for example that says I am doing curve decline and whatever fancy post comes from algorithm you wants to negotiate the key combination issue issue concern is also pretty important the main requirement is that you want the desired or the the security of the session key to be robust and by robust I mean effectively if one of the key exchange algorithms is broken it reduces to the minimum security level of the the combination of the two so if curve devanam breaks with the post quantum is fine and it reduces the postman over than vice-versa and so there\u0027s different ways you could achieve this by mixing in the result of the two key exchange steps into the key schedule one of which is to simply concatenate the results and shove them into the key schedule as currently done today in the experiments of implementations you could KDF the the two or two or more depending on you know what you choose for how many algorithms we wish negotiated KDF them together and then feed that result into the key schedule you can even explore them together and there are papers that describe the security of sort of abstract abstract models of these particular competent combiners for Kem\u0027s and specifically chems and i don\u0027t know whether or not that includes diffie-hellman based games probably does hugo\u0027s or can you say kicking said so and the last option is to you could potentially stick it in the zero slot which we just heard that potentially could be used for opaque integration if within she also used for semi static in so probably don\u0027t want to put it there it\u0027s basically because it\u0027s you know not the that has potentially other places where it can be used yeah yeah I mean just in terms of like implementation convenience I mean first of all I think like pretty clearly like the UH fake kind of Yoda Cove is obviously the most DC would implement it that\u0027s we knew we noticed what Google did in their last round so fornication convenience if you do that then clearly you clearly you want to follow one of like you know one of one two or three "
  },
  {
    "startTime": "01:51:52",
    "text": "yeah um yeah probably isn\u0027t read matter which one of those it which one of those like the ice fog refer we\u0027re talking to do um but um like yeah I think like like like we\u0027re much more likely to do it if it looks like that yep agree yeah mom Thompson I I think someone pointed out on a list that the last one means that you don\u0027t get the results of the PQ exchange in the handshake keys I think that was someone put it on a list that was that was a problem I think it depends on where you actually slide it into the key schedule but yeah because that\u0027s off to the Han Chinese derived is Richard okay so yeah there\u0027s these are the different ways you can do it there are some more technical questions to address that potentially may or may not be within the scope of this particular draft depending on where they want to go for it and specifically with relation to the Kem\u0027s that are being discussed in the the NIST and in this competition and the possibility of Kansas being added to else 1.3 later it\u0027s unclear whether or not any CEM will suffice or whether or not any certain criteria for the CEM so in particular so if you have CCA secured Kem\u0027s they are certainly more expensive than CPA secure ones and there\u0027s also comes that have a non zero none then click nonzero probability of failure so it\u0027s unclear how that actually fits into tos and what you do when you have a CEM decryption failure so maybe the outcome of this effort is that you don\u0027t use those particular chemistry hibbett you know maybe make a requirement that TLS must not use things that potentially fail randomly or with some probability and I guess from a procedural perspective because there are several efforts floating around both in the ITF and see us and an IKE the there\u0027s a question of as to whether or not the working group should be actively working on this sort of thing right now and in what kind of capacity whether that means you know looking at the different design considerations is just trying to lay out lay them all on the table to evaluate the design space or to focus on selecting one particular incarnation that you know fits best with TLS 1.3 or do nothing and just kind of let the experiments run as they are kind of relatedly a question to the workgroup is whether or not as sort of a input or feedback into this competition whether or not we should work on defining a set of requirements for Kem\u0027s or any other you know next-generation case change over them that would be necessary for it to be and you know a couple - TLS 1.3 particularly going back to the ones that "
  },
  {
    "startTime": "01:54:52",
    "text": "have nonzero probability of decryption failure so yeah merton yeah not in some Sun I think this is pretty valuable stuff particularly like the fact that there\u0027s been some citations of papers that have done analysis on these things and and so showing that various approaches to which one\u0027s work and which ones own and all those sorts of things I would like to see us work on something like this I would like to see the document say which one we\u0027ve picked and which one will be recommending having an appendix with the with the alternative isms and some analysis of those and and perhaps some explanations of why we thought though they were suboptimal would be extra valuable but ultimately what I want to have is this is the way we\u0027re gonna do it in TLS so then we don\u0027t have that have to have that debate for every single PQ thing that comes along when it comes to non-problem nonzero probability of failure no thanks yeah just remember we got to keep this quick we got about five minutes and we still have two presentations so if you can send it send it to the list that would be appreciated very quick then Claire I think there\u0027s quite a few drafts that have come up today one of which that say we need to define the new way of bringing in a new key into the TLS handshake at some point or and if we had it in the same way that we have exporter keys we should have an importer key and just have a place for importing keys into the TLS handshake and I would satisfy the needs of about three of the drafts we\u0027ve seen today hi to be a spider I find it hard to put them into two groups like the drafters talking about traditional and next-generation ones actually like Mac Elyse has been around site since 1978 and it\u0027s also in Anna\u0027s competition right now so what is it traditional or next-generation I\u0027m I wouldn\u0027t I don\u0027t have a particular ending on that topic the the we were just using two words to define two possible candidates you could use for example 2 for 9 in P 56 if you wanted to like the framework was what we\u0027re trying to get at with the framework is a weight just fit in to K change algorithms be it to classical to next-generation to post quantum to fubar whatever well drop doesn\u0027t say that right now so but I think that\u0027s I think it would I mean I\u0027m sure they would welcome some clarification around that particular language but I don\u0027t think it\u0027s their intent to limit this specifically to just classical in next-generation so for whatever next generation is there Kenny Kenny Patterson two quick points of information the decryption failure probabilities for the NIST candidates are all pretty low like 2 to the minus 40 and I\u0027m lore so you can just fail the protocol and restart in that case right you\u0027re more likely to have a TCP error of some kind or something and the second one is the the research community the crypto research community is still going "
  },
  {
    "startTime": "01:57:52",
    "text": "around in circles trying to figure out what the right way to combine different key material is and so it\u0027s still very much alive research topic which suggests it\u0027s slightly too early for the TLS working group to be really pinning down details on the right way to combine different key shares I think the papers you cited are like from the last year or two yeah and we\u0027re still developing security proofs and particularly in for quantum enforcer ease for those kinds of things yeah give it a give it a little while to settle time the arguments that were given I don\u0027t know if it\u0027s actually written in draft but the the sort of connection was that the in the abstract sense that the combiners were proved secure and the papers for example Douglass\u0027s paper that the audience was sort of just carry over to us by virtue of the fact that you know the the key derivation Achilles works a certain way so for example in the dual P rough example they give the naive broken case where you just take the like HK DF of you know shake secret share one and secret share two and use that as the traffic secret but that doesn\u0027t work due to some reason so their recommendation is to take the PRF of the KDF of key share wanted to in addition to the ciphertext the chemist I protect so and they say like that that\u0027s fine in TLS because there\u0027s this drive secret and guess the transcript and that includes Ajay protects him I think the audience for this particular conversation is very small so we will talk about it over here maybe that sounds good thank you thanks Hannes you\u0027re up you got about two minutes luckily this is that\u0027s the thing could be fast this is about IOT and I\u0027d be related topics so we are trying to find out how we can optimize the key exchange one area of investigation we are doing is besides other CTLs etc is how to shrink the size of the certificates and this proposal uses Sieber web tokens in TLS instead of the certificates if you haven\u0027t been following see WPS and gwt\u0027s there are a couple of RCS out there that define the JSON web token virtue so was originally originated in the OS working group as a way to encode access tokens and was later extended with so-called a proof of possession extension which essentially allows to embed a key inside that JSON web token not long ago we then found out that that may be a little bit too heavy for IOT scenarios and use the different encoding based on C bar which then turned into the C bar let dokkan which is what I\u0027m using here JW these are widely used in today for all sorts of identity scenarios and authentication scenarios there are set of claims defined in those and are carried in those tokens they are registered in the Vienna so you can look at the many of them for the JWT what is "
  },
  {
    "startTime": "02:00:55",
    "text": "interesting is those tokens work in a somewhat generic fashion they are using both symmetric as well as asymmetric cryptography in terms of how they are protected or how the claims are protected and or what key is embedded inside so quite flexible here\u0027s an example and this is obviously not the binary encoding but the diagnostic syntax and you can see the key an asymmetric key that is embedded inside and yeah so there\u0027s there\u0027s some other examples in the in the previously mentioned documents for symmetric cryptography as well I\u0027m not going into those and the proposal of this trap is to use the certificate pipes registry which has been established some time ago which is also used for the raw public key and and Nick was just recommending it to reuse it for also for these password based authenticated key exchange as well and it\u0027s used for the bra public key which is obviously very small but we found out that that\u0027s a little bit too aggressive for aggressive for some of the deployments because it lacks obviously intentionally it doesn\u0027t include more than the key but as you can imagine for some deployments that\u0027s our or for many deployments that\u0027s not really possible picture I stole it from Constance presentation if you have been at the SEC dispatch min being earlier this week you saw that some people are trying to replicate our profile the CW T\u0027s - and contain information that you would typically find inserts why we wrote this up is because I\u0027m trying to find out whether some of you have also been looking into this and working the IOT space and want to figure out on what the implications for code size ran over the wire overhead etc etc and so if you want to do that drop me and drop me a message the document is extremely simple obviously it just it registers the pipe but one thing it also does it talks about how to do the map or the matching between what is found in this CWD which is for example provided by this server side for a server side authentication in the subject claim with the s and I provided by the client side it focuses only on pop tokens not like in many of the older OS deployments which use para tokens chim shot worked on implementation of faith over the weekend or at the hackathon and found it quite easy to integrate but he hasn\u0027t gotten to that pop usage yet so drop me an email if you care about this obviously not ready for buying time an experiment quick question it seems like some CW T\u0027s are certificate like the possession ones but many are not um so I think your last point in previous let me have addressed this is there need here to kind of "
  },
  {
    "startTime": "02:03:56",
    "text": "profile down CWT to be appropriate so so that only the ones that are certificate like end up in TLS yeah yeah that\u0027s text also included in in the documents or it\u0027s not meant to be used for for example bearer tokens or other things that are not fit for that purpose I think that would be a security problem so will over time we\u0027d like to ask for your indulgence we do have one more presentation so if you could stick around for another five or six minutes before you run off to dinner that\u0027d be great to be tree hello I want to suggest one more solution to cheat the Perry speaking about the pie I mean the bay a partner as a part of censorship of content speaking about HTTP there was we have to notice that if those who provide censorship are ready to filter the content by IP address we can do with it anything other public information is hostname present in certificates which is encrypted when there is 103 is in use we also indicate hostname in SMI and the antithesis an IE can be suspicious itself and blocked itself and there was information but as far as I know it\u0027s not infirmed that that was a censorship based on encrypted SMI usage and there is a very suspicious idea of this oh that a white listing we don\u0027t block content if we see that hostname is in this or that white list so I suggest a way to try to achieve depay ought to make it so it\u0027s complete implementation harder as the standard does not require that SMI should be a real we want that SMI should allow to determine we first name we wanted to access we call this name through hostname and fake name can be synonymous published in for example in DNS so the client sends fake "
  },
  {
    "startTime": "02:07:00",
    "text": "it\u0027s an I in its it\u0027s an extension it goes in clear-text then when paris 1.3 is in use we get a normal handshake and this solution seems more or less reasonable more or less reasonable phobic when I\u0027m Kissin ie is sensor itself very of course it\u0027s possible not to deal with fake SMI it\u0027s possible to mine names but a fake ass and I can go for free and you hey and in most case you have to pay for registering domain that tend to live of fake ass and ie can be relatively small if it is delivered via Venice and it\u0027s not a problem to implement fake is an eye base solution in encode neither in server code no in fine code so the idea is so this the simplest ID is we just published in special record fake ass an eye that allows to implement the real hostname and server responds with a certificate containing real hostname but as it\u0027s encrypted it\u0027s unfiltered maybe it\u0027s possible to generate some rules rule providing to generate some focus of some fake names maybe it makes sense to use one-time fake name though it can be very expensive and maybe we can issue substitute Achatz but it seems more difficult solution here is the link to the draft itself and to github if anybody is interesting feel free to contact me thank you Christian we team are meaty we we did a drafter putting up requirements and known attacks against values sni encryption methods have you run your solution against this list of attacks sorry the the current dwarf I mean TLS is annoying yes a big Sun I yeah yes yeah as least "
  },
  {
    "startTime": "02:10:03",
    "text": "not not the es a nice solution the es in a s ni requirements as the least of attacks there that unknown against values s ni obfuscation or encryption systems and I\u0027m asking whether you have you black you have done that or you plan to do that as check your Papa\u0027s solution against its list of attacks okay I will I will yeah thank you III will check thank in particular I think that you have an issue with replay attacks and discoveries and six at that okay thank you very much good point Roy so two comments the first one is I think you\u0027re wasting your time because the devices to do this can do the same thing they can have a database so it\u0027s you\u0027re just moving the arms very slightly for it and it\u0027s you wasting your time second comment is that censorship is not always bad I have children and I have parental guy in software so that kind of censorship should not be avoided by this mechanism well I will live the second comment without reply regarding the first comment that proposes solution is a very cheap for a client relatively cheap for a server and being used widely it\u0027s it can be very expensive for for as centralized that censorship mechanism Erick Nygren I think what like yes and I I think the big challenge with this is gonna be configurability especially you\u0027re not going to want to have people having to do yet another DNS lookup ahead of this and a lot of the yes and I type issues in terms of DNS records not being aligned together gonna also show up with challenges um it this there are cases we\u0027ve talked about this in this passport combined with other things this could be useful like if this was an attribute of the Nikes record or there\u0027s a draft somewhere for a alt service sni attribute which covers which was covers this use case and talks a bunch a bunch of this which could also be useful on the one case it became very useful there in the discussions and on that matrix was to be able to indicate that when you have a wild-card cert that you\u0027re going to be going against to be able to specify something like wildcard at example.com rather than the 4s and I won just cuz that\u0027s a fairly easy win if there\u0027s a way to provision it it\u0027s just provisioning it\u0027s hard oh maybe maybe all right well thank you very much Dimitri and thank you all for staying over told you um thanks let\u0027s go forth and TLS "
  },
  {
    "startTime": "02:13:09",
    "text": "[Music] "
  }
]