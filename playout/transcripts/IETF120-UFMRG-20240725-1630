[
  {
    "startTime": "00:00:43",
    "text": "Chris, could you try and join the room from the like full client and then do an audio chat? Sure. I kind of hear you There's that you? That's you, yeah. Sorry try and join the room from the like full client and then do an audio chat? Sure. I kind of hear you. There's a echo going to do, yeah. That's me. Sorry. Ah Whoa. Oh. Oh about that Of all the terrible things that can happen. No, I can't, so because I've used my other computer to make the slides. I was done Sorry. I'll fix it after the session This is why I shouldn't have brought two laptops, the idea Okay idea Okay, so. I have access Can you send audio just to see if it's volume problems? problems? Hello it's volume problems? I can hear it from here. Hello. That's good. He sounds fine. Yeah Mark, I think it's just you with the audio Okay, we should start anyway anyway Okay Far ahead. Okay"
  },
  {
    "startTime": "00:02:01",
    "text": "Hi, everybody. This is UFMRG, usable formal methods We've got some interesting presentations for you, but first this is the note well, please note it well It would be really great. Everyone could be nice to each other Our agenda is not completely full, but hopefully we will not have to do my talk, which we have been not doing for several sessions in a row now So please ask lots of questions Does anyone have any agenda bashing they'd like to do? We had, who wants to move? Summer agenda bashing they'd like to do? We had, who wants to move? Uh, summer. Okay. Samma, he's gone He's gone, okay. So, Samma might have to move later, so He's gone, okay. So, Samo might have to move later, so... I mean, I put him at Penultimate, so... Oh, okay, you've moved already Um, okay, any agenda bashes? No, in which case, I think we're going to go straight to Mark Mark So Mark we'll give you control of the slides in one second once they're up How do I give control? So if you just choose the peoples? and then for Mark, if you just give them plus there, plus slides. Bunk, there you go Okay, Mark, control Mark. Off you go. All right So I hope you can hear me well. I'm trying to speak a little bit loudly so hi my name is Mark puttie you gonna So the original presentation was 30 minutes long, so I have to shrink it a bit, so I don't know how much time we will have for a question after, but we will see it at the end end So, okay, sorry, I have to switch"
  },
  {
    "startTime": "00:04:02",
    "text": "between two screen okay so this cut will go back to it. It will be become clear a little bit later in the talk but I'm going to read it my faith for more than 40 years. I have been speaking prose knowing nothing of it, and I am the most obliged person in the world to you for telling me so so So the goal of the presentation is to explain the background for an entire draft I wrote, which is about another internet draft So in 98, Dr. Revest, the RR in R&M wrote a specification for a variant of symbolic expressions. This specific was formatted as an internet drive but was never submitted to the IETF So a few years back, a year or so back, this specification disappeared from the intake And so we no longer have a way to have a reference to this specific So with Donald eastlake Lake, we started to work to publish the specification as an RFC at the I IET. And I was working I was doing the reviews on the original specification from the Revest to find all the ambiguity and the context the original specification from Dr. Revest to find all the ambiguity and the contradiction that could be in this specification to publish it. And this is something I'm doing since 20 years, is that I use formal method when I do a review and when I write a new specific from the internet But the thing is that this work is complete"
  },
  {
    "startTime": "00:06:02",
    "text": "invisible to the IETA Nobody knows that I am doing this. So I thought that this time could be a good idea to show exactly what I do what formalization and why proof I write for me to verify that my specification or other people's specification are correct or more correct So this presentation is more tailored for people from the IET that are protocol design and so on. It's less more it will be less more interesting for people from academia because they already know all these things, right? But this is the kind of presentation that I really wish I would have seen 15 years ago at the IETF and would have brought me sooner in the path for you using formal method This is not an attempt to convince anyone to do the same thing that I do I don't want that. I just want to show what I use and to let people think for themselves and try to find their own path There is no one way, just find your own way So, uh, are going to go progressively through some white proof and I'm going to start with pen and paper No, sorry, I missed a step up yeah, so first what is a proof? So a proof is a deductive argument that shows a stated assumption guarantee conclusion. That's it There is no more to it. But a proof in implies to other things that always come with it this. First is the proposition, right? This is declarative"
  },
  {
    "startTime": "00:08:02",
    "text": "statement. And this can be well at the question you want an answer to, right? So this is very important to start with your proposition and you have a proof. But there is a third thing which is equally as important which is a process that verifies at the proof is correct from that proposition right? should not trust a single things that come from the IETF and you should not trust, because you should not trust a single thing that come from the internal right? You need to take, to read it to understand it, and to find a proof of it and then to verify it yourself and this is exactly what this the third step is about so the oldest kind of proof is pen and pet paperproof, right? So the proposition is a mix of plain English and mathematical symbols The proof is a sequence of logical steps that shows that the conclusion of this proposition that gives a conclusion and shows that the conclusion is correct. The verification step is done by peer review, which means that all the mathematicians go read the proposition, read the proof, and by themselves verify that the logical step are correct The simplest is to show how it looks, and this is an example of a proof, right? So a pen of paper proof So this is called a CRM at the top A CRM is simply a proposition that have a proof Before a proposition as a proof is called a proposition After you find a proof for it, it is called a CRM. You can find also the name Lema, from time to time, but fall over all intends and purpose, this is exactly the same You have the proof after this"
  },
  {
    "startTime": "00:10:01",
    "text": "and one thing to take away from this slide is that there is a lot of things that happen unsafe. There is no definition for what a real is. There is no definition for addition to strike subtraction, multiplication, and so on so there is a lot of stop of step that have left because mathematician this is to to talk with other mathematicians and they know all these things so they don't have to repeat all this thing each time. They do time They do a proof. Now, the problem with pen and paper and proof is that the proof are becoming more and more complex and bigger each time. Now it's not uncommon to have proofs that are 10 of paper or even under odds of pages Who is going to verify this? This is a very cost process. And this is not a very well-rewarding process, right? It's not, it's great to find a bug or a flow in someone there's reasoning this is not fun to say well it's it's fine I just spent two months on this for me thing. So proof assistant are a way to use computer to improve on on this all right so here what happened is that the proposition is encoded in a logic calculus The proof is encoded into formal language, which is generally different from the proposition and then the computer does simply a verification that the proof match is correct for the proposition. This is how Isabelle Tamafi which are examples we already talk about in this research group are doing. So some proof as assistants are more specialist than other. Tamarine for example, is specialized for security proofs, which is a subset of all the proof that are interesting for a system"
  },
  {
    "startTime": "00:12:02",
    "text": "at the IETF. I want to emphasize a little bit about this, right? It's a doing only security proof is like very interesting for a system at the IETF. I want to assist a little bit about this, right? It's doing only security proof is like verifying that your entry door is secure but not verify, but not verify leak is not leaking. But not verifying that your leak is not leaking right? You can, you certainly do need to do both to verify that your house is in a good shape So, I have a programmer. I have been programming since 40 years. I have been paid to program for more than 30 years, all right? So programming is what I know and what I know very well. I am not a mathematician and I will never be a mathematician So both assistants that are based on a lot of maths that you have to learn and you have to understand and you have to internalize is not something that is easy for me. And I would bet that there is more developer at the IETF, that there is mathematicians at the IETF So there is a third way beyond Penn and paper and proof assistant, which is the key Curio Award equivalence, or the Curio World Eisen Isomophysm. So the idea is cool and paper and proof assistant, which is the Curio World Equivalence, or the Curio World Isomorphism. So the idea is quite simple, but it's quite brilliant, I would say The idea is that a programming language can be used to encode a proposition, the same way you use the logical calculus you can use a type system of a programming language to do that. And then writing code that comes is equivalent of writing a proof that is verified. And that's it This is a brilliant idea behind Curio world equivalents is that you don't need to learn all this mathematical stuff. You use what you are"
  },
  {
    "startTime": "00:14:02",
    "text": "you don't need to learn all this mathematical stuff. You use what you already know, you extend what you already know, which is, coding, which is programming And this is a very important stuff because the leap. So there is still a lot to learn, right, but this is a less work than relearning all this thing from a scratch and perhaps doing a PhD, I don't know So there is, unfortunately not all type system are equivalent and not all can encode all possible propositions. All right and not all can encode all possible propositions. There is a part of the proposition which is called quantification which cannot be done with the normal type system that you use, even something as sophisticated as others, right? So this is why to do that, we need special type system, which are called dependently typed system And there is five language that are in use and more important that are maintained, which are Koch, Agda, Lin, F star, and Idris too So that's important I use, I use in this case and I use since a few years now is Idris too. This is a programming language which has a very compact syntax, which is inspired by Askell and which has a dependent type so we can encode any proposition in the type system of Idris. In addition, in that has an extension, which is called a linear type system which permits you encode state machine which is very useful for a place like the IETF when we work on protocol and protocol are most of the time based on state machine. So this type system permits you on encode proposition related to state by machine in an easier way and safer way"
  },
  {
    "startTime": "00:16:02",
    "text": "Another thing that the reason I choose in Idris is that it's pure aske. You don't need any unicode symbol to write Idris code. And it was really useful because this permitted me to arm embed the type and the proof directly inside my internet draft and as they are available in the XML file, in the RFC XML file, you can extract the source code. And you can verify again don't trust anything I say Just go, download the XML file extract the Idris code and run it through the Idris compiler, and you will see that it verifies So what I said, assuming that the Idris compiler do not have bug, should be correct Going back to the specification so the internet draft I wrote is specific about the draft that the Dr. Rivers wrote, not about the new draft, right? The first step was to find what are the ambiguity and what are the contradiction in the original document, so we can fix it in the new Internet draft. So this is exactly what I think and what I explain inside the document. So there is three parts for each section of the original document one that said, okay, there is this ambiguity this contradiction, and we will solve it this way. The second part is the proposition in Idris that encode exactly what I just said in the paragraph in the section before And then I put some proof of correctness for the correct example, the examples that were in the document, I proved that, yes, they are copied Some were incorrect So I plan to do that again when the draft will be closed to be published to verify"
  },
  {
    "startTime": "00:18:02",
    "text": "this time that the ABNF and that the example in the new document, the soon soon-to-be RFC, are also correct so a few thought about the process of formalization. Formalization is hard, and it's incredibly hard and there is two reasons. One that goes back to what I said with pen and paper This is a computer. You have to explain everything, all right? And there is two reasons. One that goes back to what I said with pen and paper. This is a computer. You have to explain everything. A computer doesn't know about real number, doesn't know about natural number, doesn't know about string. It doesn't it knows nothing. And so you have to find someone who already explained something to a computer or you have to explain it yourself So this is the first step which is difficult The hardest past is the fact that when you write a proposition, you have to know everything about the subject you're talking about And this is the art part. When you record, especially with not so good language like Python and so on, you don't really have to understand everything you do, right? As long as a test pass and your employer doesn't fire you, then who cares, right? When you write a proposition, you have the choice. You need to understand every single thing. And I have to do a format. I did the formula last year for the beginning. All right. The world DNS sec. That's 13 do a format, I did a formalization last year for DNS, right? The world DNS, including DNS sec. That's 1,300 page of ARFCs. It took me a whole month to write the format. Only the formalization, no more than this, a whole month to understand everything. And trust me, thank you an answer in 1300 page of RFC is not simple. So this is a very difficult part and this should not be underestimated Now for the same reason formalization is good one because you know everything"
  },
  {
    "startTime": "00:20:01",
    "text": "about the subject, right? And you have to first resolve all the ambiguity in contradiction. You can just say, oh, it's just fine. The worst sentence you can hear at the IETF is, oh, don't worry, this is a corner case. No, I worry this is a corner case. So the million of calling my case that I make each day will be a problem for a substance number of people. Knowing every about the subject on this very, very important. And I would say that you don't even need to go to model checking or prove writing and all this stuff Just do the formalization. You will find an amazing number of bugs just by doing this All right. So this is very important to do this. For the record, I do not go directly writing proof and this is similar to something Jonathan said I think it was an interring meeting, is that I sketch thing into a another language in fact I use Petrinet to sketch what I want to do to verify more or less that things are working before doing this And using that Petrinet is a thing that helps a lot. I'm currently designing a new distributed protocol that I plan to submit to the IT before doing this. And using that Petrinet is a thing that helps a lot. I'm currently designing a new distributed protocol that I plan to submit to the IETF. And you have no idea how many bugs I found each day that I have to fix in the protocol, thanks to this kind of form formalization. And the last thing is found each day that I have to fix in the protocol tanks to this kind of formalization. And the last thing, this is my last slide, is that formulation is useful You already encoded your protocol, your specification as in a programming language. So now you can derive things, right? You can provide programs to extract information and generate other stuff. I use it to generate ADNF automatically I don't need to write the ABNF form, and I certainly don't need to format it by on. This is a program that"
  },
  {
    "startTime": "00:22:02",
    "text": "is doing that. Diagram generates same thing, right? State the diagram You don't need to draw a state diagram Just let the system draw it for you from your type, from your specification I do that with another thing, which is that specific test or a which is a piece of code where the programmer can put an input. The test oracle gives the correct input, and then the programmer can compare it to their own code, right? And the next step is automatic testing, where I have a generator of all the possible input for the system and I use that to test the code of the development automatically in a bunch of servers to find bugs automatically without having to write a single unit test. They are generated and they are applied automatically for me. And this is the end of my presentation I hope you have a question. Again, if you don't have time, we will Okay Any questions for the speaker? While people are thinking of some interesting questions I think this is very interesting because Sam Scott when he was doing an analysis of Tamarin, did something very, very similar, although much less formal. I just posted a link in the chat where he literally had a whole page on of the spec on one side and how a description of how it was modeled on the other side and you get this very yeah, I don't know if I can share it easily, but you get this very very nice formulas So doing it in line definitely sounds like a"
  },
  {
    "startTime": "00:24:01",
    "text": "good approach Kris P Yeah, thanks very much for the talk Can you, I just want to ask a really high-level question. What is what is usability? mean to you? Like what's the usability problem? here that's being solved? All right So, and I said this multiple times usability is in the is in the eye of the deal one what i find usable some else will not find usable I don't find tamarine usable at all, right? But this is because I was not trained in this kind of things. I find Idris and the idea of the curious word isomorphizing usable Why? Because I am a programmer who thought a lot about type in his career. So I be believe that, well, I think that for development, and as I said, there was more developers than a mathematician at the IETF curious word, I, I see, I, I see, I, I, have a slight edge of a other way of doing proof and formal method just for this reason So this is what I meant by unusable formal method for specification Metriism, we're into it So I have a question mostly like, say, currently we're starting to work on some security RFCs and question like for this part like, what is starting? point? Like how to make this specification from friendly for formal proof? from beginning like what practical steps here for specification yeah if you start with new specific how make it more provable from beginning? Well, so an interesting thing is that if you, if"
  },
  {
    "startTime": "00:26:02",
    "text": "the kind of things I did for a for symbolic expression, is that you way you do your formalization change the shape of your specific right, is that you have more logical step so it's a good thing. So I have to explain something. I did certainly have enough time to explain all the things I do, but I use something which is called literate programming, right? And in fact, the Idris code that you saw is mixed inside the text of my internet draft, right? and when I write an internet draft now I I have the specification, the formal specification why at the same time that I write this internet draft. And there is a piece of code piece of text, piece of code, piece of text, and the code is the formalization of the text, which is just above Idris has this thing that it doesn't do easily forward reference, backward reference. So you have to order your Idris code bottom up, You have to start for the bottom And I found that if you do that, it gave you a nice organization for your code logical, for your text logical organization to organize your specification. Now, there is the second thing This was the one shot, what I did to show the formalization behind. But what I want to do is to show that it's possible without any GitHub separate anything outside to publish the specification in a separate draft, but also entirely in a draft And I think that this is something I will make or may undo in the future to show, well, here how I guarantee that the thing I said in this other internal draft are caught"
  },
  {
    "startTime": "00:28:02",
    "text": "Does that answer you? question? Daniel Hi, this is Daniel Smallen speaking So one thing that I've observed in formal verification, especially the languages that you mentioned environments maybe is a better word, is that the seem to excel at, you know, create proofs around things like cryptographic primitive, and, you know, sort of things that for many folks in software engineering, they might call those as, you know, low-level abstractions. And there doesn't seem to be a strong way to connect this to higher-level abstractions. So, for example, there are, you know, formal ways of modeling things like architectural description languages where, you know, you're really not reasoning about primitive types, but maybe you're reasoning about overall properties with the system quality attributes and so on Now this is sort of a two-parter question. I guess the first part is, have you ever encountered any problems? where this lack of an interconnection in these modeling types is you know, come in the way of you doing your work? And then the second part is, do you see then there being any kind of benefit to having some sort of a solution to bridge this gap or not? Yes definitively, my dead job is a software architect, specialized in distribution system, all right? So this is a kind of things and what I no longer write code because I'm apparently to expect for that not code that goes in production, at least. What I write is code to prove that an architecture is working an architecture is correct, and things like this It's difficult. I mean, there is not a lot of guidance. There is not a lot. You cannot find a lot of books that explain how to do this. There is a few If you look at the domain, the document in design, it goes in the wide direction It's not exactly that. But there is already a lot"
  },
  {
    "startTime": "00:30:02",
    "text": "of tools that determined to do that. But yes definitely, you can use this kind of tools but you have to be creative and you don't want to put you in a place where you know so well a tool which is not adapted to what you want that you cannot see how to do other things. You really need to see a lot, a lot of different systems, a lot of different language and so on to find the ones that really can solve the problem you want working on. Thank you I would just say that there are definitely tools that are higher language, a higher level, right? If you think of like Tamarin is completely abstract and doesn't do anything to do with code Well, yeah. Technically, you can shake right? If you think like Tamron is completely abstract and doesn't do anything to do with code. Yes, and by the way, this is abstract. The thing I do didn't say is that, and this is a thing which is very surprising for a programmer like me, is that you don't run the code that you write. Running the kernel has no meaning at all. Oh, it runs so what? No, no. The thing is in the compilation. This is a compilation that gives you the answer you want. This is not running the code all right? So, but you can run the code. Same way that in time Tamarine you can extract code, right? This is the same idea you can extract code, but this is not the point. The point is to show that your proposition can be implemented, does not loop, all the properties that you want. And you don't, so this is still high level. And you need to go low level. So there was a paper published last December, I think, by the people from F Star, which shows how to match these high level things with the format of a PDU the format of the data which is sent and how to verify this. This is a very interesting way to link the high level and"
  },
  {
    "startTime": "00:32:02",
    "text": "the lower level Great. Thanks, Mark So I guess we'll move on to our next speaker Thanks. Thank you So Ben. Let me give you control of the slide Okay, well, good morning everyone. My name is Ben Greenman. I'm an assistant professor at the university of Utah, and this is joint work with folks at Brown We have Sheram, Krishnamurthy, and Tim Nelson, and the All-Star PhD student, Sudartha Prasad All right and I can control the slides. So Forge is a tool for D debugging your designs. You, the program, are start off by describing what data types do you have in mind for your system? and then you write down relationships between those data types or constraints. And you did this all in the language, which is based start off by describing what data types do you have in mind for your system, and then you write down relationships between those data types or constraints. And you did this all in a language, which is basically a scripting language for set theory Then Forge spins its wheels, and you get two modes of use out of this tool. One is you can explore the data types that you've created and see what consequences they have And then second, you get bounded verification So you write down conjectures and forage and you can verify whether or not those hold up to limited balance so let's look at an example to see how forage works and we're going to go way back to the 1990s and talk about cross-site requests forgery. So what's happening? here? We've got a user who loads up a web page and this user wants to make requests to a good server over on the right. We can get into trouble here because the web page is going to forward forward authorization cookies to any request that comes from the web page. So those will all go along to the good service So if we have an evil server which provided the page, or an evil server that did a cross-site scripting attack, and is able to make requests via the web page, both of those requests at the end of the good server, are going to have the same cookies inside. So we could want to be with something like I'm talking to Netflix to order a DVD to my house, and the evil server has sent in a request to change the address. So now the DVDs go to another"
  },
  {
    "startTime": "00:34:02",
    "text": "place, not to me. So this is a problem What can we do to fix it? Here's an idea. Let's add an origin to every request. So scripts that we provided by the evil server will now have an origin that says I came from an evil place and the good server can check these origins and make sure that they match up with where the request actually came from. That seems like a plausible idea, so let's try it out. Let's model that and forge We start off with our data types. So we have an abstract signature that says we have endpoints Endpoint is like an object. Right now, it's just an opaque thing. There's no members inside of endpoint One type of endpoint is a client Client is also just a black box The other type of endpoint is a server Server has a set of causes that come with it So these are things. Causes here is our source of truth. So every request and request later on that was a consequence of what the server has done is going to show up in our ground truth set causes And then we need a few more data types. So here's just a sketch to show what's happening with the rest of them. We have an abstract signature for HTTP events, and there's three kinds of events down below. Request, response, and redirect Every HTTP event has one endpoint that it's from, one endpoint that's going to, and one origin, which is the new thing that we've just added So the really cool thing here is just by describing these data types in a few lines of code, and this is all the code that you need for these data types we can immediately start using forage to explore what are some instances of the data So down at the bottom of the slide, you can barely read it, but it says, explore for two servers and one client, what are possible configurations of endpoints? So you can meet immediately get a sense of what data examples does this code give us. Next Now part two, we need to talk about the relationship"
  },
  {
    "startTime": "00:36:02",
    "text": "between these data types. So here's one predicate to give you a taste of that And the relations typically come in two flavors So type one here is facts about the world So one thing we want to know is every request should be paired every response should be paired with the unique request Every request has a field inside, which says here's the response and what this predicate is doing is making sure that these things are one-to-one We don't have two responses to the same request Response shouldn't be doing double duty like that the other type of predicates that we need are facts about the design that we're experimenting with So we're interested in good servers that enforce origins This predicate is saying, well, a server is good if for all requests that come in, if that request is headed to the good server, then one of the two things below need to hold. Either that request has originated at the good server or that request has an origin that matches where it's coming from And in this case, it's going to be a request from the client So what we don't want is a mismatch between origin and from where the evil server has gotten something in that looks like it's coming from the client So those are two predicates. We do need a few more for this model but I'm going to skip over those. And now we can finally test the conjection So we have a run command, which would show up at the very bottom of the forge file, which says, can we find some good and bad server where the good server is enforcing origins such that we get into a bad scenario where request is missing? and we'll explore this up to some bounds that we specify and the idea is you start off with small bounds and work your way up to build confidence that this property has no counter examples for a larger system But usually you can find a the hope, the hypothesis is that you typically find a counter example within small bounds And, uh-oh, Ford has found a counter-example to our conjecture, so we would hope that there's no instance where we can get a mismatch. And here it's shown"
  },
  {
    "startTime": "00:38:02",
    "text": "well, this is one where it can, it can appear And just like before, we have a directed graph to describe the instance So this comes out of the box from Forge Excellent so we took our idea we sketched it out in a few lines of code. We found that this idea has a flaw and now we can go back to the drawing board and say, okay, well, maybe we need to reconsider this design. What can we do to make sure that we have no evil servers sneaking in requests? One idea that comes to mind is to add a set of origins because the basic problem here is that when we have redirect mixed in, we can get things that are mislabeled But the takeaway message here is with a little bit of coding and forge, we were quickly able to find a bug So this is a much better way than just working on pen and paper and testing out examples by hand. Let's step back for a bit. Forge sits in this space that we call the lightweight formal methods arena. There's several tools in this domain Lightweight FM, that name comes from an older article by Daniel Jackson and Jeanette Wing They have a great line in that article, which says, The cost of a formal proof is typically an order of magnitude above the cost of a specification And the cost of a specification alone is often out says, the cost of a formal proof is typically an order of magnitude above the cost of a specification. And the cost of a specification alone is often outside the budget of a real project. And an IETF, that's absolutely true because everyone's a volunteer, so there's no budget So we're lucky if we can get a specification So Lightweight FM is trying to sit between this rock roll of proof of system where you get very high assurance, but you also have to put in a lot of effort. And we're also trying to navigate past the whirlpool of model checkers where you're at the wind of an automated algorithm, which is going to explore a huge space space, but it may take a very long time to get back to you But of course, there's a trade-off here So all these tools work, lightweight FM by sacrificing completeness and full verification in exchange for usability And I touched on this briefly before the hypothesis at play here is that most bugs are going to have small"
  },
  {
    "startTime": "00:40:02",
    "text": "counter examples. We should be able to find an instance which explains a problem without going too deep into the universe. There shouldn't be too big of a way most bugs are going to have small counter examples. We should be able to find an instance which explains a problem without going too deep into the universe. There shouldn't be too big of a world where this bug shows up You know, and there might still be a bug. Maybe that's the way you bring in the proof of this later on. But to build confidence at the start, this is a great lightweight way to make progress using formal methods OK, so Forge is one of many tools in this lightweight FM space. What sets it apart? There's three things that we've been working on. Custom visualization support for unit testing, and language level Visualization is just a no-brain We saw some directed graphs before those come out automatically out of the box, which is very cool but they are tough to understand So the picture at the top and the picture at the bottom are both visualizations of the same instance that came out of the solver The top one has a lot of theming and styling to make it more readable, and it's still completely incomprehensible. So this example is from a different paper related to Forge where modeling a cryptic protocol. This is Needham Schroeder with a vulnerability And the picture at the bottom, it's like anyone's guess what this is saying. And the picture at the bottom says, okay, well, here's a sequence diagram. We've got an attacker sitting in the middle and you read the text and you see, okay, the attacker has access to the public key. This is a vulnerability Custom visualization works because we have the visualized is plugged into modern web framework so you open up a browser to modern web framework. So you open up a browser tab. You have three columns here. One is the Viz on the left the middle is JavaScript code that you're in full control of to write. And then the right hand side is a browser to show you what objects do you have at play from? the solver that you can work with inside the visualization This is an area for improvement. Okay, we have the full power of JavaScript, but we also have, you know, the total unconfident of JavaScript. So we're working on building more of standard library to help make common case visualizations easier to build. But just the fact of having this, I think,"
  },
  {
    "startTime": "00:42:01",
    "text": "is a huge step above the directed graph And right, it's more than pretty pictures at play here Understanding these counter examples is critical to making progress and actually using formal methods. And we're building on research in cognitive science here, and this is a paper from Uppsla a few years ago, where we were doing user studies to find out what instances are most effective to helping users of a formal tool make sense of the examples that come out Second, unit testing. How do we know that the model we've built is correct? The models only take a few lines of code, but it's very subtle lines of code At Brown, we've been using this forged tool to get students who have no interest in formal methods using it to debug their designs. Part of what we need to do is build on experience that people have from Java and unit testing or whichever and get them to be effective modelers. The real challenge we're faced though is programming is a completely different activity than formal modeling. So programming is more like the situation on the left where we've got a robot and we're telling it step-by-step you proceed through this world after programming is more like the situation on the left where we've got a robot and we're telling it step by step you proceed through this world. You know, after time step three, you take a turn to the left There's a sequence of instructions and that's why things like print deaf debugging makes sense in the programming world It doesn't make sense at all in the modeling world. We're more like the person constructing the vase here on the right, and then the solve is filling in the blue liquid to say, well, here's what interesting shapes can come out, given the domain that you've described described So one of the constructs we have for testing is assert that was on the previous level Assert in Forge is totally different than an assert in a programming language. It's more about comparing two predicates. I assert that predicate one is an over-constraint of predicate two or I assert vice versa that we have an under-constra And you use these to hone in on the property that your ultimate interested in Third, language levels Again, we have a rich modeling language"
  },
  {
    "startTime": "00:44:02",
    "text": "scripting language for set theory, and it's very tricky What is this line of code saying? It'd take me a minute to unpack it for sure, and if we have a student who's coming in who only knows Java, it's like, well, I have no idea what's going to be This is just a non-starter We, we, we, we have a student who's coming in who only knows Java it's like I have no idea what's going on this is just a non-starter we we use surveys to introduce different levels of languages that we have to students. Okay, so going from step one to step two, this is one of the questions on the survey What does this mean that we say the course CS1 is? in the set of prerequisites dot CS2? And one of the student responses was, I hope that Forge doesn't allow me to say the thing in the code here because that would be a total travesty like method dot I object, this makes no sense So we need to have these different levels We start off with a small contained language, which is only functional relations. And then we can totally use this Java analogy. There's SIGs described objects, objects have methods, and there's no going backwards You don't have relational join yet Then step two, we can talk about relational algebra and introduce N-ary relations. And then step three, we had linear temporal logic, or LTL And the point is, each one of these languages is expressive enough to do some interesting modeling work, and you can only go so badly when you get something wrong and have an error missed You don't need to understand the full language at once to make sense of the errors. Let's zoom in for a minute on the third level language, temporal, because we You know, you don't need to understand the full language at once to make sense of the errors. Let's zoom in for a minute on the third level language, temporal, because we've been doing a lot of work recently to understand LTL and LTL misconceptions. So this has been multiple years of studies with surveys and talk aloud interviews find out. Where do people go wrong when they think that they have a correct LTL specification and it actually has a subtle bug? So right now we have a codebook of several different types of errors that show up in LTL I'm going to skip through this here, but this is a little quiz. We can talk about it after during the question period, but it's one example of misconcement"
  },
  {
    "startTime": "00:46:02",
    "text": "error. So how do we turn this into LTL? The green light is on exact once. And you can do it with just these three options Eventually, always, and next stage Here's our wrong way of expressing it, but it's very close to correct What's missing is a G on the right-hand side So what might be going on here is an implicit G misconception, and we've seen surprisingly many instances of this implicit G. But this is one of the things that comes out of the user study. The more exciting thing is we've built these into a tutoring tool So if you go to this URL, LCLTutor.XYZ, you can it'll spin up exercises for you based on what we found and it will suggest new exercises based on your performance, the ones that you've done so far So here's a question. Does this formula match this? trace? You answer yes or no. And let's suppose we get the wrong answer here. The tool will give feedback in as far as it can to say, well, your property, here's why that's the wrong answer okay so we've seen Forge it's a lightweight modeling tool takes in a description of data types, relationships between them, and gives you out two modes of use. Quick exercise and bounded verification And then the three research thrust that make Forge usable are custom visualization, unit testing, and language levels Everything I've shown so far is based on running code but each of these three pillars is also an active research area for us. So I mentioned the JavaScript toolkit The other two pillars are also things that were actively researched So we would love to find applications and collaborations in the IETF space to help build this out, find out, well, where does Ford you need to improve? to become a better lightweight tool for professionals? There's the Forge website so that you can look it up and please share with your colleagues. And then there's my email address if you want to share half-baked ideas or things that don't fit on the mailing list. Thank you Okay, Chris. Chris"
  },
  {
    "startTime": "00:48:01",
    "text": "Yeah, what a great presentation, especially for this group. I mean, it's like all of the, you know, like this is a tool. This is how we think of usability but also this is how we like evaluate whether this tool was useful. I guess it's like kind of easier because you just have like a captive audience of students to do experience with. That's really cool I wanted to ask okay so I'm not a form methods person and I'm seeing all of these like language being developed for expressing property, like symbolic property of things I'm curious why we don't start with like a programming language people already use. So if like you have students using Java can we like constrain? the semantics of Java, maybe take a subset of the language? that that at least so like syntactically they don't have to learn anything new. Yeah so that's actually where we're trying to end up, is so syntactically, you don't need to learn much new the reason you don't want to verify your Java program is the state space explosion problem it's can you explain that a little bit a don't need to learn much new. The reason you don't want to verify your Java program is the state-based explosion problem. Can you explain that a little bit? A program says a lot. There's a ton of details in a program about what needs to happen at each step. And for a model check to go through and verify that, you've got to find some way of breaking it up. Otherwise, there's just a huge space of possible states that the solver needs to deal with. And then you don't get an answer in time Okay. Okay. Thank you. Yeah. So construction things to data types and relations is a way of cutting down on the state space Done. Okay alistair woodman, can you compare this? with things like quick check? some of the other things like Koch? things that come out of the pure function? programming space, the Haskell corner Sure any others? No, I could give a long list, but I mean, I think you understand the corners. Okay yes, Cock and Idris from the last presentation"
  },
  {
    "startTime": "00:50:02",
    "text": "these I would say are in the proof assistant space, where you get very little automation to help you prove a property But on the other hand, you can get full verification in Cock, right? So you can you completely know that this theorem is satisfied once you built a cock proof A quick check on the other hand is a very lightweight tool Quick check, you specify tests as proper So instead of constructing the full input, for what I want to test, I say, well, here's a property that all my input output things should hold property that should be true of all inputs outputs and then quick check will automatically generate the inputs for you So in that sense, quick check is similar to the exploration that ford provides quick check comes with these generators for different data types like quick check at haskell it would be haskell data types we can construct instances in Quick Check And the difference is the property language in Quick Check is just the Haskell language. So you've got a general purpose programming language and it can explore things in that given the time you've given it And we have a more abstract state space in Forge So hopefully we get further and we have higher assurance that the property is true than the pointwise checking that you get in quick check I think of these things is on a continuum So quick check is like the lower end of formal methods lower quality verification. And then if Forge is a step above that. And then proof assistant is a huge step above that. Okay Thank you So that actually, I've been changed my question. That was very interesting because I was thinking on what you said with most proofs have small, most counter examples have small instances. So one of the things we found when we were doing TLS13 was actually quite a big instance with like three handshakes and 18 modes and it broke. And so"
  },
  {
    "startTime": "00:52:02",
    "text": "if you would actually want to prove you have to do the full thing anyway and I was thinking well I definitely wouldn't want to do it something in one language only to have to throw it all away and start again in a bigger language But is there a way that we can well, I definitely wouldn't want to do something in one language, only to have to throw it all away and start again in a bigger language. But is there a way that we could stitch the two together? and just be like, ah, you know, this is, we're doing it in this like very small subset, and then when I'm ready to do my full process I just like turn it onto like sport mode and it just like, I don't know Yeah, that really would be ideal. I think that's where we need to end up. But where we are now is there's this huge gap between like designing something on a whiteboard and then getting that in initial prototype. And for just trying to fill in that space where it's something just above the whiteboard and I can find many bugs The devil is really in the details of that word. What is most bugs? It could be 51% But 51% Hi, Colin Tucker So I am very much not a formal method but looking at your, you're watching your presentation it struck me that this language seems to be quite declarative and a lot of these modeling languages seem to be quite declarative in style. The way we write implementations of the protocols tend to be very imperative And this whole set of actions to perform in what order and so on and the way we write RFC tends to be very imperative and there seems to be this sort of this disconnect between the way a lot of people think about writing the specific"
  },
  {
    "startTime": "00:54:02",
    "text": "and the RFCs in the IETF and the way the people writing the modeling language is think about modeling them and I'm not quite sure where I'm going with this, but it seems that that disconnect might be part of the reason why people struggle with these these sorts of tools No, it could be. I really appreciate it with this, but it seems that disconnect might be part of the reason why people struggle with these sorts of tools. No, it could be, and I really appreciate the observation. What I would ask is if you find a point in the specification, you know, something stands out to you, like maybe this could benefit from a forge model then send a ping to the mailing list or send a ping to me So I mean, we're happy to go through and do some of the work in modeling and maybe that can help close the gap as we move forward Thank you um did you join the queue phoenix you change your mind. Okay. In which case, we thank you speaker. Thanks, better We'll move to Muhammad And this is where you'll probably see supposed to put the slides in order Yeah, which one. Which one? Which one's your slides? It said, number one which, which, which, which, which, which, which, which, which, which ones your slides? It says, number one. Oh, okay Do you want to drive the slide? or should I? If you could do it, that would be quick All right. Hello, everyone. I'm Sama from T.U Dresden and this is joint work with Artonymy, Hannes and thomas fossati, who might be familiar to you, they have been very active in the IETF already. Around the year, ago, there was a call for former analysis of a tested TLA draft, which they had already been working on and I volunteered for one of the formal verification task and I would like to thank also confidential consortium confidential Computing Consortium who actually funded me to be here next please so the agenda is that I will talk about the background of what this attested here"
  },
  {
    "startTime": "00:56:02",
    "text": "is all about just very quickly assuming that uh to be here. Next please. So the agenda is that I will talk about the background of what this attested TLS is all about, just very quickly assuming that you folks are familiar with TLS And then I will describe what the former nurse's goal was and what was the post that we took to familiar with DLS and then I will describe what the former nurses goal was and what was the post that we took tools that we used what were the challenges around in this domain and how we validated TLS 1 1.3, which was actually claimed in the SNP paper to be validated, but actually was not fully validated and then formal analysis of a tested TLS which we are currently also doing so it's an ongoing process it's a long thing to be verified and I will summarize and look forward to your feedback on this next please. TLS is very good for network security We have formally verified it many, many years ago with all the tools that we have symbolic security analysis cryptographic analysis, computational analysis, and all the handwritten proofs and all that it's very good Next please, but it's not good for endpoint security. So that means basically if the endpoint is compromised all the security is gone so so we have done all the nice proofs and all that but what if the key was actually key with which the server or the client was authenticating? That actually key is a compromise Next please. And what if the next? again so what if the software actually is compromise itself, the TLS stack which we are talking about so what if that part is actually compromised so what it's actually saying is that, hey, the network adversary in between cannot compromise but all the guarantees are only coming from that the software and the key was actually protected. And not only that, next please the platform itself, the hardware that under coming from that the software and the key was actually protected. And not only that, next piece, the platform itself, the hardware that underneath, which was actually running that software, what if there were vulnerabilities in that hardware so none of this is actually captured by the TLS standard 1.2 Next piece The use case here is basically a very strong one which is the con- confidential computing, and that's why confidential computing consortium is"
  },
  {
    "startTime": "00:58:02",
    "text": "funding me here so that the use case we have here is we actually need there is some workload running on the cloud, for instance, and how do we actually get to know that this is my cloud to which I will send over my secrets and that's where all of these these three things the keys the software the platform that I mentioned actually is required to be attested so there is I understand there are people's who are really against the attestation thing and all that privacy concerns and all these things. So I will not go into that debate, but the thing is that confidential computing actually needs this thing. So that's, that's actually requirement And there are solutions already being used for many years because it's a fundamental, attestation is a fundamental enable of that thing. Next please So I will give you a key idea of the remote attestation there is rfc 9334 which talks about all that unfortunately it actually misses the key idea of that it has like 56 or something pages about all that. But it does not really clarify the three things, the main key idea of that whole thing which is that we have three steps or three main things which we need to clarify. And a tester, so, following the terminology there so is the device which is under test In this case, so for instance, I have the workload running on the cloud so that is my attester now i as a user or that cloud, I need some guarantees that, hey, this is my workload i am only providing the data to my workload and not to someone else, not to the cloud provider and so on The other party shown here, the verifying reliant part is the one who is the in simple words, the verifier. So when I use the simple verifier, the RADS architecture, people from RADSville start killing at me. So this is just to fulfill that requirement that it's not only a verifier it's also a reliable party there. The key idea is that it will the tester, which was the entity under test is, generating an evidence somehow it has to transfer over to the verifying reliant party and then the verifying reliant party will appraise that"
  },
  {
    "startTime": "01:00:02",
    "text": "appraisal just offensive word for verification. So it will verify that what I wanted, the key actually which I wanted is protected the software is protected the hardware underneath or the platform underneath is actually protected has the right measurements, the firmware and all, everything is actually then protected, whatever it was expecting, whatever version it was expecting are up to date have no vulnerabilities inside. So next please And next, again, I think I'm just showing that attest next again so yeah so it's just showing the three steps the three key ideas of that RFC does not really clarify that what the actual thing is Next please. So having seen the TLS and rfc sorry the the RADs architecture, how do we actually combine the two things? So that was what the actual proposal was described the way, the starting point with we started. So I had a look at the other domains and other solutions in the design space what other people are doing other than confidential comp a look at the other domains and other solutions in the design space, what other people are doing, other than confidential computing also like other general solutions. And there are three ways in which we can actually now combine TLS with remote attestation. The first one shown at the top the pre-handshake attestation and then intra and then post-handshake. Next please, I have a picture which describes nicely, which is saying from the remote attestation part, we have important step is collection of the claims. I assume that basically the collect claims are collected at the right point in time, so we have no freshness issues there. And they are actually protected by the hardware. The only thing then left over is the cryptographic protection. I assume the protection here is provided by the signing of these claims which is shown here as a signature of evidence. So we're in time that actually happens is really important not only from security perspective, but also from performance perspective. So these are the designs. This is this you can think of it as the design space what i can do with all of these options okay so if I do the sign"
  },
  {
    "startTime": "01:02:02",
    "text": "before the TLS handshake has started, which is the first option in that case I will call it pre pre-handshake attestation that is before the handshake had started, I have already done that generation of evidence. The first step on the last slide. Then if I do it within the TLS handshake, meaning that more precisely in the author authentication phase, that will be the intra handshake at a station And finally, if at the end of the TLS, handshake, I do that signing, that will be the post handshake attestation. And as I said, each of them have their own properties and next please. So the solution for confidential computing consortium is a very popular solution that Intel's RATLS are most i think 90% of the solutions are actually using some variant of that solution. And that's where we actually started with it because the draft at that point in time was still being matured. There were many things which were un- unclear and so on and we started with this thing to see the explore the whole design space and our or me volunteering was that I had worked a lot with the remote attestation before. I had formalized different options of confidential computing solutions Intel SGX, TDX, and so on. So different technologies And my hope was that TLS artifacts are already available. I will just plug in the two things and everything will work So as I said, it's been used in Grameen, Rats, TLS, and many other solutions open Alclav is by Microsoft, SCX, SDKs by Intel and so on other solutions next piece Okay, so with this background so now the goal is to formally verify next please the formally verify the security of RATLS, which is the most commonly used solution in confidential computing. Next please And now I will talk a little bit about, the approach. Next please Approach is simply I'm using the symbolic"
  },
  {
    "startTime": "01:04:02",
    "text": "security analysis as defined in the SOK And next please, and the tool I have used is Proverif. It has a nice logo, which is here And next please So the approach looks like this. We have specifications coming from, in this case, Intel had a white paper specifications coming from there we have from that we created a formal model just like a typical any analysis nothing fancy here. So we have security properties. We there were no description whatsoever of what it is supposed to provide. So we thought on our own about what could be interesting i will show you later on and then the validated process is really important that's something that I will emphasize later on as well. We do the formula analysis. We get either a proof or a text. So that's simple process which we all know next please So what were the challenges in doing all that? so starting with that i i told the so this is the reference for the white paper that Intel has since 2018 as I said it's being widely used all over the place with different variants all originating from this. The issue was that they had incomplete specifications and outdated specifications TLS 1.3 is a standard since 2018 and this paper this white paper came out in 2018 but they didn't really update it even until now up to TLS 1.3. The specifications still are in TLS 1.2 that was one of the issues that's why I say outdated and incomplete that nothing is really clear what exactly is being done and in which of order and how precisely. So we use the community input as well as the implementation which was available in case of Intel RATLS to fill all the gaps. Next please. This these two are the, it didn't work maybe next please Didn't move? OK, yeah. So this is the this is the figure from the white paper TLS 1.2 handshake this is where the show even in the white paper not only"
  },
  {
    "startTime": "01:06:02",
    "text": "that is based on 1.2 they even actually do not show how remote attestation was injected So the left figure actually shows about attestation, how it all works The right figure shows about TLS. They never actually show how the two integrate together and the funny thing is that they label it the title paper as integrating remote attestation with TLS and the paper is never describing what actually, how exactly are they actually integrate together. Next please So challenges from the formal perspective and coming to the usability perspective there were very few comments in the India's TLS formal model for TLS. Next please which actually means that there were literally no comments in the main processes. TLS 1.2 point two like clients one two server one two dls one client server-13, and so on So other processes, even the QDIs, like they had no comments at all. So how is one really used? supposed to actually use these artifacts in one own research work if they are not commented so we talk about formal methods, we are improving the quality of the software and so on but we really forget when we actually write the artifacts that we really have to be also followed all the software practices. So next please So for this, what we did is we have added extensive comments on that, so following all our discussions. Next please and the proposal from our side is that ACM has nice badges. One of them is the reusable badge which includes that how reusable that piece of code is so we suggest that the conferences should be adopting that more and more and any one of you who has impact on these conferences is very welcome to initiate this idea And if you have other proposals, of course, we are welcome to discuss that next please So there was incomplete validation of draft 20 artifacts. Next please, as a fix what we did is we designed an artifact a very simple"
  },
  {
    "startTime": "01:08:02",
    "text": "validation framework. Next please And thirdly, a simple extension of attestation to that TLS article made it running for one month and we could get any idea of what exactly is going on to give you a reference for that. On my laptop, it used to work in five minutes the TLS artifacts would they give the results and this was really surprising when it was running for one day and so on. And then I put it on the high-end server. That's the ice lake machine which the fastest server we have and even on that server for one month, it couldn't give any results. Next please so we submitted it to the pro verif device to have an analysis of this to give some idea about what is going on. We never heard that it to the Providev developers to have an analysis of this to give some idea about what is going on. We never heard back. They were busy, next please. So as a fix, what we had to do was just to build it from scratch all from real scratch. Yes, next please. So the post looks like this we have the community input at the top now which is part of the formal input formal model. That is we had a discussion with a lot of people. I will come back to this point a bit late and then all the things are pretty much standard except that we have the validation sanity checks which we added to see that the formal model actually works. Next, please so with this I will talk a little bit about the validation how we did the validation very simple framework next please so the key idea is that we have DLS artifacts or the India artifacts on the right, and then TLS 1.3 specifications 8446 on the left. What we do is we bake a corresponding from our reading the artifacts for that from the TLS specifications. What how how is it? each key should be derived and then we simply compare each and every key for instance the master key coming from the left and the master key coming from the India's artifacts, and then we simply compare the two. And then if it is, equal comparison, so if they are the same, we send"
  },
  {
    "startTime": "01:10:02",
    "text": "a success um let's say output on that channel between the two parties if not then we send over a failure so very simple framework just to see that what's going on next piece and with even with this simple framework, we see that there is a lot of failures going on so we explored it further next please and this is one of the issues why they fail. We have a on next piece and with even with this simple framework we see that there is a lot of failures going on so we explored it further next please and this is one of the issues why they fail we have on the left derived secret shown by the TLS 1.3 specifications that takes as input the handshake secret label, which is shown here L5. So that's the label which comes from the standard and M empty string, which is shown here as a zero and then that becomes a salt for that and the input key material ICM, shown also as a zero, empty string and then that derives a master secret but India's are artifact had actually used the handshake secret directly as an input of the HKDF extract to derive the master secret and that's why it led to a failure in that case. But this was somehow sorry me and and this was somehow surprising for us that this, this stage actually occurred two times in the TLS hierarchy and in the other stage, they have actually followed it correctly And this was very surprising for us to see why there is a difference between the two we kept exploring it. Next slide, please The first very thought was that okay, so they have applied formal attractions as is very common in the community that, okay, so we abstract it out that this might help us in gaining some time. That's the only benefit one might get But we ruled this out because we explored the order very common in the community that, okay, so we abstract it out that this might help us in gaining some time. That's the only benefit one might get. But we do this out because we explored the original version and the three issues that we found all combined together and individually we found there was no significant difference in the verification time by eliminating that or by adding that into that we found all combined together and individually we found there was no significant difference in the verification time by eliminating that or by adding that in two different versions of proveref. Next please Okay, an interesting tale of community input the community input that I had in the approach above there. We contacted the paper authors. Next please, Bruno Blanche We asked him, here there was this paper that you had, and we found that this was not as"
  },
  {
    "startTime": "01:12:02",
    "text": "we expected it to be. He said I was not the one who was coding, so contact the other ones. So next please, we contacted Karthik. He was on long leaves and is still on long leaves from India at that point in time. It was for one year. Now it's another year He has a startup next please. So we contact the third author, Nadim, who never responded to us. Next please so then we thought okay so what's the next step? So we check the repo that we had for team which is, which is public, the India repo. And we saw the people who are actually, who have actually also used these artifacts in their own work, and then we found the one of the works was the Lourke authors and we contacted them here there is a problem in the artifacts we don't think that is actually correct so this handshake secret and the master secret should not be the way that they they are there next please so oh so so so what this said is, we have no idea if you find out something please let us know as well. And then we went to over to the DLS working group. Here, there is a problem in the artifacts Does someone has any idea about this? Why this? key is being derived not as per the specification um i think there was hugo cross who said that, look at this paper of SNP, this is describing exactly how this is being done I said, yes, we have looked at that. There was no follow. Next please Then we contacted our nice chairs They had no opinion on this as well. Next please Oh, I have a good we contacted the CCC attestation SIG. As I mentioned, Confidential Computing Consort who has people from Microsoft, Google, and all the nice fancy things and they also had no opinion on that next please and so on long story short next please and the idea IDF-119 hackathon, we had a discussion over this. Next please um in the crypto forum research group at the IETF 9119 we again had a discussion over this, that, hey, this is a, this is something that we do not understand maybe some cryptographer by the way i'm not a crypto"
  },
  {
    "startTime": "01:14:02",
    "text": "cryptographer, so I'm not saying that we were exactly right. So I'm saying that we were just honestly curious about understanding what exactly is the difference between the two So it's maybe less so in our whole attested TLS team, including arm Linaro and so on, everyone. So there was no cryptographer in involved. So we were sincerely interested in really knowing what exactly is the problem. So crypto forum so we had no feedback except that it might be that the draft 20 artifacts which they were based on might not be the final version They might not have done it, but everyone was exactly using that these artifacts who were extending that because nobody will use draft 18 when draft 20 artifacts are available right? So that was the problem. Next please Finally, at extending that because nobody will use draft 18 when draft 20 artifacts are available right so that was the problem next please finally um at the tool session at gtmfs organized in France I met Bruno again I said, yes, I know you were not the one who was coding, but we have a problem here. Let's sit down and fix this and see into it So we had a discussion and then we figured out, yes, the internet artifacts were actually wrong and then we fixed that issue and solved it. Next, please so out, yes, the India artifacts were actually wrong. And then we fixed that issue and solved it. Next, please. So. Very dedicated. OK, so. So just a time check I see you have yeah 39 more slides okay we don't really have time for 39 more slides So maybe I skip over to the summary. So we did some formal analysis which is in the slides. I encourage you to look into it and this is for the tested TLS If you click on the formal analysis for tested, it will directly jump you to the No, no. OK, you don't have hyperlinks Okay, so. Okay, let's go on the bottom Okay, we're going to scroll on the bottom You have... So we did some formal analysis, we found that they are vulnerable to de-play attacks, and I will then just describe the summary a little bit Yes, thank you very much"
  },
  {
    "startTime": "01:16:02",
    "text": "So we found that it's vulnerable to replay attacks. The evidence actually is generated before the handshake has even started, pre-handshake solution and they provide no guarantees for that freshness. Next please So there is actually a need for standardized and formally verified or tested TLS not only from the IETF perspective, the draft being going from a real application perspective because confidential computing actually needs this kind of solution next please so the questions for discussion that I had next please are number one, is there really a practical attack? And I do not mean the theoretical one that the cryptographers talk about, hey, there is a if this happens and this happens, then that they can collide and so on is there really a practical attack if that did not mean the theoretical one that the cryptographers talk about, here, there is a, if this happens and this happens, then that they can collide and so on. Is there really a practical attack if that derived secret, which I showed for India artifacts, the mismass that was? there is there really a practical attack which is possible in that case if that stage was actually really missed in the in the formal analysis? Next please And I will come to questions uh i have a couple of questions and then i come to your question, so not. So just a final minute I come to your question and not. So yeah, I just the just a final minute. So is there a fix for RETLS? This I didn't explain next please I will skip this part it needs some and how to deal with under specification like the rital s specific that I showed was largely underspecified and the other solution for post-handshake that we are looking into is even more underspecified that they don't have even the publicly available artifact so how do we actually verify and get guarantees about that? Okay, I think that's it Next please. Okay, these are the references. Next, please. So I would really like to acknowledge all the people who have helped in this and yes so so i'm going to suggest we might just take questions offline because we're Okay, sure and yes so so i'm gonna suggest we might just take questions offline because we're we can probably get okay sure well we can skip my talk it'll be great It's okay, thanks I haven't written them yet, but I don't need to"
  },
  {
    "startTime": "01:18:01",
    "text": "No, I mean, yeah, we're over time. Okay yeah, thanks for, yeah thanks. I haven't written them yet, but I don't need to. No, I mean, yeah, we're over time. Okay, I'll keep back. Yeah, thanks for, yeah. Thank you Who we got next up? Chris Chris, Kidaki. Kidaki slides, dance slides slides Chris, where are your slides? V3. It is a V3, yes I can do it on my phone. I asked, I requested Are you requested? Okay, excellent excellent is V3, yes. I can do it on my phone. I asked, I requested. Are you requested? Okay. Yeah. I grabbed it can do it on my phone. I asked, I requested. Oh, you requested? Okay, excellent. Yeah. Routage Oh, jeez oh, geez Do you want me to stir? ear apart? Well, like, it's cut off. Okay, let me want to just run it? Let me see Well, it was cut for my thing as well. I couldn't couldn't Sure, stay. Somebody's trying? No, they're not there they were in in Like, they're definitely in the meeting materials Okay so maybe a re-chartress for us. B-3. Sorry, like, I, uh uh know? I, uh, my name is use. Uh, how do I know? 24-7, 25 There we go. This were the ones that uploaded later. Yeah, the latest, the, yeah, the last one up uploaded. It's good. All right, so you're running the slides I can give you control if you like. Sure Okay so, yeah, so you know, I'm very interested in"
  },
  {
    "startTime": "01:20:02",
    "text": "like what this group is doing and I think there are potentially a lot of good outcomes that could come from the expertise in this room Since it started off, I've been kind of wondering to myself, what is usable formal methods? mean? How am I going to use formal methods? I'm a pen and paper person I don't have any expertise in formal methods The answer I keep coming into is I don't want to have to learn them, but I'm going to find a friend who can help me prove the things that I need help proving But what am I going to have them prove? So that's the question I've been trying to answer. So this is an idea basically about how we should be writing security proofs for cryptographic protocol And I want to kind of just constrain things to IETF, because that's where we are Okay, so the problem to solve is the following We develop protocols that depend on pen and paper proofs that are very relatively easy to generate but are very hard for humans to verify and basically impossible for computers to verify So the problem this creates is our demand for provable security may one day outpace the capacity of review to actually check them carefully And I think this is being driven by two factors. One, we're doing more with cryptography today than we've ever done before. We're not just looking at authenticated key exchange, we're looking at lots of different fancy protocols that do fancy new things beyond this And we're also modeling attackers in greater detail. So that means we're looking, we want to rule out large classes of attacks. This requires us to model the execution environment in which the attack is operating in greater detail And what this means is we have more proofs to check and the proofs themselves are getting longer And this puts pressures, one thing this does is put pressure on people writing papers for conference"
  },
  {
    "startTime": "01:22:02",
    "text": "to either just sketch the proof for their security protocol or punt it entirely to the appendix of the paper. And I'm going to say the quiet part out loud, these don't often get actually checked because reviewers are not required to check them in most cases. So how? do we solve this problem? I think we all believe that mechanization can help reduce burden on human review And what I mean broadly is having a computer delegating to a computer, at least in part, the process of checking the proof But this requires expertise in formal methods today. We have to, in order to write a proof that is formally verifiable, you have to know how to use the tools So I just want to say this risk is not, it's not really hypothetical. There are examples in the literature and in standards of bugs and security proofs that were invalidated later, sometimes by an attack So I want to pick on a particular class of proofs called game gameplaying. Mainly this is because I'm just familiar with them. There are other kind of proof paradigms that are worth talking about here. Mainly, but we're focused on like computational security properties. So if Diffie Hellman, my Diffy Helping group is secure then when I instantiate TLS with it, so is TLS That's the sort of thing we want to prove. So we model the security of the program as a game that's played by the adversary So this game, we imagine running an experience in which the adversary has Oracle access to the game and it's messing around with the game state. And then at the end of the experiment, we measure some outcome, and we ask, what is the probability of the average? in in winning the game and what a proof does is relate that probability to the hardness of some underlying hard problems, like cracking AES or solving discrete logarithms, things like this"
  },
  {
    "startTime": "01:24:02",
    "text": "So the proof strategy is I think concept easy to understand. We have our real security game and what we want to prove is that this game is computationally indistinguishable from some ideal game in which the adversary has no hope of winning. So basically we can this says is any attack against the real system implies an attack against the ideal system which by definition, you can't attack So how to against the real system implies an attack against the ideal system, which by definition, you can't attack. So, how does this look in practice? Well, we proofs consist basically of a sequence of games, starting with the real game and ending up with the idea game. And each of these games we think of them as just computer programs like code, and we imagine to obtain in each game we're obtaining the game by modifying the code of the previous one. So game G1, we obtain by modifying the real game. G2, we obtain for modifying G1 and so on. And what we want to do is prove that each rewrite ends up in a game that is computationally indistinguishable from the original game And when we compose all of these little claims together, we get the result that we were looking for. So, mathematically, this is a perfectly sound paradigm, and it's pretty conceptually simple once you kind of gain the intuition for it. So how does it go wrong? Well, where it goes wrong, in many cases is the code bit the actual specification of the games and making this mistakes there. So to give you a sense of how mistakes can happen, I want to walk through just a very tricky example of a game playing proof. So this is from Mike Roselect's Joy of Crypto of how mistakes can happen, I want to walk through just a very trivial example of a game playing proof. So this is from Mike Roselect's Joy of Cryptography. It's an introductory textbook that's available on online. It's a great way to kind of get into provable security in this paradigm in particular for the first time. So we're going to look"
  },
  {
    "startTime": "01:26:02",
    "text": "at hybrid encryption, public key encryption So the way we define security under chosen plain text attack is as follows We have a game where the attacker, oh, sorry where we generate a key pair we give the attacker the public key, and then we give the attacker an encryption oracle, and it gives to this oracle two plaintexts, and we're going to encrypt one of these plaintexts In the real game, we say we'll encrypt the left plain text, and then in the ideal game, we'll encrypt the right plane text. So just the only difference between these two games is which plain text were encrored So we say the the public key encryption scheme is secure under chosen plaintext attack if no efficient adversary can distinguish between these two games two games so let's look at a concrete construction. The idea here is very simple. We have a symmetric encryption scheme, a public encryption scheme, and we're going to compose them in the following way. To encrypt something, we're going to generate a symmetric encryption key, call it TK for temporary key We're going to wrap this key with the public key of the recipient to get a string called CPUB and then we're going to encrypt the actual plain text with the temporary key to get this thing called C-Sim and that's our cipher text is CPUB plus CSIM And decrypting is hopefully obvious to focus You take the receivers secret key and unwrap the temporary key and then and then decrypt the cyp ciphertext with the temporary key So this is the thing that we want to say, prove say, prove is secure under chosen plain text attack And we're going to reduce to the CPA security of both the symmetric scheme and the public scheme"
  },
  {
    "startTime": "01:28:02",
    "text": "So that is, if both of these things, the constituent parts of this algorithm are CPA secure then so is the hybrid construction So how do we do this? So here is our starting point. This is G1 We've obtained it from the real game by just in line some of the encryption the call to the encryption algorithm. So you can see the steps that we saw on the previous slide, generate TK generate CPUB, and then generate C-SIM And so, you know, so our first job as reviewers of this proof, is ask ourselves, is the real game indistinguished from G1? And it's, yeah, in this case, it's pretty simple You can see we're still encrypting the left cipher text, and we're doing the same exact thing just by inspection we it's easy to convince our ourselves that yes, these two games are indistinguishable Okay, where do we go from here? Well, our goal is that we want to replace ML on the third line there of the challenge Oracle with MR and we can't do that immediately because the outputs depend on ML So, and we also can't appeal directly to the CPA security of the symmetric scheme because the key that's used, we're generating the key in the experiment. So what do we do next? The next step is to generate a new temporary key and use that to produce CPUBs so that we end up with CPUB and CSEM being independent of one another And what we can show is that distinguishing between G2 and this rewritten G2 and G1 reduces to breaking the CPA security of the public key encryption scheme. So this is the part that takes a little bit of mental gymnastics when reviewing a problem You have, so Mike Roselect likes, he has a very particular style in which he shows reductions"
  },
  {
    "startTime": "01:30:02",
    "text": "and once you get the hang of the style, it's kind of easy and intuitive to understand. But the job of the proof is to kind of convince you that this reduction is correct. But what does correctness mean? It's basically like it's what you want to convince yourself is, if I have a distinguisher between G1 and G1 I can construct a distinguisher that plays the CPA game for the public scheme that gets advantage in breaking the thing whenever the distinct a distinguisher that plays the CPA game for the public scheme that gets advantage in breaking the thing whenever the G1, G2 distinguisher gets advantage So I'm not going to try to instill that intuition upon you, but that's the job of the reviewer. And then we have a very similar thing for the next step where we actually replace ML with MR And here we're appealing to the CPA security of the symmetric scheme this time and it's a similar kind of reductionist style argument very similar in fact But it's so simple of the symmetric scheme this time. And it's a similar kind of reductionist style argument, very similar in fact. But it's, you know, it's so similar that Mike, you know, admitted this part of the proof. It's he didn't list the game in full detail, right? And similar for the next step, we, and this is totally like, it's not in there at all, it just says, it's, you know, just do the steps you did before in reverse. It literally says that. It's a textbook. It's fine, you know it's, it's, it's for people learning But yeah, in any case, it's left to read her to kind of figure out this next step, which is you need to use the CPA security of the public scheme one more time in order to get back to the code that we had before. But this time we're encrypting the right plaintext instead of the left plaintext Okay, so I don't totally expect everyone to understand that, but I want to take away kind of two main things One is, this is a simple example but it's very common for papers in the literature to skip steps because it seems trivial"
  },
  {
    "startTime": "01:32:02",
    "text": "And that's fine, I think, for an example like this, because it's it's for learning, it's pretty low stakes, but when we're talking about a real protocol, that gets very complicated, the detail like, it's easy to make mistakes in all the details I also wanted to talk quickly about Mike's style here which I think is really pretty Does everyone follow the style? No, they don't. This is very suitable for this textbook, but in this textbook, we're not going to necessarily be using every technique that's available to us, every proof technique And there's also the question of like how complex is the problem space? So what you find in the literature, is lots of different sets of conventions for pseudocode And these are sometimes the sophistication of the pseudo code kind of depends on the application but it's also just kind of a matter of taste of the authors And you'll see if you look at like one of your favorite cryptography if you look at their papers over time and you see how their pseudo code change, I think it's really, you can get a lot of insight about what's going on in their life I think, through that process So, I think we need to change the way we write proofs It's obvious to me that the proofs that we write need to be immediately amenable to mechanized verification. And I'll say precisely what I mean in the next slide I also think, but I also think that generating these proofs should not be harder than pen and paper. I shouldn't have to learn formal methods. I know that's a controversial thing. Maybe that's not going to turn out to be true, but I think that a separation of concerns is one way to make formal methods more usable. We focus communities on different problems On the one hand, you have a subject matter who's generating a proof who understands the domain, who understands how to design protocols in that"
  },
  {
    "startTime": "01:34:02",
    "text": "space. And they built and their artifacts are something that can be consumed by someone who's an expert in formal methods, who verify the proof, whose job is more narrowly focused and doesn't necessarily need domain expertise in order to contribute positively to the work I think the sort of thing is what we need to have just more productive interaction across communities and across disciplines that are just culturally kind of different So I think the solution is quite simple and it's, I don't think this is the first time this has been proposed but proofs should be software Protocol specifications and security definitions, and then each of these game transitions that we, that we kind of make up the bulk of the proof, this should be written in code First, I guess I messed up the order here. I wanted to give a flavor of like what formal methods people would actually be doing. They're very I guess I messed up the order here. I wanted to give a flavor of like what formal methods people would actually be doing. They're verifying claims like this. So, the, we have the, you know, we're looking, what is the probability? So we quantify overall adversaries and we say, what is the probability that in adversary, that when we run the adversary, with some experiment, we observe some outcome and wen lin different steps of our proofs we're looking at, we want to say that, you know, the probability distribution is the same And this comes up in a lot of different kind of rewrites that we see in proofs So, summary requirements that I would add for the programming language we pick would be it should be popular, it should be something that people actually use So I'm not saying, I don't think caucus appropriate I don't think F star is appropriate. I think a high level program language that is likely to be used by a lot of people It should be readable. One should be able to understand"
  },
  {
    "startTime": "01:36:02",
    "text": "the protocol without having to know this semantics of the language, and it should be deployable This is a bonus, but I think it's going to be important No translation, there should be immediate translation from developing the specification to actually code you can deploy and run And that's, I think, most important for like, you know, work at IETF So why not? Rust? So I think there's a few reasons for this, pros for this. It's widely used at IETF, it's used in production software Cryptographers love it because it solves a lot of the problems that we don't want to have to solve ourselves But that's just the software engineering side. There's also I mean, I don't know all of the tooling around here. I don't know this is, this is coming from a very narrow band of experience And I'm happy to be, I'm happy to have a discussion about whether this is right or wrong. There is this hacks tool chain being developed by Crispin. It translates to different back where you do, you can do formal analysis It's used today to produce formally verified implementations of standardized cryptographic algorithms, so ML Chem being kind of the most famous recent example They also have a formally verified or formally verifiable, let's say, implementation of TLS 1.3 So cons to this, you know, we can talk about how readable rust is compared to other languages For example, CFRG commonly uses Python for specifications. I don't think I think my impression is that Rust is easier from a tooling formal methods perspective Could be wrong about that. But yeah, that's it So I think the outcome that I'm hoping for is for formal methods people who don't necessarily know about cryptography, I would love to"
  },
  {
    "startTime": "01:38:01",
    "text": "see what you think of these this problem. I have some concrete examples that kind of illustrate the different sort of rewrites that we need to think about. And I think basically this is the thing I want you to go prove And if it works, it works But thank you very much for your attention attention Yeah. So I sort of like the idea you're proposing, but I'm pretty sure it's like the empty set in the middle I mean, when I hung out with functional programmers, the people who really do these formal proof stuff they are just different human beings We're dealing with something that is on the edge of NP complete yes yes And mere mortals that can just read something in Python don't get to that altitude that they're at. Yes is like climbing to the top of Everest without oxygen tank. I would like to go, but not happening right so I think you're constraining the situation there I think we need to get people who can do formal proof stuff working with people in the team But I don't think you'll get the scale you're looking for if you choose something that is you know, if you're trying to do stuff that everybody can use because I died don't think this is going to work in my lifetime So HACS is doing some interesting things I totally don't have a full, I think like, I have an intuition for what you're saying, like the problem that I'm, I'm, pointing to is very, very, very hard People are doing very sophisticated things with hacks. I think I don't know if this problem"
  },
  {
    "startTime": "01:40:01",
    "text": "is solvable by hacks. If we could prove claims like this Yeah, I don't know. If this is the thing that, like, if there's something technical, like, technical, if there's a, if there's, if there's something about this, I want to understand how hard this is, basically I think it is definitely different than what people are doing with hacks. This is, this isn't a use case people have envisioned for this specifically, but if we can do it, it would be great. Yeah, I don't know You're probably right Well, I do, you know, I just, I do hang around with other conferences. I've seen folks that are doing formal logic know. You're probably right. Well, I do, you know, I just, I do hang around at other conferences. I've seen folks that are doing formal logic-proof stuff and they'll, you know, they'll look at something on the page and they go, oh, line 76 is wrong and I look at it, it's just set theory stuff that I i'm going i have no idea how you a A, even read that and B figured that it was false right? I mean, and this is a skill set that folks in this community just do not have. And by whatever mechanism I can't imagine that you can get that transplanted out. So somehow I think you need to meld humans together, but I don't think the individual people here will be able to get that anyway okay, I we're running quite no on time here. I love to be true with that. Yeah, I I'll do my best to prove you wrong. Thank you So we're over time. So if you can be very very brief, that'd be good things. Sure. Linnard from ETA Zurich here. Thank you for this very inspiring talk. I think this is mostly a clarifying question. I was wondering about the deployable part that you showed in the diagram towards the end. Are we talking here about a deployable implementation? that acts like ever as a horrible, inefficient reference implementation? or do we want to have a form verified library that we can deploy? that people actually want to put into their products?"
  },
  {
    "startTime": "01:42:01",
    "text": "because it is actually fast TLS That's a very good question So my expectation is that the reference implementation should be as clear and as simple as possible without necessarily optimized But if you start with like Rust code, then or Go code or C code or whatever language we pick and optimize from there, I just feel like the gap between the thing that you know and the thing that you don't know is smaller and more constrained or constrainable, let's say So it's not, it doesn't mean like we're going tom strickx up birdie one, I think it's called birdie. So there's a, there's a, rust implementation of TLS 1.3 called bird it's not, it doesn't mean like we're going tom strickx Bertie one. I think it's called Bertie. So there's a Rust implementation of TLS 1.3 called Bertie that is formally, it's, it's you can transcribe it to F Star from rust using hacks anyway it's I think it's probably efficient enough, but, but yeah That is very quick, yeah I'm sorry, the cues, blows, we're too short of time Hi, I'm Felix. I just wanted to utter remark for my gut feeling. So I understood your motivation of the presentation as we often have problems in checking proofs, but generating them not so big of a problem. And and I was one like you basically draw the line of we have the engine that can also generate proofs, and then we have the formal methods people who check these and in my experience coming from the tamarine world that that is not really how i experienced the world. I often find that engineers even struggle with writing down a concise threat model. All this security properties they want to actually achieve They have a good intuition for them, but if you even struggle to write down a security property, I think you'd also struggle to write a proof. The person I have in mind is a, provable security person who is also"
  },
  {
    "startTime": "01:44:01",
    "text": "a software engineer so it is two skills Again, sorry for coding people off, we're a bit short of time So Robert, I can give you control of the slides thank you. Thank you Is this sound good? Sound is good. So you should have control of the slides Could you do? Great, yeah we're in short of time if you could do it in like 10 minutes that'll be super good i don't know if that's possible 78 slides All right, all right I try my best. Yeah, I wanted to see start. Sorry, Robert, before you start, I think we had planned to have Francois just give a little advertisement for some work that's happening with SSH, hopefully Francois, I don't think we'll have time for slides, but basically there's some work, hopefully restarting on SSH some formal methods work on that could be good, and I think that was Francois's four slides so forgive me for it yeah. OK, now Robert, go ahead. You have the rest of the time All right, okay, so this is about specifying protocols. And I will quickly short and talk about my biases because there's many people in the group that are doing formal analysis of software and so on So I'm doing security protocols usually. Things like T.A WPA2, things that are a bit less like this like OCSP Stapling and PKCS-11, but I do work with the two predominant tools in this domain, Tamarin ProV and to sort of fix the domain domain The specificities are that they are theorem provers specifically for security so they include by default Adolf Yao attacker, which is an attacker that assumes perfect cryptography but allows mangling messages that are being sent on the network It also targets unbounded sessions that which would sort of contrast with Ben Greenen's model checking approach where you try to look for small examples, right? where he thought of the state of the art is to look for"
  },
  {
    "startTime": "01:46:01",
    "text": "unbounded sessions, but only for protocols, not as general Where we don't have decidability so the tools tend to work well with how protocols usually look like, but we can also prove that they won't work on any protocol, and that we focus on safety properties. These are things like authenticity or secret Basically, things can go where you can see it goes wrong from a single run of the protocol what I'm missing here is things that you would use general purpose verifiers for like proving functional correctness, the verification of the implementation using program verification, properties that are specific to low-level protocols on the net purpose verifiers for like proving functional correctness, the verification of the implementation using program verification, properties that are specific to low level protocols on the network level and lifeness properties So this means that you don't get stuck. That's a property for example, resistance against DDoS attacks, right? right? So within this domain, we are doing as usual, informal verification. We have a system and we prove a property about this. A system could be a protocol and the property is a safety property Okay, but where does it come from? We read RCs and then we saw of solve the non-determinism that Osama talked about. And create a mathematical model from it and then we verify this model And I want to talk about how, which language might be, might be good for this and what these languages are currently missing. So the two languages are defined by the two predominant tools in this domain which is the Temimprover and Proverif And I try to be quick in going over how they look like OK, so we have a protocol and we'll only show you the initiator, the role that takes sense the first message. In Temerun you define, rules for every step where a step is defined by a first challenge that certain facts are in your current state"
  },
  {
    "startTime": "01:48:01",
    "text": "In this case, there's a special fact that says some ephemeral key is fresh, and then there's a part of your protocol a fact established by your protocol. We will see the rule that generates it an a second that says there's a long-term key identified with the initiator and then these rules rewrite the state They remove what's on the left-hand side unless it has this exclamation mark and then introduce a new fact. So we would introduce a fact that's says we are this new state and that we want to send a message out Right. And then if the initiator has a second step, it has to start from the previous state Usually it reacts to some input doing so So if it isn't the previous state and uh get some input then it does some verification things So these are actions, that's the stuff that's in the middle, and establish a and gets some input, then it does some verification thing. So these are actions, that's the stuff that's in the middle, and establishes a session key. And the empty set is what? we substitute with, meaning that everything on the left-hand side is removed Okay, and also set up is no different from the specification of what the parties do So the setup, the thing that says this party, has this long-term key means that we take for a fact that this is a fresh key not used before and we establish that party A for it example, has a specific long-term key. Okay? contrasting this with Pro-Verf, we have a process calculus. So this is the other way of modeling protocols that's predominant in the academic literature, where the blue rule on the left-hand side corresponds to the first part of the process, which says you create a new key your output something, that's the same output as on the left-hand side and then the green part is the second rule right? So the continuation indicated by this fact in it one would be displayed"
  },
  {
    "startTime": "01:50:01",
    "text": "as just sequential composition using the semicolon, like in C and just going into the next line And likewise, the setup of the whole system is done in a process Okay, so these are the two ways When we look more closely into the RFC, we see the system is done in a process. Okay, so these are the two ways. When we look more closely into the RFC, we see though that there's something missing more closely into the RFC, we see, though, that there's something, that there's something missing. So this is the RFC of WireGar, the VPN protocol and we see that we can pretty clear get the message flow and the setup of the RFC reflected in the model What we observed, though, is much of the RFCs is concerned with message formats. This is the message that the initials actually sends and we have to tell the image implementer how many bytes have to be reserved for the type of the message how many bytes we reserve for future applications and so on. It's something that the implementer needs. This is our we have to tell the implementer how many bytes have to be reserved for the type of the message, how many bytes we reserve for future applications and so on. It's something that the implementer needs. This is often abstracted by us So we say just everything that goes into there the identity of the party, the identity of the intended responder, and the actual cryptographic message and the signature on them, they just go into a top of this, that's the angle, angle brackets And we noticed this particular in a project that we've been working on which is for monitoring implementations for speculations So we had in the earlier talk with the discussion of whether you want to have a reference implementation or a very fast implementation and so on So if you have a very fast one, you want an easy way to check whether it appears to in specification and this is one way to do it So the idea of this specification monitor is that you have a fast protocol implementation and you observe the network output using whatever system at runs in by observing system calls or something like that"
  },
  {
    "startTime": "01:52:01",
    "text": "You also observe the call mix to a cryptographic library if you know the cryptographic library that is, if you choose a popular one, you have a good chance that you find applications that do this And you generate a stream of events that complete captures the behavior of this implementation And now this specific tool monitors if this is a possible outcome of executing the specification. And why we were doing this, we noticed that we need the format strings inside the specification that is usually used for verification in order to really understand the messages that we receive Okay, so we had to be develop a message format language that we integrated with Tamarin It contains function symbols for typing things as integers by bytes, or strings, which in the view of the verifier are in naught because it doesn't need it Concordination and operatic operations that are abstracted by just keeping everything in pairs, basically as was done before and function symbols for reversing things to adapt from big Indian little, Indian to little Indian order, and also addition because we often have, we also symbols for reversing things to adapt from big Indian to little Indian order and also addition because we often have counters in protocols Okay and this again is the snippet from the Wiregard specification and this is how it would look like. I try to be quick about this, but we can basically describe the concrete types the blue bit across corresponds to the blue bit and the code and describe the formats now Okay, and this I think is potentially, useful for specification purposes This format can express the setup of parties"
  },
  {
    "startTime": "01:54:01",
    "text": "It can describe how parties react. This is basically what I showed in the second slides. And now during due to this addition we have a way to also describe the message formats, which was not possible before There's some caveats when I think about how this should be done for RFCs in general, how I would imagine as a verification person, to idealize them One problem is that when we use something like Tamarin, we have to facts and they are named and there would need to be some standardization of this. For example, the first rule of the initiator should be called initiators start throughout the box Throughout, sorry attractive to look at pro vera to this end because there is no explicit state unless you model real complicated state machines where you need to use other means of encoding them A secondary use besides this slide addition to the existing language, for describing protocols, is that spec one allows us to validate the spec against the implementation And you might say, we don't really care for this, right, because we usually specify things before we implement them But is it really true? though, isn't it that that we are? have implementation? that we base the specification on? And isn't it also this case that the specs do in fact contain? errors when implementers start implementing based? on them? We have talked before about another specification That's not so rare. Sometimes they have errors That's not so often, but under specification, I think good part of the audience probably has experience with this kind of observation"
  },
  {
    "startTime": "01:56:01",
    "text": "The other advantage, big advantage for me, is that verification with Tamer would be possible similar to the Hexback framework What about ProVev? We have some work on ProVos that translates Tamarin to ProVerv that could help and there's also tools that translate from a third language into Tameran ProVerve So you could use that. The message form is language into tamren proverbs so you could use that the message form and so the message form and so the idea is not very tied to tamer in particular and could be integrated into other languages OK, so we try to see if this works in practice with the WireGuard protocol, the EVM protocol It's how I connect to my workplace when I'm in home office And we found the following differences between an original model put out by the authors of the protocol on the website, which they used to prove it on Tameran which is excellent that they did this, right? And we created, we adapted to bring it to the point where we can monitor the reference implementation to see what the difference are. So what is the difference between a model meant for proving things and a model good enough to actually monitor the outputs of an implementation? And we found that the message forms are necessary. We can't get around it because otherwise we can't interrupt the messages. We have to know which order they are, how long they are we have to be able to do parsing We also found that the model doesn't talk about how long-term keys are established In reality, in the implementation, they are stored persistently in a configuration file, and we need to express this step in the model again. So this is something that is also, we also look as a third category into the RFC. The RFC has the message formats, of course, but not how exactly long-term keys come to play, right? play"
  },
  {
    "startTime": "01:58:01",
    "text": "Then we found that there's a difference between how the implementation computes values and how the model assumes this is taking place or how the RFC would specify this. When the implementation gets, so we wanted to do it for performance, so we had tons of clients tons of servers communicating, and we found out that the implementation, if this happens, opens a bunch of them workers that run in parallel to really increase the throughput and that necessitates that some of the pre-computation takes place before the work spawned and some afterwards and we found that neither the model, not the RFC are, representing this I'm not sure if they should but it is basically something where the implementer makes a choice that is potentially relevant for security but it's on that one sorry robert just let you know we're down to about a minute to go okay all right so you can check the slides if you care for the differences in terms yeah so the conclusion the takeaway message is for protocol specification message formats are important, and we added them to Tamarin and we could add them to Prover and make them more useful or make them useful and maybe that's enough right as an extra it also permits monitoring which might be a very easy way to double check the specification if it's realistic enough Yeah, and I think this might open up some questions that I think I would just put out here at all the time. Yeah, maybe you guys have some comments on these questions Sure So I guess we're about 30 seconds away, but then"
  },
  {
    "startTime": "02:00:01",
    "text": "comments on these questions. Sure. So I guess we're about 30 seconds away, but let's have a quick question. We'll use the time where we have it Okay, very quick. So what's your plan for proverick? If I understood you correctly, you are saying that you have for Temel? but not get for Proverif. Is it correct? And if yes then what's your plan for Proverif? Like the problems I showed would be very nice to have such a tool. I find it very I think that would be very helpful and useful uh-huh i didn't have any plans to do it but uh if there's demand for it, that, uh, that, uh, a reason to do it. Right now, the monitor does, it might make sense to integrate this into Sepik Plus, which translates to both tools and then have this as an intermediate layer. This could probably be the simplest way of doing this Leonard from ETH Zurich here, so very cool work Congrats to this. So two quick questions Where can I find more about this? Is this already? published work? And the second question is, we didn't really talk about the monitor at all And I was like wondering, given that we are dealing here with with encrypted messages, how do you monitor that divide that go out to the network, actually, I don't know are, like, correspond to the byte of a cipher text where the right inputs were used according to the tamarine model yeah that's true magic no um observed the cryptographic the output of the cryptographic library. So we would see that this party is taking a message It's encrypting it. We remember the bit string. We know the message format. So we want to see a certain bit string on the network next And if we don't see it, we shut off communication Of course, it's more complicated than that because we have non-determinism and protocols all the time and we have to account for this, but also the nice thing is protocols are used we shut off communication. Of course, it's more complicated than that because we have non-determinism and protocols all the time, and we have to account for this. But also the nice thing is protocols are usually very good in resolving it themselves"
  },
  {
    "startTime": "02:02:01",
    "text": "We just have to be able to do it if they pay And for where to get it, it's just been accepted at CCS So come to CCS, see the talk of my students I will be there. Great, great, thank you Thanks everybody for coming along. Thanks to all our speakers, and unfortunately we don't have time for jonathan hui talk talk But yeah, so we're talking the list about whether to organize some like next steps and meetings and whatnot so thanks all and enjoy the rest of your IETF week Okay So this is your coming plan, right? You just put yourself from the tail and you get ten"
  }
]
