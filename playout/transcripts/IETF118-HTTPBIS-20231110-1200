[
  {
    "startTime": "00:00:06",
    "text": "I, I, yes, I uploaded it, but please don't do the system's funny. And stuff that we load in the last bit, it may not actually show up. So move your 6. Yeah. And it causes us stress to have to do it the last mention. But, no, you know, don't worry. Okay. We Alright. Look before I responded to you now. I know. Yeah. It's that time of the week. It's it's Normally, don't even have session goes Let's just talk me to my side. You know, Friday afternoon sessions could be a lot when you attended Imagine Friday. Investment Sorry? Yes. Yeah. Understood. No. I would probably notice Chris Should we get a minute l."
  },
  {
    "startTime": "00:02:02",
    "text": "Yeah. That's if you my lap, my lap, my lap, my lap, my top, to see the speaker people on where Cheryl not for this reconversal. Frank Freeman. Thank you. We'll get started in just another minute. And and just in case you're wondering, this is HTTP if you're here for something else. This is the wrong room. Oh, yeah. Is anyone willing to, be our scribe for this session. Can we have volunteers, please? Thank you."
  },
  {
    "startTime": "00:04:02",
    "text": "And if folks would would help out on, on on on the No. Pad, pad, that'd be helpful, especially during Ex presentation, I think, It should be like Okay. Alright. Let's go ahead and get started. As I said, this is repeat, We have on the agenda blue sheets, but that's largely, automated now. If you haven't already, please use the QR codes to check into the session with data tracker. So we can track attendance in these sessions. Have ascribed. Thank you very much, Eric. And if you're not aware of it by now, the terms and conditions of your participation here are on the note well, as we say. If you're not familiar with this, look up IETF note well in your favorite internet search engine. This covers things like intellectual property, anti harassment, behavioral, norms, things like that. And and we do take it seriously here. So Our agenda for today, we're going to discuss 2 active drafts. Oops. We have, resumable uploads. For Marius. And then we have Tempe to connect TCP. From Ben who's remote. We covered cash groups, in our session yesterday. So that shouldn't be an issue. And then we're going to move on to other topics. We have 2 more presentations from Ben security considerations for after the optimistic use of HD upgrade, and the reverse HTTP transport. Then we have Eric's presentation on secondary cert authentication of servers. And finally, we have a kind of a combined session on on two proposals in in that that have a little bit of overlap. One braid, which we've we've talked about before, and one per resource event protocol. Any agenda bashing that needs to take place. No. Okay. Let's get into it then."
  },
  {
    "startTime": "00:06:02",
    "text": "First up, resumable upload. There is. Do you wanna drive the select to show me? 5555 Okay. Alright. Welcome, everybody. I'm gonna talk a bit about resumable uploads, especially what has happened since the last IETF meeting and then also discuss a few points that currently on the agenda. Just a brief recap, resumable uploads. We try to make give clients the ability to resume and upload if a large customer requests. A large body was interrupted, just to make it everything a bit more reliable. Next slide, please. So We had, hecathon project. for the first time at this IETF, Can I have the Right? One project that we worked on was an automated server formity checker. Basically it's a suit of tests that is that is currently written in Python, and you can basically point it to your server will run a few tests. Are based on your current draft specification And then it will tell you, oh, there's a few violations with your implementation, or you did everything. Correct? It's a handy tool to improve interoperability between different implementation. It also just helps you to spot a few issues in your implementation. It could be a handy tool to run-in your continuous integration pipeline. And we've also tried this out at the Ecataan. A lot of thanks to Uptek, for working on this, there's also a GitHub repository if you wanna check it out. And next slide, please. Here's a brief image or screenshot how it looks. On the top right, you here for you. Green dots that indicate a few tests were successful. A few telists. Didn't fail."
  },
  {
    "startTime": "00:08:02",
    "text": "For example, the server returned a wrong response, and that indicates that I have to fix of my implementations. Next slide, please. There was also another hackathon project where we try to integrate resumable uploads into proxies. The idea is that proxies then can handle the resumable loads without the application server ever having to handle them, on their own. So for example, as you can see on the scheme below, the proxy will take care of handling their resumable upload requests. Concatenating the data together, once the upload is complete, it will then send it in a single request to the server. Which is pretty handy. Then the server doesn't have to take care of all of this. And you can easily add it into existing applications. Merline was so kind and wrote a plugin for Katie, works really well. I tried to do it for NGINX, but didn't get so far because I'm not good at seeing But this is, I think, a pretty interesting approach for some applications not for every application application. Next type, please. Just a brief recap, what we did since the last draft We published draft 02, shortly before the IETF meeting. And changes since Oh, one, a lot of small, but still nice. We replaced the upload incomplete header with upload complete. Just immersing the logic to avoid a double negative And we also clarified, how content length is handled I'm basically making sure that whenever the client tells the server, how big the entire upload is, this information shouldn't change. It should always be consistent across requests. We also got a few editorial changes. Thanks a lot to Mark and Lucas for those. So the document is now a lot nicer to read. And a lot more in line with HTTP in general. Next slide, please. So this brings me to what has happened in the past. There's a few things that we would like to here there's 4 issues in total, one per slide, and I will just"
  },
  {
    "startTime": "00:10:02",
    "text": "briefly like to go for them and then maybe ask you for comments if there's any One of the biggest one that we currently, Facebook is the media type for the pet requests. We use patch requests in order to append data to an upload. And the specification requires that request have a media type so a server knows how to interpret did theta in the body. We currently don't have one, so we need one. There's a few options for this. Option 1 is in line with what we're currently doing in a draft. Where we say, we send a patch request This patch request has a specific offset in ahead of field. And the body is just the data that you wanna right to that offset or append in our case. This is really simple because you just take the request body. And for example, if you write it, a file. You just append it to the file. It's real simple. It's nice. But what would an appropriate media type be for that? We could say we use application octet stream, because in the empty body, it's just like a ice cream. It threw through really have to parse it. There's no additional information in it. But it also seems kinda odd because you're effectively reusing immediate type, that is the default media type, so that might also be a bit confusing or problematic. We could also think about creating a new media type for example, application slash offset plus octetstream, which indicates the request body is just a bite stream without any special format. But it doesn't start at the beginning. It starts as a specific offset That's why you have the offset plus in there. And this offset would then be specified in the Quest Head of Field. So that will be one approach a bit more, explicit. There's also another option, Indhcp API working group, there's currently a proposal about bike rank patch with defines a message byte range media type. It's a bit more complex. It's in itself, and encapsulate a patch document, I think"
  },
  {
    "startTime": "00:12:01",
    "text": "So the offset and the file data is actually included in the request body. Meaning that you actually have to parse the request body to get the offset out. Compared to option 1 where the offset is inside a It's a bit more complex. But that's another option. Maybe there's other options as well. I'm not sure if there's other existing media types that we could reuse. For this. So is there any comments, about from the audience, about Can we use a new media type for this or whether or not? Jonathan. Hey, Jonathan Flat Apple. I think that the bite range patch does have some in use cases, but I think that in resumable uploads case, it adds some unneeded complexity so I would be in favor of either 1 or 3, anything that keeps essentially, the representation just partial representation in the body. A lot of client and server API is also manage checking header fields or adding header fields first and that would allow them not to have to modify the body. Maybe make it simpler Maybe option 3 could be like a partial put with the same content type as the original resource But I'm not sure for option 1 or 3. Awesome. Hi, Austin Wright of the, aforementioned Wait range patch. Yeah. I have no problem with, like, not using a particular media type, if I've, suggested that the partial put might actually be appropriate year, which is specified in 91, 10."
  },
  {
    "startTime": "00:14:01",
    "text": "And the only reason I actually publish proposed byte range patch was because of the case where you're want to do a partial put and you don't know if the server supports it or not, in which case that could be corrupting or something, but I don't think that's the case here. A partial put would probably just fine. And that's essentially the same thing as doing a patch with What? I suggested doing, like, a, a patch with Octet Stream, should be the same thing. If if there's really a philosophic purpose to doing using the patch method instead of a partial put, Great. Like, I support that. But, in that case. So it Yeah. Option 1 Seems good. And also, It supo presumably, this patch is being applied to a resource that represents the upload body. In which case, the server should be able to support a number of different media types for the patch button. We're just talking about the one that must be supported because we know going to be universal support for it. But, theory, there's no reason the server should reject other media types. Yes. That's a quite good point. There could also be other patch formats in the future or outside this draft. And, I I got on cue just to say, if you do take and 1, the patch needs to be the patch message needs to be self describing. So application lock to extreme probably wouldn't do it. You've actually got an extra semantic here, which is pay attention to that header there. And so you probably do need a special a a media type for that. Understand that, Plus has a very specific meeting in media types. It it introduces a suffix So you probably wouldn't use that one on the screen. But we can we can get to that."
  },
  {
    "startTime": "00:16:02",
    "text": "Yeah. That that's that's all I had to say really. Austin. Yeah. I I proposed using application octet stream for that. It it's kind of hackish I my thought process was, What else could octet stream possibly mean in a patch method? Other than saying this content is opaque binary data. And it's part of a window. And in order to understand which window that is, you have to look at like, the content range header It's hack. It only works in HTTP But, Yeah. Yeah. But content range doesn't say it's for that purpose. So Okay. I'd rather be explicit. Yeah. Anyway, Alright. Thank you very much. Let's go to next slide, please. This is also a bit of, bigger question that has arrived since the last IETF meeting. Basically, in HTTP every request has a response. For example, for uploads, this response might contain information that is extracted from the uploaded pile Now in resumable uploads, it would be really handy if we could design them in a way that they were transparent upgrade over regular uploads. Are not reasonable. The way how this could work is that you have a client and it uses a HTTP library to speak to the server. And the HTTP library can then decide if it wants to use resumable uploads under the hood, to improve reliability or to recover from eros without decline actually knowing about this. This is a really handy approach to get resumable uploads deployed a bit more widely. Depending on your application, But if we wanna do that, there's a few questions about how do we handle the responses? example, the first question is the HTTP library has For provide a response to client application. What should that response be? One approach is, to simply say"
  },
  {
    "startTime": "00:18:02",
    "text": "the last request that was sent by this by the client completed the upload. So at that point, the server had all of the data available that it needed it could then process it in a certain application specific manner, and generate a response. And this response would then also be the response that the HTTP Library provides to the client. So effectively, you could use that approach that for the client, it doesn't know the resumable uploads are used under the hood. It will just get a response back, and it might not even know if it the network was interrupted. Join the upload. Will be quite handy. But that also brings up a few the questions or opportunities. Let's call it like that. For example, what happens if the client uploads all of the data, the server starts crunching on it, and generates a response, but this response is never sent to the client because the network was interrupted at that time. We could then also say the client has self opportunity to, like, refetch the response. By sending an empty patch request. To the server. An empty patch request wouldn't modify the upload resource. It's already completed. And the server could just say, okay. Gonna respond with the same response I've generated before. Allowing the client to effectively fat chicken in response. And this also brings a third question up what, for example, happens if we receive a 500 status code, does that mean that upload should be retried because the server wasn't able to store the data. Does that mean that server was not able to process the file after the upload is complete, and resuming won't help. Should we have to distinguish between those two cases or say, whatever, retry for a few times. If it doesn't work,"
  },
  {
    "startTime": "00:20:00",
    "text": "it doesn't work. So yeah. Happy for any input here. Cassel Holcke, I say that Here, we are not trying to create a long running transaction protocol. HTTP as it is, isn't for that purpose. And consider that residual upload is about just fixing the failure, one particular failure mode of HTTP, we don't need to do something more than HTTP data. Already So I'd probably say that if the server says an error, that's an error, I'll let the client do whatever for 66 and I said, that'll be fine. And I got into I think, to say something complimentary to what Kazuko has said, which is There's a decision to be made here as to whether we're trying to model this in terms of of of a resource state, you know, they they abstractions that HTTP provides or trying to provide fidelity down to the message level. If we're trying to provide that, you know, fidelity to the message level, that's quite difficult, especially because we're using HTTP high level constructs to do this. But if you're just trying to model this in terms of resource state, then it's, well, you do get to the resource. It has representation. We've got tools like content location. There's ways to compose this in a way. A reasonable, but the application has to do if you're trying to paper over everything and pretend that this is the real response from the resource, that's a much higher bar. And I don't know if we can actually reach it. So, whatever that's worth. Yeah, Jonathan Flat. I guess, echoing a bit of what Kazuko and Mark said. I think the in response to something like a 500 status code, maybe you're retry certain number of times, but I don't know if we need to specify that."
  },
  {
    "startTime": "00:22:00",
    "text": "For number 2, I think that if it misses the response, for some reason, maybe a time out, maybe the connection was interrupted. Any reason like that, I think instead of just sending an empty patch request, the client should try to do offset retrieving again and maybe get the upload offset of the complete resource at which point it then sends the patch request. And, hopefully, it retrieves the final response. Yes. That that will be the way how that would work. Yes. Thank you. Austin. Fantastic. Austin. Right? Yeah. Treating long running operations for on the server is something I would love to treat eventually. And, actually, that's sort of what got the end to doing the bite range patch. But that is sort of a different thing entirely. For getting the what what if the client for number 2, what if might misses the response. They're, understanding is the content location header does exactly this. It's providing the URL of the document that's in the response. To the request. And you could even provide that in a 103 early hints. Thank you. Jonathan. Yeah. Jonathan Lennox. I feel like Some of these questions are asked with the intention of the Hackathon project. You mentioned we're proxy can transparently turn up will upload into a nano Zoom upload, but I feel like That's probably not in general pop without the proxy knowing something about the application semantics because obviously things like you would really rather not you know, do the whole upload and then discover the application server says, no, you're not allowed to upload here. So, I feel like"
  },
  {
    "startTime": "00:24:04",
    "text": "There probably there needs to be more coordination between the proxy and the application server if you're gonna do that. Get it moved to her looking. At which point, some of these can be application logic, not just generic. Yes. That's a very good point. Implementing this on a proxy level, which also impose a few limitations, and make it all at work. Not as seamless as if you integrated directly into your application. That's correct. Alright. Thank you very much for, the responses on the response question. And then let's move on to the next one. So this is a rather smallish thing, at the last atf, we talked about using informational responses in order to carry information about the upload progress from the server to the client. And there was interest in getting this or using this. And now we have a proposal for this. It basically looks like the response It's a bit smallish. It looks like the exchange on the bottom slide. So you, for example, the client sends a post request and says I'm gonna upload a 100 bytes. And then the server says, okay, that's great. You wanna resume, use the following URL. And then it can regularly send those 104 upload resumption and support it. Informational responses, and those include upload offsets telling, for example, in this case, Hey, I uploaded or I safely stored 50 bytes. Indicating to the client that the client can forget this data. It doesn't have to keep it in buffer or in memory or on this anymore. Because it already is completely saved on the server. The rest of the response is just the final end of the upload. And that, is a pretty nice pattern. There's a few questions around which headers should be necessary in the 104. If we make upload offset mandatory in every 104 or if we rather say just do it if you have some information"
  },
  {
    "startTime": "00:26:01",
    "text": "There were also some questions on GitHub about whether location should be mandatory in every 104. Basically repeating the location over and over in case that one informational response gets lost. I'm not sure. Is there any feedback on that proposal. Because the whole in general, I think sending off satisfying. I I'm not sure the clients can rely on that. To re to release memory, but and going to the point of proxies drop in the informational responses and not that alright. I'll be in the space. Actually, narrowing losses that every information or response has to be forwarded. By the proxy. That's my understanding. In general, I think that this is safe. If we are to allow main multiple location headers to be sent, Probably, we have to consider what? We need to do it when the values are different. But maybe that's all. Hey, Luke's party. Yeah, Zurra commented on the issue. I think my memory was tainted by something, I don't maybe when I 103 really hints because just a hint and technically, the proxy, like, has to forward them. If it didn't and it kind of doesn't matter because the final thing override the hint or something like that. So not saying, That's the right design model. The RFs are seeing 91100 is very clear on what the proxy should do. And we we come necessary. Police that. But yeah, I was I was wrong. So Thanks, Kazura. Great. John and Black? I think echoing something. I think I read on the issue. What would be nice about also including the location header in these subsequent, one of four responses is that if one happens to get dropped some for some reason, you can still figure out"
  },
  {
    "startTime": "00:28:01",
    "text": "where to zoom, in that case, from, like, a future one. In that case, though, I think we should enforce that all of the location header field values are the same Yeah. I think that was the broad up that if one of those information responses is lost somehow. That if you send it again, then the client can still resume and retrieved a location. And that's the point where I'm not really sure if that's a common scene in deployed infrastructure that you lose one informational response, but the next one will get through. I'm not sure if that might also be depending on the HTTP version, it's used. Yeah. Just my response, I that feels like premature optimization, maybe, or over engineering I don't see that as a likely scenario. And if if it happens and much worse things happening probably. Yeah. Yeah. Just just to kinda echo Mark. I think, yeah, lesson not over index on that bad think my comment on the ticket was more like you're either gonna get all of them or in the middle of them. And the model we have would would work with either of those scenarios. The thing in the middle where you get some or not no. Fuck. On if you're doing that, you've got your own problems and you need to sort your stuff I There might be somebody using a a client's side tool that that doesn't expose an API for 1xx. But but but but That's their problem. They'll know about it. So, so Alright. Thank you. Next slide, please. So this is a last one, of the bigger ones. About interoperability with HDP Digest the ideas and the last ITFs, they were also expressed interest that we use checksums to verify the integrity of uploads. And basically, decline could say, Hey. I have computed this checks for the File,"
  },
  {
    "startTime": "00:30:02",
    "text": "and then the server can after the upload verify if the checksums match. Which is for some applications, pretty important. There's no other questions about if We, if we that's probable. If in the draw for original mobile uploads, we should recently mentioned how interoperability with the digest work. For example, if the client includes a digest in a patch request, but that patch request is interrupted in the middle then there. Respond or the request body that was received by the server, doesn't match with the digest. It didn't receive all of the data. Should we, in those cases, say, okay, the server should reject it? Reject the data because it wasn't able to verify the checksum. Should that be have your application dependent. Because the digest draft, as far as I know, doesn't m or doesn't, specify rules on what should be done if check some don't match. So, yeah, a bit of question. Like, should we specify this? Should we leave it out? Yes, please. Yeah. Lucas Pado again. So there was a discussion on the mailing list think Rob's raised it, and that was a while back. My it's my mind we should just, like, mention that integrity can help, but is not needed. We should definitely punt this up again. It doesn't need to be discussed in this draft if you wanna develop a But a service that uses resumable uploads, and you wanna describe a profile of HTTP makes that service works particularly well and and things Great. But but if if a patch fails halfway through, there's there's other ways you can see that way. Like, you would have the contact length of the patch and know that the pipes didn't work. We we don't wanna explain all of HTTP in this document. So think I'm voting for do nothing more than just mentioned possibility that you could do this. And even that is maybe not required."
  },
  {
    "startTime": "00:32:00",
    "text": "But I can prepare the text. I I volunteered to do that already. I just the cops, the cops So, like a piano, and we can can review and see what people feel. you very much. Alright. Thank Just just to add to that, Yeah. Generally, our philosophy is to to this describe the components and and allow people to compose them not to have type relationships between them. Alright. That makes a lot of sense. Thank you. So this is the last slide. The things that I brought up are just like a bit of the bigger things that we would like to figure to one. There's also other open issues. There's a bit of discussion about Should we handle errors during the upload creation a bit better? Also recovering from our errors there. But, We also have another small issue about allowing the 200 status code for offset retrieval currently, we only allow 204, but why shouldn't we allow 100 as well. That doesn't seem to be much downsides to that. We have also brought this up to the, what working group, to see if there's interest in implementing this into browsers. There hasn't been much going on. If there's any comments on those, feel free. To answer it. If not, That's the presentation about reasonable uploads. Any any other comments about this one? Okay. I just wanted to say, both thank you and congratulations for having, tests and for doing that work that is so important and, unfortunately, unusual. So, other draft authors take note. Thank you. Thank you. Now now we begin the Ben Schwartz's show. Coming to you in 3 parts. Okay. Hey, everybody. Great. This is connect TCP. Next slide."
  },
  {
    "startTime": "00:34:01",
    "text": "Just in case you have forgotten about Connect TCP, here's the quick reminder. This is, HTTP connect in the style of connect UDP, but for TCP. So the proxy is identified by a template and that template generates a request with a scheme and an authority and a path. Next slide. We had a long discussion at IITF 117, related to some fun questions about HTTP upgrade security, but gonna talk about that later but Closely related to that was a discussion of optimistic content Sending So there have been a couple of changes in the draft text related to that. But the actual technical content in terms of, like, interoperability has not changed. I think this is ready for working group last call. Next slide. This is the entire diff between really between dash 01anddash02? Basically, like, control And, you know, control f find and replace, false start with optimistic and, a few other adjustments about how the draft talks about early data and the 2 early response there. That is all the changes. Between these draft versions. And I should say that these changes are are largely editorial. Clarifying the rules here. Alright. Any questions thoughts on this."
  },
  {
    "startTime": "00:36:02",
    "text": "Yep. It's Ganazi Connect Enthusiast. So I filed an issue. I put it on GitHub 4 days ago, but worries if you haven't seen it yet. And Would it make sense to have a default template the way we did connect UDP and connect IP. I think it would kind of be nice to mirror those and have for the same reasons that we put it there, I think it would be good to put it here. So the the reason I didn't do that was that if if you don't have a template, we already have a a default way to to do TCP proxying, which is using standard HTTP can so if I if you're in the situation where you only have via a host name or authority to go on, you're all set for connect TCP. So it's not obvious to me that a default template provides additional utility there. If you think it's certainly easy to define. So if you think that there's value in it. We can do So my thinking would be that in a world where we're building kind of these new proxies, like, Apple's private relay, COMES ID protection. We're always setting up these things and, as I like to call it, mask all the things. It it would be nice to kinda have those things all altogether the the the same way. And That way, if we have that, we can kinda completely get rid of, like, a connect for, and that, like, wire format spelling completely. So I Not isn't crazy, but that would be my preference personally. So so my preference here, I'm I don't object to the default template say, but my preference for those systems would be to say they should be configured by a URI template. That URI template should have"
  },
  {
    "startTime": "00:38:00",
    "text": "All of the parameters needed for connect TCP and connect UDP and maybe connect IP. That's all just one URI template. And, you know, at the point that you're stuffing a string through an API boundary, that string is this URI template. That's a fair point. Let me think about it a little bit more Alright. I added it myself next. So I have one comment. On that discussion in favor of having the well known template, while I agree, Ben, with your point, I think for future things we could do with this, such as the pattern we see in the connect like, the the work that's going on in mask for having a, like, a listener mode of connect UDP, it, like, essentially uses the default well known URI template and replaces it with, like, a star star. End. I am interested in if if if if if if if seeing a similar thing for, like, a TCP listen proxy, which, you know, we we shouldn't be doing it here, but it would leverage and, like, would work well in the ecosystem of connect TCP and That's clearly a case where you're not gonna have, like, the standard yield connect proxy will not support that. And so this still provides default location for it Okay. I definitely need to think about that a little bit more. Sure. Can you take that to the issue? The other question I had more for sharing purposes as you're saying around last call and the relationship with the optimistic upgrade. I I I I'd have to double check Is there a particular document relationship between this and that I know that this one's safe on its own. But Like, is this something where we'd want to at least adopt that other work?"
  },
  {
    "startTime": "00:40:01",
    "text": "Maybe before going to last call on this one. To make sure As it stands, there is no relationship between the documents. Okay. I would be comfortable I would be comfortable moving them independently. Okay. Thank you. Yeah. Mike? Mike Bishop. I just wanted to point out, remind everyone, that one of the reasons that we want this is because legacy connect doesn't allow you to specify the host that you're trying the host name of the proxy as part of the request. And I think having that default template Legacy connect is not a useful fallback. For those of us properly and know whether we are a proxy on this request or not. And so having that default template might be helpful. That's It's better. Okay. That that seems like, a pretty strong argument. Eric. Another plus another plus one for a default template so we can get legacy have to go away. David's Ganazi. I I was looking on the GitHub. There was, like, there's a second open issue, which was opened by Kazuma. About origin, and it actually ties into this conversation a little bit, which is The the so the original HTTP spec is really not clear about see, like, if there kinda isn't one, And so in particular because there's no scheme. And so Then when you, like, say, you get into weird situations when, like, you receive all service, on your connect response and you're like, oh, but what origins does that apply to? I"
  },
  {
    "startTime": "00:42:02",
    "text": "Okay. I'm just gonna pretend it's HTTPS. I think it might be unfair to ask you to fix all that mess here. But it might actually be useful to have that discussion of saying, like, here explicitly in this document, what the origin is there, maybe discuss as part of the motivation for the document, why that's an improvement over the order the original connect. Yeah. That's, in the list of, you know, talking points, reasons to do this somewhere at the top of the document, that's a that's another one that we could add. Yaroslav. Jiarislafrasomac Josescaler, in in traditional connect there is actually a way to specify proxy origin that is used by some services if that connect happens to be dropped in the TLS, so called https. Proxy so that can be the proxy host name can be derived from SNI perhaps It could be mentioned. In the in the draft as an option for implementers that want a fallback mechanism to traditional connect I, I am inclined to avoid talking about SNI here. I think that this specification lives entirely essentially within the HTTP p specifications, and to the extent that HTTP has anything to say about SNI, that's really kind of, something that happens in a separate layer. And and and, Ben, just to be clear, your currently not updating 9110. Correct? Correct. There's yes. This this draft does not update anything. Because as soon as you start talking about the semantics, it can act"
  },
  {
    "startTime": "00:44:01",
    "text": "you're gonna need to go there. Alright. Anything else on this one, or should we move on? Okay. So I think we are now done all of our active drafts. We're gonna move on to various proposals starting with the one we were just talking about, Okay. Here we go. Go ahead then. Okay. Hello. So As I just mentioned, Connect TCP in HTTP 1.1 makes use of HTTP up grade, it copies that. It copies and pastes that almost verbatim from connect UDP. But in the course of looking at this, I realized that there were some some issues, that that were raised here. So In Connect TCP, especially It's it would have been easy to design this in such a way that there were requests smuggling attacks where a user of the proxy could impersonate the client to the proxy. And if you want, we can talk about in more detail exactly what that was We talked about this in that's unlike in in IATF117. In the in the so in connect TCP, I think we've avoided that in a straightforward way, but also, it turns out the connect UDP, which has already published RFC, is hypothetically vulnerable to similar attacks Only under certain conditions that are not currently true. So,"
  },
  {
    "startTime": "00:46:02",
    "text": "This prompted a broader discussion of what are the rules for how to make a secure upgrade product in HDDP 1.1. And are there are there things that you're not allowed to do? And there seems like to be some interest in the working group on this. So I drew up a document and tried to figure out the state of affairs. So So this is a 00, and I just wanna highlight a couple things from this draft. So This is a document exploring the security implications of optimistic transmission in conjunction with HTTP upgrade. That's the case where you're talking to the server, you start an upgrade, you send an upgrade request, And before you found out whether or not that upgrade succeeded, you say, I think it's going to succeed. And I'm going to start sending payload. As if the upgrade has already succeeded. And this this document is really about interest in, interesting interactions between at and attacker controlled data, in that in that upgrade payload. And how that enables potential request muggling and price or exploits. Text, So I I went through an analyzed The existing upgrade tokens from the IANA registry And, basically, they all seem to happen to dodge this this issue either very explicitly on purpose or, a little bit sort of ambiguously, but but sort of obviously enough except connect UDP, which I think needs a slight alteration and so this draft updates connect UVP as highlighted"
  },
  {
    "startTime": "00:48:01",
    "text": "basically disabling the optimistic behavior in in HTTP 1.1. Which it should be said, you know, connect UDP over HTTP 1.1 is a pretty sad state of affairs anyway. So making that marginally slower is is maybe not a big deal. So, yeah, everything else seems to be okay. ConnectIP has, already been updated. To match this guidance So so it's clear. Next I just figured I would copy and paste this whole section out of the out of the draft, So This is design guidance for people who are creating new upgrade tokens here. It's, you know, it's a note. It's an IATF note to self. And so I figured that probably people will have opinions about whether these are good guidance whether there are other good practices that we should be recommending, or whether we should be prioritizing some of these over others. That's that's basically the the only that's the most novel thing in the the draft, I think. Next slide. So I this draft is seeking adoption here. And, if I remember right, I think The other slides are more details about the attacks in case anybody wants to really go through the details. Next, so let's go to questions. Okay. First up is Lucas. Hey, Luke. It's Patty. There's no questions. This is good work. Thanks, Ben. I think we should adopt this document Mhmm. Just get it get it done. May maybe some issues around scoping exactly how much guidance is enough guidance and"
  },
  {
    "startTime": "00:50:00",
    "text": "whatever, but we can just figure that out in this group. Thanks. And and I'll note this is not a new topic for us. We did discuss last time, there there there seemed to be a fair amount of enthusiasm just a desire for a bit more detail, which I think we've seen here. What do you any oh, Mike, please. Down. We have a queue now. This like, they should definitely ported option here. I think this was good work thank you again for finding the spin. I will point out that the pointing to the example of H2C for upgrade that that is now deprecate behavior as a 91 13. So maybe we can point to it as having been a good design pattern that was used But, is no longer in active use. Catalogues. Thank you for this work. This has to be adapted to fix is seeing connectivity, I think. And regarding the mitigations, a, don't think that Wait. We can we can just be fine by saying that don't do this HTTP 1.5 because we are not interested in optimizing it anymore. David's Kenazi, enthusiastic about this. Please adopt. Shorten this way. Jonathan. John, Linux, you mentioned, and I looked at all your slides that they need to in order to for the problem to assess with connectivity, you need certain capsule types? Is it worth putting a restriction on the capsule type registry to not collide with ASCII names of HTTP requests. So, first of all, sorry. I dropped out and I missed Kazuko's entire comment. The so what the draft does is to say Just don't send optimistically for Connected"
  },
  {
    "startTime": "00:52:02",
    "text": "and HTTP 1.1. Just wait to find out if the upgrade succeeded. So we don't need that restriction, because we have We could have done it the other way, as you say, and, and, of course, you know, the the group is if it adopts the draft, it's free to make that change, but I think this is probably the right approach. David's Ganazi, CAP capsule registry designated expert. Not an enthusiast. Also enthusiastic about castles, obviously. I really agree. I strongly agree with Ben here. I think this is a much better solution, I would really be strongly against reserving capsules and doing the work to see what binary numbers look at what methods. It's just an absolute madness. Okay. Shall we try the show of hands tools? Yeah. For sure. Yeah? Okay. Then then we can just confirm on the list. Sure. So, I I I'll I'll do it. I think the question is, Alan, did you have moment. I'm a humming enthusiast. I know. Aren't we all? You are allowed to while you click the show of hands tool. No one will stop you. And I shall. So I think the question is, Let's just say interest and adoption. And, yes, means you support a doctor. Do you support a doctor? Do do you support Yeah. And no means, you do not support an option. And if you don't click either, then You don't care. You don't care. You're in And in and we will ask if if there are no's, we'll ask give them the opportunity to come up and point 1. So Okay. The poll is running. You may that'll be your background music too. So let's look. So we have, so far 31. Yes. One note and 36"
  },
  {
    "startTime": "00:54:00",
    "text": "that? 32136 let's give it a few more seconds. Oh, I shouldn't promise you. I know I Alright. I'm going to stop it positivity positivity almost beat ambivalent. Today, but not quite. Yeah. Oh, wait. No. No. We did it. We did it. Thanks for pushing us across the line. What a what a rare occurrence in 2020? Cada. Cada. Okay. So that was 35 yeses, one no, and, 30 to no opinion, if the if the person, who said no, would like to, get up and say why That, that, that's great. Obviously, if, if they don't, that's cool too. If they wanna come and talk to Tommy and I afterwards plane, That's equally fine. We'll we'll take this as input, and I think we'll we'll a call for adoption, formally on the list. And, see where it goes. Very good. Thank you, Ben. But they're calling me. State right here. Kazuko, again, I missed your whole comments. So if you wanna follow-up afterwards, feel Okay. So and now for something completely different, this is a a brand new weird idea. With now several authors. Called reverse HTTP transport, but I just call it reverse HTTP. So next slide. So there's this thing from Cloudflare. I have no affiliation with Cloudflare of as far as I know, none of the co authors do. But there's this thing from cloudflare called cloudflare tunnel, and Basically, it says, like, you can put a Mac mini under your TV and then it can be a web server even if it's not reachable from the next slide."
  },
  {
    "startTime": "00:56:00",
    "text": "And the way that works is if you follow the directions of the arrows, somebody is trying to reach your website so they talk to cloudflare and then your server actually also itself acts as a client talking to cloudflare. And an attacker can't reach your server directly because your server, like, doesn't exist on the public Next slide. Clubfire is not the only large company that has ended up deciding that they need something like this. So this is an a slide from SAP cloud connector, which, is actually, SAP is actually a SAP. I don't know. Is actually the employer of 1 of the authors. So cloud connector is, again, a a similar idea, but here in a in a much more complex enterprise use case, where you have some sort of in this case, some damage may be on premise behind a firewall, some in some sort of maybe firewall private cloud, set up. And, again, they want to be able to effectively act as servers, but as a transport matter, they're only clients. Next slide. So Yeah. What are these things? So in HTTP terms, I think I think about this as the origin server is not externally accessible as a transport server. That could be because it's behind a firewall. That could be because it's like, mobile. Its IP address is changing over time. We can think of other reasons maybe. And so the origin server instead acts as a transport client to reach, I'm calling this the query source. So that could be literally the the actual HTTP client or more likely, and and the draft uses this terminology. It could be an intermediary. Most commonly, probably a reverse proxy, like in the cloudflare tunnel case."
  },
  {
    "startTime": "00:58:00",
    "text": "So, like, okay. But why not just, like, punch a hole in the firewall so that only your, So they're only you're trusted, you know, the client or intermediary trying to reach can reach you. Like, that is kind of the the mostly state of the art here, but configuring firewalls out of Band is pretty complicated, and everything has to stay the same. Your your origin server can't move your your intermediary can't change the IPs that it's using to reach you And, also, if your attack is just like a massive flood that's gonna just totally saturate your link, then the firewall isn't gonna help you. They can basically overwhelm your firewall. Ted? Ted Hardy, middle back middle box and middle box services observer. You have somehow avoided, managing to talk to anybody who Vends firewalls in a very long time if this is your view of what firewalls can and can't do. There are people who do cloud based firewall services. But have elastic compute and do all of this management of volumetric attacks. There are, firewalls that do far more complicated things than than what you assert that they are doing. In terms of managing where the origins are or managing where the any cast services distributing the firewalls are, etcetera. I think you should make your argument for this. Completely disjoint from what you think some other service does. Because your view of that other service is possibly a little bit old fashioned. Okay. Noted. Next slide. The other question that I I think is sort of clear here is why why should we standardize something like this?"
  },
  {
    "startTime": "01:00:00",
    "text": "Because after all, we have these proprietary protocols, and they're doing what they're doing. You know, they're serving however many however many users they're serving. I think there are some notable limitations, with the current ecosystem. So right now, basically, the way this works is your cloud provider or essentially the whoever you're talking to provides you with a daemon, which is like a software package. That you have to run so there there are not multiple interoperable implementations. There's just a you know, an executable If you're lucky, maybe you can get source code. Provided by by your cloud provider. And so that's That can be an integration challenge if you're trying to, like, manage containers and then then complex service system, it it's nice to be able to choose your software. Migration or you know, supporting mobility in in the sense that people should be able to switch from one provider to another one without having to rearchitect or you know, change what software they're running. And protocol evolution. So these protocols haven't necessarily kept up with all of the latest HTTP features and certainly, there's no reason to expect they will going forward because they're not maintained alongside HTTP. I believe that the state of the art here is that they are built over WebSockets for what it's worth. Next slide. Another interesting category of reasons, and I think The real reason that we felt it was a time to write this draft was because of the emergence of what I'm calling large scale proxies. I'm thinking here to some extent about things like Apple Private Relay, but also about effort that I sort of hear about from other browser vendors who also want to potentially offer all of their users a."
  },
  {
    "startTime": "01:02:02",
    "text": "A proxy infrastructure that they can share. And one of the interesting things about those proxy systems is that They Probably should, to some extent, block large scale attacks, like DDoS attacks. You shouldn't it should be difficult to run a DDoS attack. Through one of these proxy systems. And so There are different architectures you can imagine. You can imagine an origin saying, I'm going to create a copy of my whole stack for each of these each of these proxies. And then, if somebody runs a DDoS attack against my main origin, which is huge and overprovision to handle these loads, you know, at least the copies that are behind these large scale proxies, those copies will stay up. Because those proxies either aren't under attack or are handling those attacks. Themselves. Or another version of this is, like, I have a DDoS defense system. Maybe I'm already using a reverse. It this kind of reverse connection pattern, but I don't wanna have to pay the cost of my of traffic running through this DDoS defense layer when it it's going through one of these which is already effectively providing that service. Next slide. So this is a an overview of the proposed protocol. This is just a sort of placeholder, I think. But the the point of this is to say that it's just standard HTTP but with the roles reversed. Which does mean that we had to do some tricks with, with client certificates because the client certificate is the thing that certifies the origin and you need an origin frame, basically, because you don't have an SNI but Those are things that exist. We didn't have to invent maybe I'll stop here for a second, Lucas. If you"
  },
  {
    "startTime": "01:04:01",
    "text": "that you didn't then, or do you wanna finish I think Well, Okay. If you wanna, we can we can, Yeah. Can we wait? Yep. So there are this is pretty early work. There are a bunch of questions here about what we actually need. And, there's some text here. There there's particularly some where I I think we really haven't quite found the right answer. But I'm interested to hear what the working group thinks and, what direction this should go. Go ahead, Lucas. Okay. Lucas, a standards enthusiast I think, like you say, there there's lots of people who are doing this kind of thing cloud file tunnel as an example, work with cloudflow, but I don't work on that team. So I can't speak to them or that product or anything like that. But but creating a a standard for what lots of people are doing seems like a sensible thing to me. The the part of our proxies and DDoS I'm not quite sure because there's DDoS is like a term for lots of different stuff. So some of those proxies will only tunnel like we talked about. You know, you know, transport layer. Things, layer 4 DDoS is very different from Cubunk as layer 7s. I don't think we need to get into that detail right now, but I I didn't just just quite followed what some of the motivations for that we're presenting. But I'll go off and think some more on that. Specific design details, Yeah. We could figure that one out. Like, I'm interested. I don't know if this is near where we might wanna think about adopting it yet. If another draft revision or 2 might help answer some of the questions that have But, say, interesting work. Thanks. Thanks. Thanks. David Scanazi, tunneling weird stuff over at GDP enthusiast."
  },
  {
    "startTime": "01:06:00",
    "text": "So this this reminds me of a conversation I had with Chris and WITMA in the very early days of mask of like, once we got to talking about, you know, putting weird things over HTTP. 1 of his could his idea was he called it the hidden server where it was very similar here, a client that would talk to a innocuous intermediary and then Other clients will talk to that and maybe send the request down to the hidden server. Very similar. And we thought about it and you could implement that with, mass. So you need, like, connect UDP, listen. You, you need a few tweaks on top of it. To how you need the client and the, like, let's call it the best server, agree on connection IDs and stuff like that. If you wanted to do it TCP, you would need some variant of connect TCP, and then would need to, you know, NFSNI to fix support multiple. Again, But the important property there is that you would do funky things at the proxying layer, but then the top a call would be unmodified. This proposal instead modifies the top level protocol. And what worries me there is that the draft says and we do H3 with the roles reversed. There are about a queue of dragons that goes out the door in terms of how hard that's going to be because what you're effectively proposing is to keep the transport layer. Let's say TLS in the same client system, client server model, but take the applicant relationship application layer and reverse it. They're really tightly coupled, especially in quick with, like, saving settings was your RTT. So you can't just flip 1 and not the other. Not saying that doing this is impossible. You're gonna find that it's gonna be incredibly difficult to hit all of those cases and get it right. So my personal take is that if we feel like solving this problem."
  },
  {
    "startTime": "01:08:00",
    "text": "I would prefer to solve it at the proxy and layer personal Okay. I we have a long queue. I'll just note that A lot of the solutions that that take that form do end up with essentially double encryption. Or a 2 layers of HTTP. Like, two layers of of TLS. That might be okay. But something to consider. So because of the whole I I I go. There are real use cases for these kind of things. And I'd be happy to see this adopted that said, there are currently limits to this, design if it comes to the large scale deployments that you've mentioned that is does the endpoint. Is being identified by the FQDN. As indicated by the SD SNI and the pro and the authentication method is tied to the such such 6 Which is pretty inflexible. So I kind of wonder if we could use a standard connect method to create this funnel. Because then we can use URI as the endpoint and can use any authentication scheme that HTTP supports for this purpose. Interesting. Interesting. Right now, this is bound to TLS client certificate authentication for the origin. And, it sounds like you're suggesting that maybe we should consider, other ways to authenticate the The origin. Right. So from the service point point of view, it went once we send 200, we just start using H2 over there. I'm close at 8 So it's pretty simple, and it becomes much more flexible for large scale and deployment. Just Interesting."
  },
  {
    "startTime": "01:10:02",
    "text": "I I think I definitely would need to to think about that a lot a lot more, especially to understand how you convince the intermediary that you are the origin that you're claiming. Yep. Authentication is needed, but yes. Thank you. Just a warning. We're gonna be closing the queue fairly soon. Teddy again, scope creep enthusiasts. Oh, wait. What I meant to scope creep skept it. That was right. In the chat, I have put a pointer for you, for a system that was built in 2000. That does pretty much what you're talking about because this actually goes back a lot further than it seems like you recognize. And that's important because because there are a bunch of different variations which have happened over the and there's been a huge amount of kind of adaptive radiation in this space for how people do this. And I think you probably know from your experience at Google, you could look at a GFE and say, the GFE is one of these things where you could say, no. No. That's a completely different thing because the kinds of origins it's talking to are different because some of them are straight up, not HTTP. Well, it's an interesting question. I think the hardest thing you're gonna actually have to do here is to write down exactly what's in scope and out of scope when you reverse these roles of HTTP and and behave in this way. You could, in fact, say this is just for proxies talking to origins. In which case, you probably need as you suggest to have a different way of indicating it's not standard HTTP. Although, frankly, your request for a new URI scheme caused the ears on the back of my head to go, what? Please stop. No. No. I would actually say you need a boss. Honestly, if this were a dispatch group, I would say you need a boss because the scope of work you're taking on Not depending on what this current document"
  },
  {
    "startTime": "01:12:03",
    "text": "restricts you to. And I think you're gonna have to do the work to figure out what The appetite of the community is for that scope anyway. So you might as well do it the normal way and take it to a boss. Else? Alexcha Nehopsky, Google. While I'm interested in this draft in theory, I'm a little bit worried about a couple aspects of it. To echo what David said, I'm definitely concerned about the role reversal and the effects that it has. But I think I have a more pragmatic concern. Which is that I've definitely seen particular use case desired. Like, internally at Google, we have a cloud, I am proxy offering thing. And we have gotten people asking How do I go and connect my enterprise VM on premises thing to this cloud I am I don't wanna punch a hole in my firewall. So, like, the use case, I think, is very limited. But the problem with all of those things is that the the the appliance is usually an HTTP 11 speaker at best. And unless I'm misunderstanding the proposal here, if this is an two word later, technology. Means that not only are you reversing the roles of these things, you also should be 2 to 11 protocol translator, which admittedly is not that hard. We have plenty of those But, like, now suddenly the deployability of this thing, particularly if you're talking to some you know, late nineties, appliance that they wanted to protect. Comes into question, right, because you're now introducing all of these new features and your translating it down to something that works and trying to use My other concern is We're basically building a VPN for HTTP. We have VPNs. Why not use the mass technology stack in order to solve this class problem because then we don't have to flip how we treat client and server on their head. If the premise here is that there are existing technologies such as cloudflare funnel, provide this as a service."
  },
  {
    "startTime": "01:14:01",
    "text": "And we don't really know the specific implementation details. I don't really know if it makes sense to standardized the same technique would not necessarily, like, a working implementation. Right? I think if there was a, person from one of those teams that has one of these things that was saying, here's how this works. We've already worked the edge cases. Like, we're we're posing to start here as the basis for the IETF. I think it would make much more sense but otherwise, I'm a little bit nervous about the scope and everything else we're doing here. And I would kind of encourage we consider finding a way to bridge the technology gap with a less invasive solution. After Hello, Austin. Right? I've done some research in the area of HTTP over meshed networks and, it turns out H1 and H2 can send requests and responses from both clients and servers. They can talk to each other, and this is not ambiguous. I'm not actually sure there needs to be any any, specification not even, like, a new elks identifier, There might be some issues, like maybe TLS I haven't looked at that, but there's still TLS client certificates for identifying the the origin or maybe, congestion control. Like, what if the the single connection going to the reverse gateway what is congested, Kent, how does the the origin node to open up new connections, but you know, maybe those things need standardization, but, this is actually unambiguous in H1 and H2. I haven't looked at H3. That's fascinating. I, At a minimum, you're gonna need an opt in. Right? You need some kind of signal that says I'm not trying to connect to you as a client. Like, I'm actually a server. So I want you to send requests to me. And here's who I am. And I can prove it."
  },
  {
    "startTime": "01:16:03",
    "text": "Yeah. You wouldn't want to accidentally connect to the wrong or or what you think is in origin. It actually starts making requests of you, but most web browsers will just support the connection at that point because they, like, don't know, but, yeah, Yeah. You The the other thing is with H2, some implementations assume that the odd numbers are always requests That's not actually true. It just identifies the client, but you can still use odd numbers in requests or responses. Okay. Well, I'll keep we'll keep that in mind, especially for this question about bidirectional multiplexing. I think is exactly about that. Michael Tumman, distributed web enthusiasts. I I I think this is really exciting personally. I'm also connected to a group that might not frequent IATF as much, which is more of the, distributed web hobbyists or hackers. And, they would really like this for set of use cases than cloudflare. Which is serving a website from your phone even to, like a web, a web server that's in your, in your, your company or in your home, like a small scale thing. The advantage there is that your phone can go offline or reconnect in different places. And but it can still hold the data, and then the proxy can exist to serve that. And that's a case where you certainly do want standards because you're not gonna have a giant company running that little proxy And, This also, connects with what I'm gonna be presenting in a little bit about, state synchronization and more of a peer to peer web, if you have the state synchronization technology, then it's okay for your phone to go offline. And you can the proxy can even mutate state on behalf"
  },
  {
    "startTime": "01:18:01",
    "text": "of of multiple clients, and then your phone can come back on and serve and and serve as an authority where it when it needs to. So there are a broader range of use cases for this. For in the in the small scale that I wanna point out for offline use. Alright. And, by the way, the queue is closed. So Mike, if you can take that to the chat, that'd be great. I just entered myself in quickly to say that I like the direction that Kazuko was going with this, and I am a, exported authenticatives enthusiasts. I know in the next talk, we will be talking about, secondary certs and I've been pushing there like, oh, we should only do, the server driven ones, but this is actually a pretty good use case if you wanted to use it for clients send secondary service. Should we move over? Sounds good. Alright. Thank you, Ben. Let's search. Eric? You want to? Yes. Engage which I just want to get discussion with you guys in 10 minutes. Alright. Hello, everyone. I am Eric Corvadi, secondary certificate enthusiast And, let's talk about them. Have a follow-up discussion from what we talked about in 117. So A quick recap, for those of you who Either we're not at 1 17 or you know, didn't read the draft. Essentially, the TLS export authenticator spec, you know, allows us to send and receive X Five and certificates at the application layer. This draft is basically proposing that, H2 and H3,"
  },
  {
    "startTime": "01:20:03",
    "text": "get the capability to send unprompted secondary certificates to clients and make themselves, you know, authoritative for different origins in the certificates. The way that we would do that is with a new frame type. On stream 0 for like H2 and the server to client control stream for H3. Which will actually carry those exported authenticators and this work is ultimately based on an older draft that the working group has discussed, that was authored by Mike Bishop, Martin Thompson, and Nick Sullivan. Next slide, please. So before we jump into the main discussion here, thought I would briefly go over the revisions to the draft, since last time we talked about it, these changes are primarily focused around clarifying the the the vision, of the draft So For one thing, we made more clear that we are using the, server authentication flow, in the exported authenticator spec. Which means that the server is going to be sending the certificates completely unprompted. This does mean that the server ends up choosing the certificate request context kind of arbitrarily. The current drafts just kind of states it's implementation dependent, but that is something that we could fill if there is any use for it. Like, if we wanted to be able to correlate what cert is used for a given request and whatnot other than that, we are we are also more clearly suggesting the usage of the origin frame, in the absence of a DNS check or both, you can The the point is that origin is more strongly suggested. And any remaining references, hopefully, to client certs have been scrubbed."
  },
  {
    "startTime": "01:22:00",
    "text": "The h two framing is unchanged for now. As I said, the current focus is clarifying the vision for the document. Next slide, please. So, ultimately, I think the meat and potatoes of the discussion here is Like, what is different this time that makes this work? Potentially useful for the working group to pursue. I would say that there are 2 angles here, which kind of come together, to to form a story here. The first is that the scope of this document has been very intentionally reduced. From the original one. There are no client certs and even for the server authentication, we are only using the spontaneous, you know, unpropped the certificates. And the the previous draft, which you know, did have client certs ended up being very complex, And ultimately, I think the problem space that client certs and server certs kind of pose. Are separate enough that if there was interest in pursuing client certificates like for what we were just discussing. That might be done more effectively in a separate document. As for the other angle here, Is that, ultimately, there are demonstrated interests in multiple use cases here, even to the level of implementation. Main use cases that I think have been discussed are, like, for 1, like, the standard, like, CDN, like, coalescing use case. Where we are essentially you know, giving the capability to more granularly, you know, control the certificates that the client is receiving, you can send search like, reactively based on, you know, if they open a connection, for a particular origin"
  },
  {
    "startTime": "01:24:03",
    "text": "that you might know a set of search that you would wanna send in response to that. And, you know, and not using these, like, gigantic cruise liner certs The other main use case that we've discussed is this hybrid proxy use case. Where we're able to make a mask forward proxy able to sort of act as a reverse proxy for particular origins kind of using the secondary search that are coming in as the signal to switch to the reverse proxy mode for those given origins. Next slide, please. So really, ultimately, the thesis here, as far as, like, why this work might be better suited for the working group to be spending time with now. Is because this scoped down version, enables interesting use cases. And, and, I think that the scope is low enough and that the use cases are interesting enough, that it would be worth taking another shot at this. Ultimately, I think as far as what future changes would be need to be made by the working group to this document. We'd probably wanna focus on change that enables the currently identified use cases, and there's actual implementation interest and or experience, rather than change that is necessarily enabling new use cases, though. Obviously, that always be negotiated. Ultimately, the way that the draft is written now should not be blocking any future expansions on this concept. If we wanted to do client search, I don't see a reason why we couldn't do it. And and and you know, a separate document and use similar mechanisms. So with that said, you know, this document remains seeking an option in the working group. Any questions? Or concerns."
  },
  {
    "startTime": "01:26:00",
    "text": "Yeah. Let's go ahead, Lucas, I think, first. While I was getting to the mic, I'll just say, you know, like, like Harry said, we have done work in this area before. We paused or or stopped because of of, I think, the City as well as, concerns about implementers interest and support. So I that's where my interest is is in the discussion here right now. Go ahead and release. Lucas Pareau, I I I support adoption of this. With its current limited scope. I think there's some things even within that that we would want to discuss So for example, the hybrid proxy use case gonna get into a situation where you have long live connect tunnels, and and requests that would need to be multiplied together and how we prioritize those. Don't need to be concrete, but just something like you should really think about those two things together if you're using it for that model. Whatever the people come up with, like, To me, my mind, there's there's no similarity between client sits, and services except for fact that there were certs and we're trying to overload one frame to do 2 things. Of friends is super cheap. So Like, this is Pant on that one, and we'll come back and do it next year if if people really want it. So hence, yes, support for what's presented here. Thanks. Alessandra. Allison, good evening. Cloudflare. So I mean, I was in favor of the previous certificates you know, dropped and even with, like, the, sort of reduced scope, this is still useful. And as you said, you know, it opens the way for for more work in the future. I guess, I mean, as Mark said, like, part of the reason why the original draft sort of fizzled out was there wasn't a lot of interest from, like, implementers and specifically client side implementors, I don't know. If that changed, I guess with the with the new, you know, Moss,"
  },
  {
    "startTime": "01:28:01",
    "text": "use case that's maybe like less of a concern but still, like, to to actually for this to be useful, for example, for the the the smaller certificate scope use case, then And specifically for me, a class player, then it would be ful to know that, you know, there are, there are clients. So, browsers ideally that want to implement this. Right. And I think that cases potentially more challenging as far as gathering interest is concerned. Whereas the the hybrid proxy 1 is a little easier because you can potentially just special client, special server. Rather than a generalized support from browsers, But, yeah, that is certainly a point worth considering. David Kinazi, authenticate authenticating things over HTTP enthusiasts. Well, I mean, it's just briefly on the, point of implementation, I'd say if hypothetically someone working for a fruit company were working on addicts board for this in their stack for mask. It might accidentally fall into safari. Like, that's not completely out outside of the realm possibility. And that might push other browsers to be interested. I can't speak for Chrome specifically here. I don't work on Chrome anymore. But I think in general, this sounds like useful work. Kinda like the tighter scope, same way that Lucas described it. Let's I would support this meeting for it. Mike. Mike Bishop resurrecting at my old draft enthusiast? I will note that they were originally a separate draft, and the working group said to combine them. And Yeah. Now we're spending the time. Is this what I told you, so a little bit, but also I might write the corresponding draft again."
  },
  {
    "startTime": "01:30:02",
    "text": "A supportive option. Okay. Can you hear me? Yes. Hi. So obviously, I I'm, proponent of the not scoped down version of this because I want the client side stuff. I think you know, we're we were discussing earlier about unprompted authors and how so similar to export to authenticators, And one of the problems we had was like, oh, exposed to authenticators. Doesn't do like spontaneous clients and there's bad support. It sort of feels like we're just locking ourselves into doing that whole cycle all over again and, like, doing twice as much work as is necessary. Just like leave out the section that says, Oh, and we don't support clients and just support clients. Like, you you have to take out text to add more support. I wouldn't say that there are a number of mechanisms in order to support the client Side, like like like like part of the part of this that are necessary to do it, like, well and safely? And Those, I think, were a lar in large part where a lot of the complexity in the old draft came from. I'm, I'm about to close the queue, by the way. Yeah. So think I think, punting on the complexity is is fine. I If the working group is sort of at least willing to consider that that work will just come back again and we'll just do twice like, like, the work will happen twice, and it's just Yeah. K. Eric. Eric Nigrant Akamai, Also, I think I"
  },
  {
    "startTime": "01:32:02",
    "text": "was also more interested in the client side part of this, and continue to worry that the server side of this that if we don't if we do it in a way that we don't also require the DNS check, we're gonna keep having a Yes. New risks, profile show up in terms as we've discussed before, when this, when this is when we've this has come up before for discussion. The I I I should note that the current draft does absolutely recommend. The DNS check as well as origin. Are you suggesting that it should be mandatory? I think it I think that if Having the DNS check is either mandatory or or for the general use case, would would make would make sense. There's also a presentation at mapparggafewietfseago showed have an upper bound of benefit fit for this in the web case was which is pretty small. For how often this could get reused, which was which if that actually was as as that is an up to down might be somewhat disappointing in terms the value is in the general case. User what might be I well, I'd I'd like us to stay focused on adoption, and this sounds like something that could be a post adoption discussion. Tommy's speaking out as chair, but as a doing work enthusiast, Just to respond to what Jonathan was saying, like, yeah, absolutely sympathetic to having client use cases, but at least my impression was Also, given the discussion we had for, unprompted off, We may want some tweaks at the TLS level of the definition. Here. So And I I I would volunteer to help work on anything we need to do to update documents there. But I think that's gonna end up being a different track. And"
  },
  {
    "startTime": "01:34:01",
    "text": "we can do the work, I think, in this working group we've done pretty good job of pushing like, smaller documents through and without too much overhead in process. And I hope that you know, we as the chairs could help drive that. So I I don't think that it necessarily imperils the efforts on client certs to try to scope things down. And hopefully, at least what we found, like, just doing any of this, encourages more stacks to implement expert authenticators and do that basic work And once that's available, building on top of it is a lot lower effort. And I think we're gonna have more export authenticator use going forward based on that. Okay. So we're over time, but, I think we're at a place where we probably need a bit more discussion maybe a little more discussion, especially about implementation. Intense. But it it does feel like with a little more discussion personally, I think we could issue a call for adoption in the reasonable future. Yeah. Let's get a sense of the room, just to see where we're at right now. Gonna do a poll, sorry, a show of hands. I think the last working way was was I supported option or affect So, starting the poll, if you supported option, do yes. If you do not, say no. You're unsure, you don't have to do anything at all. Or if you don't care, of course. Okay. And we have, I think, about the same number of people in the room. The pressure. And and this is, by the way, adoption with the scope described, of course. We we've tried with a different scope, and that didn't work out. So, And we'll give it just a few more seconds. For those. People, hovering over the yes and no buttons. Insert name here."
  },
  {
    "startTime": "01:36:03",
    "text": "Okay. I'm gonna close now. So that was, 24 with a yes, 3 for a no, and 43 with no opinion. We heard some people expressing some reservations. If there's any new information, I think at this point, let's take it to list, and we'll we'll continue the discussion there. Thank you very much, Eric good. Alright. Moving on, finally, we have, two presentations with a little bit of overlap. Start with the braid. No. Yeah. We'll start, sorry, with the per resource. Okay. Of your share yourself. Okay. How do I he do you wanna forward the slides? Okay. So Press the slide button, the big, the big file button. And then I will say yes then you find your slide and definitely Yep. Great. Hello? I'm Raul. Notifications in TWOS guest? So Let me start with some shameless self promotion. So I'm building, operating environment called centrifies. And"
  },
  {
    "startTime": "01:38:03",
    "text": "with a teeny tiny ambition that I will one day replace all desktops and home screens with this thing. I think this stops her historical mistake, but that's another discussion. So as As part of this, I use solid for storage. Now solid is great because it provides me, place to allows me to give users a place to store data And in a vendor, independent in phase. So there's no lock in. However, the notification systems in solid looks a bit like this. It's happening. There's no animation. No. It's it's a PDF. So That Yeah. I I I'm not sure. Maybe just go on. Can you hear me now, man? Or No. This is the only option we have. That was the best part of the presentation. Yes. So it looks like this. I'm the author, and I'm one of the authors even I struggle with this sometimes. And this does not explain how the beside coordinates, So it looks a bit like this. And then the client side has to coordinate. So it looks a bit like this. Oh, we really want paying that are a little bit simpler. And, again, the picture does not load."
  },
  {
    "startTime": "01:40:00",
    "text": "the censorship going on from the IETF Okay. So this is for resource events. It's a very simple idea. You send a request You get your representation and following that, any notifications that come. Our screen to you. And you can do this on a per resource basis. So This is your ordinary HTTP request. You just add one more, headed to it. And that is a request for notifications. So if this were your ordinary response, there is the response becomes a multi part response where the first part will be your representation, and the second part will be notifications. The only important thing to see here is the events header, which tells, okay, this is prep protocol, and it has some status. This is the award the one you've shared is the old that happened. Yeah. The, sometimes it doesn't update when you do it the last minute. So that's Okay. So that explains it. So there are some there's a mistake here. So subscription. X. Anyway, So, There is then there is a very header which says accept events. Customer HTTP stuff. And There's an accept events header as a courtesy. Like, next time when you negotiate, we can negotiate with prep. And then the second part is a multi part digest which will contain all the notifications. So Yeah. Please ignore the Hello IETF part. That should not be there. So it's basically right now, it is it's in the format of HTTP, message, but really actually RFC822 format. And we've added some additional semantics, something which we can discussed for their going ahead, And"
  },
  {
    "startTime": "01:42:00",
    "text": "you get a notification, then you, the resource is deleted, other notification because the resource is deleted, we'll just close the screen. And those, separators are the standard way to do it. According to RFC 2046. So a couple of variations on the request. So you can do content negotiation for the notification. So here, I'm doing, asking for notifications and Jasonld. I believe this is the favorite format of this group. This is a second example where you can ask the notifications to be sent without a representation. So I've repurposed last event ID in this case, I'm a little ambivalent about using last event. I ID because it clashes with, service end events. But In this case, I'm just saying, because it's a star, just send me whatever notification you have coming up. So why not, use, event source or service end events? So I think everybody knows what that is. So, you you couldn't buy this in any other color except black, That was hen, Henry Ford Stick Similarly, what the BlueJeans dictate is, you cannot screen it in many other format. Unless it's text event stream So this is really unhelpful because One is you can't do binary. Of course, you can base 64 encoded, but it's not there, and you, for clients, it's just a hassle."
  },
  {
    "startTime": "01:44:01",
    "text": "Also, event sources kind of dead in the because what WG does not intend to develop this any further. Okay. So why not use, web sockets, webhooks? So the main thing is you are protocol hopping at this point. Your your resources is on HTTP, but you have to go to another recall to call to Get your notifications, And so this creates the need for discovery. So you have to go to the HTTP resource, look at a link relation or something like that. And then, follow that link and get your notifications. Your semantics are really like you have endless choice of what you can do on WebSockets and that basically every designer does their own thing and applications have no standard to follow. So That's not nice. And the last thing is typically, the design pattern is that you get, updates from multiple resources on from a single endpoint or a hub or whatever you want to call that. And, this silver send updates for all those resources, and then the client has to separate them. And then figure out what to do with the data. So that's not nice. When Here, we we just have for every resource the updates are for that resource only. So, there is since I've been working with the solid community on this, so there is I have, written a companion's, standard, which is for link data. It is called, solid reps, So solid communities have already intends to,"
  },
  {
    "startTime": "01:46:00",
    "text": "has said yes to implementing this. I have to just put in, the official feature request now, which will be done after this IETF Mind you, this has been just 3 or 4 months since I've, put up the standard itself. And there's also public interest from another solid, server vendor. And, many people who want to are implementing applications on solid have been asking me for this as well. So that brings me to you guys. So this is my first time at IETF. I had no idea about how It was here. So I'm asking you, if you can help me figure this out. So there's some specific questions I have. Are there better ways to frame messages is there a better semantic, even if we use multi part framing, can we do better semantics for RFCA 22 messages that are there. Any ideas on headers? And, of course, if you have other feedback, I'll just leave that up so that it's there for anyone to comment on. Thank you. So, we are time constrained at this point. So, I think we're just gonna ask for a question of clarification at this point. Austin was was that a question of clarification, or keeping in mind that we are time constraint. We can also ask it after that. Use partial content. Okay. That's not a question of clarification, but thank you. Okay. So let's, leave that for a moment. Thank you very much. And move on, and we'll talk about bright and then we'll have a more general discussion, hopefully, although We are unfortunately running out of time. Okay. Push this button. Yeah. Yeah. Sorry. I, I did have a clarification question. Can it wait till the end when he's back on stage? I would prefer to ask it now. Well, Sorry. We have 10 minutes left. This"
  },
  {
    "startTime": "01:48:00",
    "text": "give it a moment. Okay. Very good. Alright. So, the point here is to open a discussion on this idea of adding synchronization to HTTP at the high level. I'm gonna try to just frame the the dialogue quickly and then get us to discussion. So, Let's start. Okay. So a little bit of history, which just to so HTTP was designed initially for static pages. Okay. So it's request response. And the server will give a the page to the client, Now you can ask yourself what happens if the page changes after it gives a to the client, And that's why we we just punch it and have a reload button. And that made a lot of sense. When pages were written by hand, everything was static, but over time, pages started changing maybe once a day, once a week, and then we got JavaScript my HTTP request. You didn't have to click reload anymore. And now we have web sockets and you affect everything to be real time collaborative by default where it's all loading, but there's still a reload button because we haven't addressed HTTP. And so what's happening behind the scenes is first, we added these databases, and now you have kind of like non standard state on the server. It's not a standard, but it's being funneled through HTTP. Then we add JavaScript. That's the HTTP request. And now we have to work around HTTP because HTTP doesn't give us updates, it doesn't give us synchronization, And then you have this model view controller framework on this client. To synchronize all your state. And this is why web programming sucks. And you end up having to add even more stuff just to tame it all. And this will be a lot better if this nonstandard state could be on HTTP and could be standardized, which you could happen if HTTP supported synchronization. And the, and so there are 3 problems here. One is that it sucks for web programmers. 2 is that the, the non standard state becomes locked, it's locked into each website."
  },
  {
    "startTime": "01:50:03",
    "text": "And so you don't get interoperability across websites, and that leads to walled gardens. And The third problem is that we can't support the state on standardized infrastructure, like caches, proxies, and CDNs, is just funneling through web sockets. So The big idea here is it's time to evolve from the state transfer to state synchronization. So opening the question, what do we think about adopting to HTTP, the goal of synchronization, we'd go from representational state transfer to representational state organization. And HTTP, it's already moved beyond just hypertext. We can also move into a synchronization protocol And so synchronization starts out really simple. You just send the state from a to b, But it evolves into a lot more when you wanna have multiple editors. You want per higher performance. So you don't wanna have to push the whole thing each time. You want delta compression. You want reliability and consistency and you want new types of networks and versioning. So there's a big scope there. But but We have a well characterized scope, and this is in the Braid HTTP draft. This is a scoped version of this where everything fits together. And it provides this general guarantee that multiple riders can make simultaneous edits arbitrarily across any network delay, any top topology, within client server HTTP world, to any content type and guaranteeing consistency provides a lot of benefits and we have a bunch of implementations. This actually works. We have a Chrome extension where you can see what it's like. To go to a text file and have it live update without clicking reload. It can have collaborative editing. The dev tools have a little panel that show you the history. It works in live apps, like, up on the up in the upper right so you can build your web state this way, have it all synchronized, and it also gives you in the bottom left this abstraction where you can program with distributed state like a local variable. Because it just it hides the network from you and it works, it emerges perfectly. So implement this,"
  },
  {
    "startTime": "01:52:02",
    "text": "Most of the features are actually already there. Have to combine. So, Resumable uploads of byte range patch are important because they give you the ability to update a range of a resource. And we can actually implement with the Braid HTTP spec we can implement resumable uploads, and we need a common extension that bite rate patch is adding. We also need subscriptions like prep, And then the only thing we need to support these great CRDT OT algorithms that give you the distributed sink. Is versioning and emerge type, These are the 4 improvements we need. So this is a a feasible scope if we add these features, to the subscriptions and updates and patches These are already in the works. Versioning is a new thing, but it's very simple. It's just 2 headers, one for a version, and one for the parents. You get a tag, and merch type is a simple spec. That will give us a really great synchronization technology. So now open up to discussion. What do you think? About adopting the goal of state synchronization into HTTP in some scoped fashion. Here's a potential scope, and you can see the spec for an example. I think that's that's it. I'd love just love to hear Thoughts on that Great. Thank you. And and and thank you for doing that. So question. concisely. So, again, questions of clarification, then we can start with, without in the back. Come back up to the t, to the stage, please. Question, the clarification. Obviously, these are are big topics, but let's start the discussion now and the time we have left. David's Ganazi. So changing like the goal of HTTP is daunting. And I I understand when you say"
  },
  {
    "startTime": "01:54:00",
    "text": "HDP was designed for, you know, different era. But we've also evolved it. Since, and that's what, you know, everyone here has been kinda working on, and it's working quite well. What's doing alright? And I would say from experience having seen how things work at ATF, big paradigm shifts never happen incrementally. And so I don't think A paradigm shift for HTTP is, like, in the cards. However, when you say we're almost there, That's where you should focus, in my opinion. Take the list of things where we're not quite there yet, but we could be. And bring those individual extensions to HTTP or new, you know, all those things, bring them here. That's what we do. We maintain such that has what you need, but I don't think that, like, don't even, like, the sheep doesn't have a goal. It's not even a well defined construct that we have. So I wouldn't push for that. I don't know where it takes us. Can I have one clarification on that? What I mean by adopting the goal is let's take these some of these existing features reasonable uploads and the patches. And then let's adopts subscriptions like prep or your cure, And let's just make sure they work together. To meet this use case of doing a CRDT, and that will provide a whole breadth of abilities, general features. So in that case, then what you're describing is something kind of similar to the web, where folks here have built many extensions to HTTP and many other protocols like WebSocket and transport. And then the W3C is a separate body that kinda ties it all together and creates the way. Also use HTTP for other things. So I would say what I recommend you bring here is all the bits in HTTP. And the but the tying together would be somewhere else."
  },
  {
    "startTime": "01:56:00",
    "text": "Okay. We we we've got other folks in the queue, so let's let's move through that. Yeah. Yeah. Kevin. Kevin? No? Remote. Okay. We'll have to move on then. Do you hear me? Yes. Oh, okay. Sorry. I'm the author of a quite similar protocol called Mercure that I already proposed to the IETF working group a while ago the main difference between, Mercure and prep is that it is focused on working with the current state of the the web, including with technologies not able to maintain persistent connections like a PHP or CGI and things that. And, the the protocol is already implemented and used in a lot of frameworks. So I I can say that I support this initiative because there is a need and there is a a a strong need for standardization. Many, many frameworks are adopting but Matthew is not taking the opportunities that are granted by HTTP 2 and HTTP 3 something that has been designed to work now. And I think that we can do better if we worked at the protocol level. So, as there is a need, and there are, many people interested in these technologies it will be nice to, have the ability to standardize some Okay. I'm gonna close the queue because of the time. The Alex. Hi. I just had a quick clarifying question, which boils down to why HTTP? I'm kinda curious what the clients and servers in this model are because when I saw the the communication slides about why all these things work together. A bunch of protocols there that weren't HTTP. So I my question is, like, who's used who would use this why do we need to do this in HTTP? Why isn't something like a streaming RPC protocol or something else more appropriate."
  },
  {
    "startTime": "01:58:03",
    "text": "For one, it's much easier to use HTTP, for clients and even when we are developing solid notifications protocol, a common one of the sub protocols we ended up using is HTTP Streaming. So we just thought it might make just more sense to standardized things on HTTP itself. K. And and I got in queue to very briefly say, I I think these are interesting things to for us to be thinking about adding to HTTP. One of the goals that we have is to make sure that different functions of HTTP are the different capabilities, are well integrated. And I think that's where a lot of the focus would need to be here is for example, intermediation And how does that play into this neck? We make sure that, you know, media media areas can provide value. As well as HTTP 23 and and multiplexing and efficiency. And you know, all the other facilities from redirection to authentication to everything else. So I think that that would be the areas that I would be exploring, and and figuring out what the basic, you know, I think especially, Michael, what you're posing is a new set of functions that go along all those others, which creates a matrix that we need to consider. Mike. Mike Bishop, I wanted to say, first off, I have been watching your work on Braid for a while, and I really appreciate it. I think it's an exciting direction. I also wanna say from an IETF perspective, I feel like the closest URL here. Is David's work on mass, like, 4 years ago, you brought a vision of what mask could be. To a boss. And we're still writing drafts to make that vision happen. The pieces are not all there yet, but you wrote up the scenario. You presented it. And the bricks are coming together, and it's actually pretty exciting. And I think breed is in kind of the same space that What you're talking about here is not so much"
  },
  {
    "startTime": "02:00:01",
    "text": "a piece of work as a vision statement of where we can get if we build these 16 beaters. 12 of which we may already have. And I think there is room for that in the IETF it doesn't necessarily belong as a draft itself. So you're suggesting a ball for a working group? Maybe as a venue to get people talking about the scenario, but I'm I'm still mostly in line. Like, the actual work is gonna happen as find the pieces by themselves that we're missing. Bring them to the appropriate working groups and let's build this And if I could interject, I think it's it's kind of a little top down versus bottom up. And and the the the individual components need to be able to stand on their own as it were. Yep. Yep. You're saying this is top down? This is the draft has topped out. Yeah. Oh, the drafted, but the but the work would happens on Interesting. That that's how we generally do things. You find that you, so you you read, you perceive address. Let's talk after the meeting. Yeah, I think you've got the last word. Hey, last word. Yeah, Austin Wright, partial content enthusiast, just one of the missing components here that I wanted to identify for the working group is sparse resources where some regions are undefined either because haven't hasn't been uploaded yet, or because it has not been uploaded, like, it's live stream or a shift buffer. That's something that's going to be important for state synchronization among other uses here. So, Yeah. Secondute. Okay. I think we're done. It sounds like that's ongoing discussion, I would say. And let let's chat after meeting and and, and perhaps take discussion list. Thank you, everyone. See you in Brisbane. Yeah. It's"
  },
  {
    "startTime": "02:02:05",
    "text": "of its"
  }
]
