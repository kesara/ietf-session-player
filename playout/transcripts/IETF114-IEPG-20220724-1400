[
  {
    "startTime": "00:00:53",
    "text": "[Music] again is oh"
  },
  {
    "startTime": "00:02:48",
    "text": "chris are you a data tracker testing testing share slides okay [Music] uh yes"
  },
  {
    "startTime": "00:04:12",
    "text": "thank you okay so actually we will have someone else is this microphone actually working for remote people can remote folk hear me maybe yay okay so we have rearranged the agenda very slightly and we will first be having the um what's the actual full name um where is yeah where did my packets go measuring the impact of rpki rov and that's going to be by cohen who's presenting remotely i guess actually before we should do that we should first do the hello everybody and welcome to philadelphia i'm warren this is chris you're in the iepg meeting which is a informal gathering of network operators that happens before the ietf actually starts so technically you don't have to be registered for the meeting but i'm guessing everybody probably is anyway so this is us um hello and welcome again and let's get started i guess i how's this gonna work would you like us to share the slides or do you want to ask to share and you move them whichever you would prefer i believe i asked to share the slides but then okay click the button and take it away ah lovely"
  },
  {
    "startTime": "00:06:00",
    "text": "so yes i want to talk about where did my packets go basically we do rpg irv so what impact does it have um [Music] firstly so why do we actually care about our pki i mean i would argue that okay for some of us it's probably job security but in generally speaking we care not so much about rpk intrinsically but we care about where our packets actually go to and where the place that they go to is actually the place that we intended our packets to end up in and rpg rov is one of the ways we try to make sure that they end up in the intended place um there is a slight issue with that so what's the problem oh that is a slash 21 that is announced by as333 so to ripen to c uh and we have uh the same prefix as a slash 24 announced by as666 which i've called evocor which i know it's not the right name because but hey um this discussion about whether we used reserve prefixes for documentation or not that has been going on forever and i will gladly follow that tradition um so they both announced that uh free prefix to serve and let's for this example say that surf doesn't do rov so they receive both them except both of them and forward both of them to the university of twente so that's one on the right so if the university of tennessee were to do rpki rov then it would look at both of those pgp announcements and then see okay hey that one that goes from 1103 to 666."
  },
  {
    "startTime": "00:08:03",
    "text": "hey that's that's invalid according to this the roast that i have so i'm going to drop that great okay now we have the only the other route available except that if we send the brackets to let's say 193.0.1.1 then we take look at what the routes we have we only discarded the one big that is more specific but we still have they'll still have the one that goes to the ripe ncc we forward we send that packet to the one the next hop so sure and surfed and says hey i know a more specific route and i know so i know a more specific destination let's just send it to weaver anyway so even though i do or the university of twente does rpi rov that doesn't really mean that my packet then ends up at the what is considered valid according to the rpei the inverse is also true if surf were to do rpi rov and university 20 wouldn't do rpi or rov and it would receive the same somehow would also receive the slash 24. then even though it would uh the university of fentanyl would consider that route then it would end up with servant serves as yeah but that's no that's invalid so i'm going to send it to the right ncc anyway so then it does end up in the right place even though they don't do rov so um this is what we wanted to look into basically does this actually happen in real life and what's the impact so quickly about the experiment that we set up for the experiment we have one as that is as211321 that's to an as used by an outlet labs that i was able to borrow we have two servers actually uh one of them is that kohler clue in amsterdam uh koduku is reasonably rather well connected actually it has a lot of beer and"
  },
  {
    "startTime": "00:10:00",
    "text": "appears quite a few tier one upstreams and one ad filter in sydney which is also reasonably well connected uh not as well connected skoda clue but still it's it's not invisible to the outside world um we are announcing some prefixes namely t2a04 b905 42 we have also arrow for that that has a max length of also 32 um for this as so that one is valid we announced that one from vulture that's what the blue triangle means um we also announced two a04 p905 as a slash 43 so that one is invalid according to the rows that we have but it is more specific so if you look at just bgp then it would be more specific so we choose that but if you do rpki rov then you consider it invalid so you choose this slash ready too and then to check whether you do rpi rov we have a slash 48 that is has an as0 row associated to with it so um if you don't do rpg rov then you wouldn't even send your traffic to your upstream unless you use something like default route you basically you would just discard the packet because you have no routes available for that prefix and if you don't do rpi or v you send it up to your upstream um all these ip addresses also have their ipv4 equivalents namely in two or three one one nines 22 23 and the others below there but for convenience sake i will just keep talking about the ip6 ones because otherwise this becomes a very convoluted uh talk so what we did we set up a free rpgi publication points um we have parents.rv.now which is uh hosted on 2804 b905 8001"
  },
  {
    "startTime": "00:12:00",
    "text": "so um this is inside the slash 42 announcement but outside the slash 43 announcement so this always ends up at vulture in australia it because that's the only route you could possibly take because that's the only place it's enhanced then we have child.rov.com.l this is at 2804 b905 2 which is actually inside the slash 32 and in slides 43 so depending on whether you do rpki rv and your upstream where your upstream send your packet sku this ends up at either goloklu in amsterdam or in filter in australia um and then we have invalid.rov.gov now which is at the invalid prefix so if you hit this then you are likely not go if you're likely not doing rpg rv so um yes that's what i said so um quick intermittent about why rpki publication points because well every measurement that you do on the internet has a bias we wanted to look at uh networks that were more likely to do rov than the average uh rpki publication points also again because validators look loads of data at least once an hour they generate a nice steady stream of traffic uh so this was we thought hey this might be a good way to to actually measure this or at least get a feeling for this but uh mind you this doesn't say something about the world wide web of course because most people or most organizations are still not doing rpi rov um and the artists are and most that don't do rov also don't run a validator because you don't run a validator for fun or at least not not not most people i know um"
  },
  {
    "startTime": "00:14:01",
    "text": "so this is actually the slide that most people um are waiting for this is the the results um this is a small table um the value at the top right is actually the value that you would most likely expect namely the the [Music] amount of unique ip addresses that drop invalid so didn't manage to reach invalid.rov.com now and ended up at a filter in sydney so at the vetted location however as you can also see there is in the top left um also quite quite a few that even though they did drop in valids they still ended up at what would i would say is the wrong location or the unintended location if you look at from the um rpgi perspective so that's the category that well i mean didn't even though they did rpki rv their upstream send their packets to the to a location that they thought hey we know something more specific and then it was not the place where the uh originator expected that to go however we we also see the inverse because for the all those that don't drop in valid quite a large large fortune is actually secured or secured does go to the intended location according to the rpki merely because their upstreams do so this is quite an um interesting uh it was quite an interesting first to see this especially this this bottom one because uh i please didn't expect this i didn't expect that this would have such an impact but uh luckily it does so i would say that i'm not going to say whether this is a positive message or a negative message that's something that you may decide but um it's quite interesting to see evan"
  },
  {
    "startTime": "00:16:02",
    "text": "um so during this measurements we we ran into some challenges um the first challenge is actually quite simple uh at first 99 of traffic that went to kodaklu and now that was a bit that seemed a bit strange because why would all traffic go to coda glue i mean some at least some people will do rpki or v and dropping values right well i mean vulture didn't do rov we knew that but um they would apparently if their traffic reached their edge they would say think hey i know a more specific route so they would redirect it to another tier one and then it would make a loop around the world and end up in amsterdam i've seen trace routes where the packet starts in amsterdam goes around the world reaches australia and then goes around the other side of the world again so if you need the b2b scenic routing then this is a way to do it um not really efficient and also uh well problematics for what we wanted to test uh we managed to solve this by announcing a more the more specific filter as well and then adding a bgp community so it doesn't export outside the filter which is a bit of an ugly hack but it works and was for our measurements uh good enough however if you don't do this which most people won't then even though that you might do rpg rov if you announce something at filtering someone else on the world announces more something more specific then all that traffic will go away and yeah something to think about um another thing which because ipv4 is difficult to get nowadays because it has run out for the last five years or so we initially ran everything on ipv6 and i physics only which we thought okay networks that do ipv6 run a validator that's on an online machine that also white ip6 right well the answer to that is um sometimes"
  },
  {
    "startTime": "00:18:00",
    "text": "yes but often also no so we would see that hey why don't you understand what our route is hey um we we i i think why don't you under oh wait we you don't do ipv6 on your validator even though your network support ip6 uh and interestingly enough sometimes networks had multiple validators running and some of them on machines some where some machines supports ipv4 only and some also support ipv6 so that that triggered internal alerts a little awful lot of fun basically um message being that either run make sure that your publication point is a is available on both ip4 and ip6 are just basically many we or we need to get into a world where running ip6 only is actually feasible which i mean i would love to see but that's i'm afraid probably not going to be realistic so yeah that's um it basically for me we we've seen that merely doing rov and dropping effects does not necessarily mean that your traffic goes to the intended location we knew that from a theoretical point of view but we now also just did it in the wild and we've seen the same behavior and also um the more varied your upstreams are the more important doing rov is because what we've seen in our case where vulture was our only upstream and vulture made sure that we didn't receive any rf traffic like likewise if for the university of twente because the university of tenta also has one upstream the whether the university of trenton does rov or not has a very limited impact on where traffic actually goes um if you're interested in a live map of seeing data coming in to our rpki publication points and also where your network that you're on actually takes you to then you can do so by visiting our video um if you were more interested in reading"
  },
  {
    "startTime": "00:20:00",
    "text": "an article about this than hearing me talk about it's available on gripe labs on that url or just google it um and lastly i want to thank the people at another labs and the writebnc for letting me do b2b things with their resources which um luckily nothing went wrong otherwise they would have taken the blame so that's it for me um i'd like to open the floor for questions or um comments if there's time for this yep we have a few minutes for questions if there is anyone either in the room or online going once going twice don't see any questions but people can also obviously you know email the presenters directly or excellent so thank you very much and now i think we will do the dane portal presentation which i believe you will be presenting from your own laptop excellent so uh yep you can you can as long as you don't mind turning around and clicking the button or if you put on that end of the table we can shift it down and you can stand there and you can face the audience testing one two three we can actually keep that one too just have the notes on the side thank you guys for the accommodations don't worry and then you click the ask to share button and somebody will pick to share screen yes i want to share my screen entire screen indeed oh there's the mirror echoes all right"
  },
  {
    "startTime": "00:22:01",
    "text": "just make wanna make sure remote people can see this and it all looks okay i will assume so already take it away all right hello everyone and thanks for having me um i'm nina and we've got some guys here pavon we have some of you over there i'm sure you know um we're graduate students at george mason university and we've um working with the modular security lab and really we want to show off some of the cool tools we've been working on uh in the subject of game protocols really bringing them to life to power s mine specifically but first let's uh jump into a little bit of setup so dain is a powerful protocol suite are we good to go all right cool in front of my face gotcha good note all right let me try to be a little more 50 cent here um dane is a powerful protocol sweet it really makes doing security and privacy easier right but what can we do to make gain easier that's kind of the question that inspires us so for the everyday person why can't we simply turn on secure messaging on the internet now i know what you're thinking hey we could do messaging on certain platforms and apps right we got you know whatsapp signal uh we got like our organizational pki we can you know do email really easily um but should we really be limited on the internet for you know proprietary platforms and that kind of pki boundaries right and what about usability to get to that next stage we kind of want to be sure that everyday users and operators that make the gear spin are not burdened by the overhead of having to manage doing uh you know secure messaging secure object sharing and whatnot so the idea is like what https did for transfer security we want any entity to be able to transact end to end secured"
  },
  {
    "startTime": "00:24:00",
    "text": "with secured objects with any others over the internet right on like a wide scale and this is for all sorts of use cases so that's why we're launching this basic research into how dane can unlock these long needed protections for those uses it's like mhl smart connected cities cti sharing something i'm currently working on personally and then we got 5g next year all these hot topics dain can really bring that to the next level for those protections across the internet so let's start by securing one of those basic protocols right if we can work with the model t we can work with the tesla what's something that everybody uses email and what is this going to allow us to do it's going to let us find out exactly what people need to make end-to-end internet security seamless and turn on everywhere and the catch phrase here is kind of to make it invisible so for that we need to make usable tools right making it easy well securing email with dain if that's our use case we need the tools we need kind of two sides of the equation here we need to be able to set up dane right it requires some level of work from domain holders on dns and you know we want to make that easy for them and we also want to use the search from dane on the user side on the clients and the muas that the users will use so that's a lot of uses um they'll be able to do email easily with the tools that we made to really show that off so for the first one we made the certain management portal dameportal.net and then we made the mua add-on called courier so some fancy names there we'll show them off don't worry and really the goal here a little too hard the goal here is to find out what people need to make into a security default as i said we do know one thing for sure one thing that people definitely need is uh key management right insert discovery and we got a lot of solutions for those but you know as i kind of implied dane is kind of an excellent answer right and we just need to make use the tools"
  },
  {
    "startTime": "00:26:00",
    "text": "to make it easy gameportal.net okay i'm accidentally clicking on the thing here sorry um it's an open source federated cert management system and i'll show you what that means uh and a dedicated dns infrastructure as well to make dain easy right and literally the way it works is domain holders will enable dane for their dns section zone and the email users will simply manage their certs in like a delegated manner for specifically the emails they are given control over to the degree that makes sense for the organization so let's go ahead and see these are screenshots but we can get the idea just hop on gameportal.net and you can start to enable dain create a new user like you would normally imagine these online portals to use and log in and suddenly on the first page we'll see this dashboard page where users can add their see their email addresses and zones so this is just kind of like a subset of the page here but uh basically down over here if a new admin uh somebody that owns a dome the domain holder uri wants to enable dame for their zone they'll go ahead and try to claim it now here i can try to claim example.com even though i don't own it anybody can claim any zone and they will uh need to actually verify it obviously and the flow here is pretty straightforward we just use the acme protocol every uh to prove ownership they just need to add that txt record in their zone and have dnsec enabled and we'll verify that straight through and it'll uh we'll go ahead and try to hook it up basically to finish the delegation just like any normal dns section or uh we had the ns record yes record blah blah blah you know the stuff so this is pretty straightforward for uh kind of delegations now this is the interesting part so with dane we can do kind of the zone game zone management our portal will actually create the dame zone for you enable dane whereas the admins the actual domain holders have full control they retain control over the uh keys here they can turn it on and turn it off have it accessible and you know how do you check that it's"
  },
  {
    "startTime": "00:28:00",
    "text": "actually i'm doing the same thing sorry how do you check that as active is well you use some kind of tool like sex spider that'll go through and make sure that the delegation works because dns sec right the sec part is to it's to uh you know make sure dane requires that basically now we can go ahead and add the domain this is the part where email users can be able to manage their own search now this is the part where we go ahead and let another user on dane portal manage their own search on their email address so here we got john doe on the picture and well we added him and you can see that when you go back to the dashboard you can go ahead and click through and manage his own data on dane portal so this is another screenshot showing the uh page where john joe will be able to set a cert so we can just add that add the cert on this page and over here is pretty interesting to note the dane specific protocol usage selector matching if anybody's familiar with those that's all those options are there the defaults are given to be the most permissive and the um most complete so those defaults just allow you to start doing uh secure email and once again we give the ability just to do a quick self sign start as well but you know that's just a standard open ssl not too much fanciness there we can download it and use that sir now john doe go ahead he went ahead and added his sir here and his well he could just toggle it active if he wants and he's good to go right he can start doing secure email just as easy as that and just another thing to note there where you know you can just toggle it on and off that's not really that easy in classic pki is it cool so we saw how that was making dane pretty easy this is a straightforward flow the admin added that zone and then the email users can in a federated manner manage their own search on that um on the dean zone so yeah feel free to check it out we got more links over here now we saw that first half so we jump ahead and so how users can actually have it auto-resolved inserts on career in their muas to find out what users need to make end-to-end security a"
  },
  {
    "startTime": "00:30:00",
    "text": "default and really the motivation for this is here we don't really have wide scale ed security but by observing our tools and action we can find out what makes sense if we are to make ewe default so to that end we kind of instrumented our next tool as a live experiment where really any one of us you guys can help us get some real numbers on the human puzzle piece in security automation so just to really quickly jump in we'll show how easy it is with career right uh 10 30 you've still got 20 minutes okay alrighty in that case i'll go ahead and show this live on well first when you're trying to hook it up what we added assert on dane portal right so that means your search is available on dame you just need that private key installed in your mua now all muas and os's have their own key chains and all kinds of crazy stuff in order to do that with career is pretty straightforward you just put it into the settings and just hook it up like a normal file choose your key file and you're good to go now you can jump into a secure email conversation with a stranger so i'm actually show this off why not so in thunderbird this already has career installed i could you know do something fun just go ahead and write an email to pavan here and then the email could be you know anything our top secret uh communications that needs uh uh encryption and all that so we can you know hide whatever we want but the idea is that we can do full html email we've got the ability to add pictures and all sorts of emoji and things like that and uh we'll go ahead and send this to pavan and uh you can saw what i did there and real quick on this page you saw a little hint of it it went ahead and encrypted the email and then did something fancy with like an attachment and then just sent it natively right and on the other end well whenever it comes in there you go so this top secret email it comes in as"
  },
  {
    "startTime": "00:32:02",
    "text": "like a with this tag that's basically said that it's encrypted that allows the outlook side for example to go ahead and read this and standard s mime but it's just doing the processing for us and it'll tell us that it was encrypted and that it was signed so both this is possible because both me and pavon have our search on dane despite us not sharing any keys or having it installed on our own operating systems or clients dane allows us to do this very seamlessly right now i can seamlessly reply as well and you know i could say something like this thanks and pavan uh could have that and you can see on this view like we're conveniently given the decrypted email just as a reference and to be able to send and for uh specifically on the outlook side all you gotta do is to toggle on these [Music] and then you can toggle on signing as well and you're good to go it'll send it signed and encrypted in the conversation view so you see another reply to this once again encrypted now let's see how it looks on my end whenever it comes back so um here it goes stop secret now on the thunderbird side we'll just go ahead and decrypt and you can actually see the conversation very seamless verified decrypted gives us all that info so real easy to use for any everyday users now let's come back to the slides if i am not oh sorry there you go so you you can even see it with you know funny things like you know cat picks something you want and uh it all works so just a quick rundown of the details so secure messages are sent to standard sort of pkcs 7s mime objects and to kind of avoid stepping on the standard flow of email emails they are sent as attachments um in order to retain kind of the crypto cryptographically secured thing from end to end and the insert resolution is handled natively silently and directly so um we're not using any kind of like listener or a"
  },
  {
    "startTime": "00:34:01",
    "text": "server or anything like that that some implementations use rather this is done completely natively within the add-on environment so these are standard outlook and thunderbird add-on with no extra softwares that are up and running but really these installation flows honestly you could uh if you're curious you can check out that uh email um it's got more of the info on the installation and whatnot but what i wanted to highlight was these aren't just convenient tools they are convenient sure i hope you agree but this is a vital part of our research to discover what people need and expect to make ev security a default at scale so what do people need well we've created these options page right for the interest of time i didn't run through them on the actual demo but i hope you can appreciate that well maybe we have to do some extra clicking there especially maybe on the outlook side we have to go ahead and manually toggle on for each uh email maybe we don't want to do that maybe we want to have it automated so that every email you're gonna sign it by default right and every email you might try to encrypt it if they can find a cert on dame you know that's the kind of seamless thing we can go for maybe that's what users want just configure it to their needs right but it's not just what users want it's what we can find out about what users want so when to be silent versus explicit for example with private keys you know you can think of a use case where a you can have a separate keys for signing and encrypting or you could just have a single key for both right there's there's different people that need different things but it's really being able to understand this you know actually seeing this in action and seeing what the numbers will help us know that how this correlates with the use the use case of those individuals and the other settings that they've used what they expect effectively and this is where our study comes in we are conducting an iob approved study where users can opt into an anonymous user site right and it's basically on the options page you can just toggle it on and really you can help us see what the shape of the needed security is when accepted these specific toggles just you know the kind of anonymous general ones will be shared as statistics to our telemetry server along"
  },
  {
    "startTime": "00:36:01",
    "text": "with the dos server that we're using for the dns resolution but only obviously if it's public so only the public information users can optionally answer some basic demographic queries to really uh zone in that info as well to support the statistics that we're getting and it does not invade your privacy in any way the telemetry is shared only in specific times when you change your settings and then it never tracks anything about your emails only noting the set default conflicts and naturally you have the right to be forgiven you can toggle off at any time and it's completely anonymous and you can request all your data to be removed if you ever did toggle it on and we can't do it alone right we're trying to find the results that let us automate and enable security on the internet and for that we greatly appreciate participation try out our tools and go ahead and toggle that on if you feel you will let us actually see this stuff in action so we can step back into the big picture dane as an architecture lets us make end-to-end security more seamless for every person on the internet right we recall in the past that itf made this push for https everywhere right and we now live in a world where the internet scale transfer security is kind of solved it's a default everybody expects it you see that lock on the browser it's all good right we believe that sort of ubiquity should be the case for internet scale object security end-to-end security right where any random entity on the internet not just some public server could transact with other entities in a secure manner seamlessly but we shall start with two tools for email security as you saw in this presentation and this is just the proving grounds though going forward if you help us find what users need you know trying them out joining our study then we can further our research on the new uh uses for dane that i mentioned you know cyber threat intelligence and mhealth smart cities and iot devices to be what users actually need and that's where i'll leave you guys often thank you very much any uh questions or comments i'll be out"
  },
  {
    "startTime": "00:38:00",
    "text": "here as well to chat for anyone thank you excellent so we have jared and peter in the queue i guess take it away jared jared's in person hello jared hello um can you differentiate here between what you mean between security and privacy because there's a lot of equivalencies that the ietf specifically talks about for security where i think many of us know that html email is very good at violating your individual privacy and encapsulating that transport inside of tls doesn't really actually make that communication any more private uh than you know than it was perhaps previously because the assertion here is that this will make my communications more secure but if i'm still rendering the same html my privacy is still likely to be very violated by the people who want to engage in those act those types of activities and the community's done a very poor job of differentiating between those two and i'd like to hear kind of how you think this enhances um the two different parts because for this it seems like an awful lot of work for an organization to go through to get a secure transport um for the messaging back and forth which is already afforded by for example enabling tls so that's a perfect question because that's exactly part of the point with trying to look at bain it's not just about the transport you know we have https for tls transfer security data is looking at end-to-end security so the objects themselves are secure at the ends of them if uh i'm sorry if i misunderstood your question the yeah but that doesn't mean that the object i'm downloading isn't going to infect my machine or go and violate the security properties of my device which is one of my major concerns here is that"
  },
  {
    "startTime": "00:40:00",
    "text": "uh you don't this isn't actually affording any sort of additional security for the device sure that is actually a good point and really our goal here was to focus zone in on enabling gain for s mime specifically so the s9 protocol that it currently exists right now extensions to that protocol could definitely do some work in that regards to making sure email has objects that are like the contents of email the html emails and whatnot are more conscious of our privacy right specifically but looking at exactly s mime as the protocol exists today uh is kind of the objective of the tools that we had so implementing dane specifically but that is an excellent point and you know worth looking into in the future oh it seems like we have another question go ahead yes hi um okay whatever i'll just bend um yes i have a question about um dane porter works behind the scenes so when when you kind of claim a domain then the domain owner somewhere else the token in their dns okay that's fine and then once that's done when i create users in the dan portal and do stuff with the keys how does that actually relate to that domain and why would a certain let's say email client for example contact the dain portal server about this um is it actually like that or is some information put into the customers domain dns zone i don't know that i didn't understand that at all and regarding coor the plugin seems to be relying on dane porter specifically so what if i do my own deployment of that can i make it to use my database and where is it is it a dain portal or is it in the customer's dns domain so this is a perfect point as well um there's something unfortunately i was had to rush through here but uh dane portal actually just implements the standard dane protocol which is just part of dns you know dns secured by dns sec these are just standard dns domains that are zones that are beneath"
  },
  {
    "startTime": "00:42:02",
    "text": "like our normal structures that zone holders have just with special protocols um domain names right and there's a lot of details hidden behind that but really this is the public dns right obviously you can have private dns implementations too that do gain within your own uh organization and that is totally possible and your own male clients that can resolve that dns uh can indeed do that desert resolution of gain through that private uh infrastructure but that's kind of more like the classical pki but with the when you're connected to the public dns you have this secure route like you're doing dns tech you're starting at the root zone you're coming down it's resolvable no matter whether anybody that's hooked up just like if you have your own name server and you have your own domain on it it's just like that you know so this isn't any special proprietary database this is indeed the public dns gain portal is just a front end and a kind of the infrastructure meaning we just have a specified name server for that yes sure but when i use the dane portal web interface to configure something how does that end up in my zone because i don't remember you showing that the dns provider's api key is given to dane porter or something right that that is done through a manual delegation i think i might have done rush through those slides a little bit but like for example uh we dane portal will actually be serving the specific sub-zone for dain so it will have this known name server serves it but you know that's something it does currently and we're looking into having more like general name servers as well but really the delegation is just a standard delegation uh just use the ns record and you put it in your parent zone and it is delegated to the game portal right and the reason it does that is because underneath that zone it can add the records and change the records as needed to suit the user's perspective of the each email user being able to access the domain uh and add their own certs because dane has their own records and uh protocol for it um i think there was a question on the career side as"
  },
  {
    "startTime": "00:44:00",
    "text": "well but i think i forgot that i'm sorry i i think i can repeat it is so cooler dependent on theme portal specifically yeah that's that was the question that's that's a perfect follow-up because no it doesn't uh career is resolving through the public dns so it's using a dos server and starting from the root and just looking for the email address with the proper following the dane protocol so if i do my own deployment and using absolutely right yeah and the cool thing is the portal is actually open source thank you um there was a lot of talk around day in things and i i saw how hard he worked on this first hand and i really want to thank you for doing this that's great i hope everybody you know tries it out yeah absolutely thank you so much though for me i really do hope uh people are going to give it a shot you know it's up and running right now both those tools and their respective sites and documentations hi lars liebman from net node so where exactly are the secret keys for these certificates stored um so there are no so which certificates are you talking about dns sec zones no the user the user certificates used for s5 sure for the s mime the users will have them so um in that we briefly showed the tool that allows users to create a certificate or key pair that is there for convenience that is using just a it's a thin layer on openssl so using openssl you can generate a key pair you've got a certain and a key and dateportal promises that it's not going to save any of that and it doesn't uh it's open source people can verify but basically you're just prompted to download those and once you download them it forgets about it it's like one of those things where it gives you a web page to download it's up to the user to keep track of it technically um so yeah so which computer generates the key the actual key answer for convenience"
  },
  {
    "startTime": "00:46:03",
    "text": "the user is given the option to let the server generate it meaning run open ssl on the server right now obviously we do prefer if the users come in with a proper cacer key combo we don't ever see the key definitely we do not want users to rely on that for their end-all be-all but it is a matter of convenience to let people quickly jump start into doing secure email right but there's no proof that you don't harvest it um you can't prove it yeah yeah you are technically correct right if you trust that we are going to use exactly what's in the open source repo then yes that would be the proof but there is no true trust in that in that case perhaps maybe don't use that uh the tool and use open ssl yourselves and that's actually a very good point where we can point people at more fulsome discussion of how to make their own certs use openssl on their own operating systems as well so that that's a good point and a matter of improvement for the site so if you can improve that process where the user generates his own key on his own computer and then uploads it to use and then feeds it into the dns that would be really really awesome yes like it would be a matter of pointing to like kind of a run-through of how to do it themselves it's also worth pointing out that it is perfectly fine if you are a user that already has ca certs that you know you're already doing s mime and you just want to push everything on dane whatever you have currently right and it's perfectly fine for you to keep your own key and you never use that utility at all you know but you're right because the utility is up in front there it is worth doing a bit of a stronger messaging as well um but yeah feel free to actually go up there and see and see how you feel about the messaging that exists i believe it is pretty cautious when it says that you know it is preferred that you uh go ahead and make it yourself too but this is just kind of a convenience tool but great point thank you all right so anyone lost questions i don't think we do excellent thank you very much everyone"
  },
  {
    "startTime": "00:48:00",
    "text": "thank you guys um great presentation and i think next we will have the ipv6 extension header performance and metric diagnostic which is a very long title so hello me and will crispy bring up the slide duck excellent thank you yeah i can kick you out actually revoke there we go let me see if maybe i can share the pre-loaded because chris's machine is being difficult um here we go is that excellent and you'll just tell us what hit next yep excellent thank you so um we did some extension header testing as i think you guys know from actually some of the presentations here too at iepg um there is an outstanding question about whether ipv6 extension headers can actually be used on the internet it's been a controversy for quite a few years and a number of people have done studies um showing that they don't work um and by and large these studies sent crafted fake extension headers to a number of the very large sites on the alexa top you know whatever google facebook you know the usual suspects and so what we were thinking we ourselves have been hard at work on an extension header ourselves and we wish that to work and we do not wish to throw all our work away so if"
  },
  {
    "startTime": "00:50:01",
    "text": "extension headers don't work we have been wasting our time we wish not to do that so a very brief explanation this is of particular interest at end user sites uh enterprises because we need to do very quick triage as to say is the problem at a at a hot very high level is it in the server or is it in the network and then we can dispatch uh the right set of technicians to um to go to either way and the way we do this is we put timing and sequence number information inside an ipv6 destination option next so um the way we did our testing is um first we modified a freebsd kernel to send our pdm destination option with every packet and what could the reason we did the modification in the kernel is that we wanted to test uh real data going through and we wanted it to come through all the time so we patched the kernel and so then what we did is we chose locations throughout the world because we wanted to make sure that we were going to uh multiple transit providers and so you can see you know warsaw toronto mumbai and so forth is where we were next uh and you can see we have quite a few choices of locations from this small hosting service it does become important that it was a um a small quote-unquote like no-name"
  },
  {
    "startTime": "00:52:00",
    "text": "hosting service and not like one of the brand name providers like um you know amazon azure and so on uh next and so you can see our pdm locations are exactly where i had said before that they were next uh so let me first give a shout out to our sponsors the india internet engineering society for um uh paying for these little servers all over the world and for nitk suratkal for providing uh the young people who did a bunch of the code thank you so much and then our own organization which is a consortium of industry which is very interested in this kind of information so next so this is test results so what i did was i took a very very large ftp you can see there's a ton of kilobytes to download and i tested from toronto i based out of toronto and tested to all the locations and you can see here and pdm is in is attached to every single packet and you can see here that the ftp worked and in the background i took a packet trace because packets don't lie i mean people can lie but packets don't lie next so you can see there is pdm headers i took the psn this packet field out of the pdm destination option header and put it right out there and you can see it's in all the packets"
  },
  {
    "startTime": "00:54:01",
    "text": "uh by the way all these traces are available for anyone to look at we have them here and you can see surprisingly that the large ftp was fragmented and validly so so you can see fragment headers of large fragment headers also going to the other end next please so you can see here that everything went successful next please here is the destination option header out of the trace and you can see it is a valid destination option with all the data filled in the timing is extremely important because those are delta times that are calculated when i get a packet from one end i save it and i calculate the delta off of there and so you can see both ends are properly processing the previous pdm that was also received at their end next please now you can see both the uh pdm and the fragment header again wireshark is a delightful tool next please so in the bottom line all the traces worked i mean all the ftps worked with this very large file we also have apache set up to these and we have been doing testing from here over to i believe warsaw and we also have melbourne set up so these sites um are set up to do apache over the ietf network and if you wish to see the"
  },
  {
    "startTime": "00:56:00",
    "text": "results of those please come see us at the hackathon and i will leave it i won't tell you whether it worked or not you'll have to come down and see for yourselves next please so then some of the people were like you know internally some of people in the group are like wait a minute women okay so you're using one hosting service why are your results so different from other people's are these people using some kind of overlay network now keep in mind this is a small no-name service which i did not think had the money to have their own servers all over the world but nevertheless one wishes to verify one's results so i sent them an email and i said do you guys have some kind of overlay network and they said no we do not we go over the public internet next please i then wished to validate what they said so i did a trace route we have also done mtr between these sites and you can see there is at least one and actually multiple transit networks involved which also all passed these extension headers correctly next please so why are our results so different from other people's so what we believe is that we are using a real data and a real extension header and not fake data which may validly be dropped by thing by people we are also not going to the alexa top um"
  },
  {
    "startTime": "00:58:02",
    "text": "whatever um and this becomes important because we said well let's see whether our results are also consistent with other people's and indeed if you use the large hosting companies and go to the alexa top whatever indeed there are issues but the question is well why because in our mind uh these things are not being blocked at the core of the internet so where are they being blocked next please so what we did was we did pings and trace routes from our pdm enabled machine remember we have a patch to the kernel which will send our destination option out with every packet whether it is a udp packet or icmp packet which are both used for traceroute or ping and so what we wanted to see was let's say that our pdm enabled trace route does not get through but the non-pdm enabled trace route does get through where does it stop for example if there are eight hops between ourselves and say facebook then is it always dropped at hop um seven or is it always dropped at hop three or is there a random number where it is dropped the question being is did it get to the destination network or the edge of the destination network or"
  },
  {
    "startTime": "01:00:01",
    "text": "was it dropped randomly somewhere else next please and you can see in the traceroute packet capture that all these packets come back and you can see the psns showing that there was pdm in next police and so what we are doing now is summarizing all this information and the important thing is to do a dns resolution for the the hop of interest because it turns out that going to the alexa top whatever you're actually not going to facebook or google you're probably going to some cdn of that is you're going to akamy cloudflare at all and so where is it dropped next please i shall also leave that for next time as we will have we will have a draft in v6 ops summarizing all our results and and i'm i'm only sort of kidding about about some of this it's like we are we're actually doing all these dns resolutions and we're having some internal discussions which we wish to be completely in accord on on exactly what is it that we're seeing um and so once the team is is in sync then we will present the results we welcome collaborators we wish for others to test and validate our results and if you wish to collaborate we can make our virtual machine with pdm enabled and you may test for yourself again we want to be careful"
  },
  {
    "startTime": "01:02:02",
    "text": "because if our results are indeed correct letting this kind of thing loose on the internet um for anyone to use at all for any reason could create some potential issues so please come talk to us at the hackathon and hack demo and we can uh show you the results uh if you wish to test yourself we can work with you and you can do a trace route ping or actually go into our apache and take a packet trace for yourself and see what happens any question is that it any questions so thank you i think this is fascinating and how different these results are to some of the other ones i think it's interesting and hopefully we can find out you know why why yeah we welcome collaborators even collab and especially if collaborators have interest in in uh discussing or rather fighting amongst ourselves as to why we're seeing what we're seeing jen hi angeline actually i don't think they're really different from what other people see because most of the results i've seen is packet draw packets being dropped near the last near the destination network or even source network if it's user cpu right so transits normally let them through so i do not see really conflict with your results with any other right like so it's not normally the destination network drops it and if your vantage points permit them yeah i'm not surprised you see them going through and a question is there any reason you're doing dns lookups instead of looking at is numbers"
  },
  {
    "startTime": "01:04:00",
    "text": "because i think is number might give you a better indication who actually controls that hope we can certainly do that we can the reason i was doing dns uh well i'm not okay so a couple um all right let me answer that one and then i'll go back to some of the other comments you had it's because to me i won it's like for me it seems really obvious it's like it's like if how close did it get so like if akamai for example not to pick on akamai a wonderful company is if they're hosting your site and you're [Laughter] and you're already at the edge of the akamai network and then you get dropped then i think the question is is our packets being dropped at random points in the core of the internet or at the edge of the destination network those are two very different questions yeah i totally agree i'm just trying you trying to find out which network drops the packet right so i'm trying to understand why you are using dns to find out who owns that ip address instead of using ies number attribution for this address because dns might be my ptr might not even exist while that address definitely belongs to some is number which indicates who owns the device usually sure sure we can we can do either you know either one i mean i mean yeah fine we can use as number yeah but as i say we welcome uh collaborators and and if you want to see our results live um we're happy uh to talk to you at the at the hackathon and yeah maybe present come back next time as well we'll get oh happy birthday no happy to do that i um just wanted to put a one that was"
  },
  {
    "startTime": "01:06:00",
    "text": "in the chat um anna asked do you plan to do any hop by hop extension header testing we're actually doing that in the hackathon we're trying to break the rfcs so yeah come and we'll present actually we we're looking specifically at hot by hop and the results are a little bit interesting a little bit scary also hence the security considerations and we'll present more on it at the hackathon by the way and anthony is a member of our hackathon team and he's and and liquid is is um we're happy to have them working with us at the hackathon and i'm part of these troublemakers too and i just wanted to add to what anthony said and in response to that question that a key determining factor of why these results are occurring we think is the type of extension header like hop by hot might be different than doh and the content of the data in that so those are key factors we think we questioned whoever asked that yeah yeah yeah great yeah you guys great team and yes i we think that it wouldn't it should not be a surprise if fake data is dropped i think that actually speaks to good code at somebody's firewall hello hi i'm a little bit nasty question please what kind of bad conclusions uh do we expect to result from your claim that packets cannot lie that's a good one that's a good one they can't they just don't no no i mean again yes yes yes you know what i'm not going to distract on some people who claim to have packet traces of voting machines anyway okay i'll just stop right there i"
  },
  {
    "startTime": "01:08:00",
    "text": "will stop right there okay yeah okay okay thank you guys thank you very much and our final presentation for the iapg this time is going to be jeff houston who is going to be presenting on a quick look at quick the red square good morning all um thank you this is a very quick look at the use of the quick protocol out in the wild and i want to sort of compare and contrast theory and practice next slide next slide someone yeah okay you all know this next slide well you all know that too next slide yeah pushing a lot of buttons very quickly so as some of you might be aware we've actually tried to do large-scale measurement by actually with some support from google by by actually enrolling around 20 million people a day through use of an ad advertisement campaign where the ad actually contains a scripted set of url fetches and and if you look at the dns and regard dns query labels as actually microcoded instructions and make the servers give unique answers every time a query hits it you can actually bypass most of the internet's normal caching and expose the client to your own servers so unlike lots of other measurements like measurements the alexa where one point measures a hundred or a thousand or a million sort of remote points we're actually enlisting millions of unique users every day to come to a small collection of servers which are on vms around the world now we did this initially to actually dispel the myth"
  },
  {
    "startTime": "01:10:01",
    "text": "around ipv6 deployment is big non-existent whatever with very little measurement behind it and so we started doing this ad campaign to actually enroll a whole bunch of users and say here's the thing you can get to only if you have v6 and here's another thing you can get to which is dual protocol and we're kind of interested which protocol you use this was actually a revelation because it kind of dispelled at the time the myth that there was an awful lot of e6 around there at the time and exposed exactly where it was and why and we thought well we can go further and we started looking at dns sec because again if you say well here's a name that's not validly signed the number of people that go to it is kind of well you're not validating are you you've got a resolver out there that just doesn't care about the dns validation and then we started around ipv6 fragmentation and yeah extension headers and yeah completely different outcomes to to where nalani is reporting um completely different it has a lot to do with the experimental technique we have no control over what end users run this it's basically the ad campaigns do all the enrolling so this is really the internet as measured from the outside you and i as we look into the infrastructure and so we thought well okay let's actually expose the same thing for quick next slide and this is relatively vanilla stuff nginx has now got a beta version of its server that has all the quick functions enabled and so we're running 121.7 we looked at what apple have done recently with ios i think 15 was the first to actually release it it might have been earlier i don't know but certainly in 15 where those devices in particular will actually do an https dns query and look for that"
  },
  {
    "startTime": "01:12:01",
    "text": "alpn record and so we effectively create a dynamically synthesized alpn value for every single unique dns name that we're using and so the https record is there predominantly for those apple devices and of course there's the alt service header which has been traditionally used by chrome now i should make a note about that because you're only going to see that there's an alt service record if you go and get it and if you don't go and get it you don't know if it's there so you need to actually get the thing presented twice now with ads we normally say here's a list of urls they've all got unique names you only see them once it doesn't matter what you put in there it isn't going to get triggered so we then revise the script slightly and for a couple of these ones particularly this one we wait um and we actually wait for two seconds which is an interesting number um we wait for two seconds then we tell the user go get it again same dns name but we alter the http query args and hopefully altering those query args seems to defeat most values of https proxy if that's what you're behind so the idea is for about a fifth of our experiments we do two fetches for everyone else we just do one next slide yeah said that next so these are the big answers we've got we've only started running this in june at sort of a massive level of deployment we're doing around 15 to 20 million experiments every day across a unique batch of users thank you ad engine and i'm contrasting the second fetch to the first fetch because i can see the difference the first fetch is really really low one percent so if you put something out there"
  },
  {
    "startTime": "01:14:02",
    "text": "even with the dns https record about one in 100 users will actually go and say well i'm going to use http 3 i'm going to use quick the other 99 go yeah if you do the second fetch and in our case the browser is scripted with a two second delay now i don't know what browsers do inside browser code i don't know if any human alive knows what browsers do inside the browser code there's an awful lot of code and a huge amount of complexity we issue the get command inside the script after two second wait what happens after that is browser magic but the number is still pretty low 3.5 percent next slide so where um ads give us an enormous amount of detail origin as network et cetera they also with a rudimentary form of geolocation we use max mind with a huge amount of kerfuffle recently to accommodate apple's private relay service thank you apple really appreciate it because a lot of folk use it and you've got to sort of flick the countries around to map apple's map so this is now corrected for apple thank you but there are some surprises going on as to what sort of used extensively and what's not and what i'm seeing in front of me is much better than what you're seeing there so if you look online you'll actually see that the scandies are green india is red there's some kind of weird geolocation thing going on inside browsers because i pretty damn sure it's nothing to do with networks next slide so that's curious and here's a table malta um 28 on the second query 1.4 on the first central african republic in africa wouldn't have guessed it um but this is"
  },
  {
    "startTime": "01:16:01",
    "text": "not if you will a predictable list but there are certainly systematic variations there around locales next slide hi a question quick question have you have you asked chrome whether they have an experiment running that would match this have i also sorry i'll do that have you asked chrome whether they have an experiment running that would match that i've asked apple but not chrome and part of the idea of airing this is there will be questions to chrome as well and i'm getting there um let's actually look at the first query and sort that country list by first query hit and all of a sudden the scandi's come right up denmark eight percent of users in denmark do a first query hit on http 3 whereas the second query is only slightly more so there's certainly variations going on between first and second query next slide actually this is one of the few lists go back one sorry one of the few lists where the pharaoh islands features so all 10 of them and their sheep are busy doing a relatively significant amount of querying using quick on the first query uh next slide thank you so i have some questions here that i actually thought i really would like to understand this a bit better and sort of see what's going on and and the first is which are the clients which are the browser clients that are actually performing quick and why first and second fetch variation um the second thing is if you you read the quick specs you can't fragment you just can't it says do not fragment these udp packets so what are the packet sizes quick sessions actually use and in particular i'm interested in what the clients do when they open the session and start talking to our servers so in other words"
  },
  {
    "startTime": "01:18:01",
    "text": "not what i select but what is being selected out there as the mss values for quick what's the connection failure rate because there's been an awful lot of fear and distrust about udp port 443 is it filtered like crazy does it get through like glass you know what's going on and in this case the question i had was is quick faster or not is it really quicker next so let's go quickly and try and answer some of those whoever's driving this um this is an odd table like all tables it needs explanation so this is the i the os clients as determined by the browser stream what hardware are you running on or what operating system everybody lies right so in some ways this is just the lies i got told by the browser in their browser stream there's an awful lot of windows 3 out there you know yeah right um so to some extent it's slightly cloudy but there's patterns going on they aren't comparable horizontally it's vertical so i'm just breaking down i have separate tests that run only tcp and tls completely independent of the quick stuff and for those i see well what clients does the ad actually get to so around 5.5 percent of clients out there for an ad use ios another 1 uses mac 84.5 uses android this is sort of the market share of platforms um windows still exists and you linux folk you've got a lot of work to do if you want market share so they add vertically not horizontally so let's look at the first fetch and what you actually find is of all the folk who use quick immediately 93 percent is ios 2.8 percent is macos and the rest don't"
  },
  {
    "startTime": "01:20:00",
    "text": "count and they're probably lying so this really is an apple thing that's actually looking for the https record and acting upon it now i only had one percent but at least six percent of clients are actually using that platform so it seems that apple's doing around a one in four filtering and only selectively going to quick as determined somehow by apple not by anything else interesting and on the second fetch it's predominantly the android platform with a little bit of ios and again there are liars and all kinds of stuff because it's the browser string but predominantly it's an android behavior next slide please so let's go to the browsers and the browser clients and this kind of sort of sorts it out the world is chrome no one else matters just it is 91 of folk that we flush out in ads are running the chrome browser or so they report so it could be variants webkits whatever who knows but 91 basically say i'm chrome uh on first fetch 93 say they're safari so this again says it's apple uh interestingly on the second fetch um a huge amount in chrome but also some degree of use in safari some of you ios users seem to prefer or sorry some of you android users i really don't understand that it's higher than it should be next so who does it well it's apple it's safari it's the dns https record it only does one in four it only does about one in four not sure why the apple folk can answer that much better than me chrome why isn't it everyone"
  },
  {
    "startTime": "01:22:02",
    "text": "because if it really is a second fetch waiting and asking again should trigger it that conversion rate as i observe it is tiny we will return to this next slide okay so park that thought yeah on to the next one what's the packet size the sizes that folk send me tend to correlate pretty well with the rfcs 46 are exactly 1200 octets on that first packet coming in the packet that's padded up that the spec says must be at least 1200 octets 46 percent go yeah okay fine 1200 a few folk are more inventive we'll go 1250 um yay good on you and a few more folk go we'll go 1252. okay and a very small number go we go 1354 yay then um nothing higher so whatever you think there's no one kind of pushing the boundary here because predominantly on that first packet you have no idea what the path mtu is and so 1350 is about the extent of which people are going to say okay let's go that far but no further if that first packet is fragmented nothing works next slide quick connection loss this is unusual uh this is like seeing a 99 drop rate for hot buy hop extension headers in v6 except the other way around i look at packets that reach me so i have no idea what the client attempted because i can't instrument them but what i see is when i get that initial connection packet and quick has enough expose that you know it's an initial connection packet with an initial connection id i answer i should get a second packet so i look for the amount of time when i don't get the second packet"
  },
  {
    "startTime": "01:24:01",
    "text": "and on one day i had 19 million of these sort of initials i had 46 000 that never got the sec never sent me the second packet the failure rate is 0.24 now that's an awful lot of equipment out there that thoroughly trusts udp port 443 incoming in response to an outgoing yay good on them it's much higher than sorry that is much lower than i thought i was expecting something to three percent a bit like the v6 failure rate v6 by the way in tcp has around using the same methodology around two 2.4 failure the v6 connections don't work and it seems to be that the filters close to the customer will let the packets out but won't let the syn ack packets back in i thought the same would happen here it does not if 443 makes it out 443 makes it in surprising next slide is it faster this is getting a lot harder with quick because looking at packets is difficult because most of it's encrypted and actually trying to trace packet and ack when the entire thing is potentially multi-threaded tends to make my brain explode so i went for rough and ready and inside the browser yay is a timer and i have no idea how accurate this timer is i have no idea if it's just a random number or whatever but it does seem to be that if it takes longer to load the browser's timer value is more time than if it loads quickly and i think that's about as accurate as you get so what i did is i asked the browsers to go well how long did it take you to fetch this using quick and not using quick because there's a bunch of folk who don't use it and every user actually gets to do quick and non-quick so i take the user and"
  },
  {
    "startTime": "01:26:00",
    "text": "compare their quick and non-quick timer values next slide and so this is the center point of each individual user where i see quick and not quick and compare the two values because it's the same server it's the same network path in theory so if quick is faster the time elapsed to complete the entire transaction should be lower it should be on the right hand side of the zero point if quick is slow it'll be on the left there's a huge amount where it said near same time which is fine um but there's certainly a bias to faster and these are in milliseconds it's sort of visible around the first 50 to 100 milliseconds as being most obvious it's clearer in a cumulative distribution next slide around two-thirds of the time the browser believes the quick fetch was faster now i've just got to take the browser's word for it i don't have a clue what's inside a browser and i never want to know but if i just do that what you see is for two thirds of the time the browser is reporting that the http 3 load was faster question can you come back to a slide before above the comparison when you say tcp over tls you say http 2 or do i care whether it's http 1 or http 2 no i don't it's just you're not using quick and if you're coming to my server and you're not using quick you're not using um tls anymore it's all tls so it really is tcp over tls versus quick is what i'm comparing yeah it's just because http 1 does not have header compression so it may change a lot as a result i didn't look okay yeah just did not look in fact what i was doing"
  },
  {
    "startTime": "01:28:01",
    "text": "was saying it's either an https fetch or it's an https slash three that's all so you know i didn't factor in header compression uh go forward two slides were we yeah so this is a summary of what i said next now my answers are weird and i tried to find some other public answers and the most immediate one is on the cloudflare radar site where they're reporting served from cloudflare 30 of their web fetches occur over quick so what i'm seeing is far far lower now i have no reason to doubt that it's a fine number but again this sort of why are we seeing very different things next slide so i certainly agree but i actually think cloudflare is low because if quick is enabled by default in chrome it should basically all these connections should head towards quick and so in cloudflare's case i suppose the real question is to what extent is cloudflare seeing first fetch versus subsequent fetch what's their breakdown now i can't see in behind cloudflare so i really don't know and maybe that 30 is a reflection on how many times the client got there once received the signal saying if you go there again quickly within the case time you'll go and use quick but they didn't get there so only 30 of their folk went there twice or chrome uh is not doing it all the time i have no idea so that number is kind of difficult to unpick but ours is certainly a lot lower so why is my number so low"
  },
  {
    "startTime": "01:30:00",
    "text": "i think that two second timer is way too fast for browser behavior inside chrome personally i might when i say at the script level wait two seconds and do another fetch the incredible internal scheduling issues of the chrome browser with their multiple execution cues etc etc make that two second timer almost a random spin and i may well be doing the second fetch way too quickly but that's something only chrome's going to answer and i for one am not going to look at the source code so i might be being too aggressive here i could make that a really long time but you focus users when you see an ad displayed if it's displayed more than 10 seconds you are and click the xbox and kill the ad so if i bring that timer out you're just not going to see the ad anymore and i'm not going to get the measurements that's why the timing's so aggressive next slide so i don't understand a few things i started actually looking at how many folk asked for that https um that https resource record a lot three to four times the number of folk that actually do the first fetch so surely if you knew about that https record you were going to do quick if you could and if you get a positive signal yeah i can do this you'd go and do it but they don't somehow there is a control function sitting inside that safari browser that goes yeah i could but i choose not to and my suspicion is with the country-based variations that this is the locale that changes the default behavior of safari but you know that's for apple to confirm and for me to guess the question about that two-second scripted wait time in chrome you know i don't actually know how long i should be"
  },
  {
    "startTime": "01:32:00",
    "text": "waiting for and i don't know how long chrome keeps it going well i found this directive how long is that case of that directive lasting i have no idea and the other thing about chrome is will it always use quick or if you live in burma or vietnam will it do it differently than if i live in australia you know are there locales that change that default behavior and the same question for safari to what extent is this behavior triggered by various locale setting defaults i think that's it is there another slide one more slide oh okay whoa and there's a web page where all this gunk is is there uh as pretty pictures and graphs uh any questions or preferably answers because you know i have no idea some of this stuff uh question about i guess maybe back one slide uh or yeah either slide 12 or 16 i can't remember but your https response does it have an ipv6 or ipv4 address hint in it no hints just the alpn field so my guess is there's connection racing going on here it's happy eyeballs between http 3 and http2 and if you don't return the ip address in the https query because these are one use domain names the client is also doing a a record lookup and a quad a record lookup and an https lookup it gets the a or quad a and the https let's just say even say all at the same time what it's going to start doing is maybe tls http 2 you know if the http 3 comes back and said you know like there might be i don't depending on the exact ordering it might help if there's a hint thanks eric i will add a hint this week to the experiment and see if the numbers change yes you could well be right you know in some cases this is a black box i don't work for these companies i have no idea but yeah it could be very useful"
  },
  {
    "startTime": "01:34:01",
    "text": "thank you there's a question in the chat is anthony in the room or no not sure if it's a question or a possible suggestion of a solution but how much does ad blocking pose an impact on the test data particularly some vendors os's are more prone to doing ad block by default and do not track and all that this is a larger question of selection bias because i get to see the folk who get to see ads right and so some networks um there's a mobile provider in south korea i think it's ska seems to be massively ad blocking that's fine i don't get to see them at all and so whether you use quick or not doesn't matter if you get to see the ad the full measurement set runs right now oddly enough i don't get to see russia very clearly we all know why and at some points i don't get to see iran very clearly for i guess similar reasons china quite clear oddly enough the ad systems work in china just fine using you know double clicks infrastructure a lot of folk use ads so the selection bias is there absolutely for ad blocking but it's not for each individual ad for each individual type that's a different problem about censorship and the labels that folk use my labels are as bland as they get and so i'm trying desperately not to say this is content that is banned all that kind of stuff my labels don't do that so yes it's a problem but not in this context thanks i was also going to ask about slide 16 i think you mentioned the ipv6 connection failure rate versus the quick connection failure rate uh is this um"
  },
  {
    "startTime": "01:36:01",
    "text": "what's the v4 vs v6 split for udp 443 so your ipv6 connection failure rate is that is around okay tcp tls or is i mean what i see in v6 is i see a sin i send back a synack i never get the ack the failure rate is approximately 2.5 it's it's tracked on one of these pages you'd say well what's it compared to v4 and that's kind of weird because the defaults to get you to this point were v4 you had to speak tcp to the server to get the ad up and running and so in some ways the folk who fail on four don't get to see much of the ad at all in any case so that's why i've never tracked the v4 failure rate the same as i've tracked v6 so what i'm asking is is the udp 443 success rate the same in v6 and v4 or is it skewed in one way i haven't differentiated that out and i i'm still concerned that i'm only measuring the second half i have no idea about the packet coming out because the client can't tell me i tried and i failed so this is if it comes out and i answer these are the ones that respond but again eric um i'll check v4 v6 and if they're all v6 that's a a bit of a weird smoking gun yes no okay thanks hi brief question um do we verify that the initials that you see at the server was triggered by the client by the ad actually did you verify that the initial that you see at your server was triggered by the advertisement every advertisement has a unique generated dns name at the initial conversation of the script so all of the queries with that dns string which is actually again a piece of micro code ultimately came from the same client so"
  },
  {
    "startTime": "01:38:01",
    "text": "even if you're using apple private relay that name filters through and emerges it was you now yes there's tracking there's logging there's query replay so the label has a timer field inside its label if it's more than 10 seconds it's not you it's a replay i really don't care it's something else okay so yes i know it's you almost irrespective of how so even if you're behind a very aggressive gnat and you're changing your source address every rtt which is about as extreme as you get with quick it's still you it's still the same dns name okay and so in particular we exclude also scanners and spoofed ip addresses to the extent i can yes there's an awful lot of scanners that trigger really quickly to the user as if there's a real-time feed from the user's browser to the scanner which happens a lot they're much more challenging to detect hi this is peter from the esec i would also like to ask a question on this slide i assume there is also some failure rate for the second packet when you do tls of a tcp especially in slow mobile environments for example and i wondered what that number is and if it's different from this one and whether you have parameterized the measurements by mobile versus desktop for example by ap address or user agent or something so are you referring to a failure rate in tls and tcp oh so as i said that's what i referred to in v6 sin synack okay i don't get the ack and again i can only measure in six because to get to the ad four had to be working so the failure rate in four doesn't really matter in a straight tcp thing"
  },
  {
    "startTime": "01:40:00",
    "text": "all the ones that fail in four never got enrolled in the measurement sadly well but i guess even if it doesn't seem to matter there may be still be the possibility in before that it does fail at this stage with tcp and if that number be comparable to this one then i think the conclusion that this number here stems from client-side filtering of incoming four for three packets over udp is not necessarily true i think the point to take home is that this isn't twenty percent yes yes and it isn't ten percent and that's great it's actually it's actually really bloody good yes and it's about as good as this technique will ever give you yes and it's probably the measurement not the infrastructure and so as far as i'm aware once you get to this point network blocking on incoming udp 443 for those who are doing quick doesn't seem to exist yay okay then i agree i thought you were saying this number is from that cause and that's what i was questioning okay cool um then you have this um diagram that shows the results that quick connections are quicker than non-quick connections back two slides oh forward again yeah yeah i don't necessarily need the the diagram yeah so my question is if um there are shared results with the dns lookup would benefit for one part if you do it first so perhaps the the quick look up is always the second one then it may be faster for that reason i thought you thought about this but perhaps not so when i asked the browser in my script to measure the elapsed wall clock time when it's given a url to fetch i have no idea what it's measuring none when i ask 20 million a day to measure it i'm relying on the fact that no matter what stupidities happen inside"
  },
  {
    "startTime": "01:42:00",
    "text": "that browser it's all the same stupidity and so the structural difference between these two collections of measurements is the transport protocol and a bit of the dns and therefore this is relatively rough and ready it's not a measure of packet rtt times or anything like that it's just simply the browser's view of the elapsed time and you go what does that mean i would say go ask the browser folk i have no idea right but in case it does include any latencies from dns for example and if there is a bias by always first doing the non-quick connection and then doing the quick connection you could average that out by randomizing the order across clients and 50 clients doing quick first and then on quick and the other 50 percent the other way around that might average out some of it and when i give them an ordered list of things to do they almost never appear on my servers in that order the browsers seem to add their own random the universe's noisy component and it is okay that's very interesting um yes that's what i got okay thank you thanks peter hi john o'brien you pin uh just uh checking my understanding when you say that in order for the browser to participate in the measurement ipv4 must be working um i to infer that that means that the ad platform is an ipv4 only ad platform no so i don't understand you know oh there's a lot more mechanics about limitations in ad delivery networks and what we are trying to do we originally had scripted an ad that when we went off to the admission it said run this set of code and we kept on changing things and every time we did that we had to go to the god of advertising saying could you please approve our new ad and as far as i'm aware google is an incredible advocate of ai"
  },
  {
    "startTime": "01:44:01",
    "text": "incredible there's not a human in sight in the entire ad machinery platform so when you submit a new ad in the answer is random yes approved no not approved yes wait for another day and we got pretty frustrated with this you know understandably and so what we did instead was to go okay we're going to load a skeleton inside the ad machinery which then is its first thing to do is come back to us going hi give me some some tasks so we could change the tasks without changing the ad yay that step is b4 why to maximize the reek because at that point we were fixated on measuring v6 and so in some ways the v4 always has to work for this entire thing to flow through so don't blame the ads don't blame google blame me that was the way we designed it yes it could be dual stack but in some ways this stuff works and google has approved it yay very scared to touch it thanks thanks no other questions so do you plan to to as a quick extension you see name spin bit to make additional measurements we do full packet capture full packet capture there's an awful lot of packets sitting in a you know spinning storage out there somewhere do i plan to look at it um good question currently we're fixated on extension headers uh we'll probably get back to quick once questions get resolved the data is there never looked i asked the question because it's something that is activated by the client it's already part of the implementation but it's a client-side activation this is why yeah you've got to look for it and that's hours in the day people unfortunately"
  },
  {
    "startTime": "01:46:02",
    "text": "okay there's an awful lot of privacy issues because you're seeing way too much of end users that i think isn't comfortable with so you don't want me exposing it i don't want to expose it it doesn't get exposed but my question was if you plan in the future to to ask the client to activate this feature to make a lost measurement somewhere on the path you see it's about your connection lost uh if engine x has it for free i'll do it if engine x doesn't it's probably not going to happen it's mostly to to enable on-pass measurements okay great we're done thank you very much excellent thank you very much everyone thank you to the presenters as always and we will see you next time in london and if anybody's got any suggestions for things i'd like to present on start thinking about them now wow"
  }
]
