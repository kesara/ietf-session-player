[
  {
    "startTime": "00:00:04",
    "text": "I mean we\u0027d like to see one this is the TSN tutorial we have put together with norm Finn and Pat Taylor I\u0027m Janos fucker so Erikson I am the chair of the time says it is not working at a zoo before we start we need to highlight that what you see here is a personal private you of the presenters not an official review of IT Plato to rato 2.1 or anything like so what we have today is after some introduction we dive into the details of TSN TSN twos TSN features like reliability deterministic latency resource management and after the tutorial after the summary and questions and answers we have a brief demonstration of the reliability feature recently published or finished almost almost published in anatomy which is also one of the major work items in the.net data plane on how to provide this reliability so this proof of concept includes both so let\u0027s get started actually there is quite wide use "
  },
  {
    "startTime": "00:03:08",
    "text": "case interest or many areas see TSN interesting so there are a number of potential markets that can use leverage TSN so this is not another full list of course one of the the key ones is industrial automation so in industry 4.0 TSN is kind of the new networking technology that is planned to be used and automotive for vehicular networking is very keen on having eaten at in the car we controlled by TSN tools and you see many others including 5g that can also leverage TSN that is very boring going on how to provide transport Eterna transport for the frontal interface which is part of 5g so TSN or this TSN TSN is in actually one of the task groups in the IT plato 2.1 working group which is part of the 802 Landman standards committee that develops language standards and mainly focuses on on the data link and physical layers into 2.5 as illustrated in the figure sits on on top of the 802 scope this Greenfield do I have a laser pointer here so II 22.1 sits here and TSN is one of the test groups there so 802 dot one is the one that is responsible for the 802 architecture and for the interworking among the different among the 802 Lance provides also security and also responsible for the overall management and protocol layers above the Mac and there are C layers the SN was actually started as audio/video bridging so you may hear the a VB abbreviation it was started in to 2005 in order to address professional audio and video market has reflected reflected by the name including consumer electronics and and actually automotive infotainment so it is already a VBS used in automotive there\u0027s a new alliance which is an associated group to provide conformance and and marketing for a VB and it got very interesting for other use cases and the scope of the work has been extended in a task group and the name maybe we did not reflect the extended score so we decided to change to cover the scope that\u0027s why it is corner TSN today which includes industrial and automotive and "
  },
  {
    "startTime": "00:06:11",
    "text": "many other use cases as I mentioned before ok so what is TSN what we are after we are after to provide ground guranteed data transport with bounded low latency low delay variation and extremely low lows so TSN is in fact includes 4 add-ons to generate networking or they can call them four pillars synchronization is one of them it\u0027s very important and provides grounds for many other tools queueing tools reliability is crucial for the mission-critical applications and we have dedicated standards tools for reliability like the film application and illumination and well we will see details on this shortly this is the high-level overview and as even the name of the group such as we are after low literacy for which multiple shapers and cueing mechanisms have been already developed and work is still ongoing and together with the resource management which is essential in order to provide zero conscious shadows that\u0027s what we are after so the this oops these two are to provide the zero congestion loss and for the reliabilities is against loss due to device failure then I would give the word to norm to elaborate why are we doing this and what is in scope thanks norm in what way we\u0027re targeting real-time applications and by real-time I mean something happens and you have to respond to it or something horrible will happen like two parts of the machine will bang into each other the plane crashes so that means what we care about is worst case latency most of the applications not all of them by any means but many of the applications are control loops I measure something I respond to it I measure something I respond to it and cause an action control theory depends very strongly and what\u0027s the period of your control loop and so getting the information there before you need it doesn\u0027t help anything because I have to do the thing at the time that\u0027s scheduled to do it so all that matter is that you get there soon enough average mean best-case latencies are completely irrelevant so that\u0027s not what you\u0027re commonly what we commonly think of as optimizing because I\u0027m not optimizing for example buffer occupancy "
  },
  {
    "startTime": "00:09:14",
    "text": "two fundamental ways to balance your latency throw away anything that\u0027s late grossly over provision the network lots of intensive engineering you\u0027re testing or provide zero congestion loss we chose the latter solution so what am I talking about I\u0027ve got several hops at each hop there are some queues now if the given is constant input rate and I should flag you here we can\u0027t solve all everybody\u0027s problems there\u0027s one kind of problem that we are interested in solving and that\u0027s constant bitrate streams if you have a widely varying bitrate stream we can\u0027t help you it\u0027s got to be relatively constant and it has to be last last long enough that it\u0027s worth your effort to make a reservation for resources before you start using it so assuming you\u0027ve satisfied that given that you have a constant input rate you want a constant output rate otherwise you\u0027re filling up or you\u0027re draining so in the long term the output has to equal the input we find that then the latency is necessarily bounded if you\u0027re not throwing anything away so how do we get zero congestion loss what we do is at every hop the packets per interval n equals the packets per interval out the data rate n equals the data rate out now packetized data is not a constant bitrate stream and i\u0027m typically have a bunch of streams that i want to do this with all going out the same port they have different data rates so I can\u0027t perfectly spaced everything so there are so it going to be gaps and bursts in each individual flow so what we do in TSN is we were observe enough buffer space we\u0027ve reserved the buffer space before the flow starts we use a cueing and reserve we use a cueing and shaping discipline that minimizes the interference between flows and also provides a predictable gap and burst behavior so that we can figure out what is the worst case of the bursts versus gaps in two adjacent hops that tells us how many packets we have to buffer in each node to ensure that we never miss an opportunity to transmit so that it went when it\u0027s my flows turn to transmit I have to have a packet there that means I have to have a buffer of a few packets so we want queuing algorithms that will give us a small number of the smallest number of packets as that gives us the best latency but it\u0027s got to be predictable its number one thing and then for certain known "
  },
  {
    "startTime": "00:12:14",
    "text": "delay variations for example it may take a varying amount of time to get from the input port to the output queue it may take a slightly varying amount of time to get from the output queue having selected for output until it actually hits the wire there are lots of variations in there you have to have a little bit of extra buffer for the variations because the variation in delivery time means you\u0027re storing data that\u0027s the only way you can have a variation in delivery time is that you\u0027re storing data and when that time varies you wind up dumping the data there has to be a place for it so having computed the buffer space we can get the latency now to give you an idea of why what we\u0027re doing the usual best different kinds of service this doesn\u0027t have a okay the usual best different kinds of service the more buffers you allocate the fewer you draw where is it that one thank you okay more buffers you allocate the fewer packets you drop if you look at what is the latency for an average packet you find there\u0027s a minimum it\u0027s got to get to where it\u0027s going so there\u0027s a minimum latency most packets don\u0027t take too long because the network is not is typically not congestion but if it networks a bit congestion some packets can take a longer time some packets can take a really long time if there\u0027s a topology change it gets ridiculously long and the variation in latency sorry most packets occur at roughly the same time near that peak but you can have very long latency so if I\u0027m trying to engineer my network and I say this is pretty good but this is my this is my cutoff if I want to if I want this to be my cutoff and I say I don\u0027t want anything to be take any longer than that to get into n this is the worst end-to-end latency I can accept if I push that in from a higher number that means I have to drop packets that\u0027s the way and that\u0027s what we don\u0027t want to do so what we do instead is we try to come up with the scheme such that we know how many buffers it takes so that I\u0027ll never lose one if I exceed that number of buffers doesn\u0027t matter I\u0027ve still got some religion residual packet loss because foxes can fail wires can fail cosmic rays can hit the packet my latency is now a curve that has a minimum because it takes a certain amount of time to get there no matter what but I\u0027ve got a maximum latency that I do not exceed and there is no tail past that typically that maximum latency "
  },
  {
    "startTime": "00:15:14",
    "text": "there\u0027s my average latency my average latency is typically somewhat higher than it would be if I simply gave the packets higher priority that diagram right there is the difference between TSN and what we want to do with that net and typical best avert algorithms so why TSN without TSN you can do it you can over provision the network you can use priorities you can try to set the timing on your transmitter so that they don\u0027t interfere too much there\u0027s lots of things you can do and it winds up being people do this today a lot and they wind up having to spend a very long time testing especially the corner cases especially what happens when I stop these three machines and the other two machines are still running you can make it work but with TSN it\u0027s trivial to engineer because you ask it will that work yes because the algorithms we use give you a yes or no answer can you add this stream it works even when you have the hard to test corner cases shutting off all but one stream does not cause the one stream to suddenly burst or change change the way it works which makes it cheaper because your costs your people costs go way down and because you can use the same network for the TSN and for ordinary traffic I mentioned the only thing that makes that tail that keeps the curve on the tail no matter how many buffers you have is failures failures of links failures of packets failures of devices so there is one other big technique that we use which is per packet redundancy now the normal way you do redundancy is you have two paths from A to B taking different routes through the network and you use one path and if you discover it fails you switch over to the other paths sometimes you send the packets on both paths sometimes you don\u0027t but you notice that it failed and you switch over this is kind of like that except what we do is we serial number every packet and we receive both packets from both streams or all three streams or however many you have and by looking at the serial number we throw away the ones we don\u0027t want so there is no detect failure cycle and respond to the failure with some kind of action that doesn\u0027t happen we\u0027re always taking both paths and that means that it "
  },
  {
    "startTime": "00:18:15",
    "text": "would be unusual to lose even one packet even if something goes down even if something goes intermittent we\u0027ve all had the experience of a link starts to degrade you start to lose more and more packets from it at some point after losing a bunch of packets you decide it\u0027s out of its service parameters it\u0027s giving you too many errors now you switch over in the whole process you lose a lot of packets the idea is here you don\u0027t lose any packets so that you can get extremely high extremely low packet loss rates even on the long term we have another important thing which is the packets come in we have filtering and policing and then the queueing shaping and output this doesn\u0027t show the forwarding but of course there\u0027s a forwarding choice in here too an important part of this is that you can\u0027t have misbehaving streams mess up well-behaved streams if you look at a system with lots of parts especially a real-time system packets flying everywhere it\u0027s extremely extremely hard to analyze all failure modes okay and these things go into airplanes they go into automobiles where such analysis is very very important because people can die if you don\u0027t so it to reduce the number of failure cases sometimes even at the expense of perhaps increasing the chances of a failure as long as you can analyze what will happen and show that it\u0027s okay that if it happens so every frame can we can identify which flow it belongs to we can market the usual red green yellow we can also have timed gates that say this port is open for packets of this type now it\u0027s closed so we can insist that the break sensor not only transmit only the brake sensor packets but it only transmits it at these times when it\u0027s supposed to and that it only transmit the one okay now a breaks entered sensor that runs off at the mouth and starts transmitting continuously can\u0027t screw up the rest of the car various ways to deal with that you may be you turn it up we have in our standard we have various ways to view with that maybe you drop the offending packet maybe you cut off the offending port one bad thing about that is that might maybe you programmed "
  },
  {
    "startTime": "00:21:16",
    "text": "it wrong so don\u0027t fix that in testing but on the other hand when it comes to running off at the mouth so what what is it transmitting well that\u0027s a good question that\u0027s a lot of failure modes if it\u0027s transmitting too many packets when it shouldn\u0027t then you know something\u0027s wrong and I guarantee your failure analysis did include the case where that thing dies so if it does anything weird you shut it down your failure analysis has covered that that\u0027s a good thing when you\u0027re trying to figure out what\u0027s broken so we can protect against bandwidth violations this decisions can be per stream per priority we have a the gate can be operated on a time schedule all of the devices can be synchronized so the time schedules are running all the time so I can set my input gate to match the output gate at the other side and now we have the next section for Janos to take ok so after the reliability aspects which is the includes protection in the ingress let\u0027s see what happens on the other box at the output port the queueing man isms in order to provide deterministic latency I guess you are all familiar with the stick by the queueing but it was started in 1998 so that\u0027s that was the basic queuing mechanism we had and - that waited for awaited queues were added with simple hooks in order to not to over specifies the details and this is the first mechanism that was actually specified by the ABB test group for audio/video bridging streams at the time it is called credit based shaper I will explain in the next slide why why the name this is she so the shape queues oops the shape queues are these so this is the in these diagrams not mark marking for the shapes shapes cues a have higher priority than any other queues and this man is of still guarantees bandwidth - the highest priority that is not shaped like a priority 7 in this example the credit base shaper is similar to a typical run late birthday at the shaper but very nice with very nice properties it has only one parameter to set up and as I mentioned already the impact on other queues or the number of registers papers is extractable and it doesn\u0027t depends "
  },
  {
    "startTime": "00:24:17",
    "text": "the impact to the other queues on how many shapers you use let\u0027s see what it is so why why it is guided by shaper so in in the Idol periode here in the idol slope you each queue or the queues collect credit which are used actually when a frame is transmitted so the credit gate goes down when frame gets transmitted so if you go negative with your credits then the note transmission is possible from that queue so that\u0027s actually a key difference compared to token bucket that the credit can go negative this credit accumulation can be awaited so you can add the weighted aspects if you want to the credit by shape or operation so that was that\u0027s the avb queueing if you heard that term and what I\u0027m explaining from now on was specified as TSN well it\u0027s all one package together today so this was this is called schedule traffic which introduces time based scheduling to the queues it is for it is to reduce the variation delay variation for CBR streams which are periodic with known timing actually it is time based programming of the bridging qs8 queues in and the tool is these time gates up front of each queue so there is a gate before the transmission selection a front of each queue which can be open and closed and these gates are programmed according to the time so it gives a time schedule and of course you need to know the time so time synchronization is needed so you can establish times thoughts or or can do quite fancy things with this with this tool the time gated queues for example the time gated basic tool we use for the cyclic queuing and forwarding which is in essence uses double buffers to establish cyclists in the network and and the goal is that each packet spends exactly one cycle at each hop this example shows two pairs so four and five are one pair this green and blue and two and three are as well one pair but let\u0027s focus on a pair of cues so what happens is that the frames that you receive are collected for a certain time period and then with one of "
  },
  {
    "startTime": "00:27:20",
    "text": "the queues and then they are transmitted and the collection happens in the other queue so that the two queues are served in an alternate fashion and that\u0027s what provides the DC click queuing and in order to achieve this alternate operation oops we use we use the time gates up front of the queue so actually we program the time gates according to the cycle time we want to achieve what I described up or actually all of us this kind of up to this point are all done work published or close to memory standards the next slide is ongoing work it is called a synchronous traffic shaping which aims to provide zero congestion loss without time synchronization so it what happens is that we aim to smooth a smooth traffic pattern by shaping / hope and give priority to urgent traffic over more relaxed traffic this figure illustrates it so what you see these are the classic use so this is a kind of you can see it as a kind of hierarchical queuing that up front before that using the ingress filtering and policing tools we have specified and provides a lots of possibilities to David one of them is to imply and a synchronous traffic shaping so that\u0027s where the decision is made which flow or which packet is more urgent than the other and then after making that decision they get into the regular it accused so that\u0027s how the shaping from one hope to another is implemented and I give the word two paths who was the editor of the same preemption standard right so I was the editor for the frame preemption the part of preemption that went in 802 dot three because in this case this is a facility that needs to be supported by the Mac and as well as by the part of the link layer above the Mac so when we have when we have the scheduled traffic that scheduled traffic is like rocks during the the transmission time that those rocks can\u0027t move they always happen in that spot now we would like to handle the non scheduled traffic the the traditional network traffic as efficiently and as possible and use the bandwidth between the rocks to to send as much of that traffic as we can now if we just leave things as they are and send them we\u0027ll have a situation shown here where that\u0027s "
  },
  {
    "startTime": "00:30:32",
    "text": "void okay so you know here you we\u0027ve sent packet one in this gap and then we have packet two to sit sent but it\u0027s a little to fit largely for the remaining gap so this bandwidth is wasted and then it doesn\u0027t fit in this gap either and so the bandwidth is again wasted and finally here we get a gap big enough to send packet two that\u0027s without preemption when we add preemption we let the traditional traffic act like sand between the rocks so in this case it\u0027s okay that there\u0027s not enough room to finish sending packet to we can start it here and use the bandwidth and then when the scheduled Rock finishes then we can finish sending packet to and start sending three and again continue it after the next scheduled Rock so that\u0027s why we did it so we could efficiently use the bandwidth while having the scheduled traffic in there and the traditional traffic sharing the same links so we also put in something called hold and release now it\u0027s not needed all the time preemption doesn\u0027t happen the minute you say I want to preempt the packet that\u0027s going for one thing in order to be compatible with all the physical layers out there and have them not have to know that preemption was going on we wanted the fragments that we create to look as much like regular packets to the physical layer as possible so that included keeping the minimum size of 64 bytes so if the packet has less than 64 bytes to left to transmit we can\u0027t preempt it if the whole packet was less than 124 octet Stan we can\u0027t preempt it because we have to be able to divide it into two groups actually if it\u0027s one has to be 128 in order to preempt it because we have to be able to get to 64 huh then the extra CRC we add counts on that which is why it comes out to being 124 and then in many use cases we can we can stand that amount of delay we don\u0027t need to do anything special and we just when we want to send a an Express packet a packet that can preamp the preemptable traffic we just send it and go ahead and live with the fact that that doesn\u0027t happen instantaneously that that we have to get to the end of the current packet or gets the point where we preempt and have an inter frame gap but in the for the most tight timing for we want to have really minimize the amount of jitter on the scheduled traffic we have the hold and release so we know the scheduled rock is coming up basically we have a guard band of time "
  },
  {
    "startTime": "00:33:33",
    "text": "before that happens that we send a primitive called hold to the Mac and the Mac then we\u0027ll go ahead and preempt even though we aren\u0027t giving it the scheduled packet yet it will go ahead and preempt and so that then when the scheduled packet arrives we will transmit that right away we will already have completed preemption and there can be a number of packets that go and then we\u0027ll release it when we get to the end of the scheduled time and we\u0027re ready to let the other traffic flow again so this is just a picture of kind of how this all fits in with the queues so the queues are up here and in order to support preemption we basically have have two Macs one Mac that\u0027s handling the Express traffic and one that\u0027s handling the preemptable traffic so that this one can pause its transmission and then resume it when when the Express frames are done and so the queues for the Express traffic go to the transmission selection for the end to the Mac control for the Express Mac and the other queues go to the preemptable Mac and then below the Mac\u0027s we have the Mac merge sub layer which is a shim sub layer that that basically takes the output of these two and does the actual preemption when necessary and then we have below that the Phi which is a regular find doesn\u0027t have to know anything about preemption going on I\u0027m not going to go into the details of this this is the structures we made for handling the and formats and basically the principles here were to make the fragments look again as much like a regular packet as possible so the physical layer doesn\u0027t see anything other than what it expects to see and also to minimize the amount of bandwidth taken when you fragment a packet so to the extent possible consistent with the first one so basically the encapsulations designs so that if you\u0027re not fragmenting there\u0027s no extra bandwidth needed so anytime you don\u0027t preempt a packet it takes the same amount of time on the wire as it would if it was if there was no preemption on the link at all and then when we do preempt we have some extra bytes we need for protecting the data integrity but we keep those down to a minimum so basically in the the data integrity goal "
  },
  {
    "startTime": "00:36:35",
    "text": "is to keep the same data integrity that we have without preemption operating so we make sure that there\u0027s some bytes in there that keep us from reassembling fragments incorrectly to make a Franken packet a packet that had a missing fragment or was had fragments from parts of two packets so that we keep the same level of protection as the Ethernet CRC gives us in unprompted traffic and okay so after these let\u0027s take a look on the fourth pillar the resources so what do we have on that front in TSN we have a document describes that in qcc that describes the DSN configuration and the three approaches we can have in TS on one of them is the fully distributed model that actually means you use the rebooted protocols for resource protocol for resource reservation we have next slide explains a bit on on the already existing distributed protocol and we have recently started the development of a new one in order to provide better scalability the fully centralized model is when both an equipment and network equipment like the bridges and the end stations hosts are under control of a central entity which can happen for example in a factory that all the end stations and and the bridges are under desired to be under the control of the same entity and the one that is also illustrated in the bottom is the centralized network distributed user model where the bridges the network nodes are under the control of a central entity which is called in this documents at a central network configuration entity and but the end stations listeners and talkers are not under the control there is some protocol exchange or some information exchange between the institutions and the network actually in a wire user network interface this document provides information model and yang for TSN configuration it is on still an ongoing project yes yeah I guess maybe it\u0027s good it\u0027s good point so what you see here when it when you see P up front of an abbreviation that means that still an ongoing project if P is not there then the work is done sorry which one is "
  },
  {
    "startTime": "00:39:38",
    "text": "well for this this is not for data centers but we switch seats in so that there are data center bridging standards the question was if whether a V switch is a bridge or a nun station and in DC standards we have protocol exchange and so on when it sits in so it\u0027s treated as sitting in a LAN station but this is more for TSN so we don\u0027t have V switch in this in this figure so these these these end stations here are sources and sinks of CBR streams and what we are after is how to provide the resources for those streams in order to provide the bound in low latency as I mentioned we we have protocol a standard protocol SRP for the reservation of resources so this is a distributed protocol that advertises the streams registers the paths for the streams even calculates the worst case latency and trend and establishes the domain for a VB domain what is it called that can be used by the stream so have the same characteristics or setups in the bridge and ultimately of course reserves the bandwidth for streams the qcc project also provides enhancements to the SRP stream reservation protocol and we have a new work that has been recently started called link local registration protocol it is the foundation of a new reservation protocol with to replicate and register the data that is needed including the changes and what you are after is better scalability so we want to optimize it in the order of the database in the order of 1 megabyte I need to highlight here that this work is not tied to bridges so can be applied on other network nodes as well so this is what we planned to talk about and we have some stuff going on and available we have no time to talk about just just to flash time synchronization I mentioned in the beginning it is very important like for example that for the time gated queues you have to know the time to program your schedule or for the cyclic queueing and we have standard for that the 802 dot 1 is timing and synchronization which is a profile of the ITP 1588 "
  },
  {
    "startTime": "00:42:40",
    "text": "precision time protocol for layer to it and bridges and there\u0027s an ongoing revision of this standard that includes all addressing all these aspects for example providing better reliability security is very important in of course in all applications but there are upcoming use cases I mentioned automotive networking where it is a hot topic how to how to make it secure so this is a just a set what we have in 802 dot 1 again no time to talk dive into the details so to summarise TSN has these four pillars synchronization for which we have the 802 dot when a specification reliability which covers multiple aspects as we heard the replication and elimination for against protection for providing extremely robust protection for device failures and other asked for protection against misbehaving streams and at the shapers queuing mechanisms to provide the latency together with the resource management in order to achieve the zero congestion loss there\u0027s a survey for each tutorial this is the link for this particular one please fill in the survey and there\u0027s time for questions hi thanks for information you mentioned there were different types of scheduling algorithms you\u0027re looking at and it sounded like at least one of them would not deliver early as well as not deliver late right so it was just as worried about early delivery is late delivery yes but the other one I wasn\u0027t clear about and you may have said it and then you know a be jet-lagged and just missed it but the one that\u0027s under development does that also worry about early delivery so that will not deliver early either so there is a there is timing you give for each each for when when it can go out so that\u0027s what happens in the higher in the if you if you think of it as higher hirako queueing that happens at the first expeditious time yeah I wasn\u0027t sure about because since it was almost you could look at as strict priority hewing "
  },
  {
    "startTime": "00:45:41",
    "text": "it meant that if something was if you had a packet there but you weren\u0027t quite ready to send it does that does it block completely or would have no send a little priority and in we have two basic we have two kinds of queueing mechanisms and one kind is synchronized time-based and the other is very much like intserv for those of you who are very old and the ones that are like in served candle liver early if the source varies its speed and there\u0027s a certain amount of jitter on the output it takes it a while for all the buffers to fill up before it becomes stable and and so the jitter decreases with time but you start all over again if you shut up and start over again just like very much like answer the other is time based and the time based ones with synchronized clocks can deliver the packet within a fixed time window that\u0027s the same for all flows that are coming out of that port or it can be down to a nanosecond if you want to do that that gets a little tricky because of how many queues it will take and how many timers you have to handle a lot of different flows but it can be very accurate yeah I was asking it I got on the very accurate one that you wouldn\u0027t deliver early it was really on this one or it wasn\u0027t clear and I think I heard Jana say it won\u0027t delivery early can deliver a can deliver early yes ok if if the flow is running and you stop for a while and then send one packet it\u0027ll go through real quick yeah what\u0027s a while right and two questions so the first one has to do with the source and the destination which looks like if I got it right there or excluded and many times I don\u0027t know what the assumption is for what type of devices those are but many times significant amount of jitter and latency can occur at both ends so how is that actually addressed here in the overall scheme so so the one of the use cases is industrial automation and in those cases the sources and destinations are controllers and and actuators and the the control flow is a CBR flow for them for the control loop between those so that is very periodic and that\u0027s why we can have this time based man isms in in the queues okay and "
  },
  {
    "startTime": "00:48:42",
    "text": "the other one so obviously i Tripoli\u0027s eight eight queues but you may have many flow so what is the in each priority level so what what is the mechanism by which you are dealing with competing flows so yes it\u0027s eight queues and it\u0027s a it\u0027s up to you I mean you can you can allocate for competing or I mean put competing flows in two different queues and then you can have some control on those or what you can do is this in these green box that the person is filtering and policing that can make you can do it very fancy things before they end actually in the queue before the packets are put in the queue and this is what the our synchronous traffic shaping uses like internal priority value for example or yeah if I were building a strictly standard box I\u0027d be limited to eight queues if I were building something if I were building a product for somebody I\u0027d very likely have a lot more than eight queues so still still at the wire level I\u0027m gonna have that interference no matter what and and so are we so so is that something that you guys are interested in the IDF to work with you on or what is what is the areas that you\u0027re looking for because I Triple E obviously is limited at some point and the question is exactly as you said if I would build a product that has the head I obviously am NOT going to be satisfied with the number eight so how are we dealing with that to create an end-to-end story right so realize that the the per stream part the per stream filtering and Kirk Ewing that that can be operating that\u0027s not limited to the eight cues that that\u0027s looking at the stream characteristics to figure out what stream the flow is in so so yeah the final getting onto the network is is in the priority queues but there\u0027s there\u0027s also the this the filtering and killing and a stream is loosely defined as five tuple that\u0027s wrong way you can we have a standard 802 that one CB that allows you to identify the flow right now it\u0027s by MAC addresses VLANs and IP five tuples okay okay right now and a casual reader "
  },
  {
    "startTime": "00:51:43",
    "text": "of the standard will see that it\u0027s really easy to add a you know if you can do those flow it\u0027s obviously how to it\u0027s obvious how to do the others and stay even stay within the standard yes so I think that answers okay thank you a good this was very educational thank you one of the things yes this is packet based it\u0027s not a bitwise CDR okay so yes there will always be stagger on the output because we\u0027re outputting packets okay and in a slide that you can find some references to further reading and as I mentioned as I mentioned we have a brief demo of one of the feature yes one more question yes definitely this is very related to that nut okay so maybe so so new and I coach Odette net and the purpose of debt net is is obviously there\u0027s a lot of networks that are purely or two networks and we still want to provide these kinds of capabilities over layer three networks as well as overlay or two networks and so the purpose of debt net is to is to provide the capabilities to handle this kind of traffic and and set up the the reservations and the flows and such so that this we get these behaviors over layer three networks and not just over layer two so extending this up to the IETF domain and we meet Thursday yes we\u0027re meeting Thursday morning this week yeah thank you for and I think they tend what is coming is actually that net as well so as I already mentioned in the data plane that was one of the hot topics on how to provide this film or packet replication and elimination for the reliability in linear to data plane so what we have here is a is a proof-of-concept demo both showing the layer 2 and layer 3 data pane and the packet or or frame replication and elimination feature for reliability so we focus on on the reliability aspect in this case and at layer 2 it is frame replication and "
  },
  {
    "startTime": "00:54:44",
    "text": "elimination for reliability so the 802 dot 1 CB specification provides you the mechanism actually includes a pseudo code and provided the layer 2 data plane details and in the.net document called the data data plane solution linked linked here that provides the layer 3 data plane aspect and in that document it is called the packet replication and elimination faction so that\u0027s why the two abbreviations Bota as explained below as it was explained during the tutorial both of them based on having disjoint paths and and sending the relative identical copies with a sequence number to the elimination point which which gets rid of the surplus so what you can see here is a remote control of a balancing robot so there is a lagoon robot down there and the control logic of the balancing logic has been moved away actually to that laptop computer we have implemented the replication elimination function on PCs actually they are in the two tower pcs these are the these are the pink ones here in the figure and as a reference comparing the replication and elimination we compare it with 50 millisecond switching so as I mentioned this is the setup that the two towers are the replication and elimination functions and the this is a ring topology and the two pieces in the middle that are the simple switches when we take a look on the layer 2 data plane the the controller sends VLAN tag the traffic to the edge node and this CB mechanism adds so called our tag which includes the sequence number for the replication and elimination at layer 3 if the layer 2 network is used to provide a TSN service so we have the same and host then at the Uni we have the same format in case of an MPLS data plane we have the the mpls transport level and then which was we need a pseudo higher labor and in case of the feature the controvert is mandatory so we will have a death net controvert which includes the sequence number and all the rest become payload with respect to death net this is just a "
  },
  {
    "startTime": "00:57:48",
    "text": "virus charka capture on one on the wire and so let\u0027s see the demo part so the first scenario is when we have a link failure and if we use the protection switching mechanism with the work working working tunnel protection tunnel scheme then we can get 50 million SEK failover there are many many mechanisms for that and and the application will be impacted when we turn on the application and illumination that then actually the packet loss is eliminated so let\u0027s see so the first is when the protection switching it\u0027s on the replication illumination is not yes you can come yes [Music] so what happens is you have to start the robot in a vertical position and then the balancing learns and and so the balancing runs on the on the laptop so it is a software link failure for now at the end we will see the physical link unplugging it\u0027s easier for the demo for now to have a software link at as illustrated in the figure and this was the 15 minutes I feel over time so that that amount of I guess it was three four packets that have been lost typically it is that was too much for the balancing logic and now we do it with the replication elimination feature so you despite of the link failure nothing survives one of the flows that was something else that was the demo doing one let\u0027s see it again yes yes it\u0027s you see it\u0027s really so it survives we have one of the flows in the redundant path so we have actually with the replication it\u0027s on "
  },
  {
    "startTime": "01:00:49",
    "text": "both flows actually this is very sensitive this guy so if you touch it when it tries to burn out that\u0027s over or maybe it may be unusual to the ATF but it\u0027s a very classical metric for the applications in industrial control oops the number of loss in a row is very critical like usual - they would always let you lose a packet but if you lose three four packets in a row they will stop the production line if they have to stop the production line like twice in a year on your hardware is on the curve yes so let\u0027s see the other failure scenario which we could link flapping when thing goes up goes down and if it is as illustrated in the figure twenty milliseconds then the monitoring Mahon ISM you use for the classic protection switching doesn\u0027t even detect but that\u0027s just passed Pascale explained that can be critical for for a control loop it\u0027s very sensitive so even not that easy to stop started so this is the link flapping without the replication illumination so that\u0027s like not any better than the simpler failure was so let\u0027s see it with the frame application and elimination so multiple failures or multiple link downs as illustrated in the figure "
  },
  {
    "startTime": "01:03:54",
    "text": "yeah well whenever the demo is done yeah yeah I should be 10 minutes on this level doing and of course it works with physical I will unplug the link so thank you very much for your attention if you are interested in the wireshark capture please come to the screen thank you [Music] [Applause] "
  }
]