[
  {
    "startTime": "00:00:08",
    "text": "So I I'm in the meeting. One of the slides. To sustain. Ready? Okay. Good morning. Welcome everybody to prague in this wonderful meeting room, This is, Dinaji, decentralizing the internet. Infrastructure research group of the IRTF And, this is. I'm the and as you made it to the meeting, quick user notes. So we are following the IETF IPR policy. So in short, this means, if you, talk about"
  },
  {
    "startTime": "00:02:00",
    "text": "any IPR related material or you happen to hear anything you are supposed to inform us and soon. Alright. I need to This is, it's a bit low. So And, please take note of the, code of conduct that we are operating, on the there's an onboard team that is happy to, help you with any, issues or or concerns and also consider, the, code of conduct RBCs and the anti Residence procedures. And, quick reminder, so we are here in the Internet research task for us. So we are not setting any standards This is purely about research. Eventually work off, these research groups, may inform future standards, but, so don't expect us to to produce any of those. And, yeah, so before we start the meeting, we, absolutely need a notetaker. And, be happy to, well, thank you very much. Great. So if you Yeah. That's right. So it's linked from the, IT of meeting agenda in so if people want to, help write, So adding from some things, Well, he's taking notes online. Please do that. That's typically, quite useful because sometimes when these sessions are the tactic, it's difficult to catch everything. Okay. So, we have a pretty full agenda today, quite exciting. So really looking forward to those talks. Let us know if there's anything else that, you want to talk about"
  },
  {
    "startTime": "00:04:03",
    "text": "So we left some time in the end discussion and, So, that's that's, let's discuss a bit about future ideas for for future work or next steps then. So this is, an update. So we we last time we talked a little bit, so what we want to produce in this group. So, you know, the different IITF groups you know, have different approaches some groups who are working more on, say, experimental protocols, they produce experimental, our seats eventually since we have a fairly broad, charter in the in in this group. So we are typically inviting people from different communities, inviting talks on research papers and so on we felt that it's also appropriate for us to, produce many internet drafts that would, eventually to maybe information allow a cease and so on. So that's why, we discussed a little bit, with Colin, I am IITF chair and, So we think that, typically, the work that we were discussing here would be published, by, say other research channels. So academic conferences, journals, and so on. And, of course, there is also some other work in the, say, larger ATF IETF community. So some RSCs, people are working on some topics related to centralization. That's of course also something we want to discuss here So That means we don't necessarily plan to publish overseas, anytime soon. So may want to reconsider that later if we say"
  },
  {
    "startTime": "00:06:04",
    "text": "have some, say specific work items, that could lead to such material but but but for now, we think it's most appropriate, to operate, in this direction. And facilitate this a little bit. So we started, populating, the the Dinachi Wiki that's linked here. And So we we and open that, that up for everyone. So if you, want to add some material that you find, interesting, noteworthy, worthwhile discussing, please add to this, if we want to also at say, new headings or change the structure there a little bit. And that's also, the fine. So we do this responsibly and, yeah, please share your, material that you get you come across and then, we can then discuss how to, take this on later, perhaps, on discuss this at future meetings. Are there any questions related to this I would just either bid to what the Turkish side. So currently, we started things into a few for example, marks, draft. That's, I think, is under active documents because it may also further revisions because it has gone through what you're saying now. And for, published the papers. That's, you know, it's a published. And therefore, it's a fan of origin therefore it's under a different category. The, like, a third set, the Wiki is open to everyone to edit please feel free. To, create a new categories and put, the relevant documentation onto the Wiki at the research group which depends on everyone's contribution. To collect all the relevant material. Related to centralization, otherwise, the decentralization. To contact that research."
  },
  {
    "startTime": "00:08:01",
    "text": "Okay. So he's just how it looks like, but, I guess you can click on a link yourself. And so in terms of topics, so this meeting is largely focused on these two topics of, like, measurements of centralization, and then we had discussing few approaches, that might be helpful towards decentralization in in the, say broader sense, And, so we we we are currently discussing with some, like, people from, say regulation, and economics communities, for the next meeting. So that would be probably brisk bearing. Australia. So where we would talk more about, yeah, economics and also policies that are currently being prepared or put in place in in, different countries. Of course, you input, to that is highly welcome. So, if on if you have additional ideas Please don't hesitate to contact us. And, so with that, we be ready to start the technical program, and I would like to ask 2 years to Shannon's work. A bit campus nights. Yep. Okay. So welcome, everybody. Settlino Tymer, So, I'll be briefly talking about the case of Ace 59645. Which is basically making a ping and doing stupid things, things, things, first, a little disclaimer."
  },
  {
    "startTime": "00:10:00",
    "text": "I know that this is the MPI informatics Slack template. But I'm mostly speaking for myself and not on behalf of my employer. And, ace 59645 is a project of Mine, and, like, you know, not endorsed by NPI informatics. For in any form affiliated besides me working for the MDI And I'm reporting personal experiences here for running an AS I, of course, reflect on them through the lens of, measurement researcher. But it's like, you know, the this is not endorsed. By the MDI. So what is it? Well, 1st of all, it is an autonomous system, like We know them. I mean, AT and T has 1. Dutch Telecom has 1, and apparently, I also have 1. Consisting of, systems servers put into several locations. This is the pop and dusseldorf, So pop point of presence where I have a couple of virtualization servers and a little bit of routers, And, topologically speaking, it's more like this. Don't know if this works. Of course, it doesn't. So in the center, you see, location Amsterdam than to the Left from your perspective. There's a lot of with a couple of upstreams and downstreams. And on the right, we have Berlin. In total, it is, currently originating 5 ipv4 prefixes and 10 RPV6 prefixes, which accumulates to a total of 7 slash 24s in ipv4, and roughly a 100000048 in ipv6. Because was There are 2 slash penny lines in there. There are a total of 6 upstreams over several locations, and 9 downstreams. So it's it's actually providing transit as well. And the theoretical maximum bandwidth of 34 gigabits outbound. This accumulates, my upscreens. But also, to real IXPs. So D6 in, Tusunorf and, interacts"
  },
  {
    "startTime": "00:12:02",
    "text": "an Amsterdam, and 3 more, like, you know, virtual toaster IXpace and the whole thing has been going since. 31st January 2022. And I I kind of acknowledge, I and somewhat privileged to have some IP V Four rounds. The reason for that I applied for it in 2009 when there was still enough Yeah. But as with all stupid things I tend to do, the standard question is like wife, So first of all, everyone needs a hobby and I'm kind of operate at heart, my my you know, show a little bit. And my day job as a scientist is you know, kind of not so operator it's more like writing papers, writing grants, handling funds, supervising students, teaching and not playing with Robert's there's also this thing about learning because, you you don't learn how to run things on the internet's core. APU down front things on the internet's core. And, happens to be somewhat difficult to run things at the internet score. If you're not working for somebody running things at the Internet score. Kind of kind of kind of then there's, of course, independents. It's my network, my rules, my rules, There's no funny spammers sitting in the same IP block because if there are funny spammers in the same IP block, those funny spammers are wrong, quite quickly. And if I move somewhere, if I get a wreck in another location, I just my IP with me. And of course, this thing I always kind of like is giving back to the wider community. That's you know, if you have these resources, it's surprisingly easy to have things that have impacted positive impact So Learning. Running Networks, I said, it was always a thing for me. I always liked that. And If we're all honest, the internet, used to be a network of networks with like this nice end to end principle. And if you want to ruin your mute,"
  },
  {
    "startTime": "00:14:01",
    "text": "moot, you usually have to listen to Jeff Houston, who does measurements on how this no longer the case, and how it basically is like a unidirectional content delivery platform for Hypergys. Which, I mean, I can't you know, disagree with us. I don't think that this is the internet we should be living Also, if you Get boxes from, for example, Amazon, you have to deal with these funny net for 4 things. I, I, I heard, like, I never used Azure. But I heard they're doing that 66. No clue why, but apparently they do. And, I mean, it's it's it's like, you know, not best practice infrastructure again if it's with few rules, you can actually things like they are supposed to do. And Sometimes you just want to, you know, quickly spin up something and with rising ipv4prices, it's also a lot easier if you just know, run your own infrastructure if you need, like, a 28 for something. You just then independence, and this is a very important point. I tend to have, like, very strong opinions about whole centralization, quantification stuff, And then there's people going to Amazon, and I'm like, But Why don't you just, you know, Do it yourself. And They are like, well, you it's easy for you to say it because you don't have to build it. Like, doing things to yourself is really, really hard. So running an AS and actually doing things myself, it it kind of puts some butter to my well well well well research work, software well not being that difficult if you want to. Also, as a researcher, again, this is like this. Doing independent research thing. It it it helped me to do some of my research because for some things, you might need control about reverse Ian has, you might need to specific prefixes, prefix sizes, the ability to announce them somewhere, And if you have to get that through your So your, IT department can have really interesting lengthy effects."
  },
  {
    "startTime": "00:16:00",
    "text": "Again, renting is expensive. Goes into into independence. I think at the moment, ipv4 prefix goes for around 1000 again, if you, if you, if buy it. If you want to rent it, it's usually between $1 $200 a month that gets pricing quick And the last thing, it is surprisingly difficult together network with working reverse DNS via and not for ipv6, that is also DNS Exxon. So, great feature, it's your network, Hue handles of delegations, you make sure there's the SDS records things work. Yeah. Then the giving back thing. And I like I I I like to room people's mood in the morning. I don't know if you heard about my certain propositions on an internet for a learning world. Base summary is, like, shits on fire. Yep. And what I really fear is that we are living in a world that is burning to the ground while it's being burned by us. And we have this progressing centralization, which leads a capability loss to people being less and less able to not only maintain the infrastructure we rely on, but also rebuild it and keep maintaining it when this horse infrastructure around it Doctor. Happ. It's just gone. Like, we are losing end to end understanding of what makes the world and that is That's If the world keeps working, That's disastrous as the world burns down. So by giving back by enabling people to also lose things at the Internet score to get there on ASN to see how you announce stuff. I'm kind of investing in people knowing how to how to how to run stuff. And I hope that at some point, that will help us. And being an LAR, So I'm, like, a local internet registry, which is a really nice feature and we'll we'll briefly touch on that."
  },
  {
    "startTime": "00:18:02",
    "text": "A slide later. I I can also ponder things for people who want to learn. For example, students, I can also run nice infrastructure, which helps all of us to do network measurements. Like, there's, funny sensors of diverse sort of projects. There's Right? Atlas Anchor, which, again, also gives me the benefit getting a lot of ripe Atlas credits, which then can be used for further research. There's an airline an ARC ring note I can provide a this measurement of network. I mean, I had talk about that at, NRW at, 117. My funny, back to the future, who is email security scans, or only resolve, I think, is ASC Ingalls' team, which is a system to, organized shifts of volunteers during a large event. Kind of known from the CCC events. At the end of the year. And, something that happened during the hackathon, there's also now, like, IPV 6 only, like only net be, 64 for outbound, public resolver. To a06D1C7 Sorry again. Then there's, of course, the thing about doing stupid things. So I have this nice block doing dash stupid dash things dotas59645. I I can also if I run my own network, I can also do things that are somewhat stupid. Stand by the way. Of somewhere Yes, if you So, it's been about 5 minutes would be great. Okay. That would have been good to know. So, Sometimes you want to do stupid things like the main front, commonly sends up news websites, you want to maybe do some, like, funny censorship, evasion, you want to tunnel, real IP address including legacy to your home. You might want to run tour stuff. And you might want to try out new things, which are for big networks and Aspa, for example, and I haven't played with Aspa yet. But but but but that are not yet stable enough to be tried out by those big networks because honestly, It's kind of sad for me if AS59645 goes offline,"
  },
  {
    "startTime": "00:20:02",
    "text": "but nobody really dies. It's a little bit different for DTAC. I can also use it to debug stupid things. So was a funding where an NGO had an issue where they felt a bit rate limited by a very set, AS So this 3 20, Deutsche Telekom, which could be easily traced to an MTU issue, And there's a lot more debugging you can do when you can actually control in and outbound to the DFS because basically, pickup out didn't happen, and then you can get those pickups on well. Where it hasn't been inflicted on this was ultimately helping slaying a sixteen year old QA Mo but in the rtl8139. Here still knows the rtl8139. Okay. I I I see Jeff shaking his head that is probably the shaking effect. Like, I worked really hard on not remembering that, Nick. So it's very infamous for bad driver support and, as I said, there's a fund block article about this. So if you want to get those resources in the right region natural persons like me can hold resources and become members So it's INA IR. So in this case, Ripe and then LIR, which, it's me. And in the region, there's different types of resources. So you have provider independent resources, which are direct assignments from the right NCC to end users. Initial persons or companies. You have the which are allocations to LIRs to members of the RIP NCC Paul, right, from which they can make assignments to end users. And AS numbers are always kind of direct assignments, and they come in 16 and 32 bits. If you become an IR, it's relatively simple. You apply with a ripen see. Send a couple of documents, And you're reoccurring, 1550. And then you can request your initial allocation of a 32 to 39, 29. And as in And you can queue on the activity or waiting list, maybe someone eventually If you just want to get PI,"
  },
  {
    "startTime": "00:22:00",
    "text": "you find the sponsoring layer, like me. Like me. Like me. Like me. Like me. Like me. Like me. Like me. You sign in Lea agreement with them. You ask them to apply 4 48 and then an exam. The currently ASNs are free, but the right has some opinions about that. The leer will then usually charge you a bit more. Because they also have to, you know, You have to somehow get, like, before at least, Slash 24 to be able to also announce it You can buy it. It's 15,000. You can rent it. It's round about 100 plus. And then there's also little things you could do, but don't even think about it. You have to get connected. So, There's a proper way, which means you get colocation in a proper facility, and that's usually at least 200 bucks a month. You put at least one router, switch in some servers into that rack, You get cross connects to IXs and potential Upstreams, and those are usually like 4200 to hop Then you get art. PKI and IRR data in order for your prefixes. You agree on pricing with the and Upstreams. Set up your sessions, get announced, make it pay course, in the spirit of stupid things, there's also the yellow color white so you get any device that can route run a routing demon, thank Raspberry Pi. You get one of those free tunnel upstreams from any of the fun upstream providers like free transit dot CH. HE stopped doing this, but there's a couple more. You get RPK and IRR data and auto, set up sessions, get analysis and make a thing. There's a teeny we need little little little little point about this not actually being a good thing. Because, in the end, if you're doing this, you are playing with sharp knives. Right? And you can cook with Shopnights, but you can also, like, really cut yourself And there are basically 2 types of BGP operators. There are those cost an international incident and those who are Well, it's"
  },
  {
    "startTime": "00:24:02",
    "text": "the same with OSPF. OSP is o OSPF. You can break a network in ways you don't understand. With BGP, you can break everyone's network and waste in a long does, does, does, tastes more than just not Costs, costs, It's not like a raspberry pie. You can pack away. Leave in a box and just, you know, get it out when you're on holidays again, it's more like a pet a responsibility. You have to keep maintaining it. You have to keep being there for it. And funny, your local set ups, just for the sake of it, don't really make the world a better place. There's routing table growth, there's, the issue of getting the NTEU on the internet below 1500, which shouldn't be the case. And there's like a myriad of lists which really make operators who do this for a job, not really like personal ASMs. This, if you want to do a little bit more, Yolo, and Lescolo, DM42, It's a little bit like the internet. It's a fun small scale internet with all shortcomings of the real one. They have RPV 4 exhaustion and 2 large LA integrations. They are running out of 16 bit ASMs, They have a slow introduction of ipv6 despite a pressing need that interconsistent databases and the slow adoption of our guy. So it's essentially like the real thing. But you make a few orders of magnitude people, less angry at you when you do something stupid Contusion, Is it fun? Definitely. Should you do it? Maybe, maybe not. I'm kind of a doctor well, of engineering, not, crop. So 0, you? Is it useful, well, in a burning world moving towards full scale societal collapse while losing, people with a full end to end understanding of most important technology stacks due to the increasing complexity of the production of multiple layers of transaction, driven by the practical needs of large businesses, cloud Companies And Market Encumbents, further reinforcing the role of people as users and consumers of content provided by centralizing internet Well, maybe it is kind of useful So getting to the end of this do something good for yourself, start each day with an apple,"
  },
  {
    "startTime": "00:26:02",
    "text": "make an evening jogger thing, stop decentralizing the internet 1 AS at a time. Thank you. Thank you for Okay. Good start from Monday morning. Are there any questions? By the way, make sure you scan the QR code so we get a bigger room next time. Hi, Tobias. This is Karen Katz, and nice to see you present in, in person from University. I'm gonna ask you a question that I've asked you at a different times that you've given some presentations about your work. And, That is this question of you look at the cloudification, and you present, like, one way forward by having this technical by saying that we should all run our own ESs. Which is, I think, an interesting part towards it, but does it address sort of the the business and the economic power that is there. Okay, first things first, This is all a social problem, and there's no technical solution for a social problem. So I'm also not proposing a technical intervention. It is not about everyone having to run their own AS. It is about having enough capabilities within society that there are enough people to run small scale assess to care of their community if we completely lose those Hypergiant. Because as it is with stars, hypogines, at some point, will go bust. So We lost, how was it called MySpace? For, like, the older people in the room. It's, like, Facebook only older and with funny sounds. And that is the thing. We have to preserve, sufficient capability to build and rebuild infrastructure in case of a loss of the supporting infrastructures we are currently relying on, which are mostly controlled by these you. That's a helpful clarification"
  },
  {
    "startTime": "00:28:03",
    "text": "Okay. Thank you. I think nobody else. So friends to cure scan the QR code too, Yeah. Again, to this. And So the next talk is gonna be, remote from, Michal Cole, and let me bring up his slides first. And I hope you control Can you guys hear me okay? Yep. Sounds good. Perfect. How do I control the slides? Oh, okay. So in the in the new new midaka tool, you are moving up and So I'm looking good for sure. So she'll work now. We'll tell you off the floor. It does. Yes. Thank you. Right, self present our paper, which is on investigating IPFS, which is interval system. This is the joint workflow for, and Magic Sorry. My boss is, leading me. Leo Magic, owner, and admin, church and and Okay. Arrow Keith works as well. So I'm gonna quickly talk about, IPFS in general because the kind of the measurements are important. For the measurements, it's important to understand how it works. This is currently the largest peer to peer data storage in the trigger network. It's used for traditional hosting kind of these blocks in reference data adopt and so on. So this is kind of"
  },
  {
    "startTime": "00:30:01",
    "text": "the storage technology for the web 3, but but not people host their their websites and stuff like this on on IPFS now. So it's kind of like Torrance, but at the same time, with torrents, usually you were to set up And a download of your movie and then you wait series for Bobby Moore that he will ask a website and and wait for it actively to capture. It has a pretty growing use we've seen tons of downloads, requests per day. There's certainly many more and the network is around 30,000 nodes, at any given time. Now I'm gonna briefly explain how it how it works IPFS is this kind of traditional peer to peer system. So anyone can host some data. Now if you want this date that you have to send the request, that they defels back and and then you have it. And the good thing is that once you have it, you, by default, also start it. So it gives you the really nice auto scaling, feature where if a file becomes popular, more people downloaded, the more people as a house The problem is that the traditional, naming scheme as we see the internet such as youtube.com/video.mp4. We had this location part. And this makes content migration very difficult because now if you want to move your video from you to to Vimeo. Well, then the whole name changes. This would be obviously very bad for IPF as because we want to migrate content figures all the time. So that's why IPFS using, location independent naming, basically, if you're on the name of the file, you just has it using cryptographic hash function and the digest of this file becomes the identifier of of the context. This is good for this application because if there are 2, the same files, they will have the same name. And, also, once you download the file, you can right away, hash it and then then check the integrity of the file. Now the problem of this is that if you receive the identifier of the file you want to download it, now you don't know where to download it from. Because, well,"
  },
  {
    "startTime": "00:32:01",
    "text": "the the name doesn't have the location. So to To work correctly, IPFS needs some kind of name resolution, which will map the content identifier as well as the providers from whom you can download the file. And IPFS is 2 protocols to for doing that. One is bitswap, which is a very simple protocol just connect to random peers, in the prerecord network And when you want to download the file, just ask them where they they they do have file. If they do, they will provide it to you if they bond well, they will demo. This is pretty fast because, well, all the requests are to be sent in parallel. It's a bit expensive. Because if you want to have a decent chance of getting the file, you need to send probably a lot of requests. And you're not guaranteed to find a file. Because you're contacting on the facet of the peers so it usually works only for popular files and it doesn't for for less popular ones. That's why IPFS also had a second protocol based on the distributed hash tables. This is built using Cadenia. And basically every single server in the in the network is responsible for storing some mappings between between the content identifiers and the list of providers if you want to advertise yourself as a provider, you just do a tht put operation if you want to get a resolve file I just do a disease to get. Now it's slightly slower with the with the bit slow because now you're the routing that you have to go through, however, it's still efficient with the the network size and you're guaranteed to find the So that's the core protocol on top of that, IP has built some some additional things to make life or users easier. 1st of all, we see quite a lot of, realized, sorry, a lot of peers be behind not This is obviously problematic when you're hosting a content because people cannot connect to you. And that's why IPFS allows, realize. So, basically, when you advertise yourself, You can say, hey. This is my ID, but you want to talk to me. Please talk to this relay. And then really we'll be able to, well, relay the the traffic. It's nice because it allows"
  },
  {
    "startTime": "00:34:01",
    "text": "not appears to communicate with with the network. And on top of that, I keep vessels implement, HTTP gateway. So by default every single node in the network, can receive exposes an API where it can send HTTP get request specifying the content identifier and the note is supposed to fax it from, from the FS network and give it back to you using HTTP as well. And this is a nice feature because now we can use any any major browser. It just type, type, URL of a of a browser, of a gateway slash put the colon identifier, it's kind of, fly to you using HTTP. You don't need to install anything and it plug ins nothing. Just works using pure HTTP. Now the brave browsers also has set up this support, where you can just, like, put, the content identifiers in the browser and the private browser will understand that this is IPFS address and we'll fetch it for you automatically Right. So with the measurements methodology, we are looking two aspects we wanted to see. Well, this is kind of the main decentralized so we wanted to see how decentralized is it. We're looking, 1st of all, at the traffic distribution, we need to see whether have some high heaters or maybe the traffic is distributed equally And also, we wanted to look at the reliance on the cloud infrastructure. This is important, especially for all this tech web 3 and blockchain based ecosystem because they usually do not want to rely on the on the cloud. And for that, we built quite a lot of software. We wanted to have kind of a holistic view at the network. So first with a network crawler. Then we have a DHT and bits of traffic differs. Which allows us to to get all the, content advertisements and and download requests. We have a gateway detector it's because the the HTTP gateway sometimes have different IP on the HTTP side and different of the IPFS side. So this is the tool just to correlate them. And then finally, we we did some also DNS,"
  },
  {
    "startTime": "00:36:01",
    "text": "measurements because now we can also use DNS to reference, IPF as content. An ENS crawler, ENS is Ethereum name system. This is this kind of blocked in equivalent of of the DNS if you wish. Right. So for the results at the beginning, we looked at the network itself. We were, and he found out that 85% of the APs are actually hosted in the in the cloud. This is a pretty scary result and also it was a bit surprising because 1 year before, there was a compaper, presented, and it found out that there's only 3% of the notes in the cloud. So going from 3% more than 85% in a year was it was a bit surprising. And, actually, it was a difference in the methodology. So for us, we were taking a snapshot of the network. We're taking all the IP addresses and we're classifying them cloud or non cloud, and then we're aggregating those results across multiple scans. For the sekum paper, they were scanning, continues to the network. They're gathering all the IP addresses And at the end, they did the classification between all the IPs. They they see So when we took our dataset, but applied their methodology, we need to share seen a very different result because now the non club ratio jumped to to around 60%. This is because the the non cloud nodes actually rotate their IPs very frequently. So this way, if you aggregate your results across multiple scans, the same note will be represented by multiple IPs and thus will be counted multiple times as well. So here we have a graph showing on on on top of that, we have the blue line, which is our method OT more scan you aggregate it doesn't change much because the network is pretty stable. However, with the methodology from the from the sick on paper, the more scans you aggregate. The more and overcounting of the non top notes you have, and that's Well, eventually, it'll get down, indeed to the next 3%."
  },
  {
    "startTime": "00:38:02",
    "text": "And then we looked also at the network, at the cloud providers, for the cloud And, again, we see that the the first 3 operators, and this is 2 power, vulture, and then they control basically more than 50% of the but the network which is which is again a bit scary. At the same time, you can see that for the compared to a different methodology, the the picture is is is quite different. So just to highlight that little differences in kind of counting or evaluating how something m, how decentralized something is. Can paint a very different picture. we look at the topic. So we were using a DHTN, but stop, sniffer Then and those are simplified parental charts. So on the x axis here, you have the percentage of the IPs And on the y axis, you have the the percentage of the the traffic, that those SPS are responsible for. So, basically, this data point means that there's like 5% of the IP addresses are responsible for more or less 95% of the traffic. So so so the traffic is is highly centralized in the DHT the best stop is slightly better, but but it's still a pretty centralized At the same time, we also looked at what percentage of this traffic comes from the cloud notes and, the DHT, we see around 90%. Is coming from the cloud and also all the heavy hitters here, on the left hand side they come from the top. From this swap, it's slightly better because it's only 47%. And then, yeah, I'll tell you why in in the next slide. So now for the old cloud based notes, we did reverse DNS and just to make just to try to guess what what they are doing for most of them, we don't know because we just see that it points to some Amazon AWS so we see that even though Amazon AWS was not there are not a lot of notes, in our crawls at the beginning. Actually very active in the network"
  },
  {
    "startTime": "00:40:01",
    "text": "for the advertisement to see, we have seen a lot of kind of this web free storage, NFT storage, and so on. So those are kind of block to an ecosystem related services that that helps a lot of content such as NFTs they have to periodically, advertise them. That's why we see a lot of traffic coming from them and we don't see that for the download traffic. And that's why also in the previous slides, you can you could see that for the bits up, we have less we have less, cloud traffic in the in the best of traffic because bits of it is download only. So basically, the the advertisement or the hosting of the content is much more centralized in terms of cloud reliance, than the download traffic. And then we also looked at the content providers. So so notes hosting content, and we found out that 30 percent, are non twelve notes, but behind the knots. 46% are are the cloud notes and 18% are non cloud, not with the public IP address, Then if we zoom to the NASA, peers, I told you before that they they rely on some on some relays to be able to communicate with the network and actually 90% of them used, a Relay Ultimate Cloud. Even though they're not in their cloud, rely on the cloud to communicate with the rest of the of the And then for the content itself, we found that only 5% of the of the content that's hosted uniquely by not 12 notes. Vast maturities halted by a mix of cloud and non cloud notes and finally get around 25% in, hosted uniquely by by the club. And then finally for the gateways, as I told you before, there are, like, different IPs on the decide and and different IPs on the the FFS site. And, but in both cases, you can see that the cloud for it dominates the market they they they had dealt rate that the largest, the largest, gateway infrastructure the it's actually not surprising because cloudflow is quite heavily in IPF as they"
  },
  {
    "startTime": "00:42:02",
    "text": "they they they added quite a lot of support and they kind of integrated with their software. But then again, you can see that the non cloud platforms, and not non cloud gateways are almost non system. There's a very small portion of, of this kind of gateways. This might be Kind of understandable because it requires handling quite a lot of traffic if you run the public gateway so maybe that's why people decide to, that it's too difficult to run-in a it helped. And then finally, we look at the internal name system. So we again, a term name system, a term name system is this kind of DNS puts it on the blockchain. It represents sometimes some some IPFS data. We looked at this data and we wanted to tech. Who host this data. And, again, even though it's kind of, you know, referenced by the Blockchain see that the majority of that is hosted by the clouds. Only 18% of the content is sausage on non cloud notes and the rest it's either kind of, regular, cloud providers such as Amazon, but also digital ocean which are kind of more specialized in this, in hosting this kind of blockchain based data. Right. So for the summary, we found that almost 80% of the DG servers hosted in the cloud. And the top 3 cloud providers host the majority of the network. The traffic is highly centralized, because the top 5% of the notes are responsible for up to 95% of pipes nearly in 95% of the content is provided by at least 1 cloud based notes, However, 25% is provided only by, by cloud notes. And for the entry points, such as public, HTTP gateway, they're also dominated by a few a large players such as a cloud cover. So just a few takeaways. If you look at the design of the IPFS, it's it looks like a robust base for the centralized labs. It's pretty standard in terms. It looks kind of like a mix between the DHTs and the big print. However, it seems kind of"
  },
  {
    "startTime": "00:44:00",
    "text": "it's very difficult to to achieve decentralization performance and security at the same time. Have another paper coming. We'll publish it on the NDSS next year. Where we we show an attack where it can basically delete any file from my PFS using a single laptop, because there was a vulnerability in the in the DHT. And since then, we we passed it. However, the the problem is that, well, now you get a small performance hit, at it's usually the the pay that you have to pay in the decentralized system. And especially the performance is difficult as the convolution layer, because now that the HD query takes around 1 to 2 few seconds, which is pretty slow. Protocol apps try to build, kind of hydroboosters, which are, kind of DHT Super notes, that didn't work out that well because the the improvement was really good. And now they're actually introducing centralized indexes. And now the the resolution time went down from seconds to milliseconds, but now we have all those problems with with privacy, frustration, and censorship, and and so on. So, obviously, maybe censorship is less of a problem because you still have that DHT has a fallback, but the privacy concerns are are there. And that is still a problem. There are quite a lot of notes behind behind that. And there are some protocols to well, to be able to connect to them, but they're still a bit slow. In general, there are, like, no tab notes that come and go. So the vision where you kind of host your website on your laptop is not very realistic in this sense because if you shut down your laptop, your red light also disappears. We see a lot of this kind of content detector. This is hosted only by a single note, and it's just, like, comes comes up and down, periodically. And I think what is commendable is there is a huge progress tikality and the ease of use. And I think it's amazing that now you can, you can kind of use a browser and then just, like, type the click on the link and and content is going from"
  },
  {
    "startTime": "00:46:00",
    "text": "at the centralized network and you don't even know that but definitely there's, like, more work needed in in this direction. And I think this is it from me. Thank you. Okay. Thank you So while we are asking questions, please make sure you use the online tool for for queuing because otherwise it's have to fill notetakers to to get your names Oh, okay. Wait. Go ahead, didn't we? You so much. Great talk. Just two comments. So we wrote the second paper you were referring to. And, so I think IPFS is kind of a moving target that sort I wanted to say. And you mentioned it's like So the payables published 1 year before, but actually the data is from even 1 year earlier. So it's two years old. And since then, the network has tripled in size, so I'm I'm not surprised that the cloud reliance has increase over time. Nevertheless, I think your methodology in counting and aggregation of numbers is more sound. So this is something I wanted to say. But I want to ask, did you form. Include relay addresses into your cloud notes as well. Because they could artificially inflate the numbers of cloud reliance. This one, one question, and the second one would be have you used your aggregation for our data datasets as well? Just to see how the numbers would compare with your aggregation. Because we've published our datasets. Right. So starting with the second one, no, we we haven't we haven't checked your data set with with our methodology. We did the the the the reversing, but, yeah, this will be actually very interesting to, to see if we have all the script it's probably adapting the data. So, yeah, I'd be happy to actually have a look at that. And then for the realize, I'm trying to think. So you for the for the DHT data, right, or Yeah. So those are announcing relay addresses, which are most likely also like like like,"
  },
  {
    "startTime": "00:48:04",
    "text": "these relays are hosted on cloud notes, but still they are behind nets or non cloud notes, basically. Right. So I think for the traffic, we use the the actually sizing notes. So those will be non relay notes, if I understand, if I if I correctly because in the I think in the hyperposter, logs, yeah, we we've seen Yeah. Like, they will be advertised. We'll see the relay, but, but the message will be actually coming from, from the note that is advertising So we should count the notes and not the relays in this case. And one last thing, you meant you said the hydras didn't have, like, such a huge performance impact. I think this is true for the beginning of the year or end of last year, but when they were introduced, far as I say no, Actually, I I think they had, like, quite a significant performance improvement because back then. Notes were behind nets and still participating in the DHT. But, yeah, just a comment. I I think this is yeah. Okay. I mean, I mean, just to be fair, I I don't know, I actually it's difficult for me to measure the the performance boost of the hydro boosters. I just know that there were some this continues. So maybe it was more about the ratio cost to the to the performance plus debt. Was, I said, but probably, you know, better, actually. Okay. Thank you. Okay. Next in line would be Jeff. Look, I I can't help but muse about this. And think about what you're trying to do and why I think your direction is is kind of anti gravity. The problem about all of this is that We actually engineer our networks in their infrastructure based on the majority use profile. Our networks are asymmetric her Raisin, And the reason is that most end clients suck like crazy and push nothing. So almost from the word go, download speeds,"
  },
  {
    "startTime": "00:50:00",
    "text": "were a whole lot higher than upload speeds because this isn't telephony. And so we build networks that fan stuff out. And that's what we're good at and what we're cheap at. Once you try and try and create content at the edge, and suck it back in. You really are pushing hard against where the engineering and the capital investment actually went. And this is kind of the first problem here that these kinds of models the cloud was an obvious answer because it's in the middle, not at the edge. And if you try and create in software edge based systems, you're pushing against where the engineering is. The second part is that We coped with address exhaustion and address scarcity for this by the same reason, clients don't have addresses. And they really don't. They're just clients. It does matter what address they have. Because you're only sucking and never pushing, I never have I'm reliant on your relays. Now there's a real problem with reliance on relays. And a lot of it comes right here in the IETF. Because when we talked about NAT based solutions. There was almost a religious further in an idea that we were never ever going to stand that I was nat behavior because they were satanic inventions in the airport didn't deserve standardization. The result was that every single engineer assigned to build a net had no standard to work from they're all different. Every last one. Even the same model, is different from each other in different years of manufacture. Nets are a disaster, and Nat Transition is a disaster. And so when you're talking about a direct, a decentralized system that relies on edges, this where clients don't have addresses. And and and Tabias if he's still here."
  },
  {
    "startTime": "00:52:03",
    "text": "Has addresses lucky him without them Lock would be crap. And and we all know this. The nets are just horrible. And and so to some extent, what you're doing is academically interesting and curious and wonderful. But in terms of templates for where the industry goes, as I said, I really think you're in the anti gravity area. We don't see the money in this we don't build according to this template. Where we make things cheap, and why we make things cheap is actually in massive centralization. And this is kind of the conundrum in entire research group. That the real issues here Capital And Economics. The technology you can bend in many ways that we're just not building these kinds of networks. So It's a curious thing, and I love it. You know, it's just sort of tore all over again and distributed hash tables etcetera, etcetera. That unfortunately, I don't think it's gotta be, you know, a a bright and shiny future in the commercial world. Thank you. So I think there it was not a question. I just want to clarify. I I'm not from my PFS, I'm a I'm a researcher. I just measure IPFS. I mean, be honest, it's difficult for me to to comment on the economics because nothing about it. For me, it was encouraging to see, for instance, as these such as clubs were kind of going into the space because for me, this was a sign that Well, I see the money in it. I don't know how. But but if they do, I I I hope they, Yes. Yes. Like it can it can still works, but but maybe right. I'm I'm I'm not I'm not I'm not I'm not I'm not I'm not the good person to comment on Okay. I think I'm a next one in line. First, I want to say thank you, Jeff, for your great comments. Agree. You accept the 1. You were saying that they not my help. Perhaps he said he's not IPFS. You said they were doing academic research. And I'm in academia. I wasn't sure. Because I look at the the results,"
  },
  {
    "startTime": "00:54:02",
    "text": "look at the the ticket where it's still on the slides. It says that IPFS. It's a robust base. For the decentralized work. That memory time. But the the slides are read before it. At the summary, of the results. It gave you very different effect. As the essentially sides, you know, 80% of DHT servers are in the cloud. Type of 3 servers. Serves more than half of the traffic. In a the nearly 95% of the contents provided by at least the one cloud based nodes. How those numbers support the conclusion about IPFS as I look past the base. For decentralized as academia, I mean, I noticed the numbers and just noticed I inconsistency between the numbers, and the resulting statement. I think so my my perspective, when I wrote the the the the takeaways, from the engineering perspective, I think that IPFS is coming from a kind of long line of research about the decentralized of PTP systems. And I don't see any any kind of major flow or or, you know, something all should do it something like this, and then it will work better. I think then that maybe it's like possibly the the economic or or sides delicious. Mean, I mean, as I said, it's probably unrealistic to think that people just like, is it the lack of host content ads will be fully peer to peer. And maybe that that's not that's not the model that we should go go towards, because as we as we see, like, people use it differently, I mean, it's it's just much easier, probably cheaper to just, like, run some some hosting space in at at Amazon rather than house something yourself, especially for the majority of the people. But but, yeah, so so side"
  },
  {
    "startTime": "00:56:01",
    "text": "just wanted to say that for me, I don't see this, the problem with stating that IP has the robust days for the centralized web because I think technically, technically, it's But then the other point is how is it actually used by, by the players in the network and and yeah, we see that it's actually, I use quite a lot with the with the cloud providers. And maybe we could do something on the on the design level. I, I don't see that that single thing, but but maybe also it's like we cannot solve the societal problems as as a previous speaker said, with with engineering. I think we have to move on. So, we have 2 more comments. I've got 2 more questions. 1 from, Are you yep. Yep. So I don't really have a question. I have a comment, So I'm here a little bit, by accident, about what got my attention is the item that is still a problem. So as far as I'm concerned, not is not a problem at all because I'm aware of, technology that is presented in the in area. Which I've I mean, I don't want to advertise it right now, but what I'm saying, it does exist. I'm probably going to respond to this to be not attention to it. So that technology does essentially traverse not the traverses IPV4, IPV6 also. So that issue as far as I'm concerned is just gone. And I don't think there would be any relays anymore in that tech technology care developed so much. And it also kind of makes me think maybe I should get involved in this research group. I don't know. So I'm just gonna try to see how this place, whether it's useful at all, Thank you. So just for this note, I just want to say that, like, IPF has now also is, like, having this protocol, how punching protocol where have"
  },
  {
    "startTime": "00:58:00",
    "text": "where the collection establishment is with a relay, but then the data transfer is actually peer to peer. But, actually, I'd be also very, very interested to to see at the solutions you mentioned because, yeah, that's not something that that does interesting potentially beneficial for the network as well. Yeah. So bottom line would be great if you could, follow-up on this on the mail list. So in general, we wanted to, expose IPFS, more to this community And, I'm sure there are many more questions. And great if you could discussed this morning this Yeah. I I will definitely follow-up. The Briefy Technology is called IPref IP addressing with the references. Okay. Thanks. So we have Christian Turima in the queue. Yeah. I I would like to make a a comment on this business of having, DHT nodes in the cloud. And I think that is not actually a video, It's more like a natural reaction to what Jeff was observing. Is that, yeah, I mean, if I am in my home, I could run myself off my home. It's pretty hard. It's much easier to say that will have a point of presence in the cloud. I would have some kind of hybrid system in which the important data is somehow set for my home, But, relate to the cloud or maybe the cloud is used for just for the connection. And I think that, Evolving the architecture of things like IPFS to be this kind of hybrid between point of contacts in the cloud, and a data server in the homes of the publisher might be an interesting, if you do architecture. And we should both embrace that and say, well, yeah, I get rid of it. No. No. It's Marlborough. Balancing it."
  },
  {
    "startTime": "01:00:02",
    "text": "I think this is a fair point, especially, and we're quite systems and and blocked ins. And I think that's also what we see a shift from, you know, everyone should run a blocked in on their up to maybe let's let's give to the cloud what what the cloud does best. And then we can make it to some say maybe it's just oversight procedures to have some performance, like security guarantees. And then, and then this kind of model can be much more efficient. Yeah. Yeah. You you have Yes. You you have to look at the fair amount of the cloud. The fair amount of the cloud is you'll you'll gonna be a some sense of shifting or or something like that. But If you start from the failure mode of the cloud, then you can investigate what kind of, decentralization you want. We function do you want to put in external nodes, which are maybe behind nuts and firewalls? Which function you want to put in the clouds, Like, I think real time real time real time video, I think the data in the cloud That's not gonna give too much power to the cloud managers. It's seems like that should be evaluated, I think. Yeah. That that's a fair comment that might be the way to go. Okay. But it's but it's good. Thank you. Yeah. Thanks for this discussion. We we have to move on. Yeah, let's let's, follow-up on the main list. I think there are many more sessions that we would like to ask and just let's do that and, So next, we are going to have, Martin Plattman, also joining remotely. Let me just bring up his slides quickly. Please contact about local first software. Hello, everyone. Sorry. How how do I control the slides? Yeah. Just a second."
  },
  {
    "startTime": "01:02:03",
    "text": "You can use your Casa keys now. Very good. Thank you. Okay. Well, I'm really happy to have been invited to this group, and, I think we have a lot of, like, minded people here is wonderful. I'd like to talk a little bit about, the research we've been doing over the last is on something that we call local first software. So the context for this is a collaboration soft where sort of types of apps where users can edit some sort of file typically collaborate with some colleagues and have several people editing it at the same time. So you notice from Google Docs or MS Office has a cloud version overlay for late act documents. Figma for graphics and so on. There's a a huge, class class of applications that have this sort of real time collaboration aspect. And of course, the way these apps are built currently is as cloud software where you've got the primary data storage, for the app living in the cloud, users can access this through maybe a web browser or through some client apps, from their local devices. But, really the the storage of the data here is in the cloud. And if the cloud is not accessible, whatever reason, be it, that the cloud provider has locked you out or something mundane like you don't have an internet connection right now, then both you can't access your Utah. Is particularly problematic in the case where cloud providers might just close people's accounts. For example, because some automated system has thinks that somebody is violating that the terms of service and dentists closes somebody's accounts really without any, option of recourse and you keep getting stories from this. But I think I'm preaching to the choir here. So I think everyone's aware of these kinds of problems. And that actually rather large numbers of accounts actually get closed every year of which only a small fraction gets restored again."
  },
  {
    "startTime": "01:04:01",
    "text": "So this seems kind of dangerous if, we are the mercy of somebody losing potentially huge amounts of work that they've put into say the documents that I've written on Google Docs at the whim of a cloud provider. And moreover, I feel there's a sort of as a a principled objection here we can still read some of these texts from 5000 years ago been written on clay tablets. Whereas Will we still be able to in a 100 years, even read the documents that people are writing in Google Docs today, I personally don't have great optimism that's we will still be able to do this because who knows what the state of Google Docs will be in a 100 years' time, but I suspect it will have probably been replaced with something else, whatever that something else might be. So That's have been sort of one set of objections to the current state of cloud software. Also another objection to the way we currently build cloud software which is, more to do with the the effort that the developers of that software have to put in It's actually developing cloud software is incredibly complex. It started out. Been, you know, the previous decades being not too bad where you would essentially have a a server that sends them HTML to the client, which is a web browser. Proud. Render that HTML. And then if you want to have some inputs, the web browser sends a form back to the server. Simple enough, but of course, now in the meantime, people have decided that that's not interactive enough. And the modern way how all of these highly interactive real time collaborative web apps are built is actually there's a huge client side JavaScript app that does all of the rendering. It talks to the server via some API, typically, like maybe a a REST API over CTP to server to send some JSON to to the client. Now if you think about what's actually happening here, That's a huge pile of layers of technology that are stacked upon each other. Starting from the client end,"
  },
  {
    "startTime": "01:06:01",
    "text": "in the web browser, if the app wants to render something, The app probably has some state in some variables in JavaScript which it can render, for example, using React or some other sort of web UI framework. And any user inputs, like the click events from the mouse, the keyboard events and so on. They go into the JavaScript application and update that JavaScript application state. Now if the application wants to persist anything, well, the only persistence is on the server, so it's got talk to a rest API. So it's got to convert this state from the in memory state of the JavaScript application into RPC requests and the responses have to come back from this API and to be turned back into application state. On the server side now, well, this API has to be implemented somehow. So, typically, the way it would implemented is that there's some sort of layout of model objects, which, encapsulate the business logic on the server side So any HTTP request on the server, gets converted into some sort of updates to these models and then any response takes the models and sends them back out to the clients. These models now, they have to be persisted somewhere. And so will typically go in the database, maybe using an relational mapping framework to turn the these in memory objects into SQL, and then the database health, well, that has some way of then encoding its data invites and writing it persistent storage and replicating it and sending it over to network and so on. If you think about it, this is just madness that so many layers of data conversion happening here. And huge amount of application code has to be written just to convert data from one representation into another. So I feel like the the current state of cloud software is is very much a a local optimum if you think about this, if you were designing app development platform from scratch. Today, you would surely not have 6 different representations of the same state and have lots of application codes that has to convert between all of these So"
  },
  {
    "startTime": "01:08:04",
    "text": "what we've seen is a bunch of problems with the way cloud software currently works. Certainly has some great things, you know, the real time collaboration, the fact that you can access it from any device all great stuff, and we don't want to lose that. But we do have to very precarious situation where the cloud provider controls all the data Also, the software just doesn't work offline very well. And essentially the ownership of the data is taken away from the users. Could contrast this with the old fashioned software that we had before the internet was big, you know, you would have Microsoft Word running on your own computer storing the files on your own computer. Which gave users a huge more, huge amount, more agency over their files and over what they did with it, of course, now that you have to downside that you don't have real time collaboration. And so you have to either, you, you know, send files by email, or use suppression control system like it or something like that, which is not accessible to most nontechnical users. So what we've been looking for is what is the middle ground between cloud software with all this real time convenience, and the sort of old fashioned desktop software which stores files on your own computer. And that's compromised the best of those to both wells, is what we call local first software. So the core idea of local first software is actually really simple. It's it's the users have some storage locally on their own device. In the, in the case of native apps, they can just write to the file system in the case browser based apps. There's like index DB. And so and so on. And even some new APIs for file system access. So it's possible for cloud software to store data locally on the user's device. There may still be cloud services. Absolutely. But at least if you've got the data locally, that's, something that is much harder to take away from the users. And so the really what local first software is just kind of kind of change of focus, change of priority. That in the traditional"
  },
  {
    "startTime": "01:10:00",
    "text": "Cloud software model, Ready. Any data that was not stored in the server database basically didn't any data that's on the client is is basically just a cache. It's very ephemeral. Whereas with local first software, which going to do is make the data storage on the client the primary copy, Data storage on servers probably still exists mean, you could use a peer to peer network, but, I suspect that for many practical applications, cloud services will still be continued to be used. Butts, we just changed the emphasis that cloud storage is no longer the primary storage. Cloud storage is now just essentially a backup mechanism and it's a way of syncing data from one device to another. And so this Change of emphasis looks like a small thing, but, actually, it has a very big impact on the way that the software is developed and also on the way the data is controlled by the users. So one example, for example, one thing that, offline network just works absolutely fine, with, local first software because you're just updating data that's stored locally on your own device. And then sometime later when you come back online, then there can be a protocol that syncs that syncs those changes. In the background And so the user interface doesn't have to wait on whether you're online right now. Or not or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or not, or another, advantage is that the programming model becomes vastly simpler. So compared to this traditional web app model where you've got huge number of layers of application code, what we're saying with local first softwares, essentially, the application code exists only on the client. And so you've still got if you're in a browser app, you still got your JavaScript application state But when you want to persist that application state, you just talk to a local library in your browser and store the data locally on your machine and then the data synchronization process that, takes your data and relays it to to to the machines of any collaborators who are working on the same document, for example,"
  },
  {
    "startTime": "01:12:02",
    "text": "that is just a general purpose library code. You can just use an shelf implementation. Office, of this data synchronization framework and any servers that are involved in the data synchronization similarly are just generic pieces of fungible infrastructure. So This is in the same way, like, if you're writing a server side application, you will probably just use a data like Postgres or MySQL or something like that. Probably won't try to your own, your own database because these open source databases exist that are perfectly fine, and you can just use those, for doing your persistence. Likewise, in a local first model, What we're trying to move towards is that this storage and synchronization line 33. That you talk to on the client side, that is just general purpose infrastructure. It's not specific to any one application. It's just a general purpose library that you can use. It's just open source. The protocols by which it talks over the network are hopefully in the fullness of time, open standards where there are many different server implementations that speak to same protocol and the server's not application specific. Server is really just a way of relaying some bytes from one client to another. Another nice thing with this local first model is that also opens up the option of end to end encryption. And so you know, from apps like signal and WhatsApp, end to end encryption has become very widespread when it comes to messaging, but there's this huge class of other applications like document, collaborative document editors, that are currently generally not end to end encrypted where the cloud provider just has access to the plain text. So what I mean with end to end encryption here is that rather than just encrypting data as it goes over over the wire, using TLS, for example, the data is encrypted in a way that the server, even even the server just sees the ciphertext and doesn't see the, plain text And now if we're doing all of the application logic on the clients anyway,"
  },
  {
    "startTime": "01:14:00",
    "text": "and the server is just a relay for some bytes from one place to another. That now opens up the possibility because the server doesn't need to interpret the contents of the operations. Anyway, it's it's just pushing some bikes around. Okay. So He's a comparison, between cloud software model and local first. So in the cloud software, this building real time, soft features where several people can edit and see each other's updates in real time. It's actually very hard And with local first software, we've got the opportunity build in real time collaboration from the start. Like, this is just part of the programming model. That you can support this. With, look, with cloud software making it work offline is kind of difficult. You know, it it is possible but it just pops out very naturally if you're writing software in a local first style. I talked about already the risk of users losing data if they become locked out of a cloud service. So local fest. Users have a copy of the data locally, And for example, they could then just choose to push that existing data to a new, a new synchronization service And then they continue being able to work and they don't lose any data in the process. And finally, what we're working towards with local first software is that the synchronization service is generic. So it's not tied to any one particular application. But it's just a general purpose piece of infrastructure. Whereas in cloud software, everyone who runs a cloud app, needs to write their own server back end. Which talks to some databases, which means they then need an on call team of engineers who can be woken up in the middle of the night if their cloud service goes down. Which means it becomes very expensive to actually operate cloud software. And we're hoping that with local first software. More of that synchronization and storage can be outsourced and push just to vendors so that the application of developers themselves don't have to have these on call rotations"
  },
  {
    "startTime": "01:16:00",
    "text": "Now I don't want to pretend that local first is amazing for everything, I think it really applies to a certain class of applications and not to others. So let me try to, pull that apart a bit. So think the local first works very well. For what you might call file editing software. It doesn't really matter what the file type is. Whether it's text or spreadsheet or slides for a presentation or vector graphics or or graphics or video editing, whatever. But with all of these types of software you've got users creating a file and the user can edit that file in whatever way they like. That's essentially the the characteristic of this type of software. And wherever there's this sort of productivity software, class, which is like note taking, shared to do lists, issue trackers, calendars, and those types of things. Which, again, the users can update in whatever way they like. Where local fuss does not fit well is where there's some sort of real world resource involved actually has to be managed centrally. So for example, say it's the the balance of a bank account and the transactions on the bank account, Yes. I could have a copy of my current bank account statement on my own device. I could try updating it locally, but if I update my own local copy of my bank account, bank does not care. You know, the the bank cares whether the money actually came in to the account or not, my local edits to my bank balance have absolutely no importance from the point of view of the bank. So this is different. I can't just edit my bank account in whatever way I like. Whereas in a text document, I can edit it in whatever way I like. And the same is true of like warehouse inventory, for example, or vehicles or things like that whenever there's these kind of real world resources. Involves that's actually a place where I feel centralized cloud infrastructure works just fine and, don't actually need to change that. So What I'm interested in here is just this left hand column. Of software where I think local first is a good fit."
  },
  {
    "startTime": "01:18:02",
    "text": "So very briefly, I thought I might tell you about a particular software implementation of these ideas that we've been working on so Automerge is an open source library. That can be used to build local first software. That's what it's designed for. And, the data model it provides is essentially JSON. So if you wanted to make a to do list, using auto merge, you would represent it perhaps like this. Example, you've got a a list of to do items and each to do item has a title. Which is like the text that the user has entered and a Boolean called Done, which indicates whether the user has checked the checkbox on whether that's, item not to do list has been done already. Can add further bells and whistles, but this is a sort of basic thing. And now let's say that a user wants to mark an item as done. The way you would do this in auto merge is the the state of the JSON document is actually an immutable object And, you can call this automerge dot change function in order to update any part of this JSON object in whatever way you like. So here, for example, you would now pass in the current state of the documents called before here, and you would get back an updated document in which that change has been made. And in this case, there are change that, we want to do is, done here in this callback where, in this case, I take index 1 of the array of to do items and set its done field to true. I can also associate a textual description of that change with that optionally think of this like a commit message in Git, for example. So it's just a a description of what to change did, but it's otherwise not interpreted. Also, you might have another user on their device because it's you know, this data is present on multiple devices, and each user might update that device independently. Perhaps while offline, say that another user independently on their device wants to add a new item. To this to do list. And so here, would call to dude's dot push."
  },
  {
    "startTime": "01:20:04",
    "text": "That's just the JavaScript API for, appending a new item to a list. And they would add this new item, do laundry, is not yet being done, and this then just gets added as a new item in the to do list. And so this is the basic API on how you can model data changes in auto merge. Now What I said just now is you might have different users on different devices independent really updating this data. And so, for example, Here, we have user a, setting water the plants to true, and user b on a different device adding do laundry. As the third item to do to this, to do lies. And so Automerge now handles the data synchronization so that the updates on one device get relayed to the other device through whatever network is available. And it now also merges these changes together. And it does so automatically. So unlike Git We try to not expose merge conflicts to the user except in very narrow cases where we can't do any better. But in this case, we can do an automatic merge quite reasonably using the inform using the structure of the JSON document that we have here, And so in this case, the merge that auto merge will do, is just, well, it preserves the fact that water of the plants had its done field set to true, and it preserves the fact that a third item do laundry was inserted into the list. And so this way, the different users all end up in the same state. They see the same state of the document. And in that state, we have preserved all users edits as much as possible. This also applies to text editing. So, for example, if you wanted to build a a Google Docs clone, or many app other applications, you also get text, you know, if it's a presentation slide deck software that will typically contain text on every software on on every slide. And, likewise, you could have different users editing a text in different ways. So here, for example, let's say the red user"
  },
  {
    "startTime": "01:22:01",
    "text": "edits the word, the document hello to add the word world before the exclamation mark and the blue user on a different device adds a smiley face after the exclamation mark. And the way auto merge will now merge these edits together in the way That's that's that's hopefully reflects, what all of the users wanted in this case. The wet world, will stay before the exclamation mark and the, the smiley face will be after the exclamation mark. Now we can have a longer argument on whether you think the sort of automatic merging is really the right thing to do, you might think that maybe manual conflict resolution is the right thing to do that's a sort of longer debate but I didn't really want to get into too much today because I really wanted to focus on this idea of local first software. But the idea is that, a library like auto merge can provide the sort of general purpose storage and synchronization, mechanism that allows applications then to hopefully build, collaboration software in an easy way. The way how this works internally in case you're interested, auto merge is what called a conflict free replicated data type or CRDT. Essentially just means it's a data structure that can be independently updated by multiple nodes and the changes are merged together in a determined way so that everyone ends up in the same state. Moreover, CRDTs have a nice property that they work over any type of works. So you can synchronize your data by a server if you want. But the server is not really involved in the conflict resolution. It just forwards the messages around, which means that if you want to run over a peer to peer network that actually works just fine as well. Because the we're not depending on a server to perform any particular role in the protocol. And so if you can get your bike from device a to device b via a peer to peer network done That is perfectly fine with auto merge as well. Just briefly, I'll finish with a a few words on how auto merge is implemented, So this is a library that's, written in rust and"
  },
  {
    "startTime": "01:24:04",
    "text": "you can use it through a Rust API if you want, but we picked Rust mostly because it's portable. To many platforms. And so example, we compile it to web assembly and then have a JavaScript and TypeScript wrap a library around it which we you can then use to build applications in web browsers but there's also a C API, which is then wrapped by a Go API if you want tried servers in Go, for example, if you want to write mobile apps in iOS, there's a swift API Likewise for Android. There's a Java slash Kotlin API. And so on. So the idea here just being that there's a single implementation of the data structures and algorithms and synchronization protocols and you can build apps on any platform that, that is built on top of this kind of general purpose layer. Underneath, the automer itself is really just the data library. It doesn't do any IO. But if you want to save your data persistently to disk, or if you want to send data over the network, well, you need adapters to particular storage and networking libraries. And so for storage, there's, Automotive repo, which is a a companion library that provides IO for storage we currently have adapters for index DB in a web browser and file system. For native applications, but you could plug in whatever storage happens to be available on the platform that you're running on. And then likewise, for networking, You could go via web socket server, you could go peer to peer via web RTC connection, you could use a plain TCP connection if that's available on the platform. Wouldn't be on the web, but, for native apps, of course, you could just use plain TCP any sort of way of getting the bytes from a to b is absolutely fine. Yeah. And that's really all I wanted to talk about. So the concept of local first is articulated in this essay, which is the first URL on the slide. Open source implementation of these ideas ought to merge is the 2nd URL."
  },
  {
    "startTime": "01:26:02",
    "text": "And, yeah, I hope we have a few minutes for questions still. Very much. Thank you, Martin. Okay. Other questions. One thing I might, than just say, like, as a the IETF and IRTF audience, in particular, is, like, right now, you know, open sources auto merge is just like one project in the space. They're other libraries trying to do similar things. But where we're hoping this will go in in the medium to long term. Is that we will work out a data synchronization protocol that works for a wide, wide variety of applications. And it would then once we've got that general purpose protocol, it would make sense to try to standardize that in an open way so that then hopefully my ideal end state for this is that various cloud providers or various vendors of of any sort will provide interoperable hosting services for local first software. And so then if you have your local first presentation app, for example, You can then just give it the URL of your syncing service, log into your syncing service, and that will then that will then interoperate with anyone else using using that same software. And if you have a separate app, maybe a text editor, you could likewise plug it into the same syncing service and just have a single thinking protocol that applies for a wide range of applications. So maybe that's something to sort of think about in the future as these protocols evolved, be really great. To keep talking with the IETF community on how it would make sense to standardize those kinds of protocols."
  },
  {
    "startTime": "01:28:04",
    "text": "Yeah. Great. Thanks. I mean, we are happy to discuss those further. So in the meantime, we have people lining up for questions. So, Dino, I think he went for This is Dina. Regarding your example with world and smiley face being merged to use a global timestamp, or is it first message arrival. I I on, why did you put the world before the smiley face versus the opposite way? And that's because in the when the original edits happened, the the refuse that inserted the world world before the exclamation mark, and the blue user inserted the the smiley face after the exclamation mark. And so in this case, the The order is unambiguous because the exclamation mark act as a separator essentially. So it wouldn't make sense to suddenly put the word world after the exclamation mark, for example. Now if 2 users insert text at exactly the same position in the document, In that case, the resolution is essentially arbitrary. So it's based on a a logical time stamp in the in the events, we then pick which order to put them in. Everyone ends up in the state state. So you support global ordering. What do you mean exactly with Global ordering? If if it was multi participant, you would know when the source inserted before it arrived at since there's no central point, It's source based with local time stamps, you said. True. Yes. Exactly. So there there's no server. There's no consensus protocol in here. It's just looking at the timestamps in the messages essentially to determine in which order they you placed Okay. You know, Real Yana Gita from University of Glasgow. Hi, So I think the idea of the, local first application has, well, is is something I think is a very good idea. And then I think it's nice to have sort of put in a sort of definition of some sort. But then I struggle to sort of see"
  },
  {
    "startTime": "01:30:01",
    "text": "what's the sort of the leap from, say, should of application that have server side components, a local first, but has a server side component. Was quite common. I mean, say, for example, Adobe Creative Cloud Stuff, all of that stuff, all of that stuff, mainly works on local, or the files are local, and you have a cloud component, right, that helps you synchronize and collaborate. Is is this a is this the model that you're going with? I I what's the departure from that. From the existing model. Yes. So so the Adobe Software is is actually fairly close to, to the local first model already. It doesn't have the kind of open protocols, open, generic servers that that we're trying to aim for. It does have the local data storage, this is very different from Google Docs, for example, where there's no local data storage, really. I mean, yes, you can just about enable offline editing if you install an extra browser extension, and then it's still incredibly fragile. So I think the Adobe apps here are the exception rather than the norm that we're trying to move a greater set of software in that direction. Okay. Thank you. Okay. One more question, Raul. Hi, Rahul Gupta. Just a comment. I was just downstairs at, dispatch and there is a Ozul for a state synchronization working group slash research group. So you might be interested, and I just wanted to point that out. Awesome. Yeah. If if you could, like, share a link to that or something like that, I I would Alright. Would love to hear about that. And thank you. Well, sorry to jump to the queue. If you talk about the steady synchronization, I'd also like to contact you that we have done a number of works in that area. Guess, love to talk about it further. Okay. I see no further questions."
  },
  {
    "startTime": "01:32:03",
    "text": "Then, thank you very much again, Martin, for bringing, your work to us super interesting and, hope to talk to you again soon. You. Bye. Thanks so much for me. Alright. Yeah. And last but not least, we have, Mark McFadden, and, I'm bringing up his lights. Thanks. When you're sitting in the front, you don't actually see that there are people slamming spam in the back. That's It's amazing. Okay. This is a contribution to the conversation we've been having about validation. And this is a sort of a reflection on some of the work that other people have done, and taking a look at sort of the broader work that is available on consolidation. There's lots and lots of excellent work here. There's, of course, the IAB workshop that took place in in 2019. There's also an ice hockey white paper, that dates from around the same time. Although it, it focuses pretty pretty strictly on economic issues. There's a full issue of the peer reviewed journal, cyber policy journal, in 2020 that was devoted. To to consolidation. There's a lot of work here. And of course, there's the Nottingham draft, which is in, I think the RRC editor's cube at the moment. There's a lot of academic work as well. And so one of my one of my interests is to actually read this work and understand what's going on. And one of the things that I found was that given that there's this very broad set of work going on here that, there's some interesting characteristics And that's going to be my contribution here. In with this draft. Now one of the things in a previous"
  },
  {
    "startTime": "01:34:01",
    "text": "DIN RG meeting, we got asked to as well is there a definition of what consolidation is? And in fact, not only is there a definition, but you can celebrate a little diversity because there are many definitions. Right? But they all take on sort of a similar property. And here are there are 3. There's one from yari archives draft, which is now expired. There's one from the Nottingham draft, which is moving towards, RFC. And then they're they're from the ISOC paper, there's also a a definition. I won't read these, but you can see that in general, the tenor of each one of these definitions is pretty similar. And I think I think there's a case to be made that, yes, you could come up with a definition of what consolidation is. And so I think that that objection, which has been raised in the past, no longer applies. But one of the things that I reflected on in doing some reading in this space was that A lot of people are talking about consolidation, but they're talking about different things. And so that's why I'm proposing a taxonomy here. And I was I was thinking about this very slide Now I was thinking about Jeff getting up at the microphone in an early presentation and saying, Hey. It's economic stupid. That's not what he said, but that that's what he meant. And so I'm going to, what I wanna talk about is that there are other people talk about consolidation and come to it with a different point of view. They consolidation is real, but people talk about it in different ways. And, your perspective often is determined by the kind of stakeholder you are. What your role is in the network. Are you purely consumer? Are you a provider? Are you a network engineer or so forth? In terms of a taxonomy and how it would affect the IRTF, A lot of people have talked about the relationship between consolidation protocol development. Now I'll have some words to say about that later. I I certainly have"
  },
  {
    "startTime": "01:36:00",
    "text": "some strong opinions about that. This is intended to be a piece of research supporting the research group. Into the kin kinds of large groups of consolidation work that people are are doing in the community. And I think, when, this research group recharted, this is the perfect place to do this work. My draft, the draft that we're talking about here breaks consolidation into 4 big categories. The first one is economic consolidation. And I can tell you that that's the dominant area that people talk about And so that when you're actually going through a bibliography and looking at all of the people who are talking about consolidation, this is the one in which you see the most work being done. I'm also gonna talk about traffic and infrastructure consolidation. This is, if you read, Jeff Houston's blog, this is something that he's talked about in the past as well. I'm also going to suggest that there's another category that isn't talked about often, and that's architectural consolidation. And I'll describe that in a moment. And then finally, something that we're well aware of is service and application at the at at the top of the stack at the application layer, service and application consolidation. Let me say a few words about each of these categories and why I think it's important to have the category in order to have a rational conversation about consolidation. Economic consolidation, as I said, is the dominant theme of work and consolidation. The ice hockey white paper is notable. Because it emphasized economic consolidation, talks a lot. And by the way, that's that's a long piece of work. That's more than 50 pages to get through. And here's a quote, a handful of actors play a significant role in our increasingly connected society. In this context, it's important to consider what the implications of those trends are. But the implications that the ISOC paper looks at are primarily economic ones."
  },
  {
    "startTime": "01:38:01",
    "text": "Right, and what those economic what those economic trends mean in terms of consumer choice in terms of, network diversity in terms of security and so forth. Other authors argue, repeatedly that that economies of scale are the reason that we see, consolidation. It's a really common theme. We see that in paper after paper, especially academic papers. And the the economic reality here is the source of some gloom in certain quarters. I won't accuse Jeff of this directly, but it Jeff's not the only one who has and and forgive me for characterizing it this way. Over a period of time. Measured in years come to the conclusion that not much can be done. Right? That's that's sort of the conclusion. And it and just not alone. The nodding him draft has changed over time. It was originally somewhat optimistic. It's no longer, if you look at version 13 of it, it's no longer very optimistic. But I also believe that it's important to know that there is a group of people who come to consolidation and talk about traffic and infrastructure consolidation. That what we're seeing is traffic flows on the Internet are actually dominated by the networks of a small group of people. Small group of organizations. Right? So what we're seeing here, for instance, an an example that was given to me by one vendor is that if someone creates a cat video in Hong Kong, and someone wants to see it in South Africa. That very cat video may never traverse any other network then the network the video was actually created on It's incredible to me. And so the globalization of those networks has meant that traffic is consolidated into individual vendors networks. In certain markets, access itself, so so so"
  },
  {
    "startTime": "01:40:02",
    "text": "consumer access to the network is dominated by a small group of players. So what we're seeing here is infrastructure actually, controlled by a very, very small number of players. And so as consumers consumers, we have, very few options. But also ISPs have a few options as well. And my last bullet here, is that in the mobile space, what we've seen, especially in developing markets is that the dominant incumbent player have come to And so once again, consolidation has taken place. Primarily for historical re historic reasons. I'd like to convince you that there's another group of consolidation, another category of consolidation. Called architectural consolidation. We had someone had a slide in a previous presentation today. It was a sort of talked about the fiction of the end to end prince well. I I'm old enough to know that the end to end principle actually used to be true. So that makes me, what, seventy years old. But the end to end principle no longer applies. And that's just not the way we do network anymore, and it's not built that way. It's not the way that we make it affordable. And now what we're doing in protocol design here at the IETF, is we're actually putting intermediaries in place. Right? Over and over again, we have, protocols privacy paths. Oh, hi. There's a whole bunch of them. Where we're starting as an IETF to design protocols that deliberately have intermediaries in the path, in the network path. And some protocols, privacy passes an example of this, actually demand that the number of intermediary in the middle of the path, be small. K? It's actually built into the protocol. Baked in. So an awkward argument was, I heard this many times, even from people on the IB, is that consolidation is not an for the IETF."
  },
  {
    "startTime": "01:42:03",
    "text": "But if we're designing protocols that have consolidation built into them, It truly is a topic for the IETF. And certainly, for, researcher And lots of people talk about this other category. Service and application consolidation. What we see are one stop shops. We see application layer tools that are extremely effective and are built in such a way so that if you use that tool, you're part of the eco system that uses a set of tools, and you have a sort of one stop shop. I have some statistics here. These statistics are kind of old there from 2000 and 1021. But some examples of of particular vendors who provide those ones to spec shops. Now the size of those dominant companies, and the companies like them. Is due to network effects that if Users like those tools, more users will come along and take advantage of tools become better, more users and so on and so forth. Right? You have that network effect in place. Now now The goal of this draft is really to help us organize our thinking. That's the idea is that there's not just one kind of consolidation here. That there's actually 4 big categories of consolidation. And I've tried to lay them out in a way so that it it makes sense for further conversations here in the research group and in other as well. And providing a framework to think about consolidation has sort of been my go Now what I'm looking for here, from the group is comments on whether or not the taxonomy as it stands. These four items are, granular enough Okay? Does it make sense in terms of organizing or thinking? Are there things missing here? Now primarily where these 4 categories came from. Is reading the literature that has arisen around consolidation on the internet."
  },
  {
    "startTime": "01:44:02",
    "text": "And the question that always comes with a draft like this, are there things that would be useful add to the draft. And with that, Dirk, Thank you very much. That's fine. Alright. Yeah. Yeah. Yeah. We'll then. Thanks a lot. And we do have questions. So Nick in the queue, Thanks. Me. Hey, Nick Merrill. UC Berkeley. Thank you so much for that talk. That was great. I have a lot, to talk about probably more than can fit in this question. So we'll leave that for offline. There's one thing as far as what was missing. I did some work with the internet society and all you see Berkeley about centralization, not just of the companies that manage this infrastructure, but the legal jurisdiction in which those companies operate. So we did a bit of research on operators there are. There are even fewer countries in which the jurisdiction for the, you know, their jurisdiction is, I'll give you one guess which that country is unless you're Canadian. It's the country we're both from. So I wonder how that fits into the taxonomy, if at all, and if not, why it should be excluded And then the second bit here, and I'm curious to what you think about either of both of these. Is, you know, there's this apparent paradox that I find myself discussing and defending a lot when I talk to people about this. On one hand, the internet's physical infrastructure has never been more decentralized. If you want a copy of a web page, it's in a 1000, 100,000 places at every, you know, eyeball facing ISP. On the other hand, it's management has never been more centralized. They're 3 companies and manage those, points of presence at those ISPs. And I feel that that that theme is somehow lurking in this taxonomy, but I'm curious here your thoughts about whether it should be more explicit or whether it's adequately captured or or how you think that that, that tension arises. Well, so for the first one, which was jurisdiction, I'm glad you brought that up. I I there actually is"
  },
  {
    "startTime": "01:46:00",
    "text": "besides the work you've done, there are, other academic pieces of work that have been on the talk about, consolidation of jurisdiction in and and it's an important issue because it informs some regions approach towards regulation. For instance, the European Union, as an example, so I think jurisdiction should be a part of, a part of a future draft, maybe not a category, but a discussion of of jurisdiction. The tension that exists between decentralized sort of decentralized devices. When you think about the internet of things, for instance, right, and you think, boy, There are lots of devices all over the place that are providing data, but also consuming resources, right, And you say, well, that's a very diverse community of devices out that that are out in the in the network, right, much much more diverse than, say, 15 years ago. But I I'm not sure there's a place for that conversation in this taxonomy. That would be an interesting piece of research on a separate plane, I think. But for me, your first comment about, jurisdiction really rings true for me, and I'll think about think really carefully about that. Okay. Oh my god. There are five people in the queue. What have I done? That's me. Okay. So following up on that, I think I actually am doing a self advertisement this research group ran a workshop back in 2021 and we published eventually, the workshop reports early this year. And that actually attached upon the issues, people just mentioned. The pointer to that workshop report is on the thing that Dirk showed you earlier. Go there and find it, read it. If you have not. I've I've I've for Also in the references in Minecraft."
  },
  {
    "startTime": "01:48:02",
    "text": "Yeah. Okay. Yeah. That's the point there too. Origin reason, my question, just to clarify one specific point. Regarding to end to end principle. I think today, it's not just a bit lost, the end to end principle. We lost that end to end. When I communicating with you, the end is not you. The end is a cloud. That's where we are. The end to end get fractured. We no longer have direct. And to end user communication. It's hold like, going to the cloud, and then the crowd will come back. To the other end. So this is this is not just a press was broken, the infrastructure are no longer supported. Ignacio. Hello. Thank you for the presentation. Two disclaimers, one question and one comment. One disclaimer I have a background in economics or unbiased on that sense. I have not read the draft, just and so everything I'm saying is based on that. As you were talking about this taxonomy to me with my economics background, it also sounded to me, like, that's economics. That's economic consolidation of the architecture level. That's economic consolidation of the application level. So, first, I wonder whether that could make more sense and whether maybe make sense to talk with some economies that they have better, a more solid background on the topic than myself. And the other question is that as Jeff was saying before, economies of scale are powerful gravitational forces standing on their way. Is, frequently, not a very good idea because you are very likely to be cross. And even if you don't, you might make the system more unaffordable as a whole. However, there are ways to reduce the problems that they entail them. For example, if you think about interoperability that has been one of the ways in which monopolies in appliques, application level are being, contested and similar thing with, for example, internet access providers. Making the contracts less sticky"
  },
  {
    "startTime": "01:50:03",
    "text": "is one of the ways in which, even with a relatively small number of providers can increase competition because it's not only the number of our players. It's how easy is the people to switch among them. Those are 2 different things to be taken into account. Well, thanks, Ignacio. First of all, I'm I'm not an economist nor do I play one on TV. And so my background is pretty much network engineering. And the fact that I think, especially in the academically literature, people take these 4 categories and very, very often wrap them around a economic narrative. That the economic narrative is what's driving these 4 kinds of consolidation. What I see in the conversation and the dialogue in the community is that these four strands, these four sort of run contexts are are very different, and people talk about them in different ways. And so that's why I'm bringing the taxonomy to the table is that when I read the literature, I see a lot of conversations about consolidation as I said, when I was introducing this, the economic part of it dominate the conversation absolutely does. But also the application layer consolidation also is a very interesting and much discussed part of consolidation. So I'll take it under advisement. To, reconsider the economic part of this and maybe maybe think about how, it could either be extended or made as a rapper for the narrative for the whole thing. Thanks for that. Okay, Jeff. Uh-oh. I've gone and done it. Sorry. Jeff Houston. Took I was actually reading a book called The Chip Wars a couple of months ago. And it and it kinda surprised me that when you look hard at the underlying industry in which we work, It's all grinding to a halt because of physics."
  },
  {
    "startTime": "01:52:02",
    "text": "When we're down at 3 nanometer chips, We actually have no idea how to make it smaller unless you get into 3 d mask King. And we can't do that cheaply anymore. And in fact, the cost per gate is now rising, not falling. What that means is it the previous 30 years. Where any business plan in our community did not survive more than 5 years because of Moore's law. But by the time things got 32 times cheaper, 32 times faster. That whole doubling process inexorably, disrupted and ground out the old incumbents and it bought in agile money. And the reason why the American companies dominate our world totally enough. Is indeed Economics but it's because in America, it's okay to go broke. And go bust and go bang and do it all over again. Whereas in Europe, it's a crime. You know, trading while insolvent's frowned upon in this part of the world, in America, it's kind of our year. It'll work against it. You know, So so what actually happened was that that technology impetus counted some aspects of consolidation and bread other parts of it. That's coming to a close as we speak. Because next year's chips are this year's chips. Next year's Mac is just this year's Maccolored black. There there's no change in the fundamental economics of the technology base. I can't wait for 3 years and get much more humongous software that'll work at the same cost basis today. No. No. No. I lay our stuff up. It's gonna get more expensive. So now when we get back into being a more conventional business, where technology is no longer a constant, dramatic disruptive force. Does that mean that the existing incumbents right now are much, much harder to unseat That when the music stops,"
  },
  {
    "startTime": "01:54:04",
    "text": "then the ones that are there are the ones that are there, and there's nothing you can do and the real argument in the chip sort of world is. Has the music just stopped this year? Is it over? And I would actually like you to think about that when you think about some of these themes of consolidation, because I believe that that the final missing theme is actually the underlying theme of our technology lifetime so far. Moor's law has been prodigious. But it ain't gonna do it anymore. We've reached physics limits. And at that point, The entire landscape changes, in a bad way. In a bad way because the ones that are there are there, and no one else can be. Yeah. So to the rest of the group, and I mentioned that some people have a review of this. But but but I I agree with one of your fundamental and that is the end of Moore's law. Right? That that's fundamentally coming up as you say against the laws of physics and and smarter people in Maine are well aware of that. The end of Moore's law and the effect that it has on networking something I'm really interested in but I haven't done a lot of work on. Right? I think that that's something that needs to be exposed to the larger community. It's not instance, talked a lot about in academic papers. I can't find it anyway. And so I think I think that would be a great future contribution either here or in other places to start to talk about But what does that mean that we can't keep counting on technological improvements to drive innovation. Right? What does that what does that mean from an economic point of view? But I also think from corporations point of view as well. Right? Then I I'm not gonna have better and better routers. Gonna be stuck with the ones I've got. Right? Or you pay more for better ones, but my point was What are the implications of that shift"
  },
  {
    "startTime": "01:56:00",
    "text": "for consolidation. Right. Right? And I think that's actually one of the biggest themes out there. Because when Moore's law grinds to a halt, everybody's got a problem. Right. Right. But, but, and I think that's gonna be a dramatic theme over the next couple of years. Let me I I'll talk to you offline as well, but that that's something that would be useful to actually have in this draft. Wonderful. 4 more minutes, 3 more people in the queue. Hi, Rahan Gupta. Very interesting talk. I'd like to talk a lot more offline also. Right now, just maybe one issue I'd like to bring Where do you see the role, in your tax on me? Data decentralization fitting. So, for example, I use and also contribute to a project called solid where we are trying to a store individual's data separately, even if they are being accessed by a centralized application So how would you, where would you back fit in your So I think that would be a reaction to the second of my categories here, and that's the traffic and infrastructure, category. That there one of the things that this draft doesn't have is people's responses to these kinds 4 kinds of consolidation, right, And I think what you're suggesting there is an example of a response to that kind of an attempted response to that kind of consolidation. That might be something helpful to add to, to the draft in the future. Thank you. Mix. Hi, Mark. This is Caroline. I agree to see you. I guess I wanted to bring in, a brief comment and perspective from the Internet governance form that I just participated in just to welcome, I think the the work on on this taxonomy, for decentralization centralization, I should say. So it the IGF has been"
  },
  {
    "startTime": "01:58:02",
    "text": "struggling with defining what internet fragmentation means for different stakeholders, and they finalize this process, through a policy, network. I think they call it where they put out a a a taxonomy, if you will, for internet fragmentation. I think that's being very useful in terms of organizing the conversations I welcome your efforts, for a taxonomy for centralization at IDF reacting to your ask for feedback. You know, to me, in the context of IT, if it makes sense to think of centralization across layers of the internet stack. So the sort of economic aspect of it felt sort of off place at first, but then I think it's somewhat of the invisible hand the, you know, drifts the whole thing. I think it makes sense to have it. I second next comment about I think that's an interesting one. And I wanted to borrow, a a category of the internet fragmentation taxonomy, and offer it to you and that fragmentation from the perspective of the end user. So maybe also thinking about centralization from the perspective of the end user. You know, what are people sort of, you know, missing out on what does centralization mean for people that are unconnected you know, perhaps it's not an element of the taxonomy, but, you know, you know, in in merits, you know, conversation or discussion within getting the draft I know I'll get a chance to talk to you offline about that as well, but I I do think, I have a separate drafts, that is titled the effects of consolidation, but I've come to the conclusion that not incomes draft as far superior, and I'm just not gonna continue with it. But, and and shall I say darker? But but but but but but but but the idea of, talking about what consolidation means from the end user perspective is actually worth following up on. I think that's that would have a place in the taxonomy. Thanks."
  },
  {
    "startTime": "02:00:02",
    "text": "I have a from Open Exchange. And by the way, given the previous comment time the guy that has been write the definition with implementation in the paper that was mentioned. So but I'm, no, I I noted one thing. I mean, nice to have a taxonomy of of the consolidation, but apart from the economy of scale, with one of the main causes. There's a lot of technical clicks that are employed by this company promote the consolidation and basically comes down the market and ensure their users, their customers don't go anywhere else. So I'm wondering whether we also need a taxonomy of tractionable things like, closing down interoperability, preventing portability, because there are these terms, are often used and they're used in different ways too. Maybe we need to do some work on that. Yeah. Maybe. I think that goes in a different direction than I was intended this taxonomy, but I think that's worth thinking about that, that it it's not just effects, and it's not just a discussion of the kinds of consolidation, but also the kinds of techniques that are used by companies to enforce that consolidation. Thanks for Okay. Great. Thank you very much, Martin and and on the would be great. Thanks, everybody, for the really great discussion and great questions. We really should, give Mike additional feedback, on the main list. So there were many good points raised, but just let let's not, waste this chance, to to make move this work forward. This concludes our meeting. Thank you very much for attending. This was, super interesting, super to see you again soon. Thanks. Bye bye. So"
  }
]
