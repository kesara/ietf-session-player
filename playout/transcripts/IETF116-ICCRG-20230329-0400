[
  {
    "startTime": "00:00:26",
    "text": "Folks welcome to the Internet congestion control Message group. This is Luis En. I'm acting as a delegate for Ac today because the chairs unfortunately couldn't make it to the meaning. Piece for those in the room, Please join our mid session by scanning the qr codes a prompt corner by using the name from the agenda? And also, please remember to your mask unless you are speaking at the microphone or briefly poor drinking So here's reminder of our note specifically for I f sessions. So this is not an Ie session, but we are following the Etfs Ipo disclosure rules, if you're not familiar with those, please read up on them. Also quick reminder that we are recording the presentations and anything people might be saying at the mic. So if you participate online to on your camera rental microphone, then you will appear in such recordings. And if you participate in person, and speak at the microphone, you may also appear in our recordings. And then finally, our usual privacy notice and code of conduct also applies and if you know for familiar, I invite you to familiarize yourself."
  },
  {
    "startTime": "00:02:07",
    "text": "Quick reminder also that the Of the Internet research task force conducts research, we're not the standards development organization And we can publish documents, but our primary goal is to promotes research collaborations in exploring research issues. And some our. Our goal is to generate insights and we will have to hopefully, insightful four talks today, But first, I just wanted to ask if we can please get another note taker, would anybody be willing to take notes for our discussions that we are. Having okay. See anna. Anybody else when to us pickup or Alright. Thank you, Anna. We have two talks on our agenda. Oh, and we also have somebody in the chat volunteering hearing as backup. Thank you, Vincent. We have two talks today, and without further again. I am going to share slides and then pass it to ios. Okay. Hang on. So can you request instruction yourself case because I... For some reason, I can check this my right now. Oh, oh, okay. Sure."
  },
  {
    "startTime": "00:04:12",
    "text": "I can't see my slide deck. Yeah. It is still being loaded. I suppose. Okay let's try that again. Nope. Just a lot there. Okay. I think I got it now. Okay. Great. So do I have control of the slide deck? Or And now you shall have control of the slide deck. Okay. Yeah. That works. Thank you. Okay. Hello everyone. I'm Ay. Finally a Phd student at in Us. And today, I'll be shedding one of our people called Avi heading towards a B dominant into internet, which was presented in I'm see last year. So this entire work was actually motivated by the fact that Bib was quickly gaining traction once it was used in twenty sixteen. And, you know, early adopters were"
  },
  {
    "startTime": "00:06:02",
    "text": "reporting high throughput and lower delay after switching to Bp r from traditional congestion control algorithms like Q quebec. And according to a measurement study that we did in twenty nineteen, it was clear to us that these performance gains were not going on unnoticed by the rest of the Internet as well. So in late twenty nineteen when we did a measurement study of the top twenty thousand Websites, we found that d are actually accounted for eighteen percent of the websites that we classify. And in terms of, because most of these websites actually streamed video Dvr r controlling close to forty percent of the downstream traffic on the Internet. So clearly, the trend that we were seeing was that B b is becoming a dominant force in the internet connection control landscape and a lot of it seem to be motivated by Bp performance gains over Q. More particularly Super games or Cubic. So given the strength, the research question really that me and my especially wanted to answer was that given that Bb is performing really well on the internet today. Are we heading towards an old Internet in the near future. Or can we expect Pb to completely replace q on account of its performance benefits over traditional control algorithms. So to the to answer this question, we actually had to answer two sub questions. The first thing we wanted you to understand was how will Current throughput gains evolve as more flows on the internet running P. So as people start realizing that actually performs better, and this start switching from qb to Bp, how is that actually going to impact the performance gains that current P flows are seeing. So for this, I'll be briefly going over a mathematical model that we built to model the steady state behavior of competing Q and Pb And the second question that we want to answer is that"
  },
  {
    "startTime": "00:08:02",
    "text": "given whatever trend in throughput gains we can expect to see in the future. How will this actually impact the future congestion control landscape? And for this, we actually extend our mathematical model to help us perform a game theoretical analysis that sort of Tweets Q and Bp as two separate strategies to optimize a social utility on a network, and then we sort of try to predict how this cable payout and in turn dictate the future of the internet suggestion landscape. So as we all know, and I'm sure I don't need to present this primary to the Icc, but just as a reminder, Cubic is I is currently the most dominant congestion control algorithm on the Internet. And this is what Db is poised to replace today. So Cubic is as we know congestion based, it uses packet loss as a congestion signal and when it sees packet loss backs off. And because it only backs up and it sees a packet loss, often referred to as a buffer fellow because no matter how deep buffer is, Queue is gonna fill the entire till a packet loss and only then it's going to back off. B r in contrast does not elect to use packet loss as a condition signal. Instead of that eight elect to build a model of the network which is based on the minimum Rt and bandwidth with estimates. And based on this model, it sort of figures out what the correct number of packets should be and then set it's sending rate accordingly. So what's crucial to Bb is that cause its model based, the measurement of this bandwidth and minimum is very crucial to be operating correctly. So what Pb does to measure the minimum for example is that all P flows collaboratively back off every ten seconds. Empty the bottleneck, like buffer in order to measure the minimum rt. And actually it's this mechanic which introduces the problem when q and people are interact."
  },
  {
    "startTime": "00:10:03",
    "text": "So we know that B wants empty the bottom length of every ten seconds. And the measurement that it makes when bottleneck buffer is empty is super crucial to be we are working correctly. But on the Internet today, Pb is actually competing with Cubic, which is a buffer filler, and it will not collaborate be empty the buffer when Wants to. And this is this introduces something called Rt over estimation in D. Which often tends to make it more aggressive when it's competing with loss based congestion control algorithms. And this estimation actually comes because when Dvr feels back off, that don't see a completely empty buffer. Still see some residual packets that are left behind by a competing loss space in this case Qb flow. And then these residual packets contribute to bloated Rt Rt estimation and intern Pb becomes even more aggressive. So in this paper, when we were modeling the interactions with between in cubic and Pb. I think this is the key mechanic that we want to store and see how it impacts fairness competition between Cubic and Flows. So before I go into the details of the model itself, I think it's valuable to go over the assumptions that we make so that we can put the results that I'm going to present later in context. And then can also discuss what happens to our results when we start suspending each of these assumptions. So starting for for for for the simplicity of the model, we assume that all the completing flows have the same Rt. So this is one of the strongest options that modern mix because even though most of your traffic on the internet today is delivered by cdn, which sort of know, did you still window of That you can see that's... That doesn't mean that there's no rt diversity on the internet today. You'll still see"
  },
  {
    "startTime": "00:12:00",
    "text": "flows with different. But for the simplicity of the model, we assume that we have similar between flows. And then later on, I'll present results and show what happens with the predictions of our model once we suspend this assumption. The other four assumptions we make are somewhat more reasonable then the similar assumption we assume that the buffer is a case one B, and the link is always fully utilized. From previous results, we don't that when D competes with loss space flows, it tends to be limited. So we assume that Pb pro always have twice their Bd Gdp estimate worth the packets and flight. We also assume that the back are uniformly distributed buffers drop here and Reduction in bandwidth while probing for is negligible. So once we have these assumptions at place, really what he need to model the interaction between Cubic and D I three key quantities and you need to model them only in the steady state because be interested in the steady state. So the first quantity we model is, of course, throughput of the competing cubic and Pb flows? Which we simply set as the a floor might have divided by the total end to end delay that they see. So here an equation one, We can calculate P throughput as twice its P estimate. And notice over here calculate the I'm not using the rt but rt plus which denote Pb overestimate And this is divided by the end to end delay that all the flow c which is the base rt plus the queuing delay inflicted by the package that are queuing at the Bottleneck buffer. The second quantity we need to calculate is the rt of estimate itself. So Since as I just discussed, Bp Of estimate is actually created by residual packets that q quebec leaves in the buffer when D tries to"
  },
  {
    "startTime": "00:14:01",
    "text": "the minimum. So in this case, we calculate rt plus as the base rt plus the amount of residual packets in the buffer. Divided by the amount of time will take to train in them, which is simply your capacity. And the last piece of the puzzle is figuring out how many residual your packets are competing Cubic flow. Is going to leave and it's competing with the B flow. And this is fairly straightforward. To calculate because we know that a cubic given a maximum window for a cubic flow, you know it's gonna back off to seventy percent of its window size. When it sees a bucket loss, and then we can simply subtract the B and figure out how much is left to the buffer. So for a two flow bottle, actually, we can just take these three equations with a couple of more general equations and just solve them as a cy equations and derive the bandwidth that the competing cubic and media flows are going to receive. So I won't go into the details here, but basically, when you solve the math, is the equation that you get for Q throughput. And because we assume that the link fully utilized, you can just subtract this value from the link capacity to calculate what Sup is going to be. So the validate to flow model, be actually than completing cubic in B. Over a variety of network conditions with variety of buffer sizes and we pump these numbers for model as well as calculated the throughput over a real test. And as you can see our simple model, which is based on only the steady state behavior, actually predicts the be a part of the competing Bb flow with reasonable accuracy. And it's likely true in network conditions other than the one presented on this slide. You can check out these network conditions in the paper. For more details. So we know that as simple to flow model works really well. But as we all know the Internet has"
  },
  {
    "startTime": "00:16:01",
    "text": "more than two floors. So we need to augment model from two floors and make it work for multiple flows. You have multiple cubic multiple Bp flows completed. So what changes is when you have my cubic and multiple Bp flows compete? When it turns that there's only one key thing that changes when you're trying to model how B flows and Cubic flows track. And that's actually the number of packets that the cubic flows are going to collaboratively leave in the bot during Db pro phase. So if you think about it, for a single qb flow, we know that it's going to back off by seventy percent whenever it sees a packet loss. So it's very straightforward to calculate. But when you have multiple cubic flows, depending on whether their backup of behavior, synchronized or not simply I speaking actually leave different amount of packets in the bot buffer. And this can introduce variability in Pb rt men over estimate. So to solve this problem, we actually do away with a single equation. And we move to predicting bounds, which are based on two scenarios. The first scenario is when all your competing cubic goes up perfectly synchronized. So then so in this scenario, we that all the flows back off together. And then the other bond that we calculate is the scenario where all Cubic close up perfectly they synchronized. So this would mean that only one cubic flow backs off at any given time. And we assume that in any given network, the actual behavior is going to lie somewhere between these two pounds. So now what we can do for the multiple model is multiple flow model is that we can simply replace our single equation which was for a single cubic flow by these two bounds, and now we can calculate two bounds instead of a single solid line."
  },
  {
    "startTime": "00:18:02",
    "text": "So that's exactly what we did and we validated its predictions for multiple flows. So in this experiment, we've launched five cubic and five bear flows and we vary the bottom length of a size in multiples of the Pdp And then we show how our predicted region which is highlighted in light blue compares with a empirical observe values, which are the black quiz on this graph. And again, we see that that the actual throughput is within the predicted bounds of very recently. Simple model. So we know that our model works reasonably bell for two floors, well as for multiple flows. But let's take a step back and ask ourselves why actually set out to do all this smart, like So the reason we wanted to do this meeting was we wanted to understand how Throughput gains are going to evolve as more flows at the bottlenecks starts switching from Qb to V. So what happens to a throughput when more flows are running Eb and less flows running Cubic. So to look at this behavior, I'm in this graph, I've plotted it the empirical observed throughput as well as the throughput predict model for given D flow. And in this experiment, began twenty flows. And across each iteration, we progressively switched one floor from qb to B r, till in the end, we had all the flow running Bp r. And in each of these scenarios with each of these different distributions. We calculated B r average bandwidth and so how it evolves as more flow switch to Pb. So the key trend that we actually see in this graph is that as the number of Flows at the bottom like, increases, there buffalo flow average bandwidth decreases, which means that in a given network as more flow starts with switching to Bb r Bp throughput gains over Q are going to diminish. And in fact, they seem to diminish exponentially. In this graph."
  },
  {
    "startTime": "00:20:04",
    "text": "So we know that Bib gains are going to go down as four people switch from qa to Bb. But the question we want to answer now is how low is too low. Or is there a point at which that Bp starts performing so poorly, that Qb starts becoming competitive again. So to answer this question, I'm going to define new quantity called the Nash Equilibrium distribution of Cubic and D flows in the network. And for this distribution, what I by this is that given distribution of Q and Bb flows in the network. Is the Nash equilibrium distribution. If none of the flaws have the performance incentive to switch algorithms. So let's say if in a network, the Nash equilibrium distribution lies at five q flows and five Bb flows, that means that none of the Cubic floors benefit anything from switching to Pb, and none of the Pb r flow been read anything from switching. Two cubic. And the reason we model this quantity, this way is because If you assume that the websites on the Internet are going to Ab b test, you and b are decide between them based purely performance, then our calculated Nash equilibrium is most likely what the internet congestion control landscape is going to move towards. So let's actually see what this slash equilibrium might look like given the trend that we have we have already seen in Diminishing returns. So here plotted a very general form of the graph that I presented earlier. So on the Y axis, I have Bp flow bandwidth. And on the x axis have the number of Bp flows at the bottleneck like at at given point. And we are already familiar with the exponentially decreasing. Gains of Pb. One additional thing I've done in this graph is you notice, have added dotted line, which I call the fair share. So this pressure line is basically the"
  },
  {
    "startTime": "00:22:00",
    "text": "fair share bandwidth at the bottleneck. And what this means is that if your red line dips below the fashion line, that means that your B flows are getting less than their fair share bandwidth with. And if the red line is above the fresher line, that means your Pb flow are getting more than their fresh share bandwidth. And because in this case, we assume that the Bottleneck link is completely utilized. Db getting less than the fresh bandwidth directly fits into competing cubic flows getting more than their fair bandwidth. Or in other words, that would mean that if your distribution lies in the blue region of this graph, Pb flow generally do better in that network. Whereas if your distribution lies and the green region of this graph, that means that your cubic flows are doing better in your network. So you know this is what the graph is going to look like but where is the nash we're going to lie on this graph. And this is important because Nash equilibrium is going to basically predict what the feature show of the internet is going to look like. So let's actually look at the point that I've highlighted in yellow on this graph, which lies at the intersection of the red line and the fair line. And try to understand what happens when flows try to switch algorithms at this distribution. So this distribution, where the Pb froze are getting the fair share of the bandwidth. If a Qb flow tries to switch to Bb, distribution is going to move towards right basically, the distribution is going to enter the green zone where cubic flows are doing better. Which means that at this distribution, the cubic flow does not have the incentive to to. Because it's only going to do worse. Can conversely see if the opposite happens, where a Bb flow tries to switch to Q, nay distribution is going to move the left of the graph where you enter the blue region where the Bb flow actually superior. So again, your Qb flow does... Your Flow does not have the incentive to switch to Q. At this point again."
  },
  {
    "startTime": "00:24:03",
    "text": "And this is actually quite perfect because this is exactly what Our to distribution of what our definition of a nash equilibrium distribution is. So based on the simple interaction and given Given how Bb performance gains evolve, we know that the point... The distribution at which all the flows are getting their fair air. Basically represents the Nash equilibrium distribution of the competing congestion control algorithms. So we can take this result. We can use it to augment the model that I presented earlier, and now we can start calculating the Nash distribution in different network conditions. So that's exactly what we did. Again, over here, the predicted nash equilibrium exists in a region and we don't predict a concrete point because our multi flow throughput water predicts two bounds and not a solid line. So as we can see, In this... In on this slide, basically, in the stress experiment be beyond fifty floors, through a fifty mbps s forty Link and tested all combinations of competing And Flows and it directly calculated where the nash and compared this empirical calculate nash equilibrium with the Nash equilibrium predicted by a motor. So the first thing again that we notice here is that our model, again, does a good job of predicting the general region in which the Nash equilibrium is supposed to lie. And we also see that the majority of Nash equilibrium distributions actually have cubic flows. Which means that despite At games. Over Qb when you have small number of Bp flows, in variety of buffer sizes, you're actually going to see mixed distribution network settle on a mixed distribution of Cubic and Bp r. Or basically that Qb is here to stay on the Internet. If our model is valid. Well, all this is to"
  },
  {
    "startTime": "00:26:04",
    "text": "unless, of course, the buffers on the Internet are extremely tiny, because when you buffer size is less than one B, then Dvr r is still clearly dominant strategy and one should converge to using Pci. The last thing that I want to go over is actually suspending the earlier assumption that I mentioned. So early on, I mentioned that assume that all the flows have the same Rt. What actually happens when you remove this assumption and you start giving flows different So in this case, I only plotted the imp observe Nash. And not the region predicted by a model itself because our model only predicts for flows that have the same rt. And the first thing that we see is that again, does exist for scenarios where different flows have different. And again, nash equilibrium distributions are mixed distributions, which means that they have both And Bb flows. Other than that, another key that we saw was that when you have flows with different competing. The flows with the shorter tended to top. Q over Bp and the flows with longer Rt adopt B more than q. In the paper we go over why we think this is because of how It fairness works in Q Db differently. Lastly, we all know that D is slowly going to be phased out by Bb two. So we all so interested in seeing how our predictions actually fair when you to replace Bb flows with Bb two. So over here, the blue region is the region predicted by a model for Bb. And the points are the apparently observed Nash in all these different network conditions. And the first thing that we see is that our model seems to overestimate the number of cubic flows at the Nash. And the reason this happens is"
  },
  {
    "startTime": "00:28:03",
    "text": "because in general Bp r is more aggressive than B two. So in summary, we present mat medical model that predicts the throughput for the steady state interaction in between Cubic and Bp, and we show that as the number of Bb flows in pieces at the bottleneck, the throughput advantage will reduced. And we also show game analysis to show that in most networks the Nash equilibrium distribution of Cubic and Flows is going to be mixed. Before I end the presentation, I think it's important to contextualize the key results that we've got from these people. Because there was a different discussion on the Pb dev mailing less about this people. And there was some interesting points, but bought up about how context matters when you're actually trying to figure out how different flows are competing. So I think it's worth noting that this people only expose the steady state behavior, but bulk Qb and Bb flows. And for more complex workloads, with different flow sizes. We probably have to utilize more accurate fluid motors that also model the transient states for Cubic Bp. And the exact nash equilibrium... So we predict some Nash distribution in the people but of course, the exact cash equilibrium distribution is going to depend on a variety of test which is going to include network characteristics, the flow sizes, as well as the choice of your network utility. Because over here, we only assume that people came through but But in the real world, different flows optimize for different things ranging from Qa delay or any combination of each of these factors. So how do you actually... So you know it's a complicated problem, we solved it for a very simple case But how can you actually figure out if there is going to be a"
  },
  {
    "startTime": "00:30:01",
    "text": "Nash equilibrium in your network. So I think if we abstract ourselves out from all the details of the model and the actual distribution of the predicted Nash Olympia. We can still have intuition as to whether we can expect a mixed distribution of dvr r in a given network or not. And for this, again, I think we need to think of the graph that I presented earlier where we plot the network utility for the competing Bb flows and try to figure out how they evolve as more flows in the network start running Db. And the key question here to answer is does the flow utility for the Flows go down as the share of the bear increases This is... We found. This is true for throughput. And the but this might not be the case depending on what network will you care about, especially because Throughput is a unique network related that is a zero sum game. If you're trying to optimize for... Let's say, or some other parameter that's not strictly you some game. You might get reading us. So, yeah, really what you need to care about is whether Bp network utility is gonna diminish as more flow start and db or not. Or basically that given a network utility that you that you care about, do the Bb flows hurt themselves more than the Cubic flow hurt Flows. So stemming from the results that we've seen in this paper, we also want to answer a lot of other future questions one of which is a purely performance driven switch to a new congestion control algorithm on the Internet is even possible So in this people, we have discussed the interactions between Q and Bp someone might come up with a new algorithm, but there might be a general solution that says that a performance driven switch to a new congestion control algorithm. Is"
  },
  {
    "startTime": "00:32:02",
    "text": "never possible. Unless your new algorithm is horrendous unfair to the competing potential control algorithms. The other engineering question I think that we need to answer and I think this question is a little bit more urgent is how do we actually take the zoo that that today's internet congestion control landscape has become. So we have all these connection control algorithms running on the Ethernet. How we design in network solutions that don't just work by I'm the algorithms. Don't just work for Pb but actually work for both these classes of algorithms. And the last question that we have sort of explored and this was also presented in I last year. Is how much worse the heterogeneity is gonna get with quick congestion control. So as we know, most of these standard algorithms are going to be re implemented in all the numerous quick stacks. And what we found out is that not all these implementations are exactly the same. So you have the sort of depreciation going on bear different implementation of Bp actually perform differently, and that's increasing the heterogeneity on the internet even more. So, yeah, that... That's all I have for all you today. Thank you for your time, and I'll be happy to take any questions you might have Thank you. Does anybody have questions? I already see yep funny in the. Hello. My name E from. That's a very interesting presentation. Thank you so much. I have actually two questions. The first question is compared the model and actual implementation There might be a gap between doctor implementation because typically could be a to complex because it's"
  },
  {
    "startTime": "00:34:01",
    "text": "and maybe had a phases. And The second question is it's I might be wrong, but in the middle box. But let's say if we use Each n or buy by, dot z result? My second. Okay. So I think both both of your questions are very valid and they actually question, the key assumptions that we make in a model. So the first question I think was regarding how well a model can predict the actual implementations of complex algorithms like. So in our model, we are not interested in exactly modeling you know, let's say the Siemens evolution graph of Qb quebec are exactly modeling all of the transient states of Cubic. We are more interested in what the steady state bandwidth shares between competing cubic Flows are gonna look like, which is by we use sort of estimates to predict what the throughput shares are going to look like and and assumptions it seems to work reasonably well. But of course, it's only a model for the steady state. It's by no means a replacement for more accurate fluid and model or simulation. So of course, our model has gaps there. With regards to your second question where we assume that the queue is only dropped there. Yeah. I think the the results are gonna completely change depending on what E you're. Right now we are only evaluating drop with different buffer sizes. But with Ec with by with things are gonna drastically the change. On our end, we have apparently run experiments over Code hotel. And we have seen that even though we don't have a model, we have seen that a Nash grants when exists when you run Cord at the bottleneck as well. But yeah, we haven't checked Ec or by yet"
  },
  {
    "startTime": "00:36:02",
    "text": "Thank you much. Okay. Andrew? Andrew Firstly. I think I'm comment a little on the Att distribution as seen by a Cd. Is very by factor of about two hundred. From Well under Into about a hundred and twenty five. Ish. Depends on where you look. The other thing is we ran a query over our sample We... At the end of many connections, we sample the Control and record a bunch of information about what happened to that connection. At some low fraction connections. Ran some queries over that looking for a sensible default congestion control. Mh. And the conclusion was there isn't one. It depends on it's to Yeah. it depends on there is usually a sensible default from any one of our parts to any particular Aes but on any, larger scale than that. There just isn't a reasonably a really a reasonable default. Yeah. If you had to choose, you'd lean on the side of Bb because there are fewer really awful asus for bb beyond. But really, you wanna be working per A. Yeah. I I mean, I I completely agree it. I think that's why we've been still working on cash control thirty years later because we still haven't found a at solution But yeah, maybe maybe we can catch data that with I would be interested to discuss, you know, what your metric was. To decide whether control algorithm is good enough or not. And whether you explore the things other than throughput. Tried several, but it was usually time to last buy delivered. Okay. Thanks, Andrew. We had a question in chat from V as"
  },
  {
    "startTime": "00:38:03",
    "text": "if there's any results for a nick of short and long flows. Yes. So we have we haven't conducted experiments for short long flows yet. And I mean, we we did some experiments for very small flows. But because when you start introducing in flow sizes, Rt, etcetera, then the state space that you need to floor increases exponentially. So we have only been able to do this for a baseball small flows. And you haven't really seen a trend that effort felt was worth shedding today. But, yeah. This is definitely a space that we are exploring because there might be cases where, you know, Like, how we saw for the Rt distribution, we already started the short flow tend to the same thing might be true for different flow sizes as well where your short. Therefore for one algorithm compared to the another. Yeah. So yeah, the... Like, all these things are definitely things. That we want to explore. Another thing another interesting feedback that I got from a different dock was you know, right I'm assuming only one bottle, but as your distribution of flows changes. There's also a possibility of your bottling moving in the network, which even further complicates the problem. So, yeah, I think there are a lot of interesting details to this problem that are going to impact your exact nash equilibrium prediction, But I think the key takeaway away from a result is that such a distribution can exist. And we need to be cognizant in about such a phenomenon happening on that and we need design for a heterogeneous network. Understood. Thank you, Ay. And then we have Tour in the queue."
  },
  {
    "startTime": "00:40:01",
    "text": "Hi. Yes. So it looks like the situation here like, the prisoners Dilemma where you basically you've tried to do that that on your own. You might be hurting everyone else and so the system will so optimally? Yeah. And and so the solution is basically way to try to coordinate. Rather than try to act individually. So that may shed some light on the last question whether we can transition to well the the there could be part actually that we could deploy and and do better better off so it would require cooperation. Right? Yeah. And so connected to this so very nice you talk about the patient. And I'm a single case. The the normalization of that on a multi multi neck case, would be effectively maximum fairness. Right? So yeah. So and then Maxim min is. So we could join this to next, I guess. And so... And Then would this which gonna be maximum fairness. And so then grocery some like. Going back to, you know, ten years ago when all the max explicitly rate congestion control using maxim And if you do that, then, the the location of marketing front is you cannot not increase the rate of a flow without hurting flow that's worse than you. Right? So that's that's exactly center. Do you wanna create. So, yeah, that might shed some light. I guess, if you wanted to transition to an that we would do all better off. Visiting some that Yeah. I I think the the hardest question to answer here is that is actually defining what is better off. Because for everyone better of me, it's a different thing. And you're doing better off right now or"
  },
  {
    "startTime": "00:42:03",
    "text": "So if you're doing better right now, might not necessarily mean that they're doing better of tomorrow. Because it's a function of your control algorithm distribution, links these other things that are going to evolve over time. Traffic patterns might change there might be temporal aspects. So... Yeah, I think it's it's really hard to make a decision when it comes to deciding what's better for you or not. So over here, we try to model that end by someone constantly ab b testing. You and Bp vpn and then switching whenever whichever algorithm performs better. But I think another thing I would like to shed some light on is that here we assuming that flow chooses only one algorithm. One way out of this problem is that instead of having one strategy you actually start having mixed strategies. So you actually start you know, sending some of your data over Qb, Some of your data will be. And then you load balance between them depending on which one performs better. So... Yeah. Yeah. Lots of interesting aspects to this problem. Thank you for your. I work. Thanks. Next week of minutes. Hey, Mauricio from an It. Thanks for this interesting you know, very nice to see the measurement studies. I just had one question. Have you been able to also look into your model about the fact that Bb r inherently doesn't have a response to a packet loss. So we know that know, the B v one did not have a response to packet loss, but I believe Bb b r v two has a packet a response to packet loss and more interestingly, when I looked at the source code of b r two in the kernel provided by the team at Google. I could see that the response to packet loss is pretty similar similar to what Cubic does."
  },
  {
    "startTime": "00:44:03",
    "text": "Then how has been your observations with respect to this part in your analysis? So with respect to Bb one, not looking at losses and Bb are looking at losses. Of course, in a model, we assume because we were working with Bp r v one. We assume that the Bp r was a completely lost agnostic But empirical, we have experimented with both D and D v two flows. And But we see is that because Pb v two flows respond to packet losses, they tend to be slightly less aggressive in a variety of networks compared to V. And because of this, you tend to have more cubic flows at the Nash when you have d v two, running and zero P p one. Got it. Thank you. Okay. And then we have Bj. Yeah. Hello. Beyond Tie from Lu Thanks for a very interesting talk. So my question is, I have a suspicion that the Nash Equilibrium will be the the point where the total sum of the flows. It's it's basically length filling the the link to more than sort of all the other points on the continuum. Right? So is it correct to say that Nash equilibrium is the point of which the buffer blows up the working like link is the worst. Status is it can be. An interesting point that I actually haven't seen the numbers and experiments itself. Of course, in the model, you assume that the link is completely utilized at all times, but you haven't seen a how that manifests manifest itself in terms of the buffer occupancy. But, yeah, That that's something I need to look into. So I don't have answer for you right now. Okay. Cool. Thanks. Thank you."
  },
  {
    "startTime": "00:46:03",
    "text": "Thanks everybody for the great discussion. There's no further questions or comments, and I'm not seeing any. I'm going to share the other slide set. Thank you so much again. Are you for this excellent talk. And stefan, you should have control over the slide. Can you hear me? Yes. You're a little bit quiet, if you can turn it up, just a tiny bit, maybe. Okay. I try. Okay. And thank you very much. My name is Stefan. I'm a peach student at the university of Tubing, And today, I'm going to talk about our recent paper alternative best effort for services Trading loss versus delay which has been published in my based and transactions on networking revenue. This has start work with my colleague Ga Project and michelle mint. Like to start my talk with a short activation, why did it be out of work Them I'm going to give a high level overview of our mechanism I would talk about one key point namely the credit devaluation. Also implemented our mechanism in the Linux network deck, And we'll talk about that as well. And then finally I'm going to show some results. How our can use case and then it would conclude my talk. So as we all know, and as we just here before, buffalo uploads leads to temporarily more entry delays the internet. This is not an issue for data transfer, general Tcp applications But for real time applications for example was our Ip this is not really a desirable because most of Ip with long and delay. Just doesn't work."
  },
  {
    "startTime": "00:48:00",
    "text": "There are many different mechanisms to have all these problems, for example, the framework of our various for some differentiation where real time traffic is strictly prioritized all month. Traffic. But all these mechanisms typically degrade the best effort service. So this means that best type traffic receives we're service in the system down a Pure service. And this is rather controversial in the context of network neutrality. Therefore, we have some people before us have the idea to introduce a low delay forward service at the expense of additional packet. But with the key property that the best effort service is not degraded in any way. And thereby users may choose whether they use the load it. Forwarding service. Or the service. This may even lead to the point where Isp lift choice of the P directly to the end user. Without their own policy. This data shows some example applications that could benefit from such a service for example, Online gaming systems typically have a requirement for the edge end delay of twenty to eighty milliseconds and have an accepted of packet loss rate. Of five percent cloud gaming like Now or s have even more strict delay requirements of around fifty seconds but also pe acceptable of five percent And then, of course, the bottom known voice of Ip with one hundred fifty milliseconds and one to three percent packages loss. Such mechanisms steps provide low delay but higher packet loss or not, These have already been discussed in the Ihs for example, in two thousand, the original alternative best effort from Hale and which is the basis of all work but also in two thousand sixteen whether was a discussion about a general latency loss trade of per behavior we're present however there was"
  },
  {
    "startTime": "00:50:00",
    "text": "no discussion about a specific mechanism. All existing mechanisms have one huge drawback. They are very complex. And most of them even require the flow stage, so that's not clear. Whether they are scalable incentives. Whether they are even implement on common hardware and all these mechanisms have only been evaluated in simulations. So it's also unclear how they behave. With real networks steps and with modern Tcp versions For example, model give we'd avenue two. And so on. Therefore, we proposed another schedule which we called deadlines safe credits and sdk for best efforts or the retinal service and the so called alternative best effort, which is the low delay for class. It runs locally at the bottleneck link. Or a bottleneck node where the queuing happens with the buffer of what happens and its objective is best traffic is not degraded. By other Ab traffic. Such and this node consists of several components it has two five. One forty. Be traffic. One forty eighty traffic. It has also an additional so called credit queue, which is also five and it has two class specific counters one for best and one for alternative best these credit counters act like the total buckets. Where packets can only be. If enough creditors in the bucket and otherwise, our next credit element is consumed. So when you note receives a packet it is first end q and the class specific five for q. Thereby an additional credit element is created which includes the information of the packet size and a packet class is added to the credit queue. And the Ab packets Also, I equipped with a that deadline, which is the maximum allowed q time locally, for example, five hundred seconds on ten settings."
  },
  {
    "startTime": "00:52:03",
    "text": "Packets are only if their class account has enough credit and Packets that exceed their denmark are simply dropped. There's low packets is more than the maximum love accumulate. And when a packet is successfully, the credit from the credit counter this consumed. When there's not enough credit in the credit council, the next credit element from the credit you the corresponding credit counter is increased by the size of the current government District credit is enough to package Otherwise, the next credit element is. And the credit of previously dropped Ab packets remains in the system. Which means that subsequent packets can leverage this credit doing before they are at the head of the queue. And therefore, they can be sound faster, but they can only take a place So previously, drop a packets. And therefore, the five order of the credit some that best f packets is see at least the same order in the same service as an pure B cells. Is one of the main objectives of Dc. So with this mechanism, Packets received loading delay. But with higher packet loss, and b packets do where service with without. You may have noticed that credit from any packets may remain in the system for an infinite amount of time. Which is not desired because then users may just many Led packets they get dropped the credit remains in the system add accumulates. And then they can use it for that use. For major fuss service. This shouldn't happen. And also, the credit and the low delay forwarding should only take place during contest periods you needed. Therefore the credit should vanish the congestion ends."
  },
  {
    "startTime": "00:54:01",
    "text": "Therefore, we introduced two deviations schemes where the credit is evaluated. Once it is exponentially with a given days longer, between two geek operations and this can be expressed more easily with half lifetime simple means after one half lifetime time half of the amount of the credit. This week And the second duration scheme is linearly with the link bandwidth when both are empty. Because then the congestion period is over and then with single normal deeper to behavior of the credits with a linear decrease to link bandwidth. We implemented this mechanism in the Linux Stack as so called newest. When we look at the le Snap deck when we receive a packet a packet first and q in a so called then routing on bridging decisions taken whether the packet is for the host itself, or whether it should be forwarded and when it should be forwarded, descend to the port where the unit happens when a queuing happens and there is basically implemented. We need an exponential donation for the creditor relation However, exponential function and floating part operations in general not really possible in the next call. Or at least not efficiently. And therefore, we also present efficient preservation of the spanish function. In tunnel, and We also need the link bandwidth for the linear devaluation which is unknown and maybe unstable. And therefore, we also have a precise bandwidth with estimation algorithm that runs the next that measures the available bandwidth even loud visualizations. Just very few details about the approximation of the exponential pricing we do that with a piece by a function of two to power of minus x, the details are lost that much of comparisons"
  },
  {
    "startTime": "00:56:00",
    "text": "with two different functions with one let function f x. Which we use when the value of access greater than some threshold and we use another function g x which is the d derivative for very small values of things which are lower than efficient. We to this based on these two figures on the left side, we see the approximation here in black, maybe it's not that clear to see is the original function to the power minus x in green we see the piece by linear approximation and in yellow, we see the derivative. And when we look at the error that we make when we use these approximation on the right side, we see that the derivative is better than the linear approximation for small values of x the linear function is better for larger value of x and therefore we use the derivative for small values and the linear function. For the other abilities. The bandwidth with estimation algorithm is used for the linear devaluation of credit. It needs to of estimates the current link bandwidth with not d I'll do. Real that with And therefore we use and moving average called material, which exponentially waits. Is sent bytes by the sam's transmission time that it requires We only do that when the q is backlog. So when there's at least, one other package waiting in the queue so that we see the real bandwidth and not some random rates and this mechanisms works even put flow digital visualizations. We see that's in the left corner at the bottom where we have a bottleneck link of one gigabit per second a link bandwidth changes after five seconds to two hundred fifty megabits back to one gigabit. After seven seconds. There's a special traffic pattern here, we have burst traffic. Where the offered load is only fifty percent from on average. Then link is only utilized by fifty percent us due. I"
  },
  {
    "startTime": "00:58:01",
    "text": "estimates the real report of my bandwidth very closely. And even for the difficult case, where we switch back from a low bandwidth with to a hybrid bandwidth with it matches the original bandwidth of quite closely. Okay. Enough of the theory, and now let's have some look results on some plots we evaluated these on a wide range of experiments. I will only show some basic experiments to show the situation First, we evaluated the efficiency of of dc because it looks rather complex with all these exponential functions at evaluations. We compared Bc with standard cutest of the linux kernel after call as a q And so on on a one hundred gig bottleneck, without thirty two tcp flows in the case of Dc fifty, fifty splits between Pe And maybe you and we evaluated the and the Cpu option. And what we see is that almost all Q achieve the same Tcp of around eight nine per second which is the usual behavior on a one hundred gigabit bit link. And we see and the secure also are similar but Bc has the second smallest secured load only be really easy q p five for. The lower Cpu load which simple means that you to is implemented quite efficiently in a kernel and can even the use in one hundred gigabit. Environments. The remaining of our results were conducted in the following test steps we had to see me visualize tech test that's with dedicated ten gigabit network the fist forecast had multiple o oems up to five pm sent traffic through one gigabit dc based bottleneck. The bottleneck link was controlled by top buckets. At Police we also had an additional r that applied available on the past so that we were able"
  },
  {
    "startTime": "01:00:00",
    "text": "to contact experiments with different Rt. Our default settings for all the experiments were following with one gigabit. An of one hundred milliseconds A rather a small buffer. In in the device of around twenty five milliseconds. And the maximum june delay for Ab. Was set to ten milliseconds and the half of time. Was one hundred milliseconds soft after one hundred milliseconds only half of the credit. What's due there. And get interesting, on how the mechanism behaves we first contacted conducted an experiment with so called non adaptive traffic with bursts. Which simply means, we had to udp the p based traffic patterns so no tcp, no congestion control and but very plastic traffic. With two kinds of loads first ninety five percent. So on average, the link was utilized only to ninety five percent So no queuing no packet loss. Only during those periods we had q and packet loss And then the second scenario with one twenty percent load where of course, links most of the time and only for very short periods. We have less queuing. We randomly david ninety percent effort traffic and ten percent as cpu. And the plot here shows the queuing delay for both Ab and beat for the two load scenarios we had a variety number of half five times on the x axis and also two different maximum allowed q less of five and ten milliseconds. And what we see here is the main property of obesity namely that Ab receives last tuesday and be We see on the left side of the figure around three milliseconds and four seconds depending the maximum usually whereas bd traffic sees age or nine milliseconds And in the overload scenario, B traffic is almost all june delay of the device, which around twenty two, fifty three milliseconds."
  },
  {
    "startTime": "01:02:00",
    "text": "But Ab still sees three and six milliseconds. Around shooting today. Also see that with a high half lifetime time, the q delay decreases because the duration is less heart and therefore credit remains longer system and can be used more efficiently. So the half life times the way to control the trade of between loss and delay basically When we look at the packet loss in this experiment we see quiet the opposite behavior, we see that Av receives more packet by design? In the low utilization scenario around five four and three. Percent packet loss and e around yeah. Zero partners. So technical. In the overload scenario we see at least sixteen seventeen percent because we have an and there's not in a pen for the traffic, and every ease is around for of three percent more packet loss and the packet loss decreases a half lifetime as before to queuing delay because the credit remains more in system. And can be used more This experiment validates the general of obesity less peck and less but higher packets. A more interesting experiment is the following, which is and our opinion the main use case for This give namely that we have periodic Udp based real time traffic and general purpose background, Tcp that's traffic. This plot only shows the accumulate delay for ab. We have different sending rates of Ab from three hundred kilo per second to one hundred megabit per second on a one gigabit. Bottleneck and we have a different number of background Tcp flows from eight to sixty four. And also different maximum accumulates from two to five to ten per milliseconds."
  },
  {
    "startTime": "01:04:01",
    "text": "When we look first on the right side of the plot because the left side is the special case we see the order as expected two minutes milliseconds security delay result in less than two milliseconds five hundred seconds with less than five and ten with less than ten. And we see it with increasing number of background Tcp force, the queuing net increases a bit because the congestion at the bottom is stronger. But we also see that there's no real difference in the queue delay between three and one hundred bits huge traffic So the behavior of the system it's rather independent on the standing rate and on the traffic share ab traffic. When you look at the left side, we see it as special case because when we have really low data rates, we have rather a large inter arrival times at the time between two consecutive as baby packets. Many existing approaches and then also do low delay forwarding what it with highlights. And have such a scenario packet loss in the order of twenty thirty, forty percent. Because and you pick the drop... The credits available. But until the next ab packet is received, almost all of the credits. Is damaged. So it cannot make use on it? Then the package is dropped as well. To prevent such behavior, we have implemented this that an Ab packet is only dropped if there's at least one other Ab packet in the queue that could used to credit and therefore, service differentiation is basically turned off in the case of very low Ab data And therefore we see q as an a system. And when we look at packet loss. We see again, the behavior three hundred per second. We see almost more packages because so differentiation it's basically turned off and for hey data rates we see it a lower maximum accumulate delay to higher packet loss because packets are dropped have probability because it's more likely to they exceed."
  },
  {
    "startTime": "01:06:00",
    "text": "Threshold and with a lower, I would that number packet lower But we also see that a packet us this below one point five percent lastly fits the use cases from the previous table we had acceptable packet loss rates with up to five percent. And we also see it that the packet get slower when we have higher data rates of traffic. Because there are more packets. Private times are shorter and therefore, credit can be used before it is. Yeah. This brings me to the conclusion of my talk. We presented These, which is a high performance sku that supports Ab and b scheduling implemented it in the network stack It is capable of achieving one hundred per second We also present that an efficient approximation of the exponential financial which is used in our use case for credit information and bandwidth estimation but it could be used for any other a mechanism that meets the exponential function to come. We also presented a bandwidth estimation procedure that works even at moderate ling utilization is rather difficult and our experiments show that this of slow august for a without the best effort traffic and that Ab is able to turn packet loss into delay In our paper, we also did experiments with Tcp traffic. That's also revealed that's you have to be careful what you just control does when you have different network? This is for another talk. So if it... If you're interested in that, have a look at pedal. Thank you very much for your. And I'm happy to answer any questions. Thank you, Stefan. And we already have richard in the queue"
  },
  {
    "startTime": "01:08:01",
    "text": "Literally. Thank you very much. It's a good presentation I like it, like the topics you presented. Well there were some a few points I like to have your clarification. When we talk about best efforts. You need that's a function. Implemented wireless. But a full looks to me. So all would be implemented on host. So there would be new changes for. Is that true a fence? No. So Dc has to be implemented on the bottleneck. Because. Other bottlenecks are on the router where the bottom coast Okay. Alright. So So. Okay. Clean enough. So you said how do we know it's going to be... The which router? Would it be a bottleneck neck? Well, of course, we do not know that but in principle the mechanism only affect the traffic at the bottlenecks. So even when it will be deployed at multiple notes, Those notes that are not a bottleneck act don't have any effects and only the bottleneck link has... What the bottleneck node different effect. Yeah. But in the traffic usually it's voice. So cannot make an assumption, like a no new mac. So in order to make this make let me work. So you have to implemented it everywhere so in case the allowed would become bottleneck some. Yeah. Or you know based on your statistics steps more likely that some bottleneck than another. Yeah. Okay. So I'm qualified. So Sigma one, is hard So Sometimes the when we implement reliability. Right? So tcp we can still use Udp and"
  },
  {
    "startTime": "01:10:02",
    "text": "and we implement those congestion functions and reliably here functions of slew some libraries. On the host hotel. But so suppose we do that, top your Here. So overall, we was act performance improved or not. For some of this another like. Right? It's still transport the protocol which is actually based on udp, that can. So I assume we can use a blazer layout and to support this. So my this which are experiment, you know, you have any results on that or not. Yes, you can different kinds of control systems on the host in combination with this but you have to be careful because of course, this did has the implication that you have had So when you're talking about reliability, control mechanisms then in my country applications when you tell the system via your points that you are maybe good traffic. But july and to connectivity and kind of cope with let's say, one two percent packet loss and it'll be better to still use the best effort. So this is really intended for traffic that chemical cope with packet loss. Then, of course, you can incorporate the other mechanism you come combine this for example with cornell or any other? Actually you. I'm on the router itself. Two will have fuel location of habitat muscle. So well this is acceptable or march, depends on liu case. Okay. Okay. That. So now it comes through interesting part. I have two kills. One is a b when it's ab. On top of them, Are implemented"
  },
  {
    "startTime": "01:12:01",
    "text": "some reliability functions and by using library like order to increment another protocol. So In this case, for these two queues, are going to like a wing window control, like a like, a panel or they have different like how many for the same application. Because Sim are some traffic. Like maybe you would like to put the in the b another. Hyper you would like to put in the Ab killer. There's nothing limiting once pacific of one of the queues. But you will as a design of the mechanism you will only... So you will always serve the e execute has enough credit. But that implies that the B q has no credit because otherwise, the B q would be served and but besides that, there's no control mechanism that decides which queue served and therefore, it's not red limited or anything else for you apart no. I don't. Thank you for your for answers. So the last one is no question just to give you some information. And for some volume multimedia applications. So this is important. But... But there are other approaches. It. For example, in some, you are working on something called quality communications. That's a affordable video of Ai we or human hologram application. So they are there with same problems, you are trying to solve. People call the semantic it's depends on who you are talking with? Thank you. Okay. Thanks. Okay. Thank you, Richard. Do we have any anyone else wanting to ask the question, or make it"
  },
  {
    "startTime": "01:14:01",
    "text": "comment In fact, I have a question I'm curious. So you mentioned some applications can tolerate packet loss up to maybe five percent, but then some of your slides, we saw packet losses were much higher, so I'm wondering if it's possible to have an an upper bound on packet loss if there's any way you would be able to incorporate that. There's Whether In the case that every traffic sn two high. Of course, with an overload of Api packet loss basically unbalanced But for Udp based traffic, we we did many experiments and we did not see packet loss higher than five percent We did not make a model to assess whether there's a real about but this would be of interest of course. Look that. Yeah. Okay. Thanks. And then we have Ka in the queue. Channel well. It's not the for paper, but I have some general comments something we are facing. Right right now. Is it's relates to internet control, but it's small with some coordinated actions. In the the five system, There are some components well basically, like you have radio, like an base station and you have a Functions. And those were participated in the the flow. So basically, you can use like Like the one you choose described and the other countries control in Canada And there are some mechanism to associate between"
  },
  {
    "startTime": "01:16:02",
    "text": "two here we talk about the g b and the U. But in general term, If there is some way to coordinate it took coordinate to. So when one and it is, learning, I'll find some problem like come just happened Well, at that time, it may in a Q modification to wag the other such that other can do the marking without doing like sending the traffic throughout the receiver and then do eight turn back. So basically the work can be down in the middle of the node. In the network It's going to save you a lot of long trip time. So that the mach can be sent back other the cylinder. From the middle of the... To anything. So this is something you know, Although at this time, the 5g g more using it, but at that time, It's not going turn around. The Ups function, actually you be function, while still going send on the original direction So that the receiver you can consider like the U to receive it a with the tag marked, and they is gonna come back then back to the sender for standard to do come control. So which is just the like something I'm trying to think a out of say okay. Well here, I think this is a research group. So I gave some, you know, idea No idea just some things for people to think about it, you know, whether based on the, like, young uncomfortable after him, notification such that you can see some long time. Thank you. Okay. Thank you. Anyone else going once. Going twice."
  },
  {
    "startTime": "01:18:02",
    "text": "Alright. Thank you Stefan for your presentation, and thanks everybody for the discussion. I'm sorry doing. I thought I heard somebody say something. Alright. That is the end of our agenda. So... Yeah, thank you everybody for participating today and thanks again to note takers."
  }
]
