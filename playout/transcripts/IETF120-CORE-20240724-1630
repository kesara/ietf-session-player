[
  {
    "startTime": "00:00:06",
    "text": "So we are on top of the hour and we can start the meeting. Welcome everyone This is the session of the co-working group and IETF 120. I am marco tiloca, microchairs is Jaime Hi Mejee Madison, carsten bormann. And today we also have in the room Laurent Tutan, as in-person delegate. Again, thank you very much for having us out out And as usual, we assume people at the meeting, especially those engaging in active discussion to have read the document in the agenda for today. We want to make good use of our time here to advance the documents status and solve any open issues The not will apply more in details on the next slide Your participation is recorded automatically please especially those in the room on site, don't forget to log into mita echo to be correctly tracked we have at least two minutes takers, Kristen and Rickard Thanks a lot, but everyone can find the link to the minutes in the chat. Please help out taking notes collectively And I'll keep a look at the chat So this is an official IETF meeting. The note will applies It's also about IPR patents and so on You are supposed to disclose your knowledge of IPRs related to what is discussed here. If you discuss those topics or you can refrain from discussing altogether as an alternative But even more important, it is about our code of conduct so please be professional nice with one another Some practicalities this meeting is recorded, period not well if you participate on site again be sure to log into the Meetecho, preferably using the on-site tool on your mobile You prefer to use the full-fledged client on your browser Be sure to keep your audio and video off"
  },
  {
    "startTime": "00:02:00",
    "text": "And if you want to join the Q on the mic, please join the Q queue first on Mitako and then go to the mic For remote participants, please keep your video off unless you're presenting or actively speaking at the mic queue. And if you can please use a headset. If you cannot send audio and it's your turn to tokens on, you can type Mike on the chat and we'll really your comment on question for you OK, this is the agenda for today and after the Chess introduction, we have Karsten covering two slots on the CoreComp cluster first and then on the constraint resource identifiers or HR work. Bill will give an up update on the conditional attribute document and then Martin will cover an update on DNS over co-op and as main item together with the number of related documents around it and they will be followed by also related document in that respect, co-op transport indication for which Christian will present an update I'll present an update on the work on Observe Multicast Notifications and then richard wilhelm follow with updates on item related to the OSCOR security protocol as used at proxies or concerning the app update of key material or identifiers for a OSCore. And lastly, we'll have Carlos presenting updates on two documents for co-op over the bundled protocol and in space Does anyone want to bear this agenda? Heard Nann and reading none let's continue with the document status and first of all we published a since brisbane rFC in ninety four twenty let's continue with the document status and first of all we published since brisbane rFC in 9423 on the new registry very much waited for coordinating the use on target attributes"
  },
  {
    "startTime": "00:04:00",
    "text": "Carson, do you want to add something about that? Yeah, this is probably one of the most boring drivers I ever wrote, but we have the registry here. This was the point and we should now use it as much as we can Thank you, and congratulations to Carson and Zother and the Working Group for this. So that happened in Coral With this meeting we wanted to start sort of an experiment on providing pointers to related documents that have been published or are progressing in their advanced stages in other working group, and notably in Lake shortly after our latest meeting in Brisbane, EDOC was also published as an RFC together with the companion informational rFC with trace and test vectors So back to Core now, we had a approved for publication COURC. You may have noticed on the list, depending on the role, quite a high traffic with the RFC editor as the document is in 0-48. It's almost done Carson will have more updates later today in the CoreConf slot. And we also got approved for publication Core Oscar Redoc with a very long title and it's also with the RFC editor still in the edit status yeah outside of course there's a few documents related with what we do here, one in A's as a Proto application profile for distributing key material. Now also in out 14 and two documents from Cibor are also with the RFC editor in the head status Sebor Time Tag, and the update to the CDL Grammar As publication requested nothing in core, but in other working groups we have one instance of that Proto application profile Kigrou Kamal score now in AD evaluation, and the"
  },
  {
    "startTime": "00:06:00",
    "text": "same status is for the Cibor document EDN literals Francesco says, Ziportime tag in 48, really almost done. Good Okay back to core, we have six documents that completed working group plus columnar in between the status and the publication request to come at some point. One is Group Plus Core and now on version 21 It is under shepherding review and we recently got more comments from Christian sort of confirming what happened after version 17 that received the first reviews from him and more comments on the latest version 21 and the authors have started to work on those so we plan an update as soon as possible The other two documents in this same state are from the Corcom cluster, KOMI-18, completely working group that's call some time ago. Some of work is required We expect a second working group last call happening when we have a new revision Young libraries stayed a bit in the background, expecting for the work on SID and KOMI to be completed and will follow up right after those And Carsten has some more info in his lot soon On the group of Combees document, we are mostly waiting for comments from Karsten as a follow-up from his original working group classical review, and we have recently got more comments on the latest version from Christian and all those have also started to work on addressing those And we'll prepare a revision of the document based on these latest comments Then we also got a new submission for conditions attributes that we will present later today. The main point was addressing comments received during the working group last call"
  },
  {
    "startTime": "00:08:00",
    "text": "And then we have HRF that completed the first working group plus call that Karsten is progressing and will present the status in the slot in the agenda later today looking at other working groups, there's yet another document in ACE related to group communication with OSCAR more on an admin interface, the OSCO Group Manager that also recently completed a working group called call there Back to Core. There's been a few working group documents updated recently but not in the agenda for today. So your anchor is to have a look at those and comment. Other individual submissions have been updated recently and three of the as you can see, are in the agenda from today one in the DNS related bundle two from Carls. And there's also a brand new submitted individual document that is also part of the DNS bundle that Martin will present and more other documents from other working groups? Yeah, more from A's related on Proto or concrete application and transfer profiles for group communities and one document in Schick that is going to obsolete the current RFC that describes how to use SHIC for compressing co-op headers And that was it for the update on the document status. I leave the floor to Carson for the next two slides Thank you. So one document market just mentioned was the corrections and clarifications document or clarifications and corrections and you can remember which sequence we were using Call Klar for short. And the idea here is to capture things that usually aren't captured are captured in a very"
  },
  {
    "startTime": "00:10:00",
    "text": "hard-to-use form in some errata reports and rejections of erratic reports. So just having a working document of the working group where we can collect knowledge that we find is useful to actually make use of the protocols we standardize here That's the idea This document is supported by a GitHub repository that I think has 29 issues at this point in time. These are not really all issues in the sense that the document may need to be changed They are really discussion topics So at the specific issues, we try to get some GitHub discussion going and sometimes some meeting list discussion, where we try to nail down what we know about those topics and what maybe has to be written up, either because it's not done correct in the document, which might be an errata report as well, or it may be done correctly, but really hard to find. If you have to piece together five documents to actually find out how something is done, that is not very implementer-friendly So during the decision, of these topics, we try to agree what would be the best way to handle this topic Is it just adding a section to the underlying Internet draft and use the to record the discussion or is it actually doing something in a different document, submitting a narratory? report that maybe then points back to this document and so on. So this document has been around for a long time, I think I started it in 2018, and we didn't really make a lot of progress And now we have decided we really want to"
  },
  {
    "startTime": "00:12:00",
    "text": "get some rhythm in there, some some regular pace and we want to complete on the order of at least one issue per interim meeting So we usually have, we'll have like five issues that are in our focus that any one point and so we're trying to move them forward. And we're going to try to complete one, at least one per interim meeting so for instance we had a recent discussion about the the context in which a match might be done between a request and a response So Koeb has some text about this being limited to a detail as epoch, but it never said of course, what the rules are for us score and collecting this information together and maybe collecting things we can do now that DTLS is also available as 1.3 That's the purpose of this issue number nine and the pull request that we have sitting there. Another example this time for something which is well defined, but maybe not easy to find out, is the handling of trailing slashes You know that HTTP has something special with there, and we copied this special and yes you can read section 6 of R2 to 52 and find out how this works, but you really have to do some work So we have one pull request that's already accepted in the documents, 32, and one that is still being discussed I hope will be ready soon So this is the kind of discussion"
  },
  {
    "startTime": "00:14:00",
    "text": "I just wanted to take two examples here. And what I'm asking, everyone in this room is to actually think about questions issues, topics, where they have issues or where they ran into other people that needed explanation before they could handle a particular topic well So please think about maybe contributing issues Reviewing the PRs that are in the repository and one we have merged the PR of course, also reviewing the Internet draft itself So that's really something useful you can do that those 30 issues may be intimidating but you don't have to do all of them at the same time. So to pick one that I should resonates with you and write something into the issue or review a PR and add something to the discussion there Okay, speaking about reviews, I have another slide here, because I was really interested to see what are we doing? in the way of reviews and I think this is really important because it's essentially a unique selling proposition of the IETF that our documents get good reviews So other SDOs work on secret documents until they are published or at least make it really hard to access those documents And of course that shows in the quality of review they get And we can be better and we should be better So there is one set of reviews, which is the process review So we have some reviews that are kind of baked into the process We have a shepherd review and all workbook chair review. We have an AD review once we have submitted something to the IESG"
  },
  {
    "startTime": "00:16:00",
    "text": "We have the other ASG ADs is doing something. We have directorate reviews So I think we are pretty safe on that side And for example, if you look at the story of Oscar Ethog, which is now in the RFCLETO, Q, it's a proof document that got all those reviews and that got better with those reviews. What we also have, but maybe not that enough, is spontaneous reviews of documents under construction in the working group. So Christian clearly is the leader here, so I found four reviews from him that went to the meeting minister, some additional reviews that were in some GitHub issues or request so christian hopps pretty good at this and Marco also does his share Okay, Christian kuhtz qualified with some of them where shepherds, but still it is pretty impressive if you look at it them where shepherds, but still it's pretty impressive if you look at the record there. And yeah, of course, being a share still it's pretty impressive if you look at the record there. And yeah, of course, being a shepherd, having some other role like a working group chair also motivates you. But what I'm really trying to get a little bit more here is really just by working group members that see the documents and and um questions need clarifications don't think that is solution is exactly the one we should be using Please send in a review Never has to be a full review if you just say look at section five and have this problem wonderful that that helps us We also have cross-working group reviews So for instance, Brian Zepas from the DTN crowd said something about the bundle protocol draft, or Alex Feng, who's working on notifications in Yang, sent some comments on Komei"
  },
  {
    "startTime": "00:18:00",
    "text": "and of course we can do the inverse we can also look at other drafts and send reviews there and of course the can always put this in form of in the form of GitHub feedback So you can work a GitHub issue or send a broader review in as a GitHub review if you're more comfortable with that So decisions on those will happen on the media list in the end, but starting getting the ball rolling can be done in a number of ways So that was my piece and of course, various can be done in a number of ways. So that was my appeal and of course in the various slots we will ask about who will be reviewing this. So I think we will come back to this. Marco. Thank you Karsten. Of course, I concur. Please review The last slide we have this is a summary of the already plan next series of interim meeting. We plan to start after the summary break, at the end of August, all the way until IETF 121. And as mentioned before, a recurring topic is expected to be the hands-on work on core clar, issue driven mostly okay that said, we finished the introduction a few minutes late, so let's try to catch up with the time, and the next in line is karsten walther the Corcom Cluster And that's a single slide set, and I can give you control. Thank you So you have seen essentially this slide quite a few times but it's moving forward. So the first element of call conf of using Yang in constraint environments, that was published about two years ago. So it has already had some time to be implemented and looked"
  },
  {
    "startTime": "00:20:00",
    "text": "at by people and we don't really have problems with it so far, but we actually worked on a lot in the last couple of months was the next document, SID document which governs the allocation of numbers from a linear space which is kind of an innovation in Yang Cibor. So this is almost done We have a document that passed the first working group Lasca Hall, but has enough changes now that we probably will do another one And we have one document that we probably will not be talking about today and that will probably need a little bit of reworking after the changes we agree on core combine that will probably need a little bit of reworking after the changes we agree on KOMI. So I will focus on KOMI today, but let's just quickly say something about the other documents so as I said 9254 is two years old or went two years old a week ago stable, no known problems There are two documents that try to extend this One is the Yang stand-in document that provides an efficient binary representation of data types that are text-based in Yang Yang is essentially XML at its heart so everything is text-based there, and that's of course bad if you need to manage many IP addresses or other data that have efficient binary forms. So this document is technically complete, but not editorially great So you can go ahead and implement it But we have to do this things like how do you actually negotiate the use but really please go ahead and implement it and see what happens when you do this. The other document is just a proposal how to handle Yang"
  },
  {
    "startTime": "00:22:00",
    "text": "metadata apart from 70 7950 which defines XM and 7950 which defines Yang on JSON We have an additional document defining something called Yang metadata We didn't have Ciboy Yang representation for that yet but we have a proposal now Yes, is there something for X string? that you have in Yang that represents? an exadecimal? string? Because now right now is still converted in the string instead of a byte array So maybe it's good if we have IPA addresses to have something similar for X string Right That's a pretty good comment. What this does addresses to have something similar for X-string. Right. That's a pretty good comment. What this document that is there at the moment did was leverage existing tags I don't think we have a tag that would work extremely well for hex strings, but a tag, what is it, 23, might work for that, or we might want to define a new tag to a better self-gang but that's an excellent suggestion okay so this is where going on but right now we don't have any big decisions to make we just have to complete those documents, and then at some point maybe we'll do adoption and we will probably do adoption in this CBO working group because these are really more CBO issues and not issues about using co-ab to do management access access The second document called SID, I just said we just are almost done. The document has been approved half a year ago and we finally got"
  },
  {
    "startTime": "00:24:00",
    "text": "almost out of 48 now. I just answered the last question today, I hope And what we now need to do is get a designation expert. And we have a mailing list now for coordinating the designated expert team, the RFC editor RFC Proaction Center, IANA and possibly others who want to make sure that this space works right. So this is really about setting up the process, making sure that the documents are handled properly and so on so please join that making it millionists if you are interested in making this process happen Okay, Komi is one of the three conf protocols, netcon, rest conf Okay, Komi, is one of the three CONF protocols, NetConv, you may know. Kalkenv is just Yangtzeba over COEP and of course uses the other two we already had Right now there is not that much happening because, we were focusing on costs but we had some really interesting implement discussions at the thing-to-thing research group interims and May and in June So maybe it's worth looking at the notes we have from those meetings And what we probably want to do is look at more opportunities for simplifying this so maybe using sit zero instead of get and so on there was one question whether we could have multiple IPCs or actions in one payload That would be new thinking We should think about whether this is good. And if yes, what? semantics it has. And we probably want some more examples So one thing that will come up is our"
  },
  {
    "startTime": "00:26:00",
    "text": "already is being discussed as the scaling issue KOMI was designed for constraint devices for SME small management basis. So when we are thinking about scaling, we are thinking about scaling down But still the amount of management information that is available may be large. So if every node in a six-low network has a neighbor table, and we want to retrieve some information from that the whole neighbor table may be a bit large to get. But right now we can only select subtrees. We have no other selection except some limited things that don't really solve the problem. So the question was, do we need more selective retrieval? So one thing that came up was a depth limit, but this is a very blunt instrument There also was a requirement for projection. So if some leaf or not leave some container has multiple components in the maybe you are only interested in one of them, you want to know what is the retransmission rate You don't want to know all the other things that may be of interest So there is a parallel discussion in the yang scaling under the time Yang scaling, which is about scaling up but which has all exactly the same issue So the overall plan for Komai is to get those remaining comments addressed Probably do another Worknu Plus call and maybe add a few more examples that examples are always good but they also have to be made But maybe leave this scale discussion to an extension so we should generate a proof of concept how to do scaling just to make sure we have all the extension points in place to do this"
  },
  {
    "startTime": "00:28:00",
    "text": "but maybe work on a second document that provides more ways of scaling and then obvious extension point here of course is query parameters for for fetch operation But maybe we can find other extension points as well So that would be my proposal, questions Alex Yeah, hello, so I already raised a question on the mailing list about how Seabor is kind of different encoded. So I am talking about the notifications. So there is a big project around here at the IETF about integrating Yang into the Kafka environment so that the messages can be validated I think this project also can be useful for this sort of big data collection, basically so I'm I would suggest to maybe repeat or even just reference that draft so that may it basically we have one type of notifications structure for NetConf, RESConf, and CoreConf, rather than everyone having their source of notifications. And I think that could be useful for developers to just integrate in Yang into other environments That's my suggestion Yeah, maybe you should add that the underlying problem is that when NetConge was designed, there was no Yang So when the NetConv protocol was defined, they didn't use the ability of Yenning to possibly specify the protocol information beyond the actual data. And then when RESTConf, was done, this was moved"
  },
  {
    "startTime": "00:30:00",
    "text": "back partially to the YANG domain, but not really. And I'm very much in favor of coming up with a common solution here, but I'm not sure that we actually are there enough to get this done without delaying Komai even further. So that's maybe a discussion we should be having. Thomas, you probably have an idea about that. So even if we don't have time I can give some time from the HRF slide thomas werner just maybe one quick note. If you look at the NetConchard, you will find out that actually the NetConv working group wants to obsolete 5270 that structure. As you said, a 5277 was basically designated for NetConv and we are currently discussing, discussion, have a discussion in the working group and also there are which we are proposing to define a yang module and basically replacing what the notification structure is on 5277 Yeah, so I think what we should make sure at this point in time is that we can adopt this as soon as it solidifies I'm not sure if this is a reason to actually throw out the existing notification mechanism from the Dracium this is a reason to actually throw out the existing notification mechanism from the document. I think we should discuss this on the list but we really should make sure that we can adopt this work once it's ready. Absolutely. Thanks a lot Just to remark, Kasten can you cut your mic one various questions? Because it generates echo in the room? That's interesting, didn't do that yesterday Good If there are no other questions about Kork Conf, then I would like to go to"
  },
  {
    "startTime": "00:32:00",
    "text": "the next slot, which isn't the same slide set, the CI CIIs When we started, Co-EP, we decided we were going to use URIs as they are being used for HTTP, and that has served us well but sometimes it's better to actually circumvent the incredible complexity that has created on URIs so we decided to do something called a concise resource identifier, which is like a URI, but not text-based, but it actually exposes its semantic structure. And this has been working on for a while. It was originally started by Klaus Hartke, and has been reviewed and revised a lot We currently have dash 16 out submitted today actually, which picks up in INA Ayena early review, pointing out that the CIA scheme registry that ready defining here does not negatively impact the UI scheme registry and some other editorial stuff So that's where we are. We are almost done I'd say. There is one issue that we could work on or that we could say it's not that important for us Are CIA is deterministic. And the first knee-jugary response is for absolute CRIs yes, for relative CIA, CRR references, no. And what this means is that if you put in a UI in the conversion process, you get the same CI So if you put equivalent URI,"
  },
  {
    "startTime": "00:34:00",
    "text": "you get the same CI. And I think we are almost there but we have to find out whether we can actually state that as a property or that's just an aspiration. And of course, the inverse direction know two difference here I should produce the same UI, which should be restrict the CI space enough that one UI actually has an acute that matches it CI space enough that one URI actually has a CI that matches. So really the meta of equivalence is important here Equivalence of URIs are a hot topic in the general UI word. We should find out whether we actually can prove some properties here if we actually want to pick this up so i think that is a discussion that we should dedicate an interim for So where are we? This is ongoing work, and I think the real work that happens in the background is getting more test vectors And right now we have a way to actually edit them in CSV, which is a bit faster than generating them in JSON form one thing that came up was having test vectors for zone identifiers, which are now another hotly contested issue in the URI space, and I'm not sure how much damage you will take from supporting these So that's definitely one that we need to discuss beyond the test vectors, but until we have test vectors we can only discuss that so the plan is to complete this idea with a few open questions that we still have after the summer break Questions about that?"
  },
  {
    "startTime": "00:36:06",
    "text": "hearing no questions, then thanks a lot Kirsten, for the update And the next in line would be Bill of conditional attributes So let me take the slide for you and give you control control which you should have now Okay, thanks, yes. Can you hear me? Yes, I will. Okay, good All right, good morning and good evening for those people who are in Europe. Just a very quick up update on the conditional attributes draft which has gone through enough reviews now. Well, there's never enough reviews but anyway, gone through sufficient reviews now that it's in working group last call and it also received the early review from the IOT directorate. Next slide, please Oh, you have control over this slides. Oh, I do. If you have the app with you, otherwise I can take it back on. Yeah, yeah, okay, that's good Okay, I'm gonna try this as it. I don't know if I'm really you have to flood something it's maybe on your hub Yeah, yeah, okay, that's good. Okay, I'm gonna try this. I don't know if I'm really you have to plug something. It's maybe on your app. Yeah, you're supposed to use your app app I'm not signing through my app is that any way that you can control the slides sure i can draw it for you There you go. Thank you. All right my app. Is there any way that you can control the slides for a minute? Sure, I can draw it for you. There you go. Thank you. Okay, so thank you. Thank you Marco. So now the current version is version 0.07, which was which was uploaded just at the cutoff date of this IETF meeting And, but after that, I received one review from Marco. Thank you again, Marco Marco"
  },
  {
    "startTime": "00:38:00",
    "text": "And I'll be updating the draft to address this review The early review from the IOT Directorate was given for version 6 and I think almost all the review comments have been addressed in this version Normally what I do is I outlined all the different points in as GitHub issues and then I start closing the issues one by one. So I think perhaps all of them except one have been closed now but there are there are some things I would like to discuss with the reviewer who is in a and she's on her vacation So we had a quick discussion by email today and we'll resolve that for 08 probably in August when she returns from her summer And then there was a some feedback from Ianna to clarify the registration procedure for the conditional attributes registry Next slide So this next few slides are about the different between 06 and 07. The intent that's of the draft has been changed so it went from informational to standards based on comments from the IOT directorate and also from some mailing list messages, and then we had some discussions in Yokohama about this. So this is now in standards track Next slide please and then there are some texts that that was basically already there in the editor's version, but hasn't been shifted to this 07. This was also previously already discussed about the fact that conditional attributes should not interfere with Observe the way Observe works, so we introduced resource change projections. And that clarity how this stuff works. Okay, next slide This is a example. So on the left, you see a simple observe relationship between the client and the server And on the right, you see how the server"
  },
  {
    "startTime": "00:40:00",
    "text": "behaves when it receives an observe with a conditional attribute. So essentially instead of the server, sort of using the same resource representation it creates a new state projections and then whenever there are updates to the state projection, then it's returned to the clients so next slide, please Then the other part was about how to cancel register when you have observed attributes. So now we have the resolution here, which is to do exactly what, 7641 says, so then if you do a can the client, then you must include the conditional attributes next slide security considerations have been updated based on issues raised in the mailing list, particularly for amplification attacks and then also for resource exhaustion if you use EPMAX. Next slide So this is one of the remaining issue that was raised in the review from the IOT director, it basically about how multiple conditional attributes will be handled. So obviously, we know from RFC 3986 that query parameters are given an URI with an relationship, but the question was asked whether there can be multiple conditionally attributes with all relationships, and that's actually a very valid point Obviously, I don't want to override how the generate URI in the is written so my recommendation was to was to say that if you want to have an or relationship, then you should really register several registrations independently And then each registration contains different conditional attributes. And I think that's a very clean way of doing this So if there are no objections, this will"
  },
  {
    "startTime": "00:42:00",
    "text": "go into at least the resolution of this issue. But I'm not sure whether this should be explicitly stated in the document. So if anybody has any comments on that, I'll be very happy to hear it now Can't really see the queue, sorry Oh, Karsten is in the queue now Yeah, just one obvious observation if you have an awe relationship between two registrations that actually overlap then you get two copies which is maybe not exactly what you want whether that's important in practice I don't know Yeah. Yes, exactly And then especially if you do have a combination of both and and or conditional attributes, then it gets really messy if you want to put this in one registration. So I'm going to have to look on it Okay, thank you So next slide, I think this might be coming to the end We also have Christian in the queue queue Christian briefly appeared. Yeah, just, yeah, just a quick comment. Of course, if that query language just becomes too common there's always the option of further documents specifying more complex query structures in a fetch and in a fetch pail. That's always an option and that can be done later if the need for all relations persist all right thanks Christian okay if there are no other comments we can move on to the next slide, which is thank you okay that's i've got three minutes to spare. And, uh can move on to the next slide, which is thank you. Okay, I've got three minutes to spare and any other comments or questions at this stage Marco thank you Bill so I guess the plan is just to produce a revision addressing this final thing and let me know if you want to discuss more on the comments I gave you, but hopefully the next version"
  },
  {
    "startTime": "00:44:00",
    "text": "should be ready to go to be polished for a shepherd review We have a shepherd appointed already, by the way. Yes, yes we do. Yeah, and apologies for the delay in getting this out. So I'm back on my research track so that that shouldn't take as much time as it took between zero six and three 07 Okay, thank you. If there's no other questions, looking forward with version 8. All right. Thanks. Thank you Okay, the next line is Martin with the works on the NSWERC club One second and I can give you control I use the share There you go. Okay, thank you Yeah, hello, I'm Martin I'd like to talk about DNS over co-op, but also three other drafts that are related to that now So in general, I'd want to talk about the service binding records with DNS and that includes DNS over co-ops, the DNA draft, co-op, DTSS service B, and transport indicator Transport indication is something that Christian will talk a little bit more about later, but this is also related to that So the original idea was to allow for discovery of dock resolvers via DNS DHCP, and route advertisements, and for that exists for discovery of dock resolvers via DNS, DHCP and route advertisements. And for that, there exists already a solution with service binding records or the service parents field for DHCP and route advertisements The problem is, however, that de facto only discovery of co-op over DTLS was specified via the LPA parent's field for DHCP and router advertisements. The problem is, however, that de facto only discovery of co-op over DTLS was specified via the ALPNID and RC 800,000 8,323"
  },
  {
    "startTime": "00:46:00",
    "text": "And for co-op over detail as we just need it's own ALPNID so we allocated the co-ID but the experts asked us to put this somewhere in a draft so what's missing still is what about different co-op transport? protocols and what about oscar and ad hoc and also there's a pass for the DNS of a corps needed and so in the main interim we tried to create a minimum doc service bundle the pass for the DNS of a co- is specified now in a dog pass key in the DNS of a co-op draft the value itself is defined as a CBO sequence of text strings and the text representation is then in a SIBO diagnostic notation There was a question about this is if this is maybe conflicting with, for example, zone files, but we think we put this out of the way So if you have any idea about zone files and see a problem there, please speak up then there is this DTLS service speed record draft with this basically just defines the ALPNID for COA up then there is this d tl s service speed record draft with this basically just defines the lm a lp nid for co-op over d tl s Then we have the DNR draft, which basically is just now a problem statement for service power co-op over DTLS. Then we have the DNR draft, which basically is just now a problem statement for service parrums for OSCO, and all the transport stuff beyond DTLS and T draft, which basically is just now a problem statement for service pyrums for OSCO, and all the transport stuff beyond DTLS and TLS goes into the transport indication draft now this is basically how the situation now looks like we have a lot of cross-referencing of documents, which in the May interim we already were pointed out is a bit of a mess, but we were able to basically reduce the normative references between the documents except for the already"
  },
  {
    "startTime": "00:48:00",
    "text": "published RFCs to just one And that's from the DNS of a co-op draft to the Service B DTRF, which basically just specifies this co-op over DTLS and TRL connection, and this is basically our minimal doc service parameter bundle which basically is our battery included package where now DDR and DNR is at least possible over TL and for TLS and DTS everything beyond is then on ongoing work for the rest of the year. So our question is, is stock with this DTSA? DTS-A-LPNID now ready for working group as call? I have one question you mentioned using C-bidegnostic notary notation in a live protocol using used for interchange In the root and the Zorro files for dns for example so if you need any text representation of the doc pars okay then it's in CBO diagnostic notation and the problem with zone file symptoms was we were wondering if the commas in the Cibor Diagnostic Notation might cause a problem. But as far as we saw, it doesn't Yeah, I'm just a bit worried by elevating Cibodagnostic notation into a live part of the protocol, part of a live protocol. So let me just say this and we maybe don't need to discuss it, but Christian can discuss it As far as I understand, that text representation of zone files is just as much a diagnostic"
  },
  {
    "startTime": "00:50:00",
    "text": "and configuration format as EDN is So this is basically a syntax that is internal to the DNS servers and their config files and it's just what they use to store it It looks a lot, looks very different, uh, similar between the implement different DNS implementations, but I don't think it's completely interoperable, but I might be wrong here Yeah, I think operationally, these files are used in a lot of places so I wouldn't say, even if it's not officially part of the protocol, and it might even be, I don't know, how zone transfers actually work these days we should make sure that we are not putting something in here that that is not designed to be used for interchange. I mean, it's been basically just about representing the seabor sequence somehow in a format that is textual represented Since it's in a pass, it could also, of course, be just a pass with slashes instead of commas but maybe this could also lead to some confusion that people then start putting the text path into the APN record, so yeah, it's a little bit of a discussion point, I would say, here here Just to clarify, it isn't really sending C point I would say here just just to just to clarify it isn't really sending CBO diagnostic format like there's a file that is in CBOA diagnosis format over the medium, it's just so that you have a technology representation for the CBOR sequence You can call it CBODagnostic format, but you can also just call it a comma-separated list So yeah, I'm not sure if it's that critical as you think think Okay, I hope there's an example somewhere I can look at and make up a mind on this Thank you"
  },
  {
    "startTime": "00:52:00",
    "text": "Any further? questions? Michael. This is Marco. Yeah, I just wanted to follow up more on the procedural side So I also wanted to stress that I found the relation between the different documents in the bundle sound that I was involved in that interim meeting with that was discussed and the result was also brought up on the mailing list to double check with the group and no objections were raised. So hopefully the plan the relations within the document is clear now and we have a point from Francesca of course in the chat. It'll be better to first ensure that the new document is still an individual submission is adopted as a working group document before issuing the working group on the OCE. Do you expect me up update on this version zero or from your point of view, it doesn't what it should in a good enough way to be considered for help? adoption code? Well, if there's no I know that that's not an does what should, in a good enough way to be considered for an adoption code? Well, if there's no... I know, that's not an... I don't expect to get update for that. There is an for the DOC draft, again, about the diagnostic format there might be some discussion and there was also a Yana review but that was basically just a wording issue. So that's why I didn't mention it here But I will add an update for that OK, so SBCD is stable enough to consider Carson, I think we can consider adopting it sooner than later What do you think? Of course, I was just distracted I'm sorry. What is? Let me just do a quick show of hands Yeah, who is in favor of adopting draft? lenders, core co-op detail as VCD? as a working group document Let's give it fittings give it 15 seconds ourselves"
  },
  {
    "startTime": "00:54:00",
    "text": "a five-page document we can still fix it. I mean, this is just... Of course course Oh so let's say five seconds more Okay looks good enough, and the result is 13 yes zero no for no opinion so the check will issue a working group adoption call on the sbcb document and see what else happens. And after that, hopefully the working group plus call on DOC can follow okay thank you Martin any more questions anyone Okay thanks a lot for the update then. Thank you And the next one is Christian So let's see if I can take the slides again and give them to you Off you go. I have glass control, thank you. Hello everyone this is Kirsta Amsus and transport indication is something that has been brewing in this group for quite some time and Martin kindly joined me on the world here because a lot of what is recently happening there is related to SVZ SVCP topic that she's just presented I won't go into the history of the document this time you basically know what it is about just showing the summary here. There are two changes in the latest version, one quick and easy and one is more complex, and I would like to have feedback on that matthew quick and easy one"
  },
  {
    "startTime": "00:56:00",
    "text": "was that last time we talked about this, there was the open question of whether the relation between some resource and the proxy of available for it should be should go through the host's relationship or through the the the same origin concept there wasn't much feedback there so I think others go down I picked easier one, which has simplified the document a lot The larger topic is a SVCB properties parameters and when we are starting to talk about that, I'd like to emphasize that nothing about this changes the fundamental concept that we are still treating transports as proxies so that when a new when a request is sent to a different address then would trivially be received then we'll still treat that other address as a proxy for the origin resource So let's start briefly with what we already have in the specified way. We have two common things that can be interchanged. That is we have kind of a URI with its I'm going to get back here, Fabian specified way we have two common things that can be interchanged that is we have kind of a uri with its um we have a uri we have a uri indicates a scheme and we have a message that is sent that is prepared on a particular co-op transport to be sent to a particular address and that has all those header and options parameters in there and the way we currently get from the uri which encodes not all of the information needed to send a code message, to the full core message is we use the system's name resolution mechanism, which in many cases, would result in a quad a lookup and that only works if and that requires that, that requires that, sorry that requires that and that requires that this"
  },
  {
    "startTime": "00:58:00",
    "text": "a close again and that requires that the scheme encodes everything that we need to know in order to pick the right transport provided that there are different transports in the first place for something that can result in a quarter record when looked at there are added requirements from the dns discovery and there are two ways that the so there are two ways new data can come in that can be um by discovery from DNS and that can be via local discovery and in both cases what we receive is service binding data. That means that we have a particular service in the d r and the cases that's always implicitly underscored dns service binding data. That means that we have a particular service in the DR and DNR cases that's always implicitly underscore DNS. We have a name for which this is of the service service. We have service parameters with typically contain an ELPN and can then contain the dock path and all those other things And then we can get additional DNS data in there. For example, as part of doing that service binding lookup, we can receive the 4-day record. And all this data can be used to create a corporate message, which DOC with the discovery requires to have and thus specifies So now how is this related to transport in the case? indication? DOC is the first document that needs this for co-op and given the timelines and dependencies, spells out how this conversion from the SVCB record to the co-op message and thus implicitly to the uri that is being requested can be done But this is something that I would rather not have restated in every other document for another service so transport indication now contains a generalization of this. And it fits in the topic because the same information, the same kind of information"
  },
  {
    "startTime": "01:00:00",
    "text": "that we can get through the proxy advertisement that have been in transport indications so far this is the same kind of information so it also tells us that in order to satisfy a request for that particular URI, you use these and those and those and those details Now that's so far as good and I think that is what has been the outcome of the interim There is another way of using this that has so far only been discussed in an appendix and in the latest version being moved into a dedicated section That is, we could do the same thing that HTTP does for discovery and when we have a name, perform an SVCP lookup in DNS or whatever equivalent is to the name resolution of the particular system that is being used send that request to the resolution service receive that whole bundle of service binding data which contains more than just more information than just the quad A record and compose the co-op message of that This is something that HTTP has been has started doing without any large compatibility issues. Still with what has been discussed so far for co-op, I have not described this as a change of how to resolution happens but just something that applications can opt into This is just an additional mechanism that I think is useful to describe and yeah before I go in the questions, there's one more detail I'd like to add there That is, so far this has covered the the network address and port and possibly the co-op transport which with this mechanism can be indicated in the result, what it could not be indicated"
  },
  {
    "startTime": "01:02:00",
    "text": "with the plain quad A record, but this could also provide additional data relevant to the setup of the security context so in particular if there is a TLSA record associated with the result, that could go into the creation of the co-op message and cannot be trivially expressed in the co-op you arrive. Something similar is described in the document now for ad hoc, but I will go into those details if you want to know those real document. And there's not quite a relevant for the question which I want would like to ask which is um is this whole approach of just saying that applications can opt into it but otherwise we are still sticking with the same, what we've been doing too cautious, or can we just do what HTTP? is doing and say that, like, if you can do it, query it if there's an SVCP resource record use it your your experience might be better with it and if your application describes that this is the one way you do, you can still start leaving out the quality records directly so that's the first question that next question is how precisely do we phrase that service binding? request do we want to register a dedicated resource record for that? Or do we just stick with a more generic mechanism? of having underscore growup and query the SVCP? research record for that? And third question is, do we want to have a dedicated underscore co-op as well? or do we stick with a single underscore co-op because the mech mechanism of how you verify what kind of co-op s tells you to use d ls but co-op s doesn't really tell you how to verify your peer so maybe it's better tom strickx with a single one with a single scheme rather than keeping still keeping two schemes popular at the same time Yeah, that's it from Masaso on the more"
  },
  {
    "startTime": "01:04:00",
    "text": "scale, please review this Please also have a look at the section that describes service be binding parameters in your eyelet, which I'm not sure whether we really need in this document or whether I think it's a good idea to have them but I'd like to hear more opinions there and we haven't had any interrupt testing yet The other question is like who would like to join in there? and which parts of it do you want would you like to test thank you i hope we have more than 30 seconds for questions but let's see Marco here, so first of all, I can review and I gave a first look at a document already. I had a question on the use of Hasunic Proxy as a target attribute, and the way to build on it for possibly a omitting a URI host, which seems useful in the interest of over it. I just wonder if that interest fundamentally deviation from the algorithm in the co-oprfc for producing a set of options from a URI, and if that's warrants considering an update track or whatever. Yeah, this does not come constitute a change of that mechanism because you sending it to a different resource use basically you're going through a proxy and the uri you're describing is the different resource use basically you're going through a proxy and the uri you're describing is still the same but you build um because you're sending it to every time you're sending something to a proxy this might change the options that are going in but that doesn't change the the the eventual uri of course if someone takes the co-op message and then goes um back this circular arrow and builds the URI from it, they better know that there is this proxying involved and that there is some kind of implicit aliasing going on but that conversion direction would only happen in the server and the server has advertised this and"
  },
  {
    "startTime": "01:06:00",
    "text": "knows that it's kind of they are all the same Okay thank you More comments and question for Christian Heard reading none. Thank you for the update then Thanks Thanks And I think I'm the next line Thank you again, this is an update on the observed multicast notification document now in version 9 And to give Eric up, how does Observe a normal work with Co-up in the particular case where you have multiple clients, all interested in observing the same resource at the same server where they would subscribe or register for the observation individually and as a follow-up the server would reply to each of those individually with notifications over Unicast as the resource representation changes So what this document introduces is a more efficient way to have the case where the server inquires question would start at itself a group observation with a cosmetic phantom observation request, then it provides instructions to all the clients interested in that same resource so that all those clients can listen to a multicath address and when the resource representation changes the server in question can send a single notification over multicast to all the clients And it is, of course, possible to enforce also nt1 security protected the multicast notifications using the group of score security protocol So it's been a while since the last presentation, but we have a updated regularly these documents in different"
  },
  {
    "startTime": "01:08:00",
    "text": "respect from clarifications to relatively minor protocol behavior and more recently, with the switching to the use of CRIs from the HRF document presented by Kirsten and there's even the meeting for the sake of giving instructions to the client about how to listen to the multicaz notifications. But starting with clarifications, it has been a truthful one of giving instructions to the client about how to listen to the multi-gaz notifications. But starting with clarifications, it has been a true for a while that clients may learn the instruction they need to know to receive the multicaz notifications also by alternative means then the error response typically distributed by the second in that respect. For example, they can fetch a previously publicly made available phantom observation request from the server In that case, clients can just set a everything up on their side and start listening to the multicast notification but it can still be useful, like in the original case, to approach the server in any case to help the server with the process of roughly counter the number of clients still around And on the topic of the rough counting, we have added quite a lot of text on consideration and discussions about the accuracy and the reliability of this rough counting process at the server, how that changes in that respect in different setups. So with or without proxy with the use of Antoine security or not, with the use of Entente security, particularly with Oscar protected deterministic requests as defined in christian hopps document for CashaboloScore Osco. There were also a number of clarifications on the detailed semantics of the option introduced for the rough counting and roni even earlier mentioning that the instructions of the server provides to the client is really not supposed to instruct any redirection and more on clarification we remove some text that could have been confusing. There was nothing to add on the reply check of notifications at clients"
  },
  {
    "startTime": "01:10:00",
    "text": "it's just supposed to work like the group of score document defines. We have better expressed how proxies if deployed consume the proxy related options that can be included in messages to forward. We have had for a while to appendices describing for one in Appendix C, the case where the server is also self-managing DOScore group used to protect the group of score, the multicast notifications. And in that, case, some parameters are really not needed for this particular application and we explain why they can be omitted with some parameters are really not needed for this particular application. And we explain why and why they can be omitted. We have another appendix also on for a while on the particular case where group oscar is used with oscar process deterministic requests used as phantom requests, and there was a lot of texts repeated in multiple places so we we collapsed all those sentences into a few paragraphs and we brought them up to be mentioned as quick and as soon as possible and there was also some realignment of parameter naming mostly with the group of score document But with respect to protocol behavior, it was also discussed in previous IETF meetings, and now it's in the draft that if like I mentioned before the server decides to make publicly available a phantom observation request and instructions to join a group observation to be fetched by other means by the clients, well, the server can do that, but it really has to have started a group observation first as a first thing and in the opinion by other means by the clients, well, the server can do that, but it really has to have started a group observation first, as a first thing. And in the appendix on the self-managed of Oscar Group at the server, we introduced one more parameter describing how the group works, or at least the current residuals lifetime for that group and its key material with the parameter EXC, and that was inspired by it similar related changes in ace documents actually And regarding Appendix D, this was also discussed and confirmed at a previous IETF meeting"
  },
  {
    "startTime": "01:12:00",
    "text": "If we go for the particular set-tap where group of score is used with the termini, phantom observation request, well, that's going to be the case, but in parallel, the server will not start a parallel twin group observation not based on the terministic requests. So not twin group observation we get some more details on how the proxy behaves during the rough counting process Again, in the interest of the server, counting the number of clients roughly still around. And now we're also mentioned the newly defined proxy related options, proxies herein, and proxy key number from the HRF document But the major update I would say is a revision of the field TP info in the error informative response that the servers sends to clients as they approach to tell them how they should align and set up to listen to multicast notifications and we used to in this transport specific information there with a sort of custom encoding and finally we switched to using CRIs instead that was started by Christian long ago in PR 13 CRIs matured enough to finally make it and now we well recently we created PR number 14 superseded the previous one and now merged in the document So it's really about that TP info field in the error response and you can see the need and latest situation on the right it's composed simply of two fields TPI server is a CRI with no local part, giving a transport related information on the server And then TPI details is a set of additional information that depends on basically the URI scheme and the transport over which co-op is delivered that is indicated in TPI server and to have a concrete idea of what this can mean we have"
  },
  {
    "startTime": "01:14:00",
    "text": "a cursor whole section in the draft, also summarized here on the right as an example of what you have in you specifically co-op over UDP over Multicast which is the typical case now. So again, TPI server is a CRI with information related to the server and practically the source address of the Multicus notifications Well, TPI details UDP here is composed of two pieces of information, TPI client as another CRI practically indicating the destination, multicast address for the multicast notification that the clients have to listen to and TPI token with the value of the co-op token used in all the multicast notification sent for a certain group observation as all by bound by token value to the phantom observation request and finally we extended the bit of security considerations again around the top of the rough counting of clients, And we had to revise all the examples not only aesthetically, to use ASVG, for example, but also to be aligned with the switching to using CRIs that I mentioned in the ANA considerations, other than minor revisions, we also registered a target attribute that we discussed in the past GPOBS. And it's a hint like OBS but intended to signal the support for group observations at the server and we revise a structure of the previously defined transport protocol indication to be aligned with the new structure intended for the TP info parameter in the error response from the server and the use of CRIs therein This is an outlook of the next steps. There's a few uses on the VETAP to take, especially the describe with a good example at least how this is supposed to work in the presence of a reverse proxy and some advanced details. For example, if clients want to target, multiple servers and ones each of which can use the method defined in this document and possible revocation from the server side of early advertised"
  },
  {
    "startTime": "01:16:00",
    "text": "phantom observation request. I'd like to spend some time also defining how Shik is supposed to compress the two newly introduced options but they should be pretty simple and related to what we also said at the beginning of the meeting, please do provide comments and review We got some very useful comments a few versions ago from Juran and Jaime, now integrated in the document please do provide more Thank you very much Karsten I have a question that you don't have to answer now, please, everybody think of about it and answer to the list. Do you care about deterministic CIs? End of question Something to think about also following up on the main trade in HREF Thanks. Any more questions? Okay, thanks a lot Then the next in line is Rickard, and I can open the slides and give control Right One second. Now you have to control. Yes, indeed I have control Let me share my video also Right, yes, so I love everyone I want to present some updates on this draft Oscar capable proxy which is now reached its second version So just a recap what this is about. Well, it's an update to the Oscar RFC, 8613 And we're defining here the use of OSCO in a communication leg, including a proxy So this means that you can have OS core between an origin client or server and a proxy"
  },
  {
    "startTime": "01:18:00",
    "text": "but also you can have a score between two proxies in a chain of proxies. So practically, this means that we're allowing the fact that an intermediary such a proxy can be an actual overscore endpoint, which was not covered at all in the OSCAR FC. There you could only have origin client and or in server be OSCore endpoints but now we're also allowing proxies to be OSCore endpoints so concretely it's it contains a number of things but among them the rules on how you now process the co-op options because there was concretely, it's, it contains a number of things, but among them the rules on how you now process the co-op options, because OS core classifies co-opt options seems to class that indicate whether it should be encrypted or not. And basically this rule set that we've defined now is you take a particular co-op option, how do you decide whether you should encrypt or not? And our idea is to try to escalate and encrypt as many options as possible because in the past you may not have been able to encrypt proxy your eye I mean, if you're communicating through a proxy, because, well, the proxy needs to see it But if the proxy itself can understand those core, well, of course, the client could protect that option in the later between the client and the proxy So we also admit, of course, then explicitly that you can have nested oscore protection So it's not just about one layer of Oscar protection. You can have multiple layers and we call this oscar in oscore and for example, you can first protect end to end between your client and server and then you protect that for further between the client and proxy. So you actually have two layers. And typically we get two layers one end to end and one to the next top but it's possible to have more And the Vox has no score, but the same holds for group of score So what updates have we done for version two, where it was submitted now? before the cutoff, done a number of editorial things in terms of clarifications, updated references,"
  },
  {
    "startTime": "01:20:00",
    "text": "and some readability improvements One big bigger thing we did is be clarified the order of Oscar protections for outgoing requests. So what we had already was saying that as a source endpoint X, you focus use the security context shared with a destination application endpoint Y, so you first do the end-to-end encryption or protection. But we now are a bit more explicit on this and we say that well x application on endpoint Y. So you first do the end-to-end encryption or protection. But we now are a bit more explicit on this and we say that, well, X should apply the OS core layer for each proxy with which shares on OSCore security contracts and it should apply them in the same order as the proxies are deployed and the message intended to go through But so practically this means that if you have a client and a server and a chain of proxy, in the middle as a client, well, you first encrypt for the actual end endpoint server, but then you start from the proxy closed to your ultimate endpoint, and you encrypt with that proxies of course security context and then the next one next one, next one, until you reach the closest proxy to yourself Further updates, yeah, we revise the escalation of the co-op option protection, of course, keeping the same rationale that whenever it's possible to encrypt an option, of course maintaining correct functionality, will then you should include the option. And we also discovered that the previous algorithm we had had a bug which was supported by Chrysanomysus. And thanks a lot to him. And we also had that in an issue. And this is been addressed. So this bug was practically that as a side effect of the rules that we had the URI host and URI port options were encrypted between clients and server And we thought that was a good thing. However, there's a problem here, which is that if the client is communicating to reverse proxy, well first of all the client will typically not know that it's communicating to a reverse proxy. And now if you do"
  },
  {
    "startTime": "01:22:00",
    "text": "end up encrypting your I host and your I port options as a client well, now the recipient of that message which you may think it's a server but isn't actually a reverse proxy, well, it cannot decrypt those options and thus the reverse proxy cannot take forwarding this decisions based on, for instance, you are a host So this has been fixed and the algorithm has been amended to remove this problem So in this case now, the way the algorithm works is that the URI host and your report options will be encrypted in only a single case and that is the come together with the proxy scheme or proxy scheme number options and the intended consumer of the options is a forward proxy which the sender endpoint is aware is a forward proxy only in that case to encrypt so that solves this problem with with reverse proxies More updates um yeah we actually revised a lot the section 3.1, again, about the rule set for protecting co-op options. It used to be a set of kind of abstract properties to shape but now it's more written as like a sequence of steps and mirroring the state that again we're having a appendix B and the steps also have been refraised to reflect possibly presence of reverse proxies and we should send an end point may not know about so overall it it it's a more like rule based to do this and you do that kind of approach and mirroring again this big figure we have, which I will also show later in a slide We also simplify the algorithm because we could actually merge different separate cases we could merge them together in the same case And yeah, of course, the appendix B of showing this data diagram of the rule set was also up updated according to the revised algorithm And now if I go to the next slide, now you can see this. So this is something we have in Appendix B also as an ASCARP, but this is really the rule set and the decision process, decision-making process you go through"
  },
  {
    "startTime": "01:24:00",
    "text": "You have an outgoing message M starting in the top left corner, and it includes an option OPT And now how do you take a decision? on how to process this option? Whether it process it as its original class URI? or whether to actually escalate the encryption and process it as class E? And depending on the number of properties on who added the message, and what extent options it includes, you end up with either ending up in one of these green or orange circles and green is that you process in the class E and all orange you process it as it's original class U or I So this is really a figure of this rule set for option processing Yeah, some more updates we expanded section 7 about the co-head compression with Schick and the present there has been improved in terms of what step you should take for an outdoor inner SHIC decompression and we also update Appendix A with two new examples when it regards proxies are used. And of course these are examples of we also updated appendix a with two new examples when reverse proxies are used and of course these examples are aligned with the latest fixes to the rule set for option encryption. And so Appendix A6, where we will also called both between a client and server and also between the client and proxy it's a typical reverse proxy set basically we also have a bit more advanced use case in Appendix A7, where we have all-score actually between client server, client proxy, and also between the proxy and server. And in this case, we try to mirror that the, there is operates as reverse proxy similar to the lightweight KT, lightweight M2M gateway gateway gateway gateway gateway gateway, which about that also in section 2.4 and it will take forwarding decisions based on the URI path option And then we had another update with regards to the hop limit option. So the hope limit option is defined in RFC 8768"
  },
  {
    "startTime": "01:26:00",
    "text": "where basically it's, it's update which regards the hop limit option. So the hop limit option is defined in RFC 8768, where basically it's meant to prevent loops when co-opold are used. So you set an initial value and then each hop each proxy will decrease this value by one. And if you reach zero, you will return an error response basically not forward anymore. So what did we do here? And what's the problem? Well, the problem is that RFC that 8768 does not define those called class for the whole Limit option, and thus by the OSCO RFC, it should be handled as class E because that's a default behavior for non-classified options. And well, then you will run into this potential problem is that the origin client uses oscar with origin server and will protect this option end to end, but then you actually lose the whole point of this option because the point of the option is that it should be visible to proxies so you can still, of course, let the proxies end the option, but they will in such case ignore the actual intent of the client that put that option because they cannot read the value of that option has been encrypted. And also it's just extra over and going through the entire chain so what have we done in this draft Well, we now propose an update to RFC 80 encrypted, and also it's just extra overhead going through the entire chain. So what have we done in this draft? Well, we now propose an update to RFC 8768 to say that the limit option should be the first as class U for OSC And so when using the oplimit option, with normal OSC, you do not protect the home limit option end to end. And when using the HOPE limit option, with OSCore, as in this document, well, you will not protect the hope limit option end to end. However, you will end up protecting the HOPE limit option hop by hope And then we also had a related fix about IANA to end. However, you will end up protecting the hope limit option hop by hope. And then we also had a related fix about Ianna. Next steps, we want to look at additions of outdoor options after producing a corresponding encrypted inner options so basically having both outer and inner options, which sometimes happens, for instance, in the case of Observe. We want to handle multiple responses to a SM request if they're also protected by a proxy"
  },
  {
    "startTime": "01:28:00",
    "text": "This is also related to Corvost Corv Group. Extending security considerations adding more examples, and yeah any comments and views are very welcome Yeah, there was a comment from Christian in the chat. As I understand, is supporting the power to help limit There was also a comment from Karst Karsten, you want to elaborate on it? Yeah, again, this is taken to the list of questions but I'm just wondering whether the class you have limit option actually might disclose more than we want to disclose so we would have to think about specific attack scenarios in which this information might be substantially more useful than we think about right now Yeah, I mean, you're disclosing, but on the other hand, if the cloud chooses to have the hope limit option, it seems to me that it would want the actual proxies to see it, because otherwise, well, it will end up on the server and the server doesn't really have anything to cannot really act on it so but sure you you will um the actual proxies to see it because otherwise well it will end up on the server and the server doesn't really have anything to cannot really act on it so but sure you you will you will change the such that the orient client will not protect the open limit option while as things are today you would protect it So sure, there can be some kind of privacy concern or something we should be mention with relation to this yeah so for instance nested pairs of proxies might want to protect this information and not give a client that is using them the ability to circumvent that protection. So I think we just need to think about attack scenarios here, threat scenarios And yeah, unconditionally class you are maybe something that proxies make choose to ignore. But I will also say"
  },
  {
    "startTime": "01:30:00",
    "text": "that if you're using this document, then typically you would end up encrypting the option because following the rule set you would be able to encrypt the option hop by hope. So on the wire it would be a typical would be encrypted after all And Kirsten commenting on your comment at any rate, if one defines a class you option it has to provide security privacy considerations thinking about this thing So you just anticipated something that had to be done anyway anyway Yes, okay you yes okay i switch to the next presentation Yes I can ask for it and give control to you. Thank you You should have control. Perfect. So yeah so now I wanted to move on to this next draft, which is about the key update for OSCOR, also known as kudos so just to recap what this is about, it's a key update solution for OSCore, where you end up renewing your master's seed and master salt, and thus deriving new senderner and recipient keys And these may be needed in certain situations where you have reached policies for how long you should use a certain key material. And there's also separate draft on specific on key usage limits for OSCO, where after using a key a certain number of times, it may not be secure to use that key anymore and well you achieve perfect for a secrecy with the solution. You not end up changing the ID context value. It's agnostic of the key step establishment method you originally used. You could have started from pre-shared key information or ad hoc or something else. It's loosely inspired by the appendix B-2 false core, which also provides a key appra"
  },
  {
    "startTime": "01:32:00",
    "text": "solution. So this is part one and this is the current content of this draft. It used to actually have two other sections. The first being what they are mentioned, which is the AAD key usage limits for also OSCorp, which specifies, let's say, save number of times you may use a recipient or send their key in the context of OSCorp. This was split out as of March 2023 into its separate draft And it also used to contain a precision for updating the OSCorp and sender recipient IDs which may be decided for privacy reasons And this was also split out after discussion in March 2024 so it also lives in its own drive And yes, a little bit more recal You can see on the right hand side the message flow And fundamentally, it's built on exchange two nances, N1 and N2, between the page And these nonses are transported in new fields that we defined for the osophers co-op option and the nances and some other information is taken as input to the update CTX function and that basically takes your your old master secret and keying material the nances and then it will produce for you a new updated master secret And that's how you actually perform the key updating practice And here you can see also on the bottom left, the extended form of the oscar option and yeah then i wanted to go into some of the updates we've done for version 8 So the one, the first one being that we have a note now on when it's possible for the client to use the co-op no response option because the actual message flow in kudos can be clear flexible, so you may have, you send the kudos request and you get a kudos response which is a response to that exact request. But it doesn't have to have such a strong tie. So what may happen is that the kudos response, one is in fact not a direct response to kudos request 1, but it's just another"
  },
  {
    "startTime": "01:34:00",
    "text": "response to an unrelated request. And if the client knows that this is to be the case, well, it may safely use the copno response option in its kudos request one because that particular request does not require any response. The response will regardless come as response to a different request the client may have sent or maybe there's observations ongoing so there's a notification coming. We also have more detail now on avoiding potential problems when he updates start simultaneously, because it may be the case, the two pairs, they both initially and store kudos with each other in the exact same time so they send the kudos request one and each other and then both pairs will act as initiator in that kudos execution. And when they receive, the kudos request one day, they will start acting as responder for the other pairs kudos executions. Should we have really two executions? ongoing at once. And basically, we don't want to be both of these executions to finish because then you may end up in a in a um two executions ongoing at once. And basically, we don't want to be both of these executions to finish because then you may end up in a, in a, in a, in not a good state, basically. And how did you solve this way? Well, we simply say now in section 433 that if p1 is an initiator in a QDOS execution, E1, with P2, and P1 receives a first kudos message from P2 starting a kudos execution E2, so yet another second kudos execution ongoing at the same time but in that case p1 must have bought this new execution E2 and must reply to P2 with a co-op reset message to prevent that second execution from going ahead since one is already happening at this moment in time More updates Yeah, we clarified one thing which is that when you derive a new Oscar security context which we do in kudos, the notification number in that context need to be set as uninitialized. And this notification number is used for replay detection of observed a notification"
  },
  {
    "startTime": "01:36:00",
    "text": "It comes from the OSCO RFC and it holds as a value of the largest partial IV of received notifications for enough associated observer registration. But the key thing here, what we want to do is really clarify that when Qulus arrives in your context and notification number should be uninitialized to not risk any problems you shouldn't keep the previously held value for your starting security context and yeah, we did some editorial improvements and fixes, clarifications We did a large restructuring of the section about key update with forward secrecy which was quite long and now we split that into multiple subsections and we also restructure and moved some of the content around so it should be more readable now We updated, yeah, we got some feedback from Ianna about update the, sorry, this was not about IANA feedback. We, what we did is we requested to ask update the Oscar entry in the co-op option numbers registry with a reference to this document. And why did we do this? Well, because in the Coop Options Numbers Registry, entry for OSCore, we felt it makes sense because it's good to have a reference to this document because these documents is actually extending and changing the format of those core co-op options So it's good to have a reference there there We extended security considerations. There was an interesting paper published, which actually explicitly deals with Kudos. And you see the paper name there and also a link. So we have now in the security considerations refer to that paper and taking relevant material from that paper and summarizing that to include in the security consideration This was mostly about two things. One is that rekeying with a symmetric key exchange is not intended as like a full substitute for doing fMER ephemeral diffi-Hellman key exchange So because of that, it can be good that if peers are able, they should periodically"
  },
  {
    "startTime": "01:38:00",
    "text": "perform a key update based on ephemeral diffi-helming key exchange, for instance by running the end of protocol So you may have a situation where you do kudos for your regular key update, but then we update based on the femoral Diffelming key exchange, for instance, by running the end of protocol. So you may have a situation where you do kudos for your regular key update, but then with less frequency and less often, you actually perform an execution of ad doc We also have some discussion now on possible deadlock situations on servers It may be the case that a peer can is only a cooperation discussion now on possible deadlock situations on servers. It may be the case that up here can is only a co-op server so it cannot send any requests and if that kind of pair reaches its key user image for its OSCO recipient key, well, that means it shouldn't decrypt any incoming messages because that it will be using the recipient key and that means it cannot execute kudos as initiator because whenever the client is sending a request to the server, well, the server will be unable to decrypt this request and there is a solution to this, which is that, well, the server will still be able to run kudos if the client starts kudos using the forward message flow And we felt this was the best option rather than starting to add exceptions like you may read your key user limits, but you can still use the key and invariable particular situations but we felt that um this is a simple solution rather than adding and also a more secure solution Oh yeah, so then I wanted to go in is a simple solution rather than adding and also a more secure solution. Oh yeah, so then I wanted to go into, like I mentioned, this document also contained this part about updating OSCorp send recipient IDs. So I wanted to go into some updates. It happened to that document also Just as a recap, what this was about. Well, it's about a procedure for updating sender recipient IDs for Allscore. And you can actually embed the procedure into an execution of kudos and you can also run it standalone It can be initiated by either client or the server. So both can take the initiative and on the right hand"
  },
  {
    "startTime": "01:40:00",
    "text": "side there you see a new co-op option we defined that is used for the peers to convey their desired new recipient ID to each other and the way it works is that a peer will send a message indicating the new recipient ID it wants to have in this recipient ID option which is Class E. Both pairs of course have to opt in and agree that this update is supposed to happen. Now that when you change ID, ideas, practically what will happen is because the ideas are part of the Oscar security context derivation, when you share ideas you will derive new oscar security context and that's also get new key material um there's some security considerations around you shouldn't do this immediate following a reboot. In such case, it's recommended to use Kudos first. And there's also some restrictions to kind of avoid reuse of recipient IDs that you need to be careful about not using the same recipient ID that you previously used for a particular triple off master secret master sultan ID context And we also have examples in 211 and 212 of this draft showing the exact message flow back and forth but fundamentally it's it's fairly basic it's a request with a decide recipient ID of the client, a response including the desired recipient ID of the server What updates we do? We specify now that there's an ID option may be empty because in all the core it's completely fine to have the empty byte string as your sender or recipient ID, and that's this option can be empty. And if it's empty, we clarify that option value shall be empty meaning option length zero we expanded a bit on failure cases when you're running this procedure together integrated with the kudos execution. It may be the case that the kudos procedure actually succeeds, while the ideal update procedure fails. And this could be for much"
  },
  {
    "startTime": "01:42:00",
    "text": "multiple reasons, for instance, maybe one peer doesn't want to change up change IDs. If this happens, well, since Kuto succeeded, you should continue using this new new derived ctX new that Quito's provided to you and you simply keep using the old sender and recipient ID Now, on the other hand, if those criteria succeeds, while the Kudos procedure fails, well, then you stay on your ctX old that you had before in initiating kudos, so your previous security con, you had without updating it. But of course, since the adopted procedure succeeded, you can proceed with using the new system and recipient IDs that you decided to use. We also have another part about enforcing the maximum length of the recipient IDs. So we added this as a time to use. We also have another part about enforcing the maximum length of the recipient IDs. So we added this as a failure case, basically the length of the received recipient ID option. If that exceeds, the maximum length of the OSCorp sender and recipient ID, which is based on the actually in the option, if that exceeds the maximum length of the OSCOR sender and recipient ID, which is based on the actually depends on the OSCore AD algorithm of use, in such case the procedure should be considered to have failed because you cannot switch to a recipient ID that is too long and impossible to use with OSCorp and specifically the recipient ID option must not exceed the length of the AAD nuns minus 6. And this comes from OSCore and that's basically based on on like, details on, for instance, how the OSCorp partially is calculated, but is a restriction inherited from OSCore yeah so what do we want to do moving forward well okay i can mention brief on the topic of the key usage limits document We did submit a new version three without any major changes Mostly we're monitoring any updates to this CFRG ADAD limits draft, which is kind of the draft that we're inheriting most of the information and then apply it specifically to OSCorp and we're also waiting for possible feedback because we've heard that there may be some people planning to give feedback on this in the future"
  },
  {
    "startTime": "01:44:00",
    "text": "Another big next step is we got a review from Chris and Amsius on the QD kudos documents, so very much. Thank you for that and you can see the where it's archived there in a mail So we plan to start going through that. And that will be probably the biggest goal until the next version We also have now produced an implementation of kudos in Java supporting their forward message flow. And we also have a quite mature implementation in C for the Conti K&G operating system, which is an operating system for constrained devices And this implementation currently supports the server side, so only acting as a server in the forward message flow. But that's ongoing work and it will be updated to support more features And yeah, as always, any comments? or reviews or questions are very welcome So thank you all for listening I have a question about your tIABreak process. Usually when you need tie-breaking in a protocol, you try to make sure that both sides arrive at the same way forward So if two peers start Kudos update, and exactly the same, time, they will both receive the update request from the other side at a time when they have sent there request. And so they were both abort the other guy's update request And then you're back at square one and everybody will again send an update request and they will collide again and go up in smoke. Yeah maybe having a tIABreak that has this same result who wins at the twins would be better, like hashing the requests"
  },
  {
    "startTime": "01:46:00",
    "text": "and looking at the least significant bit of the hash and decide which of the two ones wins or something like that We make, I think, we wanted to have a kind of a simple solution and our plan here is that what it is basically fine if both of the Kudos executions are aborted because it's very unlikely that the next, that this exactly collision will happen again. But I'm sure there are more like I said, there can be more advanced solutions with tIABreakers and such but we, we felt it was just, it's not interesting more complexity into the solution overall to have this very basic solution of, okay, yeah, abort and sure, both may be aborted, but in such case, you can hope that then the x this is a very unlikely collision to happen in the first place that they started exactly the same time Yeah, but even if you have an unlikely case, that gives you a life log in the end, that's not a good situation And I would expect that the sites that receive an abort are going to try again right away. Unless you have something that prevents that, from doing that, it's no longer unlikely that this happens We may, yeah, we may consider your solution or something like if you get a reset you shouldn't you should you should and you shouldn't immediately retry, you know There is a concrete solution for that very topic in my review that basically makes even a allows a simultaneous simultaneously process to complete by having by making sure that the operations commute Nice, thank you haven't managed to process your review yet, but that sounds very problem then. Thank you Thank you. Any more comment, question? Anyone? And just note in the chat that Carson appreciated the reference to John's paper"
  },
  {
    "startTime": "01:48:00",
    "text": "by the way, on the security considerations Okay, if there are no more questions thanks Richard again. And next and last in line is Carlos So, hello, everyone My name is carles gomez, and I'm going to present two drafts which are related, the updated versions of these drafts And by the way, thanks a lot to everyone who has provided comments on both drafts. Actually, it's very good feedback. Thanks So the first document is entitled copy in space My co-author here is sergio aguilar from Satelliot Next, please So this is a summary of the updates. First of all, we have extended the scope, which originally was about deep space. However, now the scope is spatial environments characterized by long delays and intermit communication opportunities. So this now includes also some Leo satellite-based scenario such as those composed of sparse satellite constellations where the satellites offer discontinuous coverage and then they require storm forward support as well and by the way there are extensions that are being standardized by the three GPP to support this kind of scenario in release 19 then we've added the section on caching and we experience there that the max h age value needs to be set according to the expect latency of the scenario, the intended scenario Of course, as long as it makes sense to consider, the response is still fresh after that max age time. Fortunately, the possible values that we can represent with the existing specific"
  },
  {
    "startTime": "01:50:00",
    "text": "for max age is up to 136 years, so hopefully this should cover our current requirements for delay tolerant applications then in section five, which was already existing on Observe, we have added that at the time between the two last notifications received is greater than 120 seconds by default the last one is also considered the latest one sent by the server, so this may be relevant to some applications And now we've added that in delayed tolerant environments the duration of 128 seconds may need to be chosen greater than that amount of time. Actually, 128 seconds was chosen to be greater than the max latency, which by default in Co-Up is 100 seconds. But now in deep space, for instance, max latency may have to be increased by one or two orders of magnitude Next please We have also added the section on co-op group communication and among others, there is, well, one parameter which is the minimum token reuse time which with the default co-op setting, and the setting for another parameter which is in the current co-op group community draft, the total time for minimum token reuse time by default would be 500 seconds. So again, in delayed tolerant environment, this would need to be set according to the expected latency of the scenario and again in deep space it would need to be increased by maybe at least one or two orders of magnitude depending on it each specific case then in section eight about security we have added that group of score protocol is used to secure group communication and also about oscore I think this was a"
  },
  {
    "startTime": "01:52:00",
    "text": "comment from christian kuhtz oscar provides this protection against replay attack and by default, it uses an anti-replace lighting window with a window size of 32 So if a greater window size is needed, because we are using co-op in a scenario with much higher latency, then the window size needs to be known by both endpoints at the moment of security context establishment Next please Yeah, so those were the main updates in the first draft. The other one is co-op over- bundle protocol, also at version 01 here And my co-author in this draft is Anna Calveras, also, well, we are both from UPC next please so we have added a section in this draft on the encapsulating bundle, the bundle that I encapsulates a co-up message so here we provide details about that bundle So the first item was already present in the document, but in a different section So what's new is the other bullets The second one is that the lifetime field of the encapsulating bundle, in our draft, we stay that it must be set to exchange lifetime for co-up con messages or to non-lifetime for co-op non-messages Of course, with those lifetimes set according to the scenario, then also because the bundle protocol doesn't have the notion of what is a response we have added some simple logic about it and we state that the destination endpoint ID of the bundle encapsulating co-op response would be the source note ID of the sender of the bundle that carries the co-op message that triggers that responds and vice versa And also we tentatively add some texts like the"
  },
  {
    "startTime": "01:54:00",
    "text": "co-op aggregation, well, the aggregation of co-b messages may be done, several messages may be aggregated in the same bundle. So there was some recent feedback on this topic by Carson on the list that perhaps we can do that by using the RFC a 3 to 3.3 message format. So yeah, we're going to use that in the next version of the draft Next please Then in section nine it's about the URI scheme. You may recall that in the previous version of the draft, we attempt to create a new URI scheme for cooperation the URI scheme. You may recall that in the previous version of the draft, we attempted to create a new URI scheme for Co-op over BP, something like Co-Up Plus BP However, after receiving the feedback from this working group, now we state that the URI scheme for co-over-P is co-op. Actually, we follow the recommendations, the guidance in the transport indication draft. So thanks a lot because it's very useful. And also to create the authority component of the URI now we request the creation of the new reserved domains in the dot ARPA namespace which would lead to the suffixes suffixes.dddn.arpa and dot IPn.arpa So also in the draft, we have the full domain name reservation considered in the IANA consideration section following the guidance of RFC 676 And here you can see a couple of examples in this case which would be the URI of the discovery resource when using co-op over BP. So yeah, by the way, in BP there are like two types of endpoint IDs. One which uses the DTM, colon slash and then name And the other one which uses IPN colon, and then the note number"
  },
  {
    "startTime": "01:56:00",
    "text": "the limiter which is a dot and then the service number So, yeah, you can see the examples how that you arrive discovery resource would be for each type of endpoint ID. And by the way, there is also some comment here about so what you can see currently is the content of the draft, but we're going to change one of the two transformations here for the next update of the draft actually for the IPN based URI, we would need to change the order of the service number and the note number. So instead of co-op colon slash-81.2 it should be the other way round to because, well, this was some comment by Karsten and yeah, probably that makes more sense than the existing proposal here to keep the hierarchy and the order in the authority component of the URI So next please Then we have added a section on securing co-op of a BP. First we explained that the co-op-based specification defines a binding to the etella please. Then we have added a section on securing co-op over BP. First, we explained that the co-op-based specification defines a binding to DTLS, and then also there is OSCOR, which can be used to provide end-to-end application layer payload protection and one interesting feature of OSCOR is that it's based on a shared security context which may be based on pre-shared materials, which avoids initial handshake So this is, well, one problem that DTLS has and involves some performance penalty, especially in these environments where latency is high. So this is why in the draft we state that for co-op over BP use of DTLS is not recommended And on the other hand, we explain that BP SEC provides security services for BP, it provides integrity and or confidentiality for one or more blocks of a bundle. And on the other hand,"
  },
  {
    "startTime": "01:58:00",
    "text": "Oskore protects with confidentiality and integrity, the co-op message payload, one co-op message has for one or more blocks of a bundle. And on the other hand, or score protects with confidentiality and integrity, the co-op message payload, one co-op message header field and also some co-op options So we need to develop a bit more this section, and in particular, since it's possible to have security at two layers, okay should we use both at the same time or maybe only one? in which cases? So yeah, we need to develop this and also any input you may have on the point is very much welcome And next please. So finally this is like a summary of some point, some item that triggered some comments you may recall that in this draft we suggest somewhat tentative to increase the co-op message ID size from 60 bits to 24 bits. The reason is trying to avoid the limitation that might be a bit severe on the message rate for a co-op endpoint, especially when you have very long delays and therefore a very long exchange lifetime However, there are some cons such as an additional bite of header overhead and also somewhat increased memory requirements for the endpoints to keep track of the message IDs used. So, I don't know if there are be opinions on this or say strong opinions like to decide whether it makes sense to continue suggesting the use of 24 bits for the message ID. And yeah, that's actually the end of my presentation Thank you, Karnas I can tell you, christian hopps three questions prepared in the notes. In the interest of time, maybe Chris questions prepared in the notes. In the interest of time, maybe, Christian, do you want to pick up one of those and then follow up on the list? I think the most prominent there is probably in the in the space IP meeting, I've heard a lot of discussion of very high bandwidth links"
  },
  {
    "startTime": "02:00:00",
    "text": "It might be worth considering when you might be her co-proxies high bandwidth links it might be worth considering whether there might be whether you might be her co-proxies in between and then use if those links support quick already use that and solve other problems like having those extreme round trip times between between the pairs that share message IDs or that would need extra extra considerations for the tokens because then those tokens are like segmented to a particular region in space okay thank you by the way one comment is that I think there are different types of links being used or proposed for using NipSpace, for instance. Some of them are very low bit rates others maybe a much higher bid rate So, but yeah, anyway, yeah, of course would you mention applies. Thank you Bill? you have a quick question Hi, it's Bill Silverajana I'm aware that we are running out of time, but I had a couple of comments Just bear with me. So actually, I think my comments basically are mostly for the co-op in space draft and not so much on the BP but something that you mentioned the BP draft also kind of resonated, but just going to back to the space draft so there was something about use of observe, and then you're recommending the later time. So I'd suggest that you look also in the conditional attributes draft which discusses P-min, P-max, E-P-P-Mex for that And perhaps also P-Sup, that might help something. So coming back to the question about BP, so we did a lot of work also on co-op, running it over low Earth orbits satellites and geosynchronous satellites, and we did a lot of work"
  },
  {
    "startTime": "02:02:00",
    "text": "on this kind of delay tolerant co-app. And I kind of echo the same thing that you mentioned that, that right orbit satellites and geosynchroned satellites, and we did a lot of work on this kind of delay-tolerant AquaWAP. And I kind of echo the same thing that you mentioned, that running it over DTLS is not really recommended but that's not specific to BP It's specific to any sort of transport that that has very high latency so something that we can discuss of offline afterwards, but yeah, I can share my experiences with what you have So perhaps that should be put in the space draft as well or somewhere else Thank you so much Thanks, Bill And Bill and Kristen especially, but everyone, please follow up on the list with more comments for for canvas Okay, thank you. We are two minutes over time. Thank you very much for the good meeting your participation and contribution. See you at in you. We are two minutes over time. Thank you very much for the good meeting, your participation and contribution. See you at the interim meetings and in Dublin, November, hopefully Thank you all. Thanks to the note take all bye"
  }
]
