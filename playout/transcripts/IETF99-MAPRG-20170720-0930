[
  {
    "startTime": "00:00:04",
    "text": "I ever ascribe someone sign in the Javan already you\u0027re willing to do it her okay Aaron parks can ascribe for Jabbar map map Ford is doing the note-taking for her minutes and we\u0027re going to start on time because we have a tight schedule today well good morning everyone I\u0027m Dave Wonka this is Mary Akula bin or the co-chairs of the measurement and analysis for protocols research group and the the group well let\u0027s start off that note well the new note will slide no well is about we are in the hi RTF we use the same intellectual property rules at the IETF so instead of this thing you should have seen this before you means that if you\u0027re going to share information here that it\u0027s governed by the rules these RFC so look at that if you get a chance or if you\u0027re going to share things in an IRT fr IETF meeting in ministry via the Charter for a group is up here I\u0027ll give you a short overview what the Charter is about measurement analysis for protocols research group is meant to be literally that measurement analysis but to serve the protocols in the operation of those protocols in the IETF so we\u0027re trying to bring the research community of which some of you are in the IETF community of what some of you are as well together to show you what some of the latest measurement data about real protocols and hope to hopefully solve some of the either design problems new architectural ideas and potentially deal with performance issues that we observe operationally we have a mailing list of course and the links are here the slides are up for today all the slides for each of the presentations are linked and we can join remotely from by a via these audio and medical links and general link is there as well so the agenda for today is we\u0027ve got two and a half hours it\u0027s almost completely full so we\u0027ll start off with once we go through here to project advertisements we invite you if you have a product that\u0027s project that\u0027s related to this but we don\u0027t have either content enough for time enough in the meeting to to present it will plug it by and the advertisement there then Michael Abramson will come talk to us for five to ten minutes about identifying ipv6 reluctant devices and hosts and then we move on to fingerprint based detection of DNS hijacking hijacks using ripe Atlas by Pappa rimsky and then a presentation by Pablo Alvarez about his experience with rate limiting of ICMP error messages and v6 trace routes so basically an example something operationally that\u0027s different between v4 and v6 and what we can do to mitigate that then I\u0027m going to present not in a chair roll but some recent measurement work that I\u0027ve been doing in a way to hopefully improve the way ipv6 address anonymization works again something that\u0027s different than the way that it might have worked in before and then "
  },
  {
    "startTime": "00:03:04",
    "text": "we\u0027ll move on to Toki talking about measuring latency variation in the Internet how deep are our queues in in bottleneck routers and such and then we\u0027ll move on to measuring YouTube override re6 with vibe Ave and lastly Jana from from Google we\u0027ve invited him to come and present some soon to be published work about internet scale deployment of quick so we\u0027ve got four things there are the six topics which is one of the directions that I want to go but that\u0027s our agenda for all and then Miriah suggested or we wanted to share with you a little bit about how we did the agenda this time the group is meaning to be reporting on nascent measurement results that would help IETF our efforts but we\u0027re also trying to do some proactive things and seeing if we can call you guys - how about so there\u0027s one item this time that\u0027s not a report of measurements it\u0027s basically a call to arms recall to help and that\u0027s the first presentation we didn\u0027t solicit for presentations this time because there was so much interest and people sent in to us unsolicited so if you have thoughts about that share them on the lists share them with us or if we have time at the end you can comment in front of the audience here but right now this is an example of one that I guess Mary and I curated the agenda this time so let us know if that works for you if you think we\u0027re doing a good job or a bad job here so let\u0027s move on to the two project advertisements the first this was from Matthias Wallisch and he\u0027s got at this URL he\u0027s got an electronic reprint of a nice paper about measuring our PKI infrastructure and determining basically this is a technology right developed in the IETF and measuring whether or not whether or not and how it\u0027s being used and not and two key observations they had is that the existing methodology from the literature was which was meaning to determine completely passively whether the RPG I was being used to filter they found some problems with that and improved upon it by introducing an active component to detect it and detected a number of ASNs that are that are indeed using the rpki to do this kind of route validation so if you want willing to the paper that those authors contact information is in there and then the second ad is from Stephen Stroh\u0027s who\u0027s with write and Stephen is in the blue shirt ripe shirt here in the front what he\u0027s wants to tell us is that ripe as is batching up daily aggregations of their measurements so for measurement researchers you can just go and plug this stuff and and much more conveniently get access to it and use it so if you have questions about that fine find Stephen or write to immense they have no self and at the bottom of the slide there mariya anything else you want to share now the blue sheets we\u0027re going to circulate those two of them looks like we\u0027ve got quite a few people again these solutions "
  },
  {
    "startTime": "00:06:06",
    "text": "hello and so I didn\u0027t send you a slide but why are you setting up so we have this ere not W workshop on Saturday for the second time and we are planning currently so unless I get on part of the steering committee of that workshop so we\u0027re currently flying to have it next summer in what we thought was San Francisco and it\u0027s now Montreal um so there\u0027ll be a call for papers and stuff so in the W hand sort of it\u0027s trying to attract apply to work on a lot of applied work has a lot of measurement in it this year we unfortunately were not really that aware that we badly conflicted with the imc deadlines we\u0027re trying to avoid that next year but keep that in the back of your mind if you have something that you think is apply it such that you know maybe it\u0027s struggling a little bit very scientifically oriented when you the ADA W specifically wants to sort of bridge this gap between practitioners and and academics um and if you are like a North American academic that it\u0027s doing good work and would like to be considered for PC chair let us know we\u0027re looking I\u0027m Lars can I ask you something where would we where do you want us to send comments and questions or ideas about nrw I think um I RTF discuss would be the home for the workshop doesn\u0027t happen zone list and under the sink I don\u0027t think we have something set up so I think the IETF discussed I\u0027m volunteering and sitting next to Ellis and she\u0027s not hitting me he would probably be a good place to discuss that all right thank you so we start with our first talk all right like presentation or call for help go ahead Mike thank you hello my name is michael ranson i work for georgia telecom so I\u0027ve been involved in ipv6 enablement there and at other places for quite a long time I\u0027ve been interacting with a lot of other operators are doing the same and why they do it and so on is we\u0027ve identified a potential future problem that we would like to measure and we try to find out we could come up with a name for this and this is why it doesn\u0027t something new so kinesics if it\u0027s available so if the ISP has done its homework and it\u0027s not what it is it\u0027s an able the home or whatever and there is now not the devices they were not using IP divide basic connectivity installs running its trafficker or ipv4 so there are a few reasons content is not available or maybe six this is a given that can probably it\u0027s going to be fixed over time so it\u0027s not that\u0027s not the immediate concern they the larger concern is that the device or operating system is not actually six capable because this is something that might not get fixed it might reside on a device that where the device and the operating system supports ipv6 but the application is done is using API is that don\u0027t support "
  },
  {
    "startTime": "00:09:06",
    "text": "ipv6 over actually the content is referenced using ipv4 literals to be hard-coded in the in the application or that\u0027s what sent to it the last one can probably be fixed but if it\u0027s hard-coded and the application is not updated then this is a problem approach one more of the traffic over ipv6 because running to both v4 and v6 at the same time increases complexity and going forward there\u0027s going to be more very great NAT and so on the these boxes are expensive both from a capex and OPEX point of view and the end goal that we will want to reach is to not have to support that before at all and just have a single address family or everything this works so there so there is some there is a worry that both historically and going forward there are new devices and and applications or so on being put into the network that does not the do not support ipv6 so my smart TV from 2012 perhaps that\u0027s like okay it\u0027s too old is never going to get updated it\u0027s the operating system is a band at that point that\u0027s not gonna that\u0027s never gonna get that music the problem is that there were still devices in 2020 being put into the network that does not support ipv6 that is a much bigger problem um anyway we would like to so this is this is the concern that even though the ISPs are now doing their part application menu what\u0027s an application vendor anyway epilation manufacturer a device manufacturer is not doing their part here and the this is going to increase the complexity and Indian potentially cause problems for because there\u0027s a lot of concern with ipv4 quality going down because prefix hijacking because of lack of addresses and all this might cause a demise that you just use ipv4 to have horse user experience than one that is using a physics so the ask to em a map or G not orgy okay is is there a way to identify this the this is possible to figure out if a device is sitting on dual stack connectivity but it\u0027s not using it and then came being fingerprinted can we figure out why can we measure this over time because I think a lot of ISPs would like to keep the ipv4 traffic at least in absolute terms not increasing we were likely to decrease but at least if it doesn\u0027t increase we don\u0027t have to do new investment in devices for to support "
  },
  {
    "startTime": "00:12:06",
    "text": "that v4 in the network so and so then we would like to measure this over time and and also if you can identify which are these devices and applications we can talk to the their manufacturers and try to lobby them or whatever to convince them in order to enable ipv6 on these devices applications content networks whatever so that\u0027s that\u0027s the ask so so Michael and I had this conversation and he\u0027s on the behalf of the chairs I said this is something we\u0027re interested in I think this is a higher bar that we should set for ourselves instead of giving reports even of nascent measurement report can we proactively do something someone else in the IETF comes to us with so we have a few minutes to talk about it now if people have ideas about observation points where we could see this how we how we might collect it without doing finger-pointing but be you know put it in a positive vein I\u0027d love to hear that bread so a question Michael um and I got this the other day from NASA a gentleman over there that operates I think G route or something like that it was looking at ipv6 deployment in some of the issues in it and one would be related he was suggesting that we go for interoperability test where we can prove that devices actually do work ipv6 only and publicize them so that people that are like your friends here might get the message in a positive manner would that kind of a thing be useful to you so there\u0027s been I think this is like for home gay to us ahead know like a goodie six ready but this would be for like applications or devices or this would be for really any kind of device that that damn one runs and Cisco is working with a number of its customers on ipv6 only deployment and you know the number of networks are working at and I\u0027m sure they\u0027re working with all their friends so you\u0027re kind of extending this to the reach of the home or other places yeah so it would the whole thing that Apple did with that they you can run nat64 you know can test that on your macbook lucky if you connected it but you can create a Wi-Fi this ipv6 only that\u0027s one way of testing this I guess so you you could say to any manufacturer or something like how does this work at least then you know that the device and the application can speak ipv6 and that would be then if the content is it becomes available pv6 it will still work so if we could just spread that approach into like more places that could definitely be a warrant way hi Tim owners from UNH Iowa so you know we\u0027re heavily involved with a ready logo program that obviously covers you know "
  },
  {
    "startTime": "00:15:06",
    "text": "stacks on devices right there\u0027s 1500 devices on that list that showed that they have ipv6 packs obviously that doesn\u0027t cover things like as you\u0027ve noticed or thought a smart TV on that list so it it does some coverage but it doesn\u0027t cover everything and the other thing that I\u0027ve been hearing from people is a lot of times at testing you know we can configure the device to turn on musics but it doesn\u0027t mean it has it on by default right and for some people that\u0027s a killer I think that\u0027s something the program might look at changing in the future saying that it\u0027s a requirement of getting the logo that you turn it on by default I think that\u0027s something that this group wanted to see that something we could push and that side that committee but right now the way it stands is basically they can configure it from the testing we do at the University we\u0027ve seen a lot more application testing in the last year in v6 only environments so we\u0027re getting a lot more vendors who build applications wanting to verify that so we\u0027re starting to see a movement in this direction there\u0027s no public list at the moment but we are seeing then there\u0027s one off come in and want to test those kinds of environments I would love if the it\u0027s a program required our basics would be default all not so that the user didn\u0027t have to do anything in order to get this working because yeah there\u0027s no reason why they don\u0027t want to do that yeah so there\u0027s a little bit of history here with this you know in some cases it would cause users maybe today is to be a different story but at the time would be kicked feedback we got was that people don\u0027t always want it on all the time I think that world has changed I mean we made that rule a while ago I think it refunds should revisit it so I\u0027ll take that action item from today and come back and see what I can get to live oh great oh that would be great because I mean I typically say when people are saying that the market isn\u0027t requiring ipv6 I usually say the market doesn\u0027t require ipv4 either it typically want Internet access the service they want is not ipv6 or v4 it\u0027s Internet and today it\u0027s both so the in order to have proper I\u0027m only connect to the internet support you need both stacks and it needs to be default on so this is a comment to the previous speaker I actually have a brief talk this afternoon and v6 ops that would be directly related to that I talked about what I had to go through to turn on ipv6 in my home router and you didn\u0027t want to go there thanks Brett so I think we cover it and we in the ten minutes what Michael and I wanted to convey is an invitation to participate this participating this or an invitation to share idea it\u0027s not how we would go about this measurement study and I guess the challenge I want to raise to the group is to say by the next time we meet which was likely in four months can we report then if we have something substantial what it would be and it\u0027s not really asking time of the the research group so it doesn\u0027t even approve all other than we can the ideas will approach it the mailing list and and if we have something to share by then we can but with this community we should be able to have something by then at least a set of ideas or a roadmap for about how we might do such measurements "
  },
  {
    "startTime": "00:18:06",
    "text": "and and how what value we can bring the community by sharing it thanks Michael yeah thank you okay we go on with our second talk problem over there and this is on fingerprint based detection of Kim s hijacking and as many people use today he also used to write bedless platform so good morning everyone my name is Paulo forum ski I work for five site security and I\u0027m a PhD student with the Polish Academy of Sciences today I\u0027m here to present you my work done with my colleague much energy in ski of masks the work was started during the recent ripe ncc DNS measurement hackathon and it\u0027s called fingerprint based detection of Venus hijacks using gravitas so I\u0027m sure that most of you know what DNS hijacking is but just as an introduction a situation in which you think for example that Google eight eight eight eight is responding to your DNS queries but in fact somewhat in the middle is hijacking the queries and responding with different data but it may also provide with device data but some kind of surveillance is happening and it\u0027s you don\u0027t know about this and so what\u0027s right at last thing is a measurement platform constants consisting of 10,000 and the three news boxes distributed around the world is run by a ripe ncc which provides a common a central API to start measurements like being trace routes things like that and collect the results back so what was our research idea for this topic well we thought about that we can send select DNS queries from each of the probe to select target IP address of the resolver and our hope here is that we would be able to spot differences in the replies if the server was hijacked so we rewrite the replies that we collect at a single probe as feature vector this is the fingerprinting stuff and finally we use machine learning to check if the fingerprint matches the model that\u0027s the detection part for this research we focus on Google DNS and Open DNS because these are the two biggest providers of Public DNS server service but the methodology is generic so you can adapt it quite easily to other operators and they wanted to know first of all how prevent hijacking is in different perspectives which are the most risky aass and finally what does it all mean to do the Internet well the features that we use I mean the queries were a bit limited by the ripe Atlas API which provides quite a restricted received an API that\u0027s for "
  },
  {
    "startTime": "00:21:06",
    "text": "purpose that\u0027s all security of the volunteers who run the probes that\u0027s good so we weren\u0027t able actually to do some low-level bit flipping stuff but we had access to the unis replies in wire format and also the platform entrance timing quite well so that was useful so we started with chaos takes the queries these are kind of queries a card class of DNS queries that lets you to britain to actually query for the state of the server rather than fetch some data from the dns we used the free most but we are queries hosting the bind person by an ID server which should give you things like the cost name of the server the software version but we also know that for example Google and Open DNS don\u0027t answer these queries so if we see a reply that\u0027s indicator that something bad is happening happening for the second set of features we check things like dinner sex support like wearing for Dinah\u0027s sec - facebook org which have which has invalid DNS SEC signatures for it should fail we are so quiet for being as key for dot DL which shouldn\u0027t fire I mean if some someone is not doing something bad like stripping off all of the DNS SEC from the DNS we also check for ipv6 support by querying for the 40 qualified domain name invitees resolvable only the resolver has full v6 connectivity which acts TCP support which occupies two non existent domains if we get got a successful reply we know that the IP address ASN of network and finally we check the unilateral case insensitivity finally we also did some more network testing I mean not really DNS strictly DNS related like running ping requests and measuring the trade routes and here we start hop count s Outlands and parameters of the exit areas that is the exits just before Google or Open DNS finally we is we use to the Enescu mi service services the first the most famous I would say when Mike amide would come who am i service is a DNS authoritative server that should reply with the IP address of the resolver I mean the non anycast public IP address but because the first service is so popular we also implemented the same service on our own domain hopefully that it is not well known that it\u0027s not filtered out for example some people could argue that the last feature is enough to deter hijacking but we actually found us resolvers in the wild that although they returned by its IP addresses that is belonging to Google or Open DNS they were hijacked for example they were returning IP addresses to non-existent domains so this feature is not enough all right as to the measurements we run them not even a month ago at the end of June this year "
  },
  {
    "startTime": "00:24:08",
    "text": "from 10000 probes from 3,000 a SS and we burned some few million ripe Atlas credits thanks to the standstill and for that and we published all of the tools and all of the data on github all everything of this project is you can use it Hackett\u0027s anything and here are the links to spreadsheet Google Spreadsheets for the raw data from Google Open DNS just to give you a glimpse of the data we collected here on the left-hand side you see the latency the histogram of latency to Google resolvers on the right hand side you see the hop count for Open DNS when we compare these to the MIDI on latency is a bit higher but the hop count is a bit less I mean the median is business all right what about the grunt roof granted is an information that is this fingerprint really a fingerprint of a legitimate server or not there is no way to obtain this from network operators because usually when you do such stuff you want to hide and also the number of networks was so so big that it would take us ages probably to contact every operator so we had to make make some assumptions so we assumed that the most common fingerprint collected around the world is the legitimate one so any deviations in this some of the features I mean 7 most obvious features like replying to non-existent domains is a clear indication that the server was hijacked and finally we use a machine learning classifier that hopefully will learn how to use all of the features that is more than forty of them however we still work on a better branch of methods finally much learning classification we compared three different machine learning algorithm we have very good first times each other and to get some good estimates of the performance by starting with just KNN with three neighbors we had something like eighty percent of accuracy and about at least a few percent of false positives so moving on through this decision tree the decision trees to random forests consisting of ten trees we obtained 93% of accuracy with close to zero false positives still 6% of false negatives which means that our method is more likely to means a hijack than to label good resolver with the hijack label so finally we classified the rest of the data using the random forest specifier and again implementation is on in github all right here are the results on this map the color encodes number of ripe Atlas probes that we believe that have very DNS Google DNS resolver hijacked as you can see the rashes on the top but if "
  },
  {
    "startTime": "00:27:11",
    "text": "we divide number of probes with hydrogenous by the number of probes within this country we get such a map where the color encodes the probability of the dinners being hijacked and the countries that are really leading this are madagascar Iraq Indonesia and China however some countries have quite small number of ripe Atlas probes still so we filter it out the country so that we left only those with more than 10 probes here Indonesian Indian Russia her the top and the on the other side Canada Austrian Norway are the country\u0027s most probes and as far as I remember zero probes had very DNS resolvers hijacked these are the reasons for Open DNS we found ninety-four probes we hide our DNS again converting them to probabilities are like the following and again filtering out a country of less than ten probes gives us such a result so again Indonesia Indian iron our ads have the most MOS transistor of DNS being hijacked switching the perspective to a SS rubberband countries it was very interesting that we found a SS with 80 to 100 percent of the probes having the DNS hijack for Google that was also the same for open dinners maybe the SS SS were a bit different however we found that when we compared the probe IDs with the Google resolver hijack with the open leanness we found that the most likely situation is which the both both of the resolvers will be hijacked we found six to eight probes with such features so finally we asked what are the most risky ISS or put it differently which are the kind of systemic hijackers I mean the ones who do this not by incident by but by purpose so we took the probes with both resolvers Heydrich we drove a SS with less than three probes we with hijack dinners that is we try to drop incidental observations or just errors of our classifier and here are just the top three results Telecom Indonesia about the following time in British telecommunications which don\u0027t belong to some oppressive government which shows that the inners hijacking is versus that not necessarily used for censorship but it may be also use it used for different purposes I don\u0027t know like making money out of I don\u0027t know some advertisements for example so just we should be aware of that that\u0027s there are different motivations for that so conclusions DNS hijacking is real of a "
  },
  {
    "startTime": "00:30:13",
    "text": "real thing happening on the internet I think that\u0027s most of you know this but we provide the data we did an open-source implementation of that so anyone can build upon our work there are some countries in the world that have more than 25 percent or even 40 percent of chances of the DNS being hijacked on average that\u0027s more more than one percent there is no necessary come from a government or from a state actor I would say same same some races have like a policy of doing so so many hide legs were in developed countries and finally there is no big difference for Google and open dinner so if you found that your google dns resolver was hijacked on network just switching the IP probably will not help you so definitely we we need a more secure internet more secure protocols between the stub resolver and the recursive resolver so I\u0027m very happy on the deprived working group taking place here or so hoping we\u0027ll have more security nests soon for future work obviously we missed ipv6 I mean querying the resolvers over ipv6 just due to time we plan this we want a better grant of methods and we also actually want to allow the data returned from hijack resolvers like where are the IP addresses for non-existent domains in the world we found something like dentistry I mean probe ID in Indonesia that was redirecting non-existent domains to China which was very interesting for us however we still need some more time and hopefully finally we\u0027ll publish a paper thanks very much that\u0027s all I had I\u0027m happy to take questions thank you very much and we already have questions and more people are coming up so I\u0027m trust Aaron Faulk Akamai I want to encourage you to publish this as some winner so he can share the link to your work this is very interesting work I two questions one I was surprised to see that round trip time was on there given the use of things like any cast for DNS I\u0027m wondering whether you found that any cast was a factor in your measurement and then my second question is did you look at whether the the contents of the response changed I mean was this just interception or was it actually data manipulation and and did you filter out the data manipulation wonder how did you know how is that folded into the results of yeah hmm so for the first question about any castes as far as I know both Google and Open DNS are any caste operators so we kind of couldn\u0027t compare it so I\u0027m not aware about any factors of any caste that are influencing our TT so I\u0027m not sure how to answer that as to the second question about the replies we haven\u0027t had time yet "
  },
  {
    "startTime": "00:33:13",
    "text": "so that\u0027s future work on Fortune\u0027s thank you thank you hi Colin Perkins and I suspect this is another thing for the future work but did you do anything to look at the replies and be able to try and infer what the reason for the hijacking was this time you could possibly gain from that yeah that would be totally interesting but yeah as I said it\u0027s probably for future work and actually it\u0027s hard to think of source of information that would give us answer to such a question what was the motivation maybe I don\u0027t know I have to think about this thank you Thank You Jenna in Google thank you for presenting this this is very exciting work and I agree with Adam that you should absolutely publish this this is the results are really cool this is perhaps more future work ideas also have you considered talk calling up some of these ISPs and just talking to them yes we are thinking about it so it\u0027s future work I would encourage you to do that I have experience with having our team we\u0027ve talked to some of these people and they tend to be usually not cool about the implications of what they\u0027re doing so they\u0027re usually you can find them quite open and willing to share because they don\u0027t know why they are doing what they\u0027re doing but they are willing to tell you what they\u0027re doing which is what you care to know so yeah I encourage you to call them at least try it out okay thank you very much I prefer great work Giovani psycheon so I also came across this problem with ripe apples before and we filtered their results out in a different way and I wonder what it would be the difference between their so when there was the root DDoS attacks cut on 2015 yeah you can also download the data from the roots and have some sort of standardized chaos theory answer so usually have a site name and what I notice is like the hijack roads they change the caves query answer and they have a very short RTT even during interview those attachments um now you can actually see you get any answers which are totally hijacked so I what I\u0027m trying to say here if you just will use the public measurements let\u0027s say for B roots or C what whatever they have standardized scale here is and the ones that are hijacked they return different contents for the answer what would be the difference between your method and just as well other one that most people have used before yeah that\u0027s right yeah that\u0027s very interesting video happiness F does your master could like show more hijacks than this simple water one I\u0027m just wondering we would have to compare that I mean the data is open we published the probe IDs we believe are hijacked so it would be pretty easy to run more measurements all "
  },
  {
    "startTime": "00:36:13",
    "text": "right thanks thank you how did you split your dataset into training and validation we did it 30 times randomly so we randomly split evenly into training and testing so okay did you perform any cross-validation not yet but it\u0027s got a question from Simone for Lydon maybe related would it also be interesting to see the user experience from these hijacked requests well yeah definitely but how should wanna collect such a data I would say but yeah definitely maybe using some crowdsourcing maybe that\u0027s an idea thank you so much thank you very much okay then our next talk is Pablo I\u0027ll say on not all so sorry there was something on ipv6 so this is on ipv6 traceroute in rate-limiting go ahead Pablo Alvarez Akamai this is work I did together with Florian après and John Rula and I\u0027m using this one here we go so traceroute is an essential technology for understanding the topology of the Internet and when you\u0027re trying to deliver lots of data to people over the world you kind of like to understand the topology of the internet so we do a lot of trace routing and we noticed that the data we got for a v6 was not as good as the data we got for a v4 so I\u0027m gonna tell you a little bit about that then I\u0027m gonna tell you how we looked at the characteristics of this data loss some of the things we try to do to get better data and finally some suggestions about what we could do as a community to to make things easier for those of us using trace route so let\u0027s start how much worse our ipv6 trace is an ipv4 traces I\u0027m gonna show you some data from one day of tracing about 20 million traces to 200,000 ipv4 and a hundred thousand i p v6 targets from about 8,000 vantage points and this is the data I\u0027m showing are if you look at the whole trace from the beginning to the last valid router how many null routers are there on the trace what is the proportion of missing answers from the beginning to the last valid router we don\u0027t look at nulls after the last valid router and so what you see here is that for v4 traces the "
  },
  {
    "startTime": "00:39:15",
    "text": "peak you can\u0027t see this the blue line is the before traces for the v4 traces the mode of the distribution is around 3 or 4 percent missing hops portrays 4 v6 traces it\u0027s around 10% missing hops portrays so that\u0027s really much worse if you this is averaging over targets if instead of looking at all the traces to a target you look at all the traces sent from a particular location you see something interesting which is that for before we get the same kind of curve with Moe you know the average loss being about 3% for v6 there are where the average loss is about as bad as before and there are some vantage points where the average loss is much higher so this was comforting to me because I was looking for a bug in our implementation and this is as well actually you know different areas the internet react differently and my code is the same so with any luck it\u0027s not it\u0027s not our code that\u0027s the problem it wasn\u0027t our code and in fact it turns out the problem is an RFC so the RFS RFC for 443 which makes recommendations for icmpv6 states that routers must rate limit icmpv6 error packets and the way traceroute works is that we the information we get back is from top limit exceeded icmpv6 error packets so if the router doesn\u0027t want to send those back we\u0027re gonna get holes in the trace rent the RFC also explains how this is supposed to happen they recommend a token bucket rate-limiting system the basic idea is that in order to send back an error packet the router needs to have a token on hand it starts with a bucket full of tokens say 10 of them and so your first end probes will get back the data and then after that it refills the bucket at a specific rate so for example if it refills the bucket at 10 Hertz after the first initial burst you will only be able to get back information ten times a second so if you send more trays routes than that too bad so the next thing was to look at these rate limiting characteristics so again we said you can do this by sending packets very fast in this experiment we did send packets at 10 milliseconds intervals to the same router and looking at what you get back and if things are reasonably clean you\u0027ll get back something like this and the clump at the beginning can let us tomato bucket size and then you can estimate the rate limit from the the "
  },
  {
    "startTime": "00:42:16",
    "text": "rest of the diagram and we did this for about that\u0027s about three thousand routers in six continents who separated out North America and South America in two separate continents and this graph shows the estimates of the refill rate and what\u0027s interesting to me about the graph is that the peaks are the same for all the continents so wherever you are you\u0027re gonna see a bunch of routers about twenty to thirty percent of them rate-limiting at one Hertz you\u0027re gonna see on the order of ten to twenty percent of the routers rate-limiting around ten Hertz about a third of the routers that don\u0027t seem to rate limit at least at the hundred Hertz rates we were testing them so this suggests that either operators all over the world have agreed on what the best rate limits are or more likely that the router manufacturers have set some defaults and nobody ever touches those defaults and when I talked to people at Akamai who work with routers they said oh no yeah we don\u0027t touch those defaults so this graph shows you the bucket size measurements it\u0027s a little bit noisier but it shows essentially the same profile which is you get big peaks that are consistent across geographies okay so given that we have this rate limiting and that some of the rate limiting is really hard at 1 Hertz and bucket sizes of 1 or 2 what can we do about it there\u0027s a couple of tricks you can play if you have a person running the trace routes and I\u0027m gonna talk about them and again this is just proof of principle stuff we have not tried to optimize these things yet but here\u0027s some data the first thing we tried was to change the order of the TTL s the simple-minded way to do the trace route to do 100 trace routes is you send a hundred packets out with TTL one and you wait till they come back and then when they come back you send 100 packets out of the TTL to and so on and so forth this is the absolute worst way to deal with a rate limiting router because you\u0027re gonna hit the rate limit on your first router then you\u0027re gonna hit the rate limit on your second router and so on and even as it spreads out you\u0027re still gonna hit a lot of rate limits so you can change the order in a number of ways you tried three different ways one is completely random kind of like what Europe does two is staggered across targets oh sorry I forgot to explain the colors black is the first packet that goes out red blue is the second packet that goes out red is the third and between is the fourth in this implementation they go out every two seconds more or less but the point "
  },
  {
    "startTime": "00:45:17",
    "text": "is is so the second is staggered across targets and the third is you pick a random starting point for each target and then you just keep going forward from that starting point and it helped so in this graph blue is the baseline method and that\u0027s set at 100% and then the other three light blue green and red are the other three methods and what you can see as we get more valid routers if we change the order of the TTL s so this is comforting up to maybe a 20% increase we get fewer and all routers with methods 2 \u0026 3 and more of them with method 1 because the the random method hits everywhere so we\u0027re not stopping traces when we see too many null routers we get a few more completed traces but this has a cost and the cost is time it turns out the original method is the fastest way to to get data the random method is essentially too expensive for us because we\u0027re trying to do a lot of traces and method 3 is a little bit slower than the baseline method and gets us more data so that\u0027s what we\u0027re working on right now another way you can try and fix it fix this is to use data you already have so if outer refuses to send you a packet back it\u0027s because it has already sent a packet back to someone right that\u0027s it it\u0027s bucket is empty so with any luck your that someone and you have that data somewhere in your last few traces so if you\u0027re willing to save the data and search through it and ignore for a moment the issue of load balancing then you can fill in traces with previously known data for so for example if you have the triplet ABC in one trace and the triplets CTU in another trace then in this third trace that has two holes in it you can fill in those two holes with B and with T you need to be careful you can\u0027t hold on to the data too long because the topology does change again there\u0027s the load balancing issue and sometimes you just don\u0027t have the data because the router sent back that packet to somebody else not to you um so how much does this help well let\u0027s look just for let\u0027s look just at the orange line the orange line shows the percentage of missing hops in traces that we are able to fill in we can almost always fill in the first hop and we can film the second and third hops are pretty effectively even after that we can fill in about a third of the hops up to 10% up to take up to hop ten and we can fill in somewhere between ten and twenty percent of the hops between hops ten and twenty so this is useful this you know it does improve our traces the other method is "
  },
  {
    "startTime": "00:48:18",
    "text": "also useful but this still is leaves us doing worse than before so in summary if you do many traces on ipv6 at the same time you\u0027re going to hit these rate limits the evidence to me suggests that the rate limits we see have been set at the factory defaults and have not typically been touched by ISPs we\u0027ve tried a couple of ways to get more data and they help but part of the reason I\u0027m here is to make a request and so the request is based on the idea that the example in the RFC suggests 10 Hertz refill rate and a 10 token bucket and that RFC is more than 10 years old I think our hardware has improved quite a bit since then so rate limit of you know 100 Hertz refill rate and the 50 token bucket seems relatively modest something that most hardware is not going to complain about this isn\u0027t just a problem for people drink trace routes I was talking to Johanna Ulrich from Vienna and she\u0027s been trying to do some research using host and reachable and network unreachable error packets and runs into the same problem which is that the routers will tell her some information and then they shut up they like nope we\u0027re done you can\u0027t have any more so if you\u0027re making routers playing a set your default a little more generously and yeah don\u0027t know if Russ white is in the audience I\u0027m gonna email him but if you\u0027re writing an RFC that tells people what v6 routers should look like mentioned this had mentioned that the rate limits should be significantly more generous that\u0027s that\u0027s my request any default values in the RFC that are recommended no there there are no recommended values in the RFC thank you very much gentlemen cover to comments the problem one of the problem here is the default default value which you could change but plenty of routers have hardware limitations which you could not change without replacing your heart burn rate and indeed an ipv6 the situation with sentient ICMP Bayaka will be much worse than before because of you do not know how much I seen people get too big the router is sending already took two other hosts and the second problem if a vendor has a common limitation for all ICMP and rate limiting all ICMP it potentially you might also be affected by neighborhood is covered so yeah obviously surprised to see that there are visitors here and administrator not always could change those defaults because it\u0027s hard very dependent yeah my Michael Irvin\u0027s "
  },
  {
    "startTime": "00:51:19",
    "text": "is so did you try changing your source address when you send the then so that they the error it so that their alter sees different destination to send back the air or to I did a very quick and dirty experiment on this and it looks like the rate limits are per router not her source or at least per port or something like that so it\u0027s not what these are yeah these are ICMP traces so it wouldn\u0027t be well per flow or I mean yeah it looks like the limitations are per router all I don\u0027t think you can may make that leap but because a lot of Rogers will actually don\u0027t like the line card CPU is the one rating the the error signal line corridor for they might have several of these they might be generated by the MTU so it\u0027s not for ultra but ok nevermind okay so the in there was an earlier graph did you differentiate between getting zero one two or three responses from a router when you\u0027re saying you\u0027re getting no response I\u0027d need to understand that from the graph we are only testing each hop once okay because when I hurried so we normally the default me for this to send three packets and more talk about holes I thought it meant zero responses but so this is like but okay so your your trigger already on not getting the the response on the first try yes okay it would be interesting to see when you\u0027re doing the try stress if you\u0027re actually sending multiple packets we\u0027re we\u0027re not getting in a response but because I thought the did you go after well it looked like you were sending three packets in the color very nice a lot of color slight said we\u0027re not know if this this this is not that default tracer at implementation having worked on a few v6 routers implementations is quite likely that there are a lot out there with much higher limits than you were able to detect but still limits and these are probably set not so much by processors in their routers but by communication between the forwarding plane and the control plane and you\u0027re likely to find that actually despite the cpu being really quite fast there isn\u0027t enough and there are not enough packets per second available between the control and fording clones to actually get the limits higher and so you find limits and exotic places inside the inside the router chassis also to the previous point about line CAD CPUs yeah that usually is the line CAD CPU doing this so I didn\u0027t completely understand what you said are you saying that there are "
  },
  {
    "startTime": "00:54:19",
    "text": "additional rate limits beyond the token bucket that we haven\u0027t seen because we were not going fast enough possibly or believe or possibly then the the limit is not actually a token bucket but a packets per second limit somewhere inside the inside the router implementation and it\u0027s for such things as all ICMP or all packets destined to the to the control plane or something like that the the data I got are consistent in most cases with with a token bucket at least for the low rate limits that would be a normal way of implementing such a low limit yes jumpers asking um one one of the things that I think is important to note is we often I work for Comcast so for largest v6 deployments we often do things that you see deliberately because I\u0027ve seen PP 6 can have pretty unfortunate side effects as far as that stability that out Creek is concerned they\u0027ll be interesting to run this test okay so there\u0027s this notion of like the internet and then you know stuff that\u0027s not the Internet like internal if used to run this internally it within from within a network to see if that changes the view or the vantage point for how the traces are permitted or not for or rate limited or not the other thing I wonder as I sat here and I listened to few comments is is addressing right and then like did you some people use you know addresses that prevent you from getting a an addressing methodology that prevent you from getting a proper response back great so maybe links aren\u0027t using globally routable addresses for example I don\u0027t know I don\u0027t know if there\u0027s anything in your data that would suggests that are not so I did not I know that in v4 traces we often see RFC 1918 addresses I did these data if we get any kind of address back we count that as a valid hop so I was not filtering out non globally routable addresses so I again it depends how things are set up but that\u0027s I can look in the data for that I didn\u0027t think to look for that sure thank you before I run away did you just say you want to run some trace notes and give the data to Pablo for further analysis okay I could actually do that yeah I mean I could you know if um you know I mean I work with a lot of other people from your companies I mean yeah look we find me after than mean we can talk - sure thanks I mean so just just on a comment on the rate-limiting staff a hundred organic here and from somebody who actually wrote some of that code you unless you have a low end routers then yeah you are grading hardware but like if you have an a decent router that is just a software that Europe if you trace ink on a port or an interface you\u0027re gonna get a different limit if you spread the earth rises over the line "
  },
  {
    "startTime": "00:57:20",
    "text": "card line car CPU does but not often takes there architectures that don\u0027t use line colors if you centralize everything if you have slices on your line cards because the vendor doesn\u0027t have a big enough CPU then if you if you go on a single slice you\u0027re gonna have a different limit so you have to be careful how you\u0027re gonna get that stuff there will you will be hitting limits you will be hitting limits from ICMP at some point those are usually software imposed limits then so can be tuned you will be hitting the limit if you gonna send everything is an ICMP test you\u0027re gonna hit some other limits so if you wanna have but really what can be done and how you should turn you should probably talk it\u0027s not secret you should talk to some of the vendors who are in the room and we can tell you how to do the test thank you very much and at the end if you wanna do more then at some point here you may you may need a better CPM Cal but you know the compiler control card but anything better that\u0027s like two three years old I don\u0027t think you you have any concerns is really the old hardware that but you would have a concern yeah I\u0027d like to know you given what you had like to know if the data that you collect is publicly available [Music] I\u0027m not sure talk to me afterwards because there are many project that collects race raw data so Qaeda ripe at last and I think if there are if we do together the project that collect data and we put the data publicly then it will be a less incentive for other companies to try to collect private data and if we have a good data set of trade data then we will have a good data set that would be available for everyone and so if you could join together with ripe Atlas Qaeda and others that are providing pubic data then everybody will have to forget about race right and nobody would have to write their own twist water to to get private data yeah I think that\u0027s a very good proposal thank you for the talk and at that point thank you we\u0027re safe I\u0027ll say this time also about ipv6 all right I\u0027m not my chair role so the meter or nicer to me than you would this is about work that I\u0027m doing at Akamai about taking a new measured approach to the way might do ipv6 address anonymization arthur berger my colleague and i have a short paper about this that\u0027s available to you now at archive.org at the URL there so we\u0027re going to talk about a really simple kind of IP address anonymization just anonymization by truncation or aggregation where we strip off the trailing end of the IP address and "
  },
  {
    "startTime": "01:00:20",
    "text": "considering what the consequences of that are is it providing providing any kind of anonymity at all is it providing a similar kind of anonymity to what you might seem v6r before if you strip off something that like the last eight bits or something and why is this used well it certainly in my company it\u0027s used for things like web analytics but you could imagine in Google Analytics does this as well you you want to be able to match the activity that you see in the Internet to network topology to routing to the service providers involved and geographic locations so if you didn\u0027t leave a public top portion they address you couldn\u0027t correlate easily to those sources so just a real simple example of what this would look like in ipv4 and I think probably something you\u0027ve seen before here I\u0027m showing you 32 ipv4 addresses in the ten 0:42 subnet and each of them has some character associated with this in this case it\u0027s just in their currents like maybe they they were you know on or off activity and the anonymization is simple right it\u0027s just you truncate the some fixed prefix length like here it is slash 27 and say well I\u0027m clearly combined the activity of 32 addresses maybe 32 actors into 10 zero 4202 slash 27 and someone would feel well maybe I\u0027ve been anonymized somehow been included that way a lot of this has been going on for decades that\u0027s a common way to anonymize so I\u0027m just gonna say like start from is if we didn\u0027t know anything or feel we knew anything about what was the right level and before what truncation based anonymization is only ideal to use if you can guarantee that improves privacy so I\u0027m wondering from a measurement scientist perspective can we tell if it improves privacy or not and I have my position is that clearly it\u0027s going to be different than B 6 because v6 addressing is very different so we\u0027re proposing something called k IP anonymization where k is the number of members of an anonymity set that we\u0027re saying we\u0027re gonna make an individual appear to be indistinguishable amongst k individuals in that set and it\u0027s based on the notion of can anima teeth at you might have heard of before from the medical world or health records where you say I can prove that I\u0027ve mixed these in a way that you can\u0027t discern which of the patients suffered this particular problem for instance that\u0027s the sole connection though it\u0027s just that I used the same letter k it\u0027s not the same kind of proof it\u0027s a very different kind of proof and all by the way it also happens to be very similar to the definition in RFC sixty nine seventy three about privacy considerations for internet protocols so we\u0027re gonna use some real data in this study four sets of data the first one a meeting network a meeting that happened earlier this year happens to be IETF 98 and then we\u0027ll talk about three different ISPs that happen to be in three different continents but I chose them not because of that but because they have interesting characteristics for instance they\u0027re a common practice today is to say suggesting anonymizing ipv4 addresses to the slash 24 level so it\u0027s tripping off the lower 8 bits in in ipv6 a very common recommendation is to strip it to the slash 48 now in this particular "
  },
  {
    "startTime": "01:03:20",
    "text": "network the Japanese ISP you could if you look here over seven days we see the same number of active slash 48 prefixes meaning ones that cover world wide web clients in that ISP as we do slash 64 so clearly if I truncate from 64 to 48 I\u0027m not anodized anonymizing anything at all and this was sort of my inspiration initially clearly I know that slash 48 based anonymization is doing nothing in some cases but there are other networks where just this u.s. ISP and the Japanese ice be very disparate numbers right on the order of 300 times different in terms of the number of slash 48 so they used to cover so just evidence that there are very different addressing schemes and we want to accommodate that in the anonymization mechanism and then but we can see that in this case the jet Penney\u0027s ISP in the u.s. IP in terms of slash 64\u0027s and full addresses they look very similarly sized so that\u0027s curious but if you look at this European ISP which I haven\u0027t told you which which they are but I don\u0027t have any reason to believe they\u0027re different in size they\u0027re using twice as many full addresses in a week\u0027s time and 10 times as many slash 64 so just give me an idea there are very many different addresses used in the v6 Internet today and an anonymity mechanism it doesn\u0027t take that into account is probably dangerous so ki P just has three steps the first step is I\u0027m going to call temporal and spatial address classification then we\u0027re going to move on to activity matrix analysis and then the last step is called anonymous aggregate aggregate synthesis we\u0027re just gonna make a table of aggregates that we suggest that would provide some guarantee of anonymity there so let\u0027s look at what these steps work like the first step classification basically it I called it address dendrochronology dendrochronology z-- the study tree rings and branches over time that\u0027s how the way one of the ways that I think about ip6 addresses I presented this last year but so we\u0027ll just go really quickly with what that was about if you take a set of these six addresses for instance world-wide-web clients I\u0027m showing an abbreviated version of 1600 those addresses on on the left there in the green and the red and I can take those addresses they just have the colons you know missing where it\u0027s just a fixed length version we can we can classify those addresses and say we have a spatial characteristic amongst them if I sort the addresses I can find the first bit at which they differ and this tells you essentially it\u0027s a measure of how far apart the addresses are from each other and you can imagine for pseudo-random portions the address they\u0027re randomly different distances apart and in they\u0027re quite far apart so that\u0027s the spatial characteristic of the calcification the other classification we can do is since we\u0027re sitting at those say a webserver we can see those addresses come back to us or not over time that\u0027s a measure of stability a temporal characteristics so we have the number of days stable in this case the last address was seen across multiple days in ones that say zero day they might have been temporary ephemeral us that privacy addresses and then lastly a kind of an really neat a simple way a nice way from Fernando gods ip6 toolkit you can classify an address by pumping it through this tool and it will tell you when it knows about the address and in this case the tool is saying I think that that address you gave me is randomized in its ID so that\u0027s that\u0027s "
  },
  {
    "startTime": "01:06:20",
    "text": "sort of the starting point we say as a foundation if for a narrows tool says it\u0027s randomized let\u0027s start there and see if we can use those randomized addresses the privacy addresses as a way to unravel how many devices might be in that prefix so the question then is simply by truncate at the / 64 boundary would you be satisfied with that probably your service provider is giving you a slash 64 so the identity that you\u0027re being assigned by your service providers isn\u0027t isn\u0027t completely captured by the slash 64 so we\u0027re not going to be satisfied with that well are we satisfied with what length and what prefix likes would you be satisfied that if we truncate it to there that somehow you are an authorized now list now let\u0027s move on to step two that\u0027s just a base classification we have all the addresses they\u0027re classified up by those metrics that I told you about there now let\u0027s look at what\u0027s called it we call an address activity matrix and this thing that looks sort of like a punched card here the red represents bins of time in this case hours where an ipv6 564 address was seen active and vertically or armed in this in this matrix is just progression through the address space so we can see patterns activity there and by studying this activity I can discern some things about about the way the addresses are used there that I\u0027m going to use for anonymity this is based on our earlier work we did just last year with Philip Rick Drew\u0027s presenting by the way this afternoon and I RTF open meeting but the the simple idea is you make a matrix and you have time and the horizontal dimension and space in the vertical dimension and you just fill in cells where interesting activity happened so that\u0027s what we did for ipv4 and and we\u0027re able to discover patterns of behavior across the entire idea view for address space but I want to take this notion and curate anti-b v6 so what Philip saw was things like you know which parts of the address space seemed not to be used on the web these angles ones in the upper right show for instance DHCP pool activity and the one in the lower right is a very heavily utilized address block so clearly this visualization offers some an analytic capabilities now let\u0027s turn to v6 here so because I\u0027m older than Philip I\u0027m going to use ASCII art instead of p.m. geez the other reason I\u0027m use ASCII art is the space for v6 is really big so if you try to make a PNG file it expresses the entire space it\u0027s going to run up your memory of the machine so so what the the characters here well let\u0027s start first with the addresses again so just like before I\u0027ve got the / 64 prefix there in green for a set of addresses just 16 addresses here that happened to be observed in one day in 1/64 so this is the typical home situation but you\u0027ve got a / 64 year for a home and throughout the day sixteen addresses were used those are the I interface IDs in red now the next thing we\u0027re going to do and I\u0027m going to point you to the paper for details here but I\u0027m going to tell you that we can use probability to try to discern whether or not those IDs are plausibly random or not and it\u0027s like the birthday problem right like if you\u0027ve got 16 people what\u0027s likely that their birthdays will collide well 516 ostensibly random numbers a certain number of bits leaving them is likely to collide and I can just play that in advance and figure out what it is but in "
  },
  {
    "startTime": "01:09:21",
    "text": "this case I\u0027m saying those 16 addresses are plausibly random in their interface IDs because that spatial characteristic the distinguishing prefix length that I have in the second column there is less than a predicted value so I\u0027m just gonna ask you to assume them for now if you look in the paper for the proof of how that works so space and time again all I did was space goes the other way from top to bottom and now what do we so the hash marks here are the places the cells that are filled in that\u0027s where the web server saw activity from that address right so the question is you\u0027d say Dave I see 16 addresses over here over a day can you tell me when was the address the the prefix assigned to a user when does it appear to be no sign and can you tell me for instance how many devices might have been using those addresses well the first thing we can do is we can take that activity and sort it left to right based on the initial activity of that particular address so they have the idea that you know it\u0027s in your home you\u0027ve got a mix of different v6 equal devices and throughout the day they move across these seemingly pseudo random IDs now the next thing I could do and this is the key to the whole technique if I see for instance in that fourth line I filled in with these read out signs I saw an instance of activity at one hour like the eighth hour of the day and then again in the thirteenth hour of the day but with that random number so that so the the inference I\u0027m going to make is that address must have been signed even though I didn\u0027t assign to some device even though I didn\u0027t see activity between it so I\u0027m going to extrapolate from those two observations of activity that it was assigned throughout that duration where these @ signs are I won\u0027t talk about that a little bit more but that\u0027s the key to the technique is that privacy addresses allow you to draw conclusions about when they must have been designed on equipment even when you don\u0027t see activity from so then we\u0027re gonna annotate this matrix a little more since that it is having the pixels it\u0027s got a couple different characters and these X\u0027s and greater than Xin less-dense they\u0027re showing the the beginnings and ends of episodes of activity from some observation point you have a qualifying question you know it\u0027s a question it\u0027s more of a question about your assumption so I\u0027d leave that to the end you might have you mind if we limited the end off just cuz I got a lot of things there so okay so then so we\u0027re seeing these episodes of activity and I if I see at a point in time and there\u0027s activity another point I\u0027m making the assertion which intimidate I will talk about it then about is that a valid assumption to make so now we can do an interesting thing right I can do some kind of funny arithmetic on these characters I quit in this matrix and say well if I saw two devices that in the same hour\u0027s time that third hour of the day at some point in time during that hour those two active the those two x\u0027s add to one meaning it there I know at some points during that hour at least one address was active I can\u0027t say two because I don\u0027t know that they were active at the same moment okay so that\u0027s that\u0027s why the funny arithmetic is here so I play out this kind of arithmetic and I get an array of totals and the conclusion is from just this sparse activity in this particular Saturday\u0027s before I can see that there was some moment during that day at which three simultaneous "
  },
  {
    "startTime": "01:12:21",
    "text": "pseudo-random IDs were assigned that\u0027s the maximum so that\u0027s an interesting measurement result in and of itself now let\u0027s take that and see what we can do with the monetization as a result of it so we\u0027re going to do is take that now that we look at the activity of individual full addresses or these interface IDs really I already proposed to you that the slash 64 is the thing that\u0027s hide your identity that\u0027s probably the thing that your network gave to you as your in identity assignment so let\u0027s just take that activity matrix and / clean it up to say when was the / 64 assigned and that\u0027s this array of exclamation points and that I then it turned into a boolean array of values where there ones there I have yet gathered evidence that the slash 64 is assigned to someone and where the zeros are I don\u0027t have any evidence it doesn\u0027t mean it wasn\u0027t those don\u0027t have any evidence from the observation point and we\u0027re going to call that these moments between the 24 our bins were to call those fence post moments so what I\u0027m trying to do is jump from at 10 I don\u0027t know exactly the moment to saying it must have been assigned in the in East one of these 23 moments that separates the 24 hours and that\u0027s the second key to the technique I count being able to count it at a moment now the way that works is consider the easy yellow boxes are those hourly bins the fencepost moments just like a fence are them are the moments between them and then when you look at activity on either side of a fence post if it\u0027s the same address we can conclude that there must have been one address available that fence post so what I\u0027m doing is building a matrix now with a fence post that says it\u0027s okay for me to add the values together because they can\u0027t happen at the same moment across addresses so we get at the bottom we get this array of values and now we\u0027re ready to move on to the step three the last step in synthesizing anonymous aggregates I\u0027m going to take those values for all the hundreds of millions of slash 64 as we see in a network or in the entire Internet and build a tree we\u0027re now just instead of the having only one counter in the tree this is a patrícia tree or base two radix tree I\u0027m going to have an array of whatever size my window of observation is for this trivial example I\u0027m showing you one day but in practice we would use a week or two weeks and there\u0027s 23 values so now again I\u0027m gonna go a little fast this is explained in the paper but basically you could imagine pruning this tree and aggregating those type arrays of counters up such that for instance those first two nodes at the top they would aggregate to that slash 55 with all the twos there and what I\u0027m saying is that if I a grenade to the slash 55 I have evidence that two simultaneous 64\u0027s were simultaneously assigned at those same moments and if I had a really compromised notion of anonymity that said it was sufficient to just mix two people\u0027s traffic together and you\u0027d consider that anonymous that\u0027s what I\u0027ve just done is produced in an anonymous aggregate prefix there and then it turns out in this case because I was using the IETF meeting network there were so few 64 is active that it turns out that\u0027s the only one I can report with the point here you know that K equals 2 / 64 is where active simultaneously so now let\u0027s "
  },
  {
    "startTime": "01:15:21",
    "text": "jump kind of quickly into so I take that I had a characterization that data sets before the part in bold is these counter to conclusions from that that algorithm that I just described and the cool things we can see here is is for instance that we had a network that had you know two in 21 point four million active / 64 is over seven days but by that counting mechanism we only saw two million at a time so I\u0027m saying I\u0027ll reliable lower bound the estimate of the simultaneous number of identities assigned in that network is 2 million and remember we talked about these three ISPs having very different characteristics based on address counts if you use this mechanism it shows that within an order within a multiple of one they have about the same numbers of simultaneous years it means we\u0027ve seen a fencepost moment between ours over the period of in this case a week that that shows that those three different networks are about the same size and now we can use those counters to go and produce aggregates so to finish up now let\u0027s say we probably decide that mixing 32 ipv6 identities together in a report that maybe my company is going to publicly release or your public your companies can publicly release we say 32 is enough to mix together can be anything right you can say 256 or whatever but when we run those numbers for the US is P this is the distribution of size of these anonymous aggregate prefixes I mean all right we get so compare this to what I think is status quo today people use slash 48 aggregation this is saying if instead of using slash 48 a gregarious the thing that plunk and burn are developed you can sometimes share slash 50s in fact most often you can share slash 50s because they meet your guaranteed requirement then it makes 32 identities together however in the EU is P slash 48 is not usually sufficient more often you have to go to 47 sometimes you have to go to 41 so you\u0027re going to get an array of different sizes of anonymous aggregates out but those are the ones that meet the guarantee and the 48 that we might be using today doesn\u0027t meet the guarantee meaning we don\u0027t have good evidence that in anonymizes individual users in this v6 network lastly the Japanese ISP which arguably had the most unusual higher orbits in terms of their aggregation such 48 never met the criteria you never mixed 32 users together when you had green at the slash 48 but you did very often when you a great at the slash 41 so it\u0027s a new way to think about a group it\u0027s not exactly rocket science we have that rhythm laid out in that paper and but that\u0027s basically how it works and we went from a place that we just assumed based on like slash slash 24 is you know ipv4 network that this is good enough kind of aggregation to say I have good evidence that I can prove that the aggregates I used actually do makes a certain number of identities in a v6 network thank you Tim Chang yeah I think this is really good thanks Dave I\u0027m trying to learn what the question "
  },
  {
    "startTime": "01:18:21",
    "text": "was there so I think it was about when you\u0027re talking I\u0027ve heard I\u0027ve heard a question related this before so I think I make the last request I privacy addresses and seventy to one seven addresses I don\u0027t think you can tell the identifies of cars whether it\u0027s privacy dress or 72.7 dress from the randomization but in one case if you go away and come back you\u0027ll get a new privacy dress because you\u0027re reattaching in the other case you\u0027ll get the same last 64 bits so making assumptions about persistence is different I think yep thank you thank you I I think I agree with you if I understand what you\u0027re saying remember in the third column here I\u0027ve got a stability characteristic of the address from an observation point now it has to be an observation point that sees a lot of traffic but if you are where you see a lot of traffic I can use a sliding window of time and look if I\u0027ve ever seen that address before your right to question whether or not my test for randomness is reasonable what we want to do is say have we ever seen that value much more or have we seen it spatially across other networks because any of those would suggest that it\u0027s not random so the assumption that it was unstated that I guess you reminded me the state is I\u0027m saying this works if the temporary slack privacy address mechanism picks good random numbers and we can reject the persistent pseudo random by looking at their days stable okay well it\u0027s me over negative 80 I really like his work because I think it\u0027s important so many other sports besides like minimization reports for example it would be useful in light rate-limiting is yes in a service yeah whenever I want to like fumble a couple of years together or whatever any users so um it would actually be create if in some way we could take this work from the IRT F maybe to the IETF so that we could create a protocol infrastructure or something like that where I could identify I would say reasonable prefix lengths for all kinds of rate limiting measurement anonymization and the other aspect is the European data protection very soon yep so this is a hot topic now and this is really really amazing for I agree I think that would be an interesting direction to go and the thing that you\u0027re proposing is are the results portable if I observe activity from one point can they be ported to somewhere else and I would argue without having thought about it too much is there portable if the place reporting them from is a place that has better visibility than where you are you know and also can we educate the data if like many people actually do those measurements aggregation algorithm can we like publish the alts aggregate them that\u0027s an idea I hadn\u0027t thought of but yeah that\u0027s great if you\u0027re thinking of caring the result you know you can find a way to not disclose the privacy addresses in the first place then maybe we can integrate our observations in some private way to get the privacy yet I mean is all kind of comedy that go to privacy and well yeah yeah that\u0027s "
  },
  {
    "startTime": "01:21:23",
    "text": "another are there are methods that allow you to share addresses in an opaque way and tell if they\u0027re the same without disclosing their value but essentially there are so many cases in which I have fun ipv6 prefix and I want to know okay where do I cut it yeah essentially and if somebody else has created a measurement that yeah came to a conclusion based on a public available algorithm yeah I think it takes so it sounds like yeah maybe it\u0027s some guidance document that we should take the guidance document that we should take on as a group yeah well I\u0027ve got a dad that would be interesting yeah I think the place I could start is I don\u0027t even know where all the places other than one RFC I cited where in the ITF community do you talk about such a thing so so so it\u0027s a candidate maybe maybe be a candidate for draft for yeah I think we can\u0027t we can\u0027t actually take on such a draft in this group and then bring it to the bring it to the attention of the IETF when we\u0027re have it in the right shape okay and I you know I\u0027m over my time so let\u0027s do these quick Oh Pablo Alvarez Akamai so in terms of reusing these data do you have some idea of their temporal stability that is if you if you do this experiment and you run it six months later right do you do the ISPs still have similar property so how often would we need to renew this data and share it we don\u0027t know that exactly but your rights to be concerned though the way we developed the algorithm is to run it continually so that you can as you\u0027re doing the anonymization you can tell whether it worked or not but so it works really well and simply for offline analysis like you\u0027re a researcher and you have a data set that\u0027s contained it was finished last month you can run it and you\u0027re guaranteed that you mix them correctly but if you could if you decide that you\u0027re going to use that set of prefixes and forecast in the future we need to do new work to figure out how long that would last okay Michael promises so combining this with your earlier work how to identify and what where the addresses are within these aggregation levels for like D there was really mean dementia I think there\u0027s some kind of anti abuse handling group here in the ITF I think that I think they would have interesting if you can define like this is P typically has like a slash 56 to each customer so it\u0027s not only you know like you\u0027re going the anonymous ation way but also like how do you identify an household where you\u0027re going there the other direction in aggregation from to identify that pivoting pivot point or like where is this where as a household what\u0027s the equation policy so thank you for reputation like identification so on you it could be a good input for them as well D or IRS have a way of the ice be publishing this or basically no but it does you could be doing this kind of a measurement I think creates that as well equally good reminder what Michaels saying is we can either discover it right or they can tell us a priori and what I\u0027d love to do is compare the two right anyway "
  },
  {
    "startTime": "01:24:23",
    "text": "thanks for the questions in the comments you know a few minutes late but I think that it\u0027s fine so we\u0027re kind of jumping a little bit from the operations partner to more performance measurements and first one is on the agency measurements talking thank you hey everyone I\u0027m Alan garrison I\u0027m a PhD student at Cal State University and this study i\u0027m going to present we presented this at connect last year and basically what we wanted to dig into was how much buffaloed exists in the internet and of course we don\u0027t really have a vantage point to look at the whole internet so what we did instead was we figured out that business thing called and lapse which has this really nice large scale at measurement dataset and then we found a passive capture from an isp which has some different her twisted time we wanted to to combine this and see if we could extract some insights from from these two datasets then as a proxy for buffer bloat because you can\u0027t really necessarily in further in all cases we used latency span as the metric so that\u0027s the idea that if you have a a certain path or certain flow then the obviously the minimum latency measurement to get over that flow is sort of an existence proof that we can achieve this latency during the on this path so any other measurements we get in the same flow that are higher than this are to a certain extent too high for this flow so the difference between the minimum and the maximum seen in the flow is sort of a a proxy for extra latency as atlas flood and then to specifically look at curing latency apart from this we did two things to try to be to estimate during it and see the first thing was was that if if we have a long TCP flow in the ax measurements we get continuous at the t measurements over this time and we also get information about to speak ingest in the minutes ie a drop leading to a reduction in Sealand and if we look at the latency right after such an event in many cases we could see a very distinct drop I\u0027ll show you an example of this bit later and we use this as a property so we we assume that in this least very distinct cases that this drug actually corresponds to the cue draining until while the flow starts down and comes back up and then on the on the dataset where we had where we knew where where the data was from it was from sort of a lake where we could catch it which correlate link load and and latency and see when latency goes up as the link get loaded that\u0027s also a very strong indication of upload so the two days that we used as I said one of "
  },
  {
    "startTime": "01:27:24",
    "text": "us the the NDT dataset which there were sort of active measurements where the user goes into I want to test my internet connection and post on a 10 second download going as fast as I can and we looked at these there\u0027s about 265 million total test runs over six years and the beta source we looked at sort of M lat publishes these statement into P state machine as the T samples for over the whole flow so we took these and then we\u0027ve got the the span of latency as I mentioned before within each flow and then we combine this with this capture from a from an isp network which are to one gigabit per second application linked with only 50 or 400 customers so these are way smaller datasets that were connected to during a period of time and in 2004 the for the catch was there we looked at the delay between the syn ACK and the act so the intuition here is when a client connects to the server on the internet it will send a syn to the server and then the cynic comes back and the client will immediately send the final act is that was the connection and since we\u0027re capturing packets on the access link at the ISP this delay between Cenac and AK correspond pretty well to the latency from the measurement point to the client because we assumed that the operating system generated the AG immediately when it gets the the synapse so for the first dataset we look at as I said latency span and if you look at the left-hand side here you have the because okay you have the the minimum and the maximum at ities and you have the attitude span so what you see here is at about two two-thirds of the flows have a have a span of evade attitude of latency above 200 milliseconds and then on the on the right hand side you can see that there\u0027s a significant difference these other of the geo assignment of the client IDs that measurement Labs provides so these are not necessarily completely accurate but they give a nice indication of where in the world are these clients and they the glands will select the nearest server for the test so it\u0027s not like this is because all the servers or the clans in Africa connect to so in the US these are actually connections to the nearest server and as you can see there\u0027s very large regional variation between the different continents so one of the interesting insights we got from this was that be if you look at the development over time you see on the left hand side the latency is basically the same for all these six years of "
  },
  {
    "startTime": "01:30:24",
    "text": "measurement whereas for bandwidth as you see on the right hand side there\u0027s a sort of steady development towards more bandwidth so whereas bandwidth gets better and better latency doesn\u0027t and of course some things you can\u0027t really change for latency I don\u0027t think we figured out how to change the speed of light yet but there\u0027s a lot of other sources of latency other than just propagation delay and these are definitely not getting better and by several hundred milliseconds of latency we should be able to do this refrain question is the same TCP algorithm the congestion avoidance algorithm used in all these measurements or did that change over time hmm that\u0027s a good question I\u0027m a feint on the server side it\u0027s the same one but obviously the clients you don\u0027t know yeah because that\u0027s that the the bandwidth increase could be partly because of better TCP algorithms as well and so the turkeys correct the server side in lips server side TCP hasn\u0027t changed over that\u0027s been but yes the client the clients may have so it\u0027s may reflect partly the change in the distribution of congestion controls in the clients yeah but we\u0027re looking at trial traffic going from the server to the client of this instance sure yes right so that that was one of the interesting things we saw that that difference between the agency and let me just say one more thing I think Windows XP came with which is being Windows cabling I don\u0027t remember if that was turned on or not seed that might actually have changed that the client population changed over this time span [Music] right so access network data where we looked at here was the the same span between but but - all connections going to the same customer the same IP address here and again if you if you look at different depending on which percentile of of all the the measurements you look at you get a span going from 50 60 milliseconds at the up to almost 200 200 milliseconds and the for for a quarter or more of the of the traces so again I a lot of them do not have a large span but we can see that there are some connections where we get this variation that indicates extra latency then there\u0027s the the curing latency thing I was talking about so this is the the pattern that we saw this is a trace from us from a single ten-second test "
  },
  {
    "startTime": "01:33:24",
    "text": "where the cross you see at the top is the congestion event so as the congestion event happens it\u0027s sort of if it in flatlined for the flat part is TCP not sending anything so that takes a while until the next sample arrives and when that arrives you get this sudden drop in in method latency and so this is the size of this drop so from there from the flat part and down to the bottom that was what we used as sort of the indication this is a a lower bound on latency so so for all flows it\u0027s like out of out of the out of all the flows we can only detect this pattern reliably and about five million of them and if we then plot the size of this graph for those 5 million flows we get this distribution so that sort of indicates that when we actually see this what we have path asides is a occurring latency then it\u0027s pretty significant from sort of in the median 200 milliseconds of which it sort of fits quite well with what we\u0027ve seen in lab tests like when you do see buffer bloat it\u0027s usually got quite a lot of it especially when the bandwidth is not very high as is the case in most of these tests and then for this is a an example of one of the traces that we saw from the from the isp where you can see the rate on the on the left hand side there this is a obviously a half megabit per second link and as soon as the the transfer rate hits the ceiling back the delay goes up to more than a second of Earth Quebec and we saw examples of this in the in the in both of the traces from the SD so what what we drew from this study is that there\u0027s a lot of latency variation in this in the internet and it has not improved over time whereas bandwidth has for whatever reason and also that there are significant regional differences both as I showed you here over different parts of the world but also in different countries women within each region and then at least some of this extra latency that we saw can be attributed to queuing and and where this peering does occur there\u0027s a lot of it so and sort of I was asked to put in some recommendations to this and my main part is that we need to pay more attention to the latency and I know this is this is starting to happen and especially like certainly in academia but also I think more and more people find you too wise not to the fact that higher bandwidth is not necessarily net in itself equals better connection to the internet with modern applications and we do have queue management "
  },
  {
    "startTime": "01:36:25",
    "text": "solutions several of which has been standardized on the process being analyzed at the ITF so it\u0027s just a matter of turning it on and it can really improve proof things the CPU space is obviously a place where we need to do this and where deployment is difficult but the technology dis exists and then of course I better congestion control there\u0027s other ways we can try to do this problem yeah thank you very much I actually have a question so did you also you detect those cases where you have this large buffers and when you have congestion you see a high delay did you also try to detect cases where you actually find a QM and identify those cases no not correctly I\u0027m not sure how you do that but like interesting and the other point is I think so your data is from 2015 yes so I guess like the most interesting part for me would be like actual data from 2015 to now yeah so the nice thing about NDT that they publish all the data and I think it\u0027s pretty straightforward to rerun this on new data set so we published the the data said in the script please do and bring it to the list yes I can do that I much pain to humanik so if I understood correctly these are measurements from taken by users towards M lab servers right so this is specific measurement traffic towards M lab which usually content providers are not optimizing for right so when making a statement like latency has an improved over time I would also put a caveat that this is actually measurements taken towards M last and it may or may not be true for a normal traffic that people actually go to yes I consider the fact that this is not speedtest.net so it\u0027s not an in the eyes P Network I consider better feature actually so I would also say speedtest.net is also not normal traffic right so traffic towards Google or Facebook would be normal traffic because this is what people usually do on the Internet so has the agency towards YouTube or Facebook improved or not over time the study doesn\u0027t tell us that right so so I would put a caveat as well right but that\u0027s fair point but the Internet is also more than YouTube chubby share from networks there the graph you show that a continuance is kind of interesting can you go back to that which one the congestion yeah this one so you certainly discover this kind of pattern in 5.7 million flows home how many Twitter flow out of food that you found this with a five-minute vote and how "
  },
  {
    "startTime": "01:39:26",
    "text": "many was the dataset Intel so yeah I said that that\u0027s 200 something million so okay it wasn\u0027t a very large percentage flows but that was partly I we obviously didn\u0027t look at 5 million flows right my name is sort of a matter of being really sure that our algorithm but we are on the side of of not having too many flows instead of having false positives right because I think that is a kind kind of a very interest because you only find this is roughly a few percentage of your total flows right out of 200 some hundred million flows I think you only see this pattern if this event is the sender induced because otherwise if the event is induced I cross track you your congestion behavior will not alter anything though so this actually gave a very good indication say there\u0027s a maybe 80 percent of the contingent events received by the lesson territory is not safe in use that\u0027s a polyethylene decision so that there was another pattern that we that we saw in some cases manually which was actually why we started looking at it which is also that certain cases there\u0027s some some days flows that over the ten sessions do not get a single congestion event at all but the latency increases quite a lot but this was not sort of something which would reliably detect in the in the they said we also filled out flows that get more than one congestion event over the whole clock yeah I just want to point out it because that percentage you said means the list of the flow which you do see are contingent events but you don\u0027t see this kind of a pattern not a most likely indication of what that events is that cross traffic induced not as an area news that could be I wouldn\u0027t say that all other flows like none of the other flows have self induced congestion events as I said we\u0027ve been we\u0027ve been very conservative with the with the detection algorithm here but obviously a lot of it is a lot of these roses that\u0027s true okay thank you very much very quick thanks sir go Tyson I was just curious about how my evolved during your measurement period were more servers being deployed and was a certain geographical bias that may have influenced the findings of HUD for example I know Africa has a very small number of M lobsters yeah and that\u0027s grown a lot over the last five years yeah I don\u0027t know it was a quick yeah thank you why butters next talking about YouTube measurements this time again with the focus on ipv6 hello hi so my name is Bob "
  },
  {
    "startTime": "01:42:29",
    "text": "of watch me and I\u0027m a postdoc at humanik I\u0027m working with your cot and today I\u0027m going to talk about measuring YouTube content delivery over ipv6 and so this paper has been recently got an exception will appear and succumb computer communication reviewing that you like issue and but the paper is already online so you can click on this link and you can see the paper as well and this is joint work with Saban from although University you\u0027re got you Minich and your controller from University Bremen okay so ipv6 is happening and Comcast tells us that ipv6 contributes to one quarter of their traffic than Comcast but this could have been this would be higher I assume this is data that I have from 2016 I would love to know the new number at some point and and Swisscom also reports that around sixty percent of the ipv6 traffic that the C is actually YouTube so it\u0027s more than half of the v6 stuff that you see within the exes network is YouTube and and that\u0027s so most of the v6 traffic is largely dominated by YouTube and it\u0027s also seen by people in academia and this is largely happening since the world ipv6 launch day so we started wondering so do users experience benefit or do they suffer from streaming YouTube videos over v6 and in order to do that we developed the test we call it the YouTube test and it\u0027s written in C and we deployed this on Sam nose probe so it\u0027s a nose if you know it\u0027s a company based in London which is handling out probes to people for doing broadband measurements and these are hardware based probes with a largely tp-link routers flashed with the custom open the plot eerily from there and and so we this is the deployment of the around hundred dual stack some nose probes that we have at this point and it\u0027s covering around sixty six different origin areas and some of the people in the room are actually hosting this probe for us so thank you for that and most of these probes are largely sitting in residential settings so these are measurements mostly taken from home networks and we had this test running on these boats and since August of 2014 so we have more than a year or two year long dataset okay so the these are all the contributions that we provide so we we saw that happy eyeballs which is RFC six five five five that it makes clients prefer streaming YouTube videos IP over ipv6 but we saw that the performance when streaming videos over v6 was actually worse both in terms of latency and throughput when compared to v4 and moving away from performance we also saw that stalling which you see when "
  },
  {
    "startTime": "01:45:32",
    "text": "streaming videos it\u0027s fairly low both over four and six and which is why the bit race that you can reliably stream are fairly comparable over four and six and all those stalls usually don\u0027t happen when they do happen the durations of the stall are actually higher over six and over four and all of this is largely happening because GGC nodes which are Google global caches these are caches deployed within the ISPs network most of them are v4 only at least from the manwich point that we are measuring from so this is the first study which is measuring YouTube content delivery over ipv6 and I\u0027m happy to show you some results and so this is the methodology so I\u0027ll take a different approach I\u0027ll show you the methodology and partially as we walk through the presentation and you will also see the results in parallel and we\u0027ll go through this using a sequence diagram so in order to measure YouTube you have to identify what destinations you want to measure towards so YouTube provides us an API which is the VP API which is 0.23 and gives you the popularity of global popularity of videos so we use that list and we apply our selection criteria that a video should be at least one minute long and should also be a level in HD and should not have any additional restrictions and stuff like that and we update this list every 12 hours and the probes are actually pulling this list from the Sun nose back end okay and we do this every 12 hours so you can actually see the profile of a specific video how it performs over four and six within this 12 hour period but we also change this list every off every 12 hours so you can also capture the profile of large number of videos over the duration of last two two and a half years note that the test is actually supporting nonreactive instead down layout modes at this point only and by step down I mean if you actually experience install the bit rate that you\u0027re streaming you will go down to the previous lower bit rate and cry screaming it again and of course the results are you should know that there\u0027s also bias by our vantage point so vantage points are actually deployed in EU us in Japan and that\u0027s the performance that we see okay so now look at the sequence diagram so what is happening is when you have this list you pick up the first video URL from the list and the what the probe is doing is establishing a TCP connection to the website that\u0027s what we use actually do when you go to watch a video on the browser and it\u0027s a sapling shingle connection to the website and fetching the HTML page once you have the HTML page you parse that HTML page and try to see what container formats does the video support and where to get that video from right and that\u0027s the host name of the media server that will that "
  },
  {
    "startTime": "01:48:32",
    "text": "is hosting that video so when you do the dns resolution from the probe the resolver will give you the location of the nearest media server which could be a GDC node if that is all level within the SVS network otherwise you will actually hit a CDN which is but away okay and once you do this DNS resolution you establish a TCP connection to the media server and try to establish new connections once for fetching audio and once for fetching video okay so once you have done all of this what kind of measurements can you actually see so the first thing that we looked at was success rate so how often can you actually successfully do this in the last two-and-a-half years a number of successful iterations through total iterations will give us the success rate and since the test is executing every hour over both address families we get a higher sample and what you see is so red is v6 and glue is refor and that\u0027s how it will be over throughout the presentation what you see is you generally are able to make a connection go to the website and go to the media server so success rate in both over four and six is fairly good so 99% of the probes are able to achieve success rate more than 94% of the time and which is slightly lower for six which is around 97% which is still fairly good and slightly lower success rate is not because of YouTube on the server side but because of the probes so sometimes they experience network connectivity issues so they can\u0027t resolve a DNS or they have TCP connection timeouts over 6 okay in situations where you can actually do connect we started looking at given that you measure both over 4 \u0026 6 as a user if you have an option what would you prefer and so there are two RFC\u0027s which are coming into the picture when making the calculation that is our SC 6 7 to 4 and which gives a preference to v6 destinations and IC 6 5 5 5 which is happy eyeballs so it tries to fall back to 4 in situations where you\u0027re not able to connect over 6 but wheel actually saw that around in our sample of around 900 videos in 97 percent of the time you will actually prefer making connection over v6 and try to keep that connection and so the takeaway here is that if you are dual stack at home you will largely be streaming videos over 6 then over core which is good and and once we have established that so in terms of TCP connect times or the first latency that you see you after establishing a connection to a media server or to the website what kind of latency that you experience and what you see is around in 63 percent of the cases the audio and video streams that you stream from the media server actually slower over six but the difference is actually not that bad if you look at the scale of the x-axis and in 14% of the cases the audio/video streams are 10 milliseconds slower so I would consider this not bad okay "
  },
  {
    "startTime": "01:51:33",
    "text": "given that we have a 2 a 2 year long data set you can actually also look at the time series of this and so this is median time series and some of you might know the kind of plots I usually draw so this is 4 - 6 ipv4 - ipv6 TCP connect time so if you see anything on the positive scale that shows that ipv6 is faster and and so on the y-axis you will see that everything is on the negative scale but the scale is fairly zoomed in and so in terms of connections to web to the web side which is the top plot you see that it\u0027s around at maximum 5 milliseconds slower when connecting with the website and with and then kind of keep in the media server the connection is around point 3 milliseconds slower and median is I would love to know what happened in the middle of the top lot where there was a drop and then it increased Oh again I don\u0027t know but if somebody in from Google can tell me what happened to the website be nice to know over 6 ok so once you have established the connection you try to stream the video so you try to fetch the video for 2 seconds and try to fill your buffer and that\u0027s what we call the pre pouring duration so this is happening when you start making interaction with the media server and try to fetch the video for 2 seconds and fill the buffer that is people paying attention and the entire thing starting from when you interact with the website till the end of two seconds of streaming the video is start of delay and so startup delays a very well-known metric in literature so this is the amount of time it takes for the video to start playing on your screen when you stream a video in your browser so using these two metrics what kind of performance has fixed to you see so this is startup delay again this is four minus six so positive is good for v6 which means faster and what you see is in eighty percent of the samples which is around 6.5 million samples over the last two and a half years you see it is actually slower over v6 and in the median here actually 100 milliseconds slower over v6 which I do not consider as a good news because hunting Michigan is something that you can actually see as a user and and this is how it looks over the time and so startup delay is the plot on the bottom the pink plot and what you see it\u0027s the starter delay has been consistently being words that\u0027s around 100 milliseconds words over 6 then over 4 and in terms of peak buffering duration which is the green plot on the top it\u0027s around 25 milliseconds words over 6 so if you remember the sequence diagram again peewit buffering duration was just the interaction with the media server and the startup delay is also including the interaction with the website so all of a sudden there is a higher jump from 25 milliseconds infinitely latency 200 milliseconds inflated latency when you start interacting with the website so so is "
  },
  {
    "startTime": "01:54:37",
    "text": "the initial interaction with the web server where you have to fetch the HTML page to identify where the media server is which is which is what is increasing latency this is something that we might have to optimize and the interaction with the media server is not that of an issue okay going forward we keep on doing this so we try to stream the video for a minute and given that we do this you what you can do is you can try to capture the throughput what you experienced over the entire one minute duration of running this measurement and so this is throughput throughput aspects of streaming the vini and what you see is an ATP 80 percent of the videos actually achieve lower throughput over v6 but this is not bad news which we will see later and in terms of time series over the last two years you will also see that the throughput is consistently lower again it\u0027s on the negative scale but it tends it seems to be improving over time so the scale is like and the slope is actually increasing towards zero okay apart from latency and throughput aspects we also looked at what happens when you experience a stall so stalling is when you expect a packet or I but it doesn\u0027t arrive in your buffer is empty and so in that case Yury buffer for a duration of one second and then the playout resumes again so this is stalling and we looked at how often do you actually see stalling over four and six and so this is cgi showing the stall rate of both over four and six so red is six and blue is for and what you see is in 90% of the cases probes do not experience much stall it is actually less than one percent so stalling usually doesn\u0027t happen on the internet as as we see towards YouTube as seen from these vantage points and as a result of this the bit rates that you can reliably stream both over four and six are actually comparable and what this goes to show that throughput is actually not a very reliable metric for judging performance because even if your throughput might be lower the bitrate that you reliably steam is still comparable but you still experience higher latency over six so latency aspects are also very important but you also saw in the previous presentation to include when when looking at performance aspects we also looked at in situations where you do experience a stall what is the duration of the stall that you experience and how does it compare over four and six and what we saw was in 80% of the cases the samples actually probes actually experienced higher stall over six ten over four and it\u0027s at least one second longer so if you are sleeping over six an experience is tall you might be waiting one second longer and then you would have had if you had been streaming or four which is not good okay so what all do we take away from all of this the first takeaway is that you might remember I showed the first plot where we showed that you actually prefer "
  },
  {
    "startTime": "01:57:38",
    "text": "streaming videos over six the performance both in terms of throughput and latency is not that good and this is also happening because of this RFC six five five five which is using a very high timer value so we recommend that the RFC should be updated with a lower timer value and on B six watts on Tuesday I presented some of the results which tend to show that using a 150 millisecond having the time of my value might actually help and this is just a background on why a 300 millisecond timer value was used because there were more relays 6,400 Ordo really is being used back in 2012 when this RFC was defined and this is no longer the case with v6 deployment anymore and as such happy eyeballs also needs an update and this is part of the Charter within v6 off so this is happening so that\u0027s good news and it might eventually work out for the best second recommendation is which is fairly obvious I think that I wish should take latency of the first class citizen and it\u0027s not just about throughput and the third one is more important where I should ensure that their caches the gge cache of a dual stack so I got told that this is the nodes are not dual stack because some of the ISPs have not asked for a p v6 perfect silicon from google and i hope if this happens then this might be solved okay so what all did we see today we saw that clients prefer streaming YouTube videos over 6 but even though you do prefer streaming videos over 6 you performance both in terms of latency high throughput is bad and when it comes to stalling of video we do not see much stalling happening at least from these probes but when a stolid does utter a stalled the duration of the stall over 6 is at least 1 1 second higher over 6 than over 4 and this is largely happening because the GGC nodes that we hit from these probes were v4 only and as a result of which you might have to go further into towards the CDN to fetch the video imaging which increases latency in order to encourage reproducibility of this work we have open sourced the test so the test is available on github so you can download the test it\u0027s written in C you can also run it in your own network try to see if you actually see what we saw and the entire dataset is also available online so you can go to that link and download the dataset and play with it that\u0027s all about from my n and I\u0027m happy to take questions mouki around yeah thank you for taking that for it\u0027s interesting presentation and just one comment about the necessity to add the DRF CG happy eyeball RFC actually there is no recommendation about the use of three hundred milliseconds in that RFC - only one "
  },
  {
    "startTime": "02:00:38",
    "text": "example that has been sitting there I see so it\u0027s after the implementers to use what goal and tamers to kick off the connection so for me there is no ABI - required to the target at all that\u0027s actually correct so that\u0027s just an example and three milliseconds actually not recommended with just given as an example because Chrome and Firefox who are using it back then and that\u0027s largely what people use on the Internet and so there\u0027s a there\u0027s a bit of it yeah I did rosin sand fine um I thought it was interesting they said the stall rates were basically the same the bit rates were basically the same which makes me think the user wouldn\u0027t really see any difference and I\u0027m wondering why that makes you conclude that latency is so important like you know I think Lane Z is important but I\u0027m not sure that that\u0027s important for videos hmm okay that\u0027s something they pick and us so I I have this impression that start a bill is also achieved it\u0027s it\u0027s an interesting metric to look at because if the video does not start quickly for me it\u0027s bad experience for me right and so that wasn\u0027t latency though that you thought was really I think you said trying before first trying to be six first and it not working in v4 oh no anyway I guess we discuss this later but you know I think you know a lot of people say Oh Layton you know if I measure the latency I\u0027m understanding what my customers are seeing but I think you kind of showed that\u0027s not the users experience with watching the video of this so and then we just got to watch what we monitor I think it is interesting if you know found this way of you know actually there\u0027s seen stalls and things I mean so the stats and interesting points I would like to highlight the sign can you please so going back can\u0027t so start a Billy this is the one that I was talking about and this is what I want people to take away so this is the amount of time it takes for the video to start playing and this is actually 100 milliseconds worse over 6 and that\u0027s how that\u0027s why I consider latency to be important because this is one interesting metric for user experience and so that\u0027s why I wouldn\u0027t consider throughput and as more interesting metric as long as you can sleep the same video with the same bitrate yeah I agree the user experience is important and Michael Abramson so I think what I\u0027m looking at these graphs you said that like things are improving over time it would be interesting to get those you know those diagrams for like the last 12 months of your data JTC like what is the current state of affairs more than the influency but the a lot of things that seems to be going on 2015 because that that is a very in it like if we had problems before there is no longer a problem I don\u0027t want it to affect like the overall conclusions "
  },
  {
    "startTime": "02:03:39",
    "text": "of what to do right now would it be useful to have an online page where this is updated regularly and absolutely that would be super ok ok Jen I ignore Google thank you for doing this work it\u0027s really really interesting I actually have a very specific question you pointed to a dip and you said you don\u0027t know what happened there yeah it\u0027s uncanny so I have a question about something that might relate to the death are you confirming that the protocol that\u0027s running underneath the client is in fact ECT this is actually TCP and yeah okay okay so one of the first graphs in my presentation is coming up next is going to coincide exactly with that dip and I don\u0027t know why in sweat kugel /youtube so yeah again thanks for doing this this is very interesting so when you introduce a methodology and maybe I just misread the slides but did you actually have one sample set which only did v4 and one which did happy apples oh so the probe is so the test is running once over ipv6 and the once a test concludes then it is learning the same thing again over v4 so it\u0027s at the same time almost at the same time and then we try to pair it so it\u0027s not like we have a v4 data set and then v6 data set one months later or something like it\u0027s not like that okay and so the GGC knows that don\u0027t speak v6 you just kind of endure them from the sample so we have to because we can\u0027t make a comparison between four and six and okay I mean this is not this is saying slightly surprising to me personally but I I will ask some folks and see if they have any intuition and if I can have any clarification I\u0027d like to ask a clarifying question to that one um so if the closest did you see no it is before only you\u0027re then doing the test to a further away yaki v6 enabled honor or or you just they\u0027re not doing a test no we are doing the test is going further away so you\u0027re going to yeah you\u0027re going to the closest v4 server and then the closest these yes sir yep okay yeah so that probably makes a dramatic difference in the results because as you said there are quite a few GGC noticed that don\u0027t have v6 enabled right just just a follow-up of you we would also like to know the expansion of GGC nodes which are dual tight or not so would like to work with Google on this perhaps combine this with ripe address or something and so perhaps we can talk about this if you\u0027re interested sure yeah Giovani si DN great work I also have more a historical question I happen to have been during your presentation during a fit networking 2015 and you had similar "
  },
  {
    "startTime": "02:06:39",
    "text": "measurements I think the papers gal who connects fester and I can do agile measure YouTube so my question first questions is this are those measurements here now comparable with those and I have the feeling that the results for v6 was far worse back in 2014-15 when you measured and what has changed since then so the presentation that Yoani is referring to was those who are latency towards websites and so this is specifically YouTube and we have a previous paper on this at Pam in 2015 which was using data set a 20-year 20 days long data set and we would suggest people to look at this because this is covering a longer time span so that those papers are not comparable alright because we are looking at two different things thank you Jenko one question it would be really interesting to see two separate sets of data when a nearest node is dual-stack nearest node is before only because obviously yes it\u0027s before only user experiences yours but is there any difference if now this drastic yes that\u0027s a very good question so we are doing work on this and I might not want to comment on this at this point because I have to look at the results again but we are also looking at when the when the node over four is also when the GCC node is also not available over four and what is the kind of benefit that you see when you actually see node but also when the duals when the node is actually dual stack so do you actually see any difference or not happy to show you results privately and comment you saying that operator should request ipv6 addresses from Google that\u0027s not true I think a pure ATAR should provide v6 connectivity for so know that then it will be the on stack we it currently deploying them like doors check by default if a curator has a canoe v6 connectivity and before all the nodes could be converted to dual stack if a curator is ready okay okay that\u0027s good to know so that\u0027s my mistake thank you very much and we come to our last presentation on quick presented by China well it\u0027s that time any better well my name is Jan Iyengar and I\u0027m going to be talking about a bunch of results and and and data that we have now published it\u0027s not a period yet but I\u0027m happy to send you a copy of the paper if you want to send me an email or wait for a couple of weeks and it will show up on the second website but this paper will appear in a team sitcom in a month so I\u0027m going to not go into the details of it but I\u0027m gonna start with just talking about what the history of quake is how many of you here have heard about quick excellent this should be good so quick is basically protocol for "
  },
  {
    "startTime": "02:09:41",
    "text": "HTTP transport and runs between Google services at this point and Chrome or other mobile applications at the clock as clients and it has reduced page load times and video latency for us in rebuff aerates as I\u0027ll show in a moment in general this is sort of a tldr slide it reduces video buffers for YouTube by 15 to 18 percent and Google search latency between three point six to eight percent it is now 35 percent of Google\u0027s egress traffic which by external estimates is about 7% of total Internet traffic and the IETF quick working group was formed in 2016 in October last year to sanitize it I will not be talking about IETF quick here I will be talking merely about our deployment Google\u0027s deployment of quake so here\u0027s a chart that shows basically the ramp up of quake that\u0027s happened at Google and we started sometime in 2014 but the numbers don\u0027t matter at that point it\u0027s low enough but then he slowly round up quick these are all driven by client and at the time it was only chrome that was opting to use quake and we would basically have a we had a way to experimentally turn this on at chrome clients all servers were always speaking quick but only some chrome clients would would opt themselves in to speak quick and we ramp that up slowly over time as we found performance benefits and so on two things to note in this graph I mean you can see of course that at this point we\u0027ve gotten up to close to 35% at the at the right end of this graph but you will see that there\u0027s a big dip there right on January 2016 viber I\u0027m going to take away from this that when quick traffic goes down TCP performs poorly I have no fucking clue why so we should talk more later and I this is this is a weird very uncanny sort of result I don\u0027t see vibe in the room anyways oh there you are so that\u0027s it\u0027s the same it\u0027s the same time right yeah okay yeah we should talk I don\u0027t know what\u0027s going on the be turned off quick at that time because there was a security we found a security bug in our code and we rapidly turned on quick and we bumped turned back on once we shipped the fix to our crimes the chrome and that\u0027s that happened in January 2016 there\u0027s also a bit of a a big bump up there near August and September 2016 any guesses as to what that is one guess somebody said mobile Lydia yeah that\u0027s exactly it that was you tube app starting to use "
  },
  {
    "startTime": "02:12:42",
    "text": "quick so if you had any doubt about how much data goes towards mobile that should put it to rest that\u0027s about half our total traffic good quick yeah I think that well the TCP performance worsening could just be the additional loads that suddenly switched over from quick it\u0027s absolutely possible that\u0027s that\u0027s the only RIA have at this point but I yeah the UNIX at this point but but you if that\u0027s true you may also he has longitudinal data and you should see sort of over time the latency changing as as quick increases but I don\u0027t think that\u0027s visible in his data but anyways here something relevant does it\u0027s something really did this happen there all right so what are we talking about what is quick well I\u0027m nervous in very quickly quick is basically a replacement of TCP TLS and a big chunk of HTTP to for for HTTP transport it subsumes a whole bunch of that and it\u0027s an encrypted transport and that\u0027s where it sits in the IP stack quick has a number of features and I will briefly touch upon these I will not get into the details here the quick was designed to be deployable and evolved evolvable and by that I mean it\u0027s runs on top of UDP and it is encrypted keeping it from getting messed around by middleboxes are getting ossified by middleboxes as i will talk about later in this presentation it was designed to have really low latency connection establishment there are at least two presentations today that I\u0027ve seen they talked about latency as being important and that\u0027s absolutely true there\u0027s actually dollars attached to that and quick gives you zero RTD connection establishment encrypted sorry secure connection establishment mostly zero RTD which means that a client can send a client hello immediately followed by an HTTP request and encrypted HTTP requests without waiting for a server to respond and sometimes the client has to fall back to one oddity and in almost no cases we do have the case for handling to our deities but that happens very rarely quick as multi-streaming I will not talk about that quick has better loss recovery and flexible more flexible congestion control the loss recovery comes from quick having better signaling to signal packet packets being received and better oddity estimates and so on and finally quick has resilience to not rebinding it runs on UDP so we sort of need a way to make sure that it that Nats don\u0027t know state mid connection that\u0027s about all I\u0027m going to talk about the quick design itself going onto the data the metrics that we evaluate for this study are basically two broad "
  },
  {
    "startTime": "02:15:45",
    "text": "genres so to speak one of them is latency the other is rebuff rate latency focuses on Google search latency and video playback latency reading playback latency is very similar to what I was talking about which is initial load time it takes to load the first frame of video search latency is an end-to-end metric this is the total time it takes for a search query to be sent to the service and then for the response we received and ready to display but Yuri buffer it has to do with again the total amount of V buffer time divided by the total amount of time spent watching the video latency goes to speak goes goes towards speaking to how quick helps small connections or early small connections and connections early in the game video buffering talks about long term health of the connection typically we focus on application define metrics because these are what matter to applications and these are what drive adoption across the industry they certainly are the things that drove adoption at Google and expect that\u0027s true elsewhere too and these tend to include non network components if whatever change you are making makes tremendous amount of difference when you\u0027re looking at just the network part but on an end to end basis it really doesn\u0027t matter then there\u0027s not much incentive to deploy it and so the metrics we show are in fact in to end our quality of quality of experience metrics that these applications define so here\u0027s the first one there are a lot of numbers here do not look at them all I will tell you which ones to look at and you can go read the paper for the rest the first one is simply the mean and the top is search in the bottom as video there\u0027s desktop and mobile in both cases but he can generally see that the mean latency improvements exist they are about 8% for a desktop on search and 8% for video on desktop as well and these are very substantial numbers just as far as application metrics go it\u0027s hard to move search latency so latency has a number of things in it Network latency is just a part of it so moving all of search latency by like 8% on desktop this is a pretty pretty big deal and we can see here that as we plot the percentiles most of our wins most of that hit percent or most of the the the mean basically comes out of the tail and this also tells you something that\u0027s important the tail is very important to moving application metrics that applications care about and the tail is where a lot of the gains still exist Venis where we can optimize things more and actually get more gains I talk about why this happens in a moment so okay so this is search in video latency so as I "
  },
  {
    "startTime": "02:18:45",
    "text": "pointed out to you search in video latency is largely impacted by things that happen up front and the reduction the zero rtd handshake that I mentioned earlier is kind of critical to this reduction to this improvement and this is a micro benchmark that just measures Han Qi latency do not read the text I am sorry that got left in I meant to remove it there\u0027s a simply showing handshake latency measured for TCP versus quick forget the subscript of G those are again you can read the paper for more details of the experiment groups are called those but the folks who are using TCP end up having that yellow line as your RTP increases the x-axis has increasing round-trip time to clients this is the minimum round-trip time measured to those users and the hand the handshake latency goes up this is expected there are about three or three round-trip times for establishing a TLS connection and so if you have a certain amount of time the the Hanshin latency simply we usually three times that round-trip time and that\u0027s what we observe in the wild with quake you can see that it\u0027s almost flat and near zero and that\u0027s because 88% of our connections on desktop get zero RTD connectivity that\u0027s a pretty large number and we are able to achieve that and even when we don\u0027t get zero RTP the thin blue line in the middle shows you what the remaining 12% of our users experience and that\u0027s still one RTP so you can you can easily see the savings here are quite substantial so zero already saves a lot and it actually contributes quite a bit to search in video Layton sees the second metric that I mentioned either second or so genre of metrics is video a buffer rate these numbers are different and the numbers look different so I\u0027ll explain them the mean again shows these are reductions in rebuffed aerator smaller fury buffers are better and and we get about an 18% reduction on desktop with quake in rebuffering and fifteen point three percent reduction on mobile at the mean as it turns out when you try to look when you look at the person died later when you lay this out up to 93 percentile up to the 93rd percentile videos have basically no Reaper force so all of these winds are coming from the tail so we can\u0027t sue quick can\u0027t improve anything that\u0027s what the Asterix is they\u0027re mean quick cannot improve anything up to the 93rd percentile entry buffer it because on TCP even these paybacks have no rebuffs and as these playbacks go into the tail as the Reaper first go into the tail quick pulls the mass in the reduction here says the at the 95th percentile this says quick "
  },
  {
    "startTime": "02:21:47",
    "text": "gives you 100 percent reductions in rebuff errs on desktop what that means is that PCP experienced nonzero Reapers and quick experienced 0d buffers at the 93rd percentile of rebuffs so that\u0027s how you see this at 94 quick reduces we buffers by 70.4% what that means is that it\u0027s starting to CD buffers in quick at this point so this sort of pulls the tail in and and and again moves the mean quite significantly so what are the causes for the rebuff a rate improving the first thing I\u0027ll point out is that all of these metrics not just the rebuff irate all the metrics the latency matrix and the rebuff irate matrix improve more quick has greater improvements as the round-trip time to the user increases now there should be obvious for the latency matrix because as the round-trip time increases there\u0027s more handshake latency and therefore quick improvements are high that\u0027s fine but that happens to be true for rebuff a rate as well and now remember rebuff rate has more to do with long-term connection health right not just initial startup so it has much more to do with long-term connection health but even there we see that the metrics that quick actually offers more or improves more as the round-trip time to the client increases and this has to do with the fact the network loss rate increases with round-trip time this is a graph that shows TCP retransmit rates plotted against minimum round-trip time to the user and this basically basically a transpiration rate here is used as a proxy for network congestion or loss in general and you can see roughly that the the trend does there\u0027s definitely very strong correlation here the retransmit rates go up as the arterial the client goes up this tends to be true because these are clients who are either in places which have put our connectivity and therefore or Google doesn\u0027t have a node sitting there GGC node sitting there and lots of reasons why this is true but we find that delay is correlated with network quality RTT is correlated with into quality so what this means what is what tells us is that as loss it increases as RTD increases quick has more improvements to offer the quick improvements show up more this is borne out in oh sorry yes I will get get to this this point in a moment the second reason we see improvements is because we found that about four point six percent of all TCP connections had were limited by the advertised receive window from the client this is where the service condition window basically hit the receive window and therefore could "
  },
  {
    "startTime": "02:24:48",
    "text": "not grow any further there\u0027s a CDF of the distribution of this you see that there\u0027s a good chunk of them that are limited by 64k receive window but that\u0027s that\u0027s what that is this basically these are things that are ecosystem problems where a server can\u0027t change anything this client advertisers receive windows and it\u0027s kind of hard to change this quick and releases a large receive window of course and so this problem doesn\u0027t exist in quick so between loss recovery and receive window limitations quick seems to have a leg up over TCP five minutes so the loss recovery thing plays in here we find that most of our improvements globally come from countries where connectivity is poor South Korea for instance barely sees any improvements in B on the other hand see substantial improvements with quick so this is just to say that quic is not a protocol that\u0027s particularly useful for her really well connected network so you are not reducing latency is from you know small latency is this actually has pretty big impact for users who already have poor connectivity and that\u0027s one of the benefits that we see for having taken control that\u0027s this data use good question so we use so we use cubic for these for this data but it\u0027s two connection cubic in the in the condition avoidance phase it does two connection emulation and that\u0027s because YouTube itself uses to subject connections it\u0027s not apples to apples but that\u0027s what we have so I have only a couple of minutes instead of talking about these things I\u0027ll just put these slides up you can look at them I\u0027m happy to take questions I have a good question to you guys because now quick is by default an iBook in Chrome yes and we have to say like so I live in a part of the world like in Singapore where you actually love helping you\u0027re making it slower because of a without our access is one gig to Gig those adoptions we have in South Korea have 500 Mac so for a slow connection and what we\u0027re seeing is some of the infrastructure is actually blocking quick for various security reasons and and then that introduces the light so quick actually doesn\u0027t help but in those in those geo locations and actually makes things worse because of some infrastructure settings you know you go somewhere you connect to a hotel you\u0027re most likely gonna be filtered in a very sensuous country so I was wondering if you guys looking at making it more geo "
  },
  {
    "startTime": "02:27:48",
    "text": "aware so so we enable it because no no questions and I will get here and I blink it in you know India countries like this great help but enabling it in countries which have good connectivity we actually contributing to a problem so the first thing is that it shouldn\u0027t hurt and if it is hurting then I\u0027d like to talk to you and find out why second if operators are blocking it that\u0027s fine chrome will end up using TCP that\u0027s what this slide says we are big drops and then you flip so then so let me so that\u0027s the delay that\u0027s what I\u0027m saying so that\u0027s what that\u0027s the delay that that\u0027s happening in certain places that\u0027s what I were saying I\u0027m not saying stops working but because in in those jurisdictions that have a good connectivity as you noticed there is hardly any if if at all improvement but then you introducing this oh so there\u0027s there\u0027s hardly any improvement in terms of the actual performance as measured by YouTube or by Google search but I think there\u0027s a general value in being able to deploy quick because we expect that this is going to be a feat more feature-rich going into the future it\u0027s an encrypted transport that by itself is an additional value that\u0027s not measured in these graphs so I would like to see quick deployed there as well and I\u0027d like to talk to you about exactly what the delays are we are working on on situations where we do have situations where middleboxes will actually drop traffic after the handshake and and that causes us heartburn and we are working on on detecting that and and and repairing those things hi : Perkins um are you able to break down the performance results based on the TCP version running at the receiver so is it that you\u0027re always winning out against all versions of TCP for example and not against new versions what do you not have the data to tell not at that not bad data not the high level metrics data that\u0027s really hard to do to do a join it mean but there\u0027s a mother data that we are taking into those are more micro benchmark II but yeah it\u0027s so I can tell you that from the chrome user base most of our desktop clients or Windows but your interest the question that you\u0027re asking is which version of Windows I think I could get some data about the chrome distribution maybe that\u0027ll help Thank You question and it\u0027s up to Janna if he wants to hang in "
  },
  {
    "startTime": "02:30:49",
    "text": "my Claire Benson are you saying packet size too large are you doing packet lever a packet level pollen to discovery we\u0027re not doing parking little path in this the side-effects shows we did a bunch of experiments we found what packet sizes work for most of our clients and that\u0027s what we aren\u0027t up using we fix it at 1350 bytes yeah Marcus select the mobile results can you differentiate if it\u0027s the clients are connected over Wi-Fi or cellular access we don\u0027t do it in this data again I don\u0027t know how far we can actually tie in the application metrics to that level of detail I let it look into it but I don\u0027t we don\u0027t have that data right now I guess that\u0027s about it please do read the paper there\u0027s a very particular there\u0027s a very fun story about middle box ossification that we already experienced if you believe it if you can quick is already ossified there are middle boxes there is one particular middle box we shall we shall remain nameless at the mic but I\u0027ll tell you when you ask me offline who took the one byte that was visible and decided to put it into their firewalls as a way to detect quick and this was the flags byte and that obviously caused breakages but read the paper thank you thank you thank you already for contributing and if you have data or maybe consider to send it on the minister to us earlier like you want to present something because we have quite a lot of requests these days which is great but the better the earlier you let us know the better thank you very much see you next time if you haven\u0027t signed the blue sheet yet come to the front you "
  }
]